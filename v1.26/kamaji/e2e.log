I0904 17:16:49.940928      19 e2e.go:126] Starting e2e run "8135def3-9470-4ff0-8b37-b89f5d81782b" on Ginkgo node 1
Sep  4 17:16:49.954: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1693847809 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Sep  4 17:16:50.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:16:50.147: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep  4 17:16:50.165: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  4 17:16:50.187: INFO: 9 / 9 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  4 17:16:50.187: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep  4 17:16:50.188: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  4 17:16:50.193: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Sep  4 17:16:50.193: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Sep  4 17:16:50.194: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep  4 17:16:50.194: INFO: e2e test version: v1.26.4
Sep  4 17:16:50.195: INFO: kube-apiserver version: v1.26.4
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Sep  4 17:16:50.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:16:50.205: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.059 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Sep  4 17:16:50.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:16:50.147: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Sep  4 17:16:50.165: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Sep  4 17:16:50.187: INFO: 9 / 9 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Sep  4 17:16:50.187: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
    Sep  4 17:16:50.188: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Sep  4 17:16:50.193: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Sep  4 17:16:50.193: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
    Sep  4 17:16:50.194: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Sep  4 17:16:50.194: INFO: e2e test version: v1.26.4
    Sep  4 17:16:50.195: INFO: kube-apiserver version: v1.26.4
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Sep  4 17:16:50.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:16:50.205: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:16:50.231
Sep  4 17:16:50.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename daemonsets 09/04/23 17:16:50.232
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:16:50.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:16:50.254
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Sep  4 17:16:50.284: INFO: Create a RollingUpdate DaemonSet
Sep  4 17:16:50.294: INFO: Check that daemon pods launch on every node of the cluster
Sep  4 17:16:50.304: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 17:16:50.304: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 17:16:51.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 17:16:51.321: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 17:16:52.318: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 17:16:52.318: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 17:16:53.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 17:16:53.321: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 17:16:54.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 17:16:54.314: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 17:16:55.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 17:16:55.321: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 17:16:56.317: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 17:16:56.318: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 17:16:57.318: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 17:16:57.319: INFO: Node tenant-000003 is running 0 daemon pod, expected 1
Sep  4 17:16:58.316: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 17:16:58.316: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Sep  4 17:16:58.316: INFO: Update the DaemonSet to trigger a rollout
Sep  4 17:16:58.325: INFO: Updating DaemonSet daemon-set
Sep  4 17:17:01.354: INFO: Roll back the DaemonSet before rollout is complete
Sep  4 17:17:01.374: INFO: Updating DaemonSet daemon-set
Sep  4 17:17:01.374: INFO: Make sure DaemonSet rollback is complete
Sep  4 17:17:01.399: INFO: Wrong image for pod: daemon-set-wtq2w. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Sep  4 17:17:01.400: INFO: Pod daemon-set-wtq2w is not available
Sep  4 17:17:04.415: INFO: Pod daemon-set-gp7vc is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/04/23 17:17:04.425
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9886, will wait for the garbage collector to delete the pods 09/04/23 17:17:04.426
Sep  4 17:17:04.497: INFO: Deleting DaemonSet.extensions daemon-set took: 15.200747ms
Sep  4 17:17:04.597: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.435596ms
Sep  4 17:17:06.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 17:17:06.002: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  4 17:17:06.010: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1559"},"items":null}

Sep  4 17:17:06.015: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1559"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:17:06.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9886" for this suite. 09/04/23 17:17:06.033
------------------------------
â€¢ [SLOW TEST] [15.816 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:16:50.231
    Sep  4 17:16:50.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename daemonsets 09/04/23 17:16:50.232
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:16:50.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:16:50.254
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Sep  4 17:16:50.284: INFO: Create a RollingUpdate DaemonSet
    Sep  4 17:16:50.294: INFO: Check that daemon pods launch on every node of the cluster
    Sep  4 17:16:50.304: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 17:16:50.304: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 17:16:51.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 17:16:51.321: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 17:16:52.318: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 17:16:52.318: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 17:16:53.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 17:16:53.321: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 17:16:54.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 17:16:54.314: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 17:16:55.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 17:16:55.321: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 17:16:56.317: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 17:16:56.318: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 17:16:57.318: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 17:16:57.319: INFO: Node tenant-000003 is running 0 daemon pod, expected 1
    Sep  4 17:16:58.316: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 17:16:58.316: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Sep  4 17:16:58.316: INFO: Update the DaemonSet to trigger a rollout
    Sep  4 17:16:58.325: INFO: Updating DaemonSet daemon-set
    Sep  4 17:17:01.354: INFO: Roll back the DaemonSet before rollout is complete
    Sep  4 17:17:01.374: INFO: Updating DaemonSet daemon-set
    Sep  4 17:17:01.374: INFO: Make sure DaemonSet rollback is complete
    Sep  4 17:17:01.399: INFO: Wrong image for pod: daemon-set-wtq2w. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Sep  4 17:17:01.400: INFO: Pod daemon-set-wtq2w is not available
    Sep  4 17:17:04.415: INFO: Pod daemon-set-gp7vc is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/04/23 17:17:04.425
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9886, will wait for the garbage collector to delete the pods 09/04/23 17:17:04.426
    Sep  4 17:17:04.497: INFO: Deleting DaemonSet.extensions daemon-set took: 15.200747ms
    Sep  4 17:17:04.597: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.435596ms
    Sep  4 17:17:06.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 17:17:06.002: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  4 17:17:06.010: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1559"},"items":null}

    Sep  4 17:17:06.015: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1559"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:17:06.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9886" for this suite. 09/04/23 17:17:06.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:17:06.049
Sep  4 17:17:06.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename cronjob 09/04/23 17:17:06.052
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:17:06.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:17:06.084
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 09/04/23 17:17:06.086
STEP: Ensuring a job is scheduled 09/04/23 17:17:06.095
STEP: Ensuring exactly one is scheduled 09/04/23 17:18:02.1
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/04/23 17:18:02.104
STEP: Ensuring the job is replaced with a new one 09/04/23 17:18:02.108
Sep  4 17:19:00.122: INFO: Warning: Found 0 jobs in namespace cronjob-5626
STEP: Removing cronjob 09/04/23 17:19:02.113
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  4 17:19:02.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5626" for this suite. 09/04/23 17:19:02.13
------------------------------
â€¢ [SLOW TEST] [116.093 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:17:06.049
    Sep  4 17:17:06.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename cronjob 09/04/23 17:17:06.052
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:17:06.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:17:06.084
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 09/04/23 17:17:06.086
    STEP: Ensuring a job is scheduled 09/04/23 17:17:06.095
    STEP: Ensuring exactly one is scheduled 09/04/23 17:18:02.1
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/04/23 17:18:02.104
    STEP: Ensuring the job is replaced with a new one 09/04/23 17:18:02.108
    Sep  4 17:19:00.122: INFO: Warning: Found 0 jobs in namespace cronjob-5626
    STEP: Removing cronjob 09/04/23 17:19:02.113
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:19:02.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5626" for this suite. 09/04/23 17:19:02.13
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:19:02.146
Sep  4 17:19:02.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-probe 09/04/23 17:19:02.147
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:19:02.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:19:02.191
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-a02e9124-17f5-440f-93e0-2e5ad30782a0 in namespace container-probe-2859 09/04/23 17:19:02.194
Sep  4 17:19:02.205: INFO: Waiting up to 5m0s for pod "busybox-a02e9124-17f5-440f-93e0-2e5ad30782a0" in namespace "container-probe-2859" to be "not pending"
Sep  4 17:19:02.212: INFO: Pod "busybox-a02e9124-17f5-440f-93e0-2e5ad30782a0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.506703ms
Sep  4 17:19:04.217: INFO: Pod "busybox-a02e9124-17f5-440f-93e0-2e5ad30782a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.01234031s
Sep  4 17:19:04.218: INFO: Pod "busybox-a02e9124-17f5-440f-93e0-2e5ad30782a0" satisfied condition "not pending"
Sep  4 17:19:04.218: INFO: Started pod busybox-a02e9124-17f5-440f-93e0-2e5ad30782a0 in namespace container-probe-2859
STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 17:19:04.218
Sep  4 17:19:04.224: INFO: Initial restart count of pod busybox-a02e9124-17f5-440f-93e0-2e5ad30782a0 is 0
STEP: deleting the pod 09/04/23 17:23:04.946
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 17:23:04.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2859" for this suite. 09/04/23 17:23:04.967
------------------------------
â€¢ [SLOW TEST] [242.831 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:19:02.146
    Sep  4 17:19:02.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-probe 09/04/23 17:19:02.147
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:19:02.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:19:02.191
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-a02e9124-17f5-440f-93e0-2e5ad30782a0 in namespace container-probe-2859 09/04/23 17:19:02.194
    Sep  4 17:19:02.205: INFO: Waiting up to 5m0s for pod "busybox-a02e9124-17f5-440f-93e0-2e5ad30782a0" in namespace "container-probe-2859" to be "not pending"
    Sep  4 17:19:02.212: INFO: Pod "busybox-a02e9124-17f5-440f-93e0-2e5ad30782a0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.506703ms
    Sep  4 17:19:04.217: INFO: Pod "busybox-a02e9124-17f5-440f-93e0-2e5ad30782a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.01234031s
    Sep  4 17:19:04.218: INFO: Pod "busybox-a02e9124-17f5-440f-93e0-2e5ad30782a0" satisfied condition "not pending"
    Sep  4 17:19:04.218: INFO: Started pod busybox-a02e9124-17f5-440f-93e0-2e5ad30782a0 in namespace container-probe-2859
    STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 17:19:04.218
    Sep  4 17:19:04.224: INFO: Initial restart count of pod busybox-a02e9124-17f5-440f-93e0-2e5ad30782a0 is 0
    STEP: deleting the pod 09/04/23 17:23:04.946
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:23:04.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2859" for this suite. 09/04/23 17:23:04.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:23:04.977
Sep  4 17:23:04.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename namespaces 09/04/23 17:23:04.98
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:23:04.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:23:05.001
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 09/04/23 17:23:05.005
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:23:05.023
STEP: Creating a service in the namespace 09/04/23 17:23:05.026
STEP: Deleting the namespace 09/04/23 17:23:05.047
STEP: Waiting for the namespace to be removed. 09/04/23 17:23:05.065
STEP: Recreating the namespace 09/04/23 17:23:11.069
STEP: Verifying there is no service in the namespace 09/04/23 17:23:11.088
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:23:11.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-883" for this suite. 09/04/23 17:23:11.095
STEP: Destroying namespace "nsdeletetest-3720" for this suite. 09/04/23 17:23:11.104
Sep  4 17:23:11.107: INFO: Namespace nsdeletetest-3720 was already deleted
STEP: Destroying namespace "nsdeletetest-6415" for this suite. 09/04/23 17:23:11.107
------------------------------
â€¢ [SLOW TEST] [6.140 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:23:04.977
    Sep  4 17:23:04.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename namespaces 09/04/23 17:23:04.98
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:23:04.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:23:05.001
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 09/04/23 17:23:05.005
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:23:05.023
    STEP: Creating a service in the namespace 09/04/23 17:23:05.026
    STEP: Deleting the namespace 09/04/23 17:23:05.047
    STEP: Waiting for the namespace to be removed. 09/04/23 17:23:05.065
    STEP: Recreating the namespace 09/04/23 17:23:11.069
    STEP: Verifying there is no service in the namespace 09/04/23 17:23:11.088
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:23:11.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-883" for this suite. 09/04/23 17:23:11.095
    STEP: Destroying namespace "nsdeletetest-3720" for this suite. 09/04/23 17:23:11.104
    Sep  4 17:23:11.107: INFO: Namespace nsdeletetest-3720 was already deleted
    STEP: Destroying namespace "nsdeletetest-6415" for this suite. 09/04/23 17:23:11.107
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:23:11.122
Sep  4 17:23:11.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pod-network-test 09/04/23 17:23:11.123
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:23:11.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:23:11.146
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-3097 09/04/23 17:23:11.148
STEP: creating a selector 09/04/23 17:23:11.148
STEP: Creating the service pods in kubernetes 09/04/23 17:23:11.149
Sep  4 17:23:11.149: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  4 17:23:11.172: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3097" to be "running and ready"
Sep  4 17:23:11.190: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.766066ms
Sep  4 17:23:11.190: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:23:13.197: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025428894s
Sep  4 17:23:13.198: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:23:15.194: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021940202s
Sep  4 17:23:15.194: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:23:17.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023181246s
Sep  4 17:23:17.196: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 17:23:19.197: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025423602s
Sep  4 17:23:19.198: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 17:23:21.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023036991s
Sep  4 17:23:21.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 17:23:23.198: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.025644975s
Sep  4 17:23:23.198: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 17:23:25.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.022566234s
Sep  4 17:23:25.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 17:23:27.202: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.030013208s
Sep  4 17:23:27.202: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 17:23:29.200: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.027961722s
Sep  4 17:23:29.200: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 17:23:31.194: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.022288246s
Sep  4 17:23:31.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 17:23:33.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.023460083s
Sep  4 17:23:33.196: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  4 17:23:33.196: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  4 17:23:33.200: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3097" to be "running and ready"
Sep  4 17:23:33.206: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.635016ms
Sep  4 17:23:33.206: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  4 17:23:33.206: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/04/23 17:23:33.209
Sep  4 17:23:33.220: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3097" to be "running"
Sep  4 17:23:33.237: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.2449ms
Sep  4 17:23:35.241: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02098459s
Sep  4 17:23:35.242: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  4 17:23:35.247: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  4 17:23:35.247: INFO: Breadth first check of 10.36.55.71 on host 10.225.0.5...
Sep  4 17:23:35.250: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.36.55.72:9080/dial?request=hostname&protocol=http&host=10.36.55.71&port=8083&tries=1'] Namespace:pod-network-test-3097 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:23:35.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:23:35.251: INFO: ExecWithOptions: Clientset creation
Sep  4 17:23:35.251: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3097/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.36.55.72%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.36.55.71%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  4 17:23:35.378: INFO: Waiting for responses: map[]
Sep  4 17:23:35.378: INFO: reached 10.36.55.71 after 0/1 tries
Sep  4 17:23:35.378: INFO: Breadth first check of 10.36.217.201 on host 10.225.0.7...
Sep  4 17:23:35.385: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.36.55.72:9080/dial?request=hostname&protocol=http&host=10.36.217.201&port=8083&tries=1'] Namespace:pod-network-test-3097 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:23:35.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:23:35.385: INFO: ExecWithOptions: Clientset creation
Sep  4 17:23:35.385: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3097/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.36.55.72%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.36.217.201%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  4 17:23:35.485: INFO: Waiting for responses: map[]
Sep  4 17:23:35.485: INFO: reached 10.36.217.201 after 0/1 tries
Sep  4 17:23:35.485: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  4 17:23:35.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3097" for this suite. 09/04/23 17:23:35.491
------------------------------
â€¢ [SLOW TEST] [24.377 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:23:11.122
    Sep  4 17:23:11.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pod-network-test 09/04/23 17:23:11.123
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:23:11.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:23:11.146
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-3097 09/04/23 17:23:11.148
    STEP: creating a selector 09/04/23 17:23:11.148
    STEP: Creating the service pods in kubernetes 09/04/23 17:23:11.149
    Sep  4 17:23:11.149: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  4 17:23:11.172: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3097" to be "running and ready"
    Sep  4 17:23:11.190: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.766066ms
    Sep  4 17:23:11.190: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:23:13.197: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025428894s
    Sep  4 17:23:13.198: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:23:15.194: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021940202s
    Sep  4 17:23:15.194: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:23:17.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023181246s
    Sep  4 17:23:17.196: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 17:23:19.197: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025423602s
    Sep  4 17:23:19.198: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 17:23:21.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023036991s
    Sep  4 17:23:21.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 17:23:23.198: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.025644975s
    Sep  4 17:23:23.198: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 17:23:25.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.022566234s
    Sep  4 17:23:25.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 17:23:27.202: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.030013208s
    Sep  4 17:23:27.202: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 17:23:29.200: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.027961722s
    Sep  4 17:23:29.200: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 17:23:31.194: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.022288246s
    Sep  4 17:23:31.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 17:23:33.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.023460083s
    Sep  4 17:23:33.196: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  4 17:23:33.196: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  4 17:23:33.200: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3097" to be "running and ready"
    Sep  4 17:23:33.206: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.635016ms
    Sep  4 17:23:33.206: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  4 17:23:33.206: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/04/23 17:23:33.209
    Sep  4 17:23:33.220: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3097" to be "running"
    Sep  4 17:23:33.237: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.2449ms
    Sep  4 17:23:35.241: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02098459s
    Sep  4 17:23:35.242: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  4 17:23:35.247: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  4 17:23:35.247: INFO: Breadth first check of 10.36.55.71 on host 10.225.0.5...
    Sep  4 17:23:35.250: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.36.55.72:9080/dial?request=hostname&protocol=http&host=10.36.55.71&port=8083&tries=1'] Namespace:pod-network-test-3097 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:23:35.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:23:35.251: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:23:35.251: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3097/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.36.55.72%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.36.55.71%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  4 17:23:35.378: INFO: Waiting for responses: map[]
    Sep  4 17:23:35.378: INFO: reached 10.36.55.71 after 0/1 tries
    Sep  4 17:23:35.378: INFO: Breadth first check of 10.36.217.201 on host 10.225.0.7...
    Sep  4 17:23:35.385: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.36.55.72:9080/dial?request=hostname&protocol=http&host=10.36.217.201&port=8083&tries=1'] Namespace:pod-network-test-3097 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:23:35.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:23:35.385: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:23:35.385: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3097/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.36.55.72%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.36.217.201%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  4 17:23:35.485: INFO: Waiting for responses: map[]
    Sep  4 17:23:35.485: INFO: reached 10.36.217.201 after 0/1 tries
    Sep  4 17:23:35.485: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:23:35.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3097" for this suite. 09/04/23 17:23:35.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:23:35.504
Sep  4 17:23:35.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename var-expansion 09/04/23 17:23:35.505
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:23:35.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:23:35.537
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 09/04/23 17:23:35.539
STEP: waiting for pod running 09/04/23 17:23:35.55
Sep  4 17:23:35.550: INFO: Waiting up to 2m0s for pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371" in namespace "var-expansion-2117" to be "running"
Sep  4 17:23:35.553: INFO: Pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371": Phase="Pending", Reason="", readiness=false. Elapsed: 2.558189ms
Sep  4 17:23:37.559: INFO: Pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371": Phase="Running", Reason="", readiness=true. Elapsed: 2.008072636s
Sep  4 17:23:37.559: INFO: Pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371" satisfied condition "running"
STEP: creating a file in subpath 09/04/23 17:23:37.559
Sep  4 17:23:37.566: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2117 PodName:var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:23:37.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:23:37.567: INFO: ExecWithOptions: Clientset creation
Sep  4 17:23:37.567: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2117/pods/var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 09/04/23 17:23:37.686
Sep  4 17:23:37.690: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2117 PodName:var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:23:37.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:23:37.691: INFO: ExecWithOptions: Clientset creation
Sep  4 17:23:37.691: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2117/pods/var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 09/04/23 17:23:37.791
Sep  4 17:23:38.311: INFO: Successfully updated pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371"
STEP: waiting for annotated pod running 09/04/23 17:23:38.325
Sep  4 17:23:38.325: INFO: Waiting up to 2m0s for pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371" in namespace "var-expansion-2117" to be "running"
Sep  4 17:23:38.334: INFO: Pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371": Phase="Running", Reason="", readiness=true. Elapsed: 8.751846ms
Sep  4 17:23:38.334: INFO: Pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371" satisfied condition "running"
STEP: deleting the pod gracefully 09/04/23 17:23:38.334
Sep  4 17:23:38.334: INFO: Deleting pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371" in namespace "var-expansion-2117"
Sep  4 17:23:38.347: INFO: Wait up to 5m0s for pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 17:24:12.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2117" for this suite. 09/04/23 17:24:12.373
------------------------------
â€¢ [SLOW TEST] [36.876 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:23:35.504
    Sep  4 17:23:35.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename var-expansion 09/04/23 17:23:35.505
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:23:35.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:23:35.537
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 09/04/23 17:23:35.539
    STEP: waiting for pod running 09/04/23 17:23:35.55
    Sep  4 17:23:35.550: INFO: Waiting up to 2m0s for pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371" in namespace "var-expansion-2117" to be "running"
    Sep  4 17:23:35.553: INFO: Pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371": Phase="Pending", Reason="", readiness=false. Elapsed: 2.558189ms
    Sep  4 17:23:37.559: INFO: Pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371": Phase="Running", Reason="", readiness=true. Elapsed: 2.008072636s
    Sep  4 17:23:37.559: INFO: Pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371" satisfied condition "running"
    STEP: creating a file in subpath 09/04/23 17:23:37.559
    Sep  4 17:23:37.566: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2117 PodName:var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:23:37.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:23:37.567: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:23:37.567: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2117/pods/var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 09/04/23 17:23:37.686
    Sep  4 17:23:37.690: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2117 PodName:var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:23:37.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:23:37.691: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:23:37.691: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2117/pods/var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 09/04/23 17:23:37.791
    Sep  4 17:23:38.311: INFO: Successfully updated pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371"
    STEP: waiting for annotated pod running 09/04/23 17:23:38.325
    Sep  4 17:23:38.325: INFO: Waiting up to 2m0s for pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371" in namespace "var-expansion-2117" to be "running"
    Sep  4 17:23:38.334: INFO: Pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371": Phase="Running", Reason="", readiness=true. Elapsed: 8.751846ms
    Sep  4 17:23:38.334: INFO: Pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371" satisfied condition "running"
    STEP: deleting the pod gracefully 09/04/23 17:23:38.334
    Sep  4 17:23:38.334: INFO: Deleting pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371" in namespace "var-expansion-2117"
    Sep  4 17:23:38.347: INFO: Wait up to 5m0s for pod "var-expansion-9cdd37e2-df8f-4af0-b121-9c30a373f371" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:24:12.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2117" for this suite. 09/04/23 17:24:12.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:24:12.388
Sep  4 17:24:12.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 17:24:12.389
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:12.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:12.423
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 17:24:12.44
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:24:12.656
STEP: Deploying the webhook pod 09/04/23 17:24:12.672
STEP: Wait for the deployment to be ready 09/04/23 17:24:12.688
Sep  4 17:24:12.708: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 17:24:14.727
STEP: Verifying the service has paired with the endpoint 09/04/23 17:24:14.755
Sep  4 17:24:15.755: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 09/04/23 17:24:15.761
STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/04/23 17:24:15.798
STEP: Creating a configMap that should not be mutated 09/04/23 17:24:15.806
STEP: Patching a mutating webhook configuration's rules to include the create operation 09/04/23 17:24:15.823
STEP: Creating a configMap that should be mutated 09/04/23 17:24:15.839
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:24:15.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1146" for this suite. 09/04/23 17:24:15.952
STEP: Destroying namespace "webhook-1146-markers" for this suite. 09/04/23 17:24:15.978
------------------------------
â€¢ [3.611 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:24:12.388
    Sep  4 17:24:12.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 17:24:12.389
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:12.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:12.423
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 17:24:12.44
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:24:12.656
    STEP: Deploying the webhook pod 09/04/23 17:24:12.672
    STEP: Wait for the deployment to be ready 09/04/23 17:24:12.688
    Sep  4 17:24:12.708: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 17:24:14.727
    STEP: Verifying the service has paired with the endpoint 09/04/23 17:24:14.755
    Sep  4 17:24:15.755: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 09/04/23 17:24:15.761
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/04/23 17:24:15.798
    STEP: Creating a configMap that should not be mutated 09/04/23 17:24:15.806
    STEP: Patching a mutating webhook configuration's rules to include the create operation 09/04/23 17:24:15.823
    STEP: Creating a configMap that should be mutated 09/04/23 17:24:15.839
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:24:15.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1146" for this suite. 09/04/23 17:24:15.952
    STEP: Destroying namespace "webhook-1146-markers" for this suite. 09/04/23 17:24:15.978
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:24:16.009
Sep  4 17:24:16.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename svcaccounts 09/04/23 17:24:16.012
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:16.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:16.034
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Sep  4 17:24:16.047: INFO: Waiting up to 5m0s for pod "pod-service-account-021c261e-09fb-42f6-b6d1-ddbfacc4549d" in namespace "svcaccounts-5946" to be "running"
Sep  4 17:24:16.051: INFO: Pod "pod-service-account-021c261e-09fb-42f6-b6d1-ddbfacc4549d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.197651ms
Sep  4 17:24:18.056: INFO: Pod "pod-service-account-021c261e-09fb-42f6-b6d1-ddbfacc4549d": Phase="Running", Reason="", readiness=true. Elapsed: 2.008218975s
Sep  4 17:24:18.056: INFO: Pod "pod-service-account-021c261e-09fb-42f6-b6d1-ddbfacc4549d" satisfied condition "running"
STEP: reading a file in the container 09/04/23 17:24:18.057
Sep  4 17:24:18.057: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5946 pod-service-account-021c261e-09fb-42f6-b6d1-ddbfacc4549d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 09/04/23 17:24:18.272
Sep  4 17:24:18.273: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5946 pod-service-account-021c261e-09fb-42f6-b6d1-ddbfacc4549d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 09/04/23 17:24:18.446
Sep  4 17:24:18.446: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5946 pod-service-account-021c261e-09fb-42f6-b6d1-ddbfacc4549d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Sep  4 17:24:18.635: INFO: Got root ca configmap in namespace "svcaccounts-5946"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  4 17:24:18.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5946" for this suite. 09/04/23 17:24:18.644
------------------------------
â€¢ [2.646 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:24:16.009
    Sep  4 17:24:16.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename svcaccounts 09/04/23 17:24:16.012
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:16.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:16.034
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Sep  4 17:24:16.047: INFO: Waiting up to 5m0s for pod "pod-service-account-021c261e-09fb-42f6-b6d1-ddbfacc4549d" in namespace "svcaccounts-5946" to be "running"
    Sep  4 17:24:16.051: INFO: Pod "pod-service-account-021c261e-09fb-42f6-b6d1-ddbfacc4549d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.197651ms
    Sep  4 17:24:18.056: INFO: Pod "pod-service-account-021c261e-09fb-42f6-b6d1-ddbfacc4549d": Phase="Running", Reason="", readiness=true. Elapsed: 2.008218975s
    Sep  4 17:24:18.056: INFO: Pod "pod-service-account-021c261e-09fb-42f6-b6d1-ddbfacc4549d" satisfied condition "running"
    STEP: reading a file in the container 09/04/23 17:24:18.057
    Sep  4 17:24:18.057: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5946 pod-service-account-021c261e-09fb-42f6-b6d1-ddbfacc4549d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 09/04/23 17:24:18.272
    Sep  4 17:24:18.273: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5946 pod-service-account-021c261e-09fb-42f6-b6d1-ddbfacc4549d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 09/04/23 17:24:18.446
    Sep  4 17:24:18.446: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5946 pod-service-account-021c261e-09fb-42f6-b6d1-ddbfacc4549d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Sep  4 17:24:18.635: INFO: Got root ca configmap in namespace "svcaccounts-5946"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:24:18.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5946" for this suite. 09/04/23 17:24:18.644
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:24:18.658
Sep  4 17:24:18.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 17:24:18.66
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:18.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:18.684
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 09/04/23 17:24:18.688
Sep  4 17:24:18.699: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd" in namespace "projected-1892" to be "Succeeded or Failed"
Sep  4 17:24:18.703: INFO: Pod "downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.906628ms
Sep  4 17:24:20.708: INFO: Pod "downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008270244s
Sep  4 17:24:22.707: INFO: Pod "downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007145599s
STEP: Saw pod success 09/04/23 17:24:22.707
Sep  4 17:24:22.707: INFO: Pod "downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd" satisfied condition "Succeeded or Failed"
Sep  4 17:24:22.710: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd container client-container: <nil>
STEP: delete the pod 09/04/23 17:24:22.894
Sep  4 17:24:22.919: INFO: Waiting for pod downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd to disappear
Sep  4 17:24:22.934: INFO: Pod downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 17:24:22.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1892" for this suite. 09/04/23 17:24:22.939
------------------------------
â€¢ [4.288 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:24:18.658
    Sep  4 17:24:18.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 17:24:18.66
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:18.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:18.684
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 09/04/23 17:24:18.688
    Sep  4 17:24:18.699: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd" in namespace "projected-1892" to be "Succeeded or Failed"
    Sep  4 17:24:18.703: INFO: Pod "downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.906628ms
    Sep  4 17:24:20.708: INFO: Pod "downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008270244s
    Sep  4 17:24:22.707: INFO: Pod "downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007145599s
    STEP: Saw pod success 09/04/23 17:24:22.707
    Sep  4 17:24:22.707: INFO: Pod "downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd" satisfied condition "Succeeded or Failed"
    Sep  4 17:24:22.710: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd container client-container: <nil>
    STEP: delete the pod 09/04/23 17:24:22.894
    Sep  4 17:24:22.919: INFO: Waiting for pod downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd to disappear
    Sep  4 17:24:22.934: INFO: Pod downwardapi-volume-4f927dc9-a42d-4a5b-9221-06ebecfcdfbd no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:24:22.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1892" for this suite. 09/04/23 17:24:22.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:24:22.955
Sep  4 17:24:22.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pods 09/04/23 17:24:22.956
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:22.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:22.983
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 09/04/23 17:24:22.988
Sep  4 17:24:23.000: INFO: Waiting up to 5m0s for pod "pod-hostip-581fe83d-e9b0-4253-bc90-2518214f6e4d" in namespace "pods-4360" to be "running and ready"
Sep  4 17:24:23.014: INFO: Pod "pod-hostip-581fe83d-e9b0-4253-bc90-2518214f6e4d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.277446ms
Sep  4 17:24:23.014: INFO: The phase of Pod pod-hostip-581fe83d-e9b0-4253-bc90-2518214f6e4d is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:24:25.022: INFO: Pod "pod-hostip-581fe83d-e9b0-4253-bc90-2518214f6e4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.022395678s
Sep  4 17:24:25.023: INFO: The phase of Pod pod-hostip-581fe83d-e9b0-4253-bc90-2518214f6e4d is Running (Ready = true)
Sep  4 17:24:25.023: INFO: Pod "pod-hostip-581fe83d-e9b0-4253-bc90-2518214f6e4d" satisfied condition "running and ready"
Sep  4 17:24:25.031: INFO: Pod pod-hostip-581fe83d-e9b0-4253-bc90-2518214f6e4d has hostIP: 10.225.0.5
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 17:24:25.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4360" for this suite. 09/04/23 17:24:25.035
------------------------------
â€¢ [2.092 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:24:22.955
    Sep  4 17:24:22.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pods 09/04/23 17:24:22.956
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:22.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:22.983
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 09/04/23 17:24:22.988
    Sep  4 17:24:23.000: INFO: Waiting up to 5m0s for pod "pod-hostip-581fe83d-e9b0-4253-bc90-2518214f6e4d" in namespace "pods-4360" to be "running and ready"
    Sep  4 17:24:23.014: INFO: Pod "pod-hostip-581fe83d-e9b0-4253-bc90-2518214f6e4d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.277446ms
    Sep  4 17:24:23.014: INFO: The phase of Pod pod-hostip-581fe83d-e9b0-4253-bc90-2518214f6e4d is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:24:25.022: INFO: Pod "pod-hostip-581fe83d-e9b0-4253-bc90-2518214f6e4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.022395678s
    Sep  4 17:24:25.023: INFO: The phase of Pod pod-hostip-581fe83d-e9b0-4253-bc90-2518214f6e4d is Running (Ready = true)
    Sep  4 17:24:25.023: INFO: Pod "pod-hostip-581fe83d-e9b0-4253-bc90-2518214f6e4d" satisfied condition "running and ready"
    Sep  4 17:24:25.031: INFO: Pod pod-hostip-581fe83d-e9b0-4253-bc90-2518214f6e4d has hostIP: 10.225.0.5
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:24:25.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4360" for this suite. 09/04/23 17:24:25.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:24:25.059
Sep  4 17:24:25.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 17:24:25.067
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:25.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:25.102
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 09/04/23 17:24:25.106
Sep  4 17:24:25.121: INFO: Waiting up to 5m0s for pod "labelsupdate43aec9b0-eb1f-4fef-8c2d-5ec56d966344" in namespace "downward-api-7404" to be "running and ready"
Sep  4 17:24:25.130: INFO: Pod "labelsupdate43aec9b0-eb1f-4fef-8c2d-5ec56d966344": Phase="Pending", Reason="", readiness=false. Elapsed: 8.848849ms
Sep  4 17:24:25.130: INFO: The phase of Pod labelsupdate43aec9b0-eb1f-4fef-8c2d-5ec56d966344 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:24:27.135: INFO: Pod "labelsupdate43aec9b0-eb1f-4fef-8c2d-5ec56d966344": Phase="Running", Reason="", readiness=true. Elapsed: 2.013703142s
Sep  4 17:24:27.135: INFO: The phase of Pod labelsupdate43aec9b0-eb1f-4fef-8c2d-5ec56d966344 is Running (Ready = true)
Sep  4 17:24:27.135: INFO: Pod "labelsupdate43aec9b0-eb1f-4fef-8c2d-5ec56d966344" satisfied condition "running and ready"
Sep  4 17:24:27.680: INFO: Successfully updated pod "labelsupdate43aec9b0-eb1f-4fef-8c2d-5ec56d966344"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 17:24:29.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7404" for this suite. 09/04/23 17:24:29.721
------------------------------
â€¢ [4.672 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:24:25.059
    Sep  4 17:24:25.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 17:24:25.067
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:25.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:25.102
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 09/04/23 17:24:25.106
    Sep  4 17:24:25.121: INFO: Waiting up to 5m0s for pod "labelsupdate43aec9b0-eb1f-4fef-8c2d-5ec56d966344" in namespace "downward-api-7404" to be "running and ready"
    Sep  4 17:24:25.130: INFO: Pod "labelsupdate43aec9b0-eb1f-4fef-8c2d-5ec56d966344": Phase="Pending", Reason="", readiness=false. Elapsed: 8.848849ms
    Sep  4 17:24:25.130: INFO: The phase of Pod labelsupdate43aec9b0-eb1f-4fef-8c2d-5ec56d966344 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:24:27.135: INFO: Pod "labelsupdate43aec9b0-eb1f-4fef-8c2d-5ec56d966344": Phase="Running", Reason="", readiness=true. Elapsed: 2.013703142s
    Sep  4 17:24:27.135: INFO: The phase of Pod labelsupdate43aec9b0-eb1f-4fef-8c2d-5ec56d966344 is Running (Ready = true)
    Sep  4 17:24:27.135: INFO: Pod "labelsupdate43aec9b0-eb1f-4fef-8c2d-5ec56d966344" satisfied condition "running and ready"
    Sep  4 17:24:27.680: INFO: Successfully updated pod "labelsupdate43aec9b0-eb1f-4fef-8c2d-5ec56d966344"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:24:29.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7404" for this suite. 09/04/23 17:24:29.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:24:29.732
Sep  4 17:24:29.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 17:24:29.733
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:29.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:29.754
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5096 09/04/23 17:24:29.757
STEP: changing the ExternalName service to type=NodePort 09/04/23 17:24:29.765
STEP: creating replication controller externalname-service in namespace services-5096 09/04/23 17:24:29.797
I0904 17:24:29.807927      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5096, replica count: 2
I0904 17:24:32.875694      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 17:24:32.885: INFO: Creating new exec pod
Sep  4 17:24:32.895: INFO: Waiting up to 5m0s for pod "execpodcs78k" in namespace "services-5096" to be "running"
Sep  4 17:24:32.899: INFO: Pod "execpodcs78k": Phase="Pending", Reason="", readiness=false. Elapsed: 3.912853ms
Sep  4 17:24:34.906: INFO: Pod "execpodcs78k": Phase="Running", Reason="", readiness=true. Elapsed: 2.01098464s
Sep  4 17:24:34.907: INFO: Pod "execpodcs78k" satisfied condition "running"
Sep  4 17:24:35.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-5096 exec execpodcs78k -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Sep  4 17:24:36.090: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  4 17:24:36.090: INFO: stdout: ""
Sep  4 17:24:36.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-5096 exec execpodcs78k -- /bin/sh -x -c nc -v -z -w 2 10.96.242.32 80'
Sep  4 17:24:36.344: INFO: stderr: "+ nc -v -z -w 2 10.96.242.32 80\nConnection to 10.96.242.32 80 port [tcp/http] succeeded!\n"
Sep  4 17:24:36.344: INFO: stdout: ""
Sep  4 17:24:36.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-5096 exec execpodcs78k -- /bin/sh -x -c nc -v -z -w 2 10.225.0.5 32349'
Sep  4 17:24:36.493: INFO: stderr: "+ nc -v -z -w 2 10.225.0.5 32349\nConnection to 10.225.0.5 32349 port [tcp/*] succeeded!\n"
Sep  4 17:24:36.493: INFO: stdout: ""
Sep  4 17:24:36.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-5096 exec execpodcs78k -- /bin/sh -x -c nc -v -z -w 2 10.225.0.7 32349'
Sep  4 17:24:36.678: INFO: stderr: "+ nc -v -z -w 2 10.225.0.7 32349\nConnection to 10.225.0.7 32349 port [tcp/*] succeeded!\n"
Sep  4 17:24:36.678: INFO: stdout: ""
Sep  4 17:24:36.678: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 17:24:36.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5096" for this suite. 09/04/23 17:24:36.726
------------------------------
â€¢ [SLOW TEST] [7.008 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:24:29.732
    Sep  4 17:24:29.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 17:24:29.733
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:29.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:29.754
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-5096 09/04/23 17:24:29.757
    STEP: changing the ExternalName service to type=NodePort 09/04/23 17:24:29.765
    STEP: creating replication controller externalname-service in namespace services-5096 09/04/23 17:24:29.797
    I0904 17:24:29.807927      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5096, replica count: 2
    I0904 17:24:32.875694      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 17:24:32.885: INFO: Creating new exec pod
    Sep  4 17:24:32.895: INFO: Waiting up to 5m0s for pod "execpodcs78k" in namespace "services-5096" to be "running"
    Sep  4 17:24:32.899: INFO: Pod "execpodcs78k": Phase="Pending", Reason="", readiness=false. Elapsed: 3.912853ms
    Sep  4 17:24:34.906: INFO: Pod "execpodcs78k": Phase="Running", Reason="", readiness=true. Elapsed: 2.01098464s
    Sep  4 17:24:34.907: INFO: Pod "execpodcs78k" satisfied condition "running"
    Sep  4 17:24:35.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-5096 exec execpodcs78k -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Sep  4 17:24:36.090: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  4 17:24:36.090: INFO: stdout: ""
    Sep  4 17:24:36.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-5096 exec execpodcs78k -- /bin/sh -x -c nc -v -z -w 2 10.96.242.32 80'
    Sep  4 17:24:36.344: INFO: stderr: "+ nc -v -z -w 2 10.96.242.32 80\nConnection to 10.96.242.32 80 port [tcp/http] succeeded!\n"
    Sep  4 17:24:36.344: INFO: stdout: ""
    Sep  4 17:24:36.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-5096 exec execpodcs78k -- /bin/sh -x -c nc -v -z -w 2 10.225.0.5 32349'
    Sep  4 17:24:36.493: INFO: stderr: "+ nc -v -z -w 2 10.225.0.5 32349\nConnection to 10.225.0.5 32349 port [tcp/*] succeeded!\n"
    Sep  4 17:24:36.493: INFO: stdout: ""
    Sep  4 17:24:36.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-5096 exec execpodcs78k -- /bin/sh -x -c nc -v -z -w 2 10.225.0.7 32349'
    Sep  4 17:24:36.678: INFO: stderr: "+ nc -v -z -w 2 10.225.0.7 32349\nConnection to 10.225.0.7 32349 port [tcp/*] succeeded!\n"
    Sep  4 17:24:36.678: INFO: stdout: ""
    Sep  4 17:24:36.678: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:24:36.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5096" for this suite. 09/04/23 17:24:36.726
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:24:36.749
Sep  4 17:24:36.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 17:24:36.753
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:36.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:36.89
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-3880 09/04/23 17:24:36.893
STEP: creating service affinity-clusterip-transition in namespace services-3880 09/04/23 17:24:36.893
STEP: creating replication controller affinity-clusterip-transition in namespace services-3880 09/04/23 17:24:36.912
I0904 17:24:36.922970      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3880, replica count: 3
I0904 17:24:39.974567      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 17:24:39.981: INFO: Creating new exec pod
Sep  4 17:24:39.987: INFO: Waiting up to 5m0s for pod "execpod-affinityvggdr" in namespace "services-3880" to be "running"
Sep  4 17:24:39.990: INFO: Pod "execpod-affinityvggdr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.543976ms
Sep  4 17:24:41.996: INFO: Pod "execpod-affinityvggdr": Phase="Running", Reason="", readiness=true. Elapsed: 2.0081182s
Sep  4 17:24:41.996: INFO: Pod "execpod-affinityvggdr" satisfied condition "running"
Sep  4 17:24:42.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-3880 exec execpod-affinityvggdr -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Sep  4 17:24:43.366: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep  4 17:24:43.366: INFO: stdout: ""
Sep  4 17:24:43.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-3880 exec execpod-affinityvggdr -- /bin/sh -x -c nc -v -z -w 2 10.96.66.77 80'
Sep  4 17:24:43.550: INFO: stderr: "+ nc -v -z -w 2 10.96.66.77 80\nConnection to 10.96.66.77 80 port [tcp/http] succeeded!\n"
Sep  4 17:24:43.550: INFO: stdout: ""
Sep  4 17:24:43.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-3880 exec execpod-affinityvggdr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.66.77:80/ ; done'
Sep  4 17:24:43.931: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n"
Sep  4 17:24:43.931: INFO: stdout: "\naffinity-clusterip-transition-klfqj\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-rgbn4\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-klfqj\naffinity-clusterip-transition-klfqj\naffinity-clusterip-transition-klfqj\naffinity-clusterip-transition-klfqj\naffinity-clusterip-transition-rgbn4\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-rgbn4\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-rgbn4\naffinity-clusterip-transition-rgbn4\naffinity-clusterip-transition-klfqj"
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-klfqj
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-rgbn4
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-klfqj
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-klfqj
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-klfqj
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-klfqj
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-rgbn4
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-rgbn4
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-rgbn4
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-rgbn4
Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-klfqj
Sep  4 17:24:43.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-3880 exec execpod-affinityvggdr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.66.77:80/ ; done'
Sep  4 17:24:44.403: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n"
Sep  4 17:24:44.403: INFO: stdout: "\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt"
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
Sep  4 17:24:44.403: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3880, will wait for the garbage collector to delete the pods 09/04/23 17:24:44.423
Sep  4 17:24:44.487: INFO: Deleting ReplicationController affinity-clusterip-transition took: 10.155032ms
Sep  4 17:24:44.588: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.031892ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 17:24:47.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3880" for this suite. 09/04/23 17:24:47.021
------------------------------
â€¢ [SLOW TEST] [10.280 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:24:36.749
    Sep  4 17:24:36.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 17:24:36.753
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:36.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:36.89
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-3880 09/04/23 17:24:36.893
    STEP: creating service affinity-clusterip-transition in namespace services-3880 09/04/23 17:24:36.893
    STEP: creating replication controller affinity-clusterip-transition in namespace services-3880 09/04/23 17:24:36.912
    I0904 17:24:36.922970      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3880, replica count: 3
    I0904 17:24:39.974567      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 17:24:39.981: INFO: Creating new exec pod
    Sep  4 17:24:39.987: INFO: Waiting up to 5m0s for pod "execpod-affinityvggdr" in namespace "services-3880" to be "running"
    Sep  4 17:24:39.990: INFO: Pod "execpod-affinityvggdr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.543976ms
    Sep  4 17:24:41.996: INFO: Pod "execpod-affinityvggdr": Phase="Running", Reason="", readiness=true. Elapsed: 2.0081182s
    Sep  4 17:24:41.996: INFO: Pod "execpod-affinityvggdr" satisfied condition "running"
    Sep  4 17:24:42.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-3880 exec execpod-affinityvggdr -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Sep  4 17:24:43.366: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Sep  4 17:24:43.366: INFO: stdout: ""
    Sep  4 17:24:43.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-3880 exec execpod-affinityvggdr -- /bin/sh -x -c nc -v -z -w 2 10.96.66.77 80'
    Sep  4 17:24:43.550: INFO: stderr: "+ nc -v -z -w 2 10.96.66.77 80\nConnection to 10.96.66.77 80 port [tcp/http] succeeded!\n"
    Sep  4 17:24:43.550: INFO: stdout: ""
    Sep  4 17:24:43.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-3880 exec execpod-affinityvggdr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.66.77:80/ ; done'
    Sep  4 17:24:43.931: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n"
    Sep  4 17:24:43.931: INFO: stdout: "\naffinity-clusterip-transition-klfqj\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-rgbn4\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-klfqj\naffinity-clusterip-transition-klfqj\naffinity-clusterip-transition-klfqj\naffinity-clusterip-transition-klfqj\naffinity-clusterip-transition-rgbn4\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-rgbn4\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-rgbn4\naffinity-clusterip-transition-rgbn4\naffinity-clusterip-transition-klfqj"
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-klfqj
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-rgbn4
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-klfqj
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-klfqj
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-klfqj
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-klfqj
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-rgbn4
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-rgbn4
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-rgbn4
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-rgbn4
    Sep  4 17:24:43.931: INFO: Received response from host: affinity-clusterip-transition-klfqj
    Sep  4 17:24:43.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-3880 exec execpod-affinityvggdr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.66.77:80/ ; done'
    Sep  4 17:24:44.403: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.66.77:80/\n"
    Sep  4 17:24:44.403: INFO: stdout: "\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt\naffinity-clusterip-transition-l88xt"
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Received response from host: affinity-clusterip-transition-l88xt
    Sep  4 17:24:44.403: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3880, will wait for the garbage collector to delete the pods 09/04/23 17:24:44.423
    Sep  4 17:24:44.487: INFO: Deleting ReplicationController affinity-clusterip-transition took: 10.155032ms
    Sep  4 17:24:44.588: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.031892ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:24:47.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3880" for this suite. 09/04/23 17:24:47.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:24:47.034
Sep  4 17:24:47.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 17:24:47.036
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:47.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:47.086
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 09/04/23 17:24:47.09
Sep  4 17:24:47.105: INFO: Waiting up to 5m0s for pod "pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b" in namespace "emptydir-1393" to be "Succeeded or Failed"
Sep  4 17:24:47.112: INFO: Pod "pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.289826ms
Sep  4 17:24:49.120: INFO: Pod "pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014815117s
Sep  4 17:24:51.120: INFO: Pod "pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01502588s
STEP: Saw pod success 09/04/23 17:24:51.12
Sep  4 17:24:51.121: INFO: Pod "pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b" satisfied condition "Succeeded or Failed"
Sep  4 17:24:51.124: INFO: Trying to get logs from node tenant-000001 pod pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b container test-container: <nil>
STEP: delete the pod 09/04/23 17:24:51.134
Sep  4 17:24:51.164: INFO: Waiting for pod pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b to disappear
Sep  4 17:24:51.166: INFO: Pod pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 17:24:51.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1393" for this suite. 09/04/23 17:24:51.17
------------------------------
â€¢ [4.143 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:24:47.034
    Sep  4 17:24:47.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 17:24:47.036
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:47.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:47.086
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 09/04/23 17:24:47.09
    Sep  4 17:24:47.105: INFO: Waiting up to 5m0s for pod "pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b" in namespace "emptydir-1393" to be "Succeeded or Failed"
    Sep  4 17:24:47.112: INFO: Pod "pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.289826ms
    Sep  4 17:24:49.120: INFO: Pod "pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014815117s
    Sep  4 17:24:51.120: INFO: Pod "pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01502588s
    STEP: Saw pod success 09/04/23 17:24:51.12
    Sep  4 17:24:51.121: INFO: Pod "pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b" satisfied condition "Succeeded or Failed"
    Sep  4 17:24:51.124: INFO: Trying to get logs from node tenant-000001 pod pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b container test-container: <nil>
    STEP: delete the pod 09/04/23 17:24:51.134
    Sep  4 17:24:51.164: INFO: Waiting for pod pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b to disappear
    Sep  4 17:24:51.166: INFO: Pod pod-a2d6f8f0-5cf9-49e2-a902-c5916fc01d4b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:24:51.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1393" for this suite. 09/04/23 17:24:51.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:24:51.183
Sep  4 17:24:51.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename gc 09/04/23 17:24:51.185
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:51.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:51.204
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 09/04/23 17:24:51.21
STEP: delete the rc 09/04/23 17:24:56.235
STEP: wait for the rc to be deleted 09/04/23 17:24:56.256
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/04/23 17:25:01.454
STEP: Gathering metrics 09/04/23 17:25:31.477
W0904 17:25:31.484343      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep  4 17:25:31.484: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  4 17:25:31.484: INFO: Deleting pod "simpletest.rc-2rjw9" in namespace "gc-7514"
Sep  4 17:25:31.548: INFO: Deleting pod "simpletest.rc-4jt4r" in namespace "gc-7514"
Sep  4 17:25:31.610: INFO: Deleting pod "simpletest.rc-5ctkz" in namespace "gc-7514"
Sep  4 17:25:31.669: INFO: Deleting pod "simpletest.rc-5p6ng" in namespace "gc-7514"
Sep  4 17:25:31.723: INFO: Deleting pod "simpletest.rc-5qth9" in namespace "gc-7514"
Sep  4 17:25:31.750: INFO: Deleting pod "simpletest.rc-5tc58" in namespace "gc-7514"
Sep  4 17:25:31.938: INFO: Deleting pod "simpletest.rc-65thq" in namespace "gc-7514"
Sep  4 17:25:32.030: INFO: Deleting pod "simpletest.rc-6ngfp" in namespace "gc-7514"
Sep  4 17:25:32.124: INFO: Deleting pod "simpletest.rc-7b2tf" in namespace "gc-7514"
Sep  4 17:25:32.178: INFO: Deleting pod "simpletest.rc-7cvs8" in namespace "gc-7514"
Sep  4 17:25:32.284: INFO: Deleting pod "simpletest.rc-7kd64" in namespace "gc-7514"
Sep  4 17:25:32.348: INFO: Deleting pod "simpletest.rc-7scfs" in namespace "gc-7514"
Sep  4 17:25:32.697: INFO: Deleting pod "simpletest.rc-85wxp" in namespace "gc-7514"
Sep  4 17:25:32.759: INFO: Deleting pod "simpletest.rc-88xc7" in namespace "gc-7514"
Sep  4 17:25:32.798: INFO: Deleting pod "simpletest.rc-8dzt9" in namespace "gc-7514"
Sep  4 17:25:32.878: INFO: Deleting pod "simpletest.rc-8q94b" in namespace "gc-7514"
Sep  4 17:25:32.961: INFO: Deleting pod "simpletest.rc-8vd92" in namespace "gc-7514"
Sep  4 17:25:33.017: INFO: Deleting pod "simpletest.rc-996w2" in namespace "gc-7514"
Sep  4 17:25:33.059: INFO: Deleting pod "simpletest.rc-9kmxj" in namespace "gc-7514"
Sep  4 17:25:33.102: INFO: Deleting pod "simpletest.rc-9zcr8" in namespace "gc-7514"
Sep  4 17:25:33.187: INFO: Deleting pod "simpletest.rc-bbh5c" in namespace "gc-7514"
Sep  4 17:25:33.259: INFO: Deleting pod "simpletest.rc-bc5cl" in namespace "gc-7514"
Sep  4 17:25:33.331: INFO: Deleting pod "simpletest.rc-bdfzk" in namespace "gc-7514"
Sep  4 17:25:33.456: INFO: Deleting pod "simpletest.rc-bjn25" in namespace "gc-7514"
Sep  4 17:25:33.514: INFO: Deleting pod "simpletest.rc-bknlk" in namespace "gc-7514"
Sep  4 17:25:34.092: INFO: Deleting pod "simpletest.rc-bvchp" in namespace "gc-7514"
Sep  4 17:25:34.182: INFO: Deleting pod "simpletest.rc-c686p" in namespace "gc-7514"
Sep  4 17:25:34.243: INFO: Deleting pod "simpletest.rc-chsgq" in namespace "gc-7514"
Sep  4 17:25:34.287: INFO: Deleting pod "simpletest.rc-cjcqf" in namespace "gc-7514"
Sep  4 17:25:34.344: INFO: Deleting pod "simpletest.rc-csmls" in namespace "gc-7514"
Sep  4 17:25:34.385: INFO: Deleting pod "simpletest.rc-cz24z" in namespace "gc-7514"
Sep  4 17:25:34.432: INFO: Deleting pod "simpletest.rc-d4k4j" in namespace "gc-7514"
Sep  4 17:25:34.488: INFO: Deleting pod "simpletest.rc-dw84t" in namespace "gc-7514"
Sep  4 17:25:34.747: INFO: Deleting pod "simpletest.rc-f5qdg" in namespace "gc-7514"
Sep  4 17:25:34.840: INFO: Deleting pod "simpletest.rc-fdzxl" in namespace "gc-7514"
Sep  4 17:25:34.884: INFO: Deleting pod "simpletest.rc-fhjgh" in namespace "gc-7514"
Sep  4 17:25:34.947: INFO: Deleting pod "simpletest.rc-fk6jj" in namespace "gc-7514"
Sep  4 17:25:35.102: INFO: Deleting pod "simpletest.rc-fsz4v" in namespace "gc-7514"
Sep  4 17:25:35.166: INFO: Deleting pod "simpletest.rc-fx5rb" in namespace "gc-7514"
Sep  4 17:25:35.187: INFO: Deleting pod "simpletest.rc-gxcl9" in namespace "gc-7514"
Sep  4 17:25:35.216: INFO: Deleting pod "simpletest.rc-hbv4n" in namespace "gc-7514"
Sep  4 17:25:35.244: INFO: Deleting pod "simpletest.rc-hcs7g" in namespace "gc-7514"
Sep  4 17:25:35.311: INFO: Deleting pod "simpletest.rc-hm6dt" in namespace "gc-7514"
Sep  4 17:25:35.378: INFO: Deleting pod "simpletest.rc-hpw8x" in namespace "gc-7514"
Sep  4 17:25:35.434: INFO: Deleting pod "simpletest.rc-hvv5c" in namespace "gc-7514"
Sep  4 17:25:35.472: INFO: Deleting pod "simpletest.rc-j926l" in namespace "gc-7514"
Sep  4 17:25:35.534: INFO: Deleting pod "simpletest.rc-jtsc9" in namespace "gc-7514"
Sep  4 17:25:35.718: INFO: Deleting pod "simpletest.rc-k5jgr" in namespace "gc-7514"
Sep  4 17:25:35.792: INFO: Deleting pod "simpletest.rc-kbv55" in namespace "gc-7514"
Sep  4 17:25:35.895: INFO: Deleting pod "simpletest.rc-kcrtm" in namespace "gc-7514"
Sep  4 17:25:35.915: INFO: Deleting pod "simpletest.rc-l2zpn" in namespace "gc-7514"
Sep  4 17:25:36.062: INFO: Deleting pod "simpletest.rc-lf6lf" in namespace "gc-7514"
Sep  4 17:25:36.151: INFO: Deleting pod "simpletest.rc-lqf2j" in namespace "gc-7514"
Sep  4 17:25:36.254: INFO: Deleting pod "simpletest.rc-lrrgn" in namespace "gc-7514"
Sep  4 17:25:36.316: INFO: Deleting pod "simpletest.rc-lrx6g" in namespace "gc-7514"
Sep  4 17:25:36.410: INFO: Deleting pod "simpletest.rc-m2szg" in namespace "gc-7514"
Sep  4 17:25:36.518: INFO: Deleting pod "simpletest.rc-m6w7b" in namespace "gc-7514"
Sep  4 17:25:36.709: INFO: Deleting pod "simpletest.rc-m7vcr" in namespace "gc-7514"
Sep  4 17:25:36.776: INFO: Deleting pod "simpletest.rc-mhqgr" in namespace "gc-7514"
Sep  4 17:25:37.059: INFO: Deleting pod "simpletest.rc-mlpf5" in namespace "gc-7514"
Sep  4 17:25:37.083: INFO: Deleting pod "simpletest.rc-n647s" in namespace "gc-7514"
Sep  4 17:25:37.100: INFO: Deleting pod "simpletest.rc-nq97d" in namespace "gc-7514"
Sep  4 17:25:37.133: INFO: Deleting pod "simpletest.rc-p4jnp" in namespace "gc-7514"
Sep  4 17:25:37.192: INFO: Deleting pod "simpletest.rc-pvrh6" in namespace "gc-7514"
Sep  4 17:25:37.241: INFO: Deleting pod "simpletest.rc-px526" in namespace "gc-7514"
Sep  4 17:25:37.286: INFO: Deleting pod "simpletest.rc-q9mbn" in namespace "gc-7514"
Sep  4 17:25:37.337: INFO: Deleting pod "simpletest.rc-qbdp4" in namespace "gc-7514"
Sep  4 17:25:37.410: INFO: Deleting pod "simpletest.rc-qd7zg" in namespace "gc-7514"
Sep  4 17:25:37.478: INFO: Deleting pod "simpletest.rc-qdkh6" in namespace "gc-7514"
Sep  4 17:25:37.790: INFO: Deleting pod "simpletest.rc-qpgdq" in namespace "gc-7514"
Sep  4 17:25:37.811: INFO: Deleting pod "simpletest.rc-qs27x" in namespace "gc-7514"
Sep  4 17:25:37.869: INFO: Deleting pod "simpletest.rc-rdhp7" in namespace "gc-7514"
Sep  4 17:25:37.910: INFO: Deleting pod "simpletest.rc-rkfsb" in namespace "gc-7514"
Sep  4 17:25:37.953: INFO: Deleting pod "simpletest.rc-rljwq" in namespace "gc-7514"
Sep  4 17:25:37.993: INFO: Deleting pod "simpletest.rc-rlqqn" in namespace "gc-7514"
Sep  4 17:25:38.040: INFO: Deleting pod "simpletest.rc-rmjpm" in namespace "gc-7514"
Sep  4 17:25:38.146: INFO: Deleting pod "simpletest.rc-s2q4g" in namespace "gc-7514"
Sep  4 17:25:38.210: INFO: Deleting pod "simpletest.rc-s82n7" in namespace "gc-7514"
Sep  4 17:25:38.227: INFO: Deleting pod "simpletest.rc-sptm6" in namespace "gc-7514"
Sep  4 17:25:38.241: INFO: Deleting pod "simpletest.rc-svdnf" in namespace "gc-7514"
Sep  4 17:25:38.300: INFO: Deleting pod "simpletest.rc-t2bm2" in namespace "gc-7514"
Sep  4 17:25:38.377: INFO: Deleting pod "simpletest.rc-t65w7" in namespace "gc-7514"
Sep  4 17:25:38.421: INFO: Deleting pod "simpletest.rc-tdwkf" in namespace "gc-7514"
Sep  4 17:25:38.479: INFO: Deleting pod "simpletest.rc-tftps" in namespace "gc-7514"
Sep  4 17:25:38.551: INFO: Deleting pod "simpletest.rc-txt9c" in namespace "gc-7514"
Sep  4 17:25:38.638: INFO: Deleting pod "simpletest.rc-v2bfk" in namespace "gc-7514"
Sep  4 17:25:38.678: INFO: Deleting pod "simpletest.rc-v95k6" in namespace "gc-7514"
Sep  4 17:25:38.708: INFO: Deleting pod "simpletest.rc-wcm6g" in namespace "gc-7514"
Sep  4 17:25:38.742: INFO: Deleting pod "simpletest.rc-wdkng" in namespace "gc-7514"
Sep  4 17:25:38.769: INFO: Deleting pod "simpletest.rc-wr25r" in namespace "gc-7514"
Sep  4 17:25:38.824: INFO: Deleting pod "simpletest.rc-xcgp2" in namespace "gc-7514"
Sep  4 17:25:38.893: INFO: Deleting pod "simpletest.rc-xdnh8" in namespace "gc-7514"
Sep  4 17:25:39.008: INFO: Deleting pod "simpletest.rc-xgf7w" in namespace "gc-7514"
Sep  4 17:25:39.026: INFO: Deleting pod "simpletest.rc-xn4vj" in namespace "gc-7514"
Sep  4 17:25:39.048: INFO: Deleting pod "simpletest.rc-z4h62" in namespace "gc-7514"
Sep  4 17:25:39.134: INFO: Deleting pod "simpletest.rc-z88t6" in namespace "gc-7514"
Sep  4 17:25:39.191: INFO: Deleting pod "simpletest.rc-zczdq" in namespace "gc-7514"
Sep  4 17:25:39.231: INFO: Deleting pod "simpletest.rc-zppcc" in namespace "gc-7514"
Sep  4 17:25:39.485: INFO: Deleting pod "simpletest.rc-zrb55" in namespace "gc-7514"
Sep  4 17:25:39.510: INFO: Deleting pod "simpletest.rc-zxpvr" in namespace "gc-7514"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  4 17:25:39.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7514" for this suite. 09/04/23 17:25:39.53
------------------------------
â€¢ [SLOW TEST] [48.359 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:24:51.183
    Sep  4 17:24:51.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename gc 09/04/23 17:24:51.185
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:24:51.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:24:51.204
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 09/04/23 17:24:51.21
    STEP: delete the rc 09/04/23 17:24:56.235
    STEP: wait for the rc to be deleted 09/04/23 17:24:56.256
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/04/23 17:25:01.454
    STEP: Gathering metrics 09/04/23 17:25:31.477
    W0904 17:25:31.484343      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep  4 17:25:31.484: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep  4 17:25:31.484: INFO: Deleting pod "simpletest.rc-2rjw9" in namespace "gc-7514"
    Sep  4 17:25:31.548: INFO: Deleting pod "simpletest.rc-4jt4r" in namespace "gc-7514"
    Sep  4 17:25:31.610: INFO: Deleting pod "simpletest.rc-5ctkz" in namespace "gc-7514"
    Sep  4 17:25:31.669: INFO: Deleting pod "simpletest.rc-5p6ng" in namespace "gc-7514"
    Sep  4 17:25:31.723: INFO: Deleting pod "simpletest.rc-5qth9" in namespace "gc-7514"
    Sep  4 17:25:31.750: INFO: Deleting pod "simpletest.rc-5tc58" in namespace "gc-7514"
    Sep  4 17:25:31.938: INFO: Deleting pod "simpletest.rc-65thq" in namespace "gc-7514"
    Sep  4 17:25:32.030: INFO: Deleting pod "simpletest.rc-6ngfp" in namespace "gc-7514"
    Sep  4 17:25:32.124: INFO: Deleting pod "simpletest.rc-7b2tf" in namespace "gc-7514"
    Sep  4 17:25:32.178: INFO: Deleting pod "simpletest.rc-7cvs8" in namespace "gc-7514"
    Sep  4 17:25:32.284: INFO: Deleting pod "simpletest.rc-7kd64" in namespace "gc-7514"
    Sep  4 17:25:32.348: INFO: Deleting pod "simpletest.rc-7scfs" in namespace "gc-7514"
    Sep  4 17:25:32.697: INFO: Deleting pod "simpletest.rc-85wxp" in namespace "gc-7514"
    Sep  4 17:25:32.759: INFO: Deleting pod "simpletest.rc-88xc7" in namespace "gc-7514"
    Sep  4 17:25:32.798: INFO: Deleting pod "simpletest.rc-8dzt9" in namespace "gc-7514"
    Sep  4 17:25:32.878: INFO: Deleting pod "simpletest.rc-8q94b" in namespace "gc-7514"
    Sep  4 17:25:32.961: INFO: Deleting pod "simpletest.rc-8vd92" in namespace "gc-7514"
    Sep  4 17:25:33.017: INFO: Deleting pod "simpletest.rc-996w2" in namespace "gc-7514"
    Sep  4 17:25:33.059: INFO: Deleting pod "simpletest.rc-9kmxj" in namespace "gc-7514"
    Sep  4 17:25:33.102: INFO: Deleting pod "simpletest.rc-9zcr8" in namespace "gc-7514"
    Sep  4 17:25:33.187: INFO: Deleting pod "simpletest.rc-bbh5c" in namespace "gc-7514"
    Sep  4 17:25:33.259: INFO: Deleting pod "simpletest.rc-bc5cl" in namespace "gc-7514"
    Sep  4 17:25:33.331: INFO: Deleting pod "simpletest.rc-bdfzk" in namespace "gc-7514"
    Sep  4 17:25:33.456: INFO: Deleting pod "simpletest.rc-bjn25" in namespace "gc-7514"
    Sep  4 17:25:33.514: INFO: Deleting pod "simpletest.rc-bknlk" in namespace "gc-7514"
    Sep  4 17:25:34.092: INFO: Deleting pod "simpletest.rc-bvchp" in namespace "gc-7514"
    Sep  4 17:25:34.182: INFO: Deleting pod "simpletest.rc-c686p" in namespace "gc-7514"
    Sep  4 17:25:34.243: INFO: Deleting pod "simpletest.rc-chsgq" in namespace "gc-7514"
    Sep  4 17:25:34.287: INFO: Deleting pod "simpletest.rc-cjcqf" in namespace "gc-7514"
    Sep  4 17:25:34.344: INFO: Deleting pod "simpletest.rc-csmls" in namespace "gc-7514"
    Sep  4 17:25:34.385: INFO: Deleting pod "simpletest.rc-cz24z" in namespace "gc-7514"
    Sep  4 17:25:34.432: INFO: Deleting pod "simpletest.rc-d4k4j" in namespace "gc-7514"
    Sep  4 17:25:34.488: INFO: Deleting pod "simpletest.rc-dw84t" in namespace "gc-7514"
    Sep  4 17:25:34.747: INFO: Deleting pod "simpletest.rc-f5qdg" in namespace "gc-7514"
    Sep  4 17:25:34.840: INFO: Deleting pod "simpletest.rc-fdzxl" in namespace "gc-7514"
    Sep  4 17:25:34.884: INFO: Deleting pod "simpletest.rc-fhjgh" in namespace "gc-7514"
    Sep  4 17:25:34.947: INFO: Deleting pod "simpletest.rc-fk6jj" in namespace "gc-7514"
    Sep  4 17:25:35.102: INFO: Deleting pod "simpletest.rc-fsz4v" in namespace "gc-7514"
    Sep  4 17:25:35.166: INFO: Deleting pod "simpletest.rc-fx5rb" in namespace "gc-7514"
    Sep  4 17:25:35.187: INFO: Deleting pod "simpletest.rc-gxcl9" in namespace "gc-7514"
    Sep  4 17:25:35.216: INFO: Deleting pod "simpletest.rc-hbv4n" in namespace "gc-7514"
    Sep  4 17:25:35.244: INFO: Deleting pod "simpletest.rc-hcs7g" in namespace "gc-7514"
    Sep  4 17:25:35.311: INFO: Deleting pod "simpletest.rc-hm6dt" in namespace "gc-7514"
    Sep  4 17:25:35.378: INFO: Deleting pod "simpletest.rc-hpw8x" in namespace "gc-7514"
    Sep  4 17:25:35.434: INFO: Deleting pod "simpletest.rc-hvv5c" in namespace "gc-7514"
    Sep  4 17:25:35.472: INFO: Deleting pod "simpletest.rc-j926l" in namespace "gc-7514"
    Sep  4 17:25:35.534: INFO: Deleting pod "simpletest.rc-jtsc9" in namespace "gc-7514"
    Sep  4 17:25:35.718: INFO: Deleting pod "simpletest.rc-k5jgr" in namespace "gc-7514"
    Sep  4 17:25:35.792: INFO: Deleting pod "simpletest.rc-kbv55" in namespace "gc-7514"
    Sep  4 17:25:35.895: INFO: Deleting pod "simpletest.rc-kcrtm" in namespace "gc-7514"
    Sep  4 17:25:35.915: INFO: Deleting pod "simpletest.rc-l2zpn" in namespace "gc-7514"
    Sep  4 17:25:36.062: INFO: Deleting pod "simpletest.rc-lf6lf" in namespace "gc-7514"
    Sep  4 17:25:36.151: INFO: Deleting pod "simpletest.rc-lqf2j" in namespace "gc-7514"
    Sep  4 17:25:36.254: INFO: Deleting pod "simpletest.rc-lrrgn" in namespace "gc-7514"
    Sep  4 17:25:36.316: INFO: Deleting pod "simpletest.rc-lrx6g" in namespace "gc-7514"
    Sep  4 17:25:36.410: INFO: Deleting pod "simpletest.rc-m2szg" in namespace "gc-7514"
    Sep  4 17:25:36.518: INFO: Deleting pod "simpletest.rc-m6w7b" in namespace "gc-7514"
    Sep  4 17:25:36.709: INFO: Deleting pod "simpletest.rc-m7vcr" in namespace "gc-7514"
    Sep  4 17:25:36.776: INFO: Deleting pod "simpletest.rc-mhqgr" in namespace "gc-7514"
    Sep  4 17:25:37.059: INFO: Deleting pod "simpletest.rc-mlpf5" in namespace "gc-7514"
    Sep  4 17:25:37.083: INFO: Deleting pod "simpletest.rc-n647s" in namespace "gc-7514"
    Sep  4 17:25:37.100: INFO: Deleting pod "simpletest.rc-nq97d" in namespace "gc-7514"
    Sep  4 17:25:37.133: INFO: Deleting pod "simpletest.rc-p4jnp" in namespace "gc-7514"
    Sep  4 17:25:37.192: INFO: Deleting pod "simpletest.rc-pvrh6" in namespace "gc-7514"
    Sep  4 17:25:37.241: INFO: Deleting pod "simpletest.rc-px526" in namespace "gc-7514"
    Sep  4 17:25:37.286: INFO: Deleting pod "simpletest.rc-q9mbn" in namespace "gc-7514"
    Sep  4 17:25:37.337: INFO: Deleting pod "simpletest.rc-qbdp4" in namespace "gc-7514"
    Sep  4 17:25:37.410: INFO: Deleting pod "simpletest.rc-qd7zg" in namespace "gc-7514"
    Sep  4 17:25:37.478: INFO: Deleting pod "simpletest.rc-qdkh6" in namespace "gc-7514"
    Sep  4 17:25:37.790: INFO: Deleting pod "simpletest.rc-qpgdq" in namespace "gc-7514"
    Sep  4 17:25:37.811: INFO: Deleting pod "simpletest.rc-qs27x" in namespace "gc-7514"
    Sep  4 17:25:37.869: INFO: Deleting pod "simpletest.rc-rdhp7" in namespace "gc-7514"
    Sep  4 17:25:37.910: INFO: Deleting pod "simpletest.rc-rkfsb" in namespace "gc-7514"
    Sep  4 17:25:37.953: INFO: Deleting pod "simpletest.rc-rljwq" in namespace "gc-7514"
    Sep  4 17:25:37.993: INFO: Deleting pod "simpletest.rc-rlqqn" in namespace "gc-7514"
    Sep  4 17:25:38.040: INFO: Deleting pod "simpletest.rc-rmjpm" in namespace "gc-7514"
    Sep  4 17:25:38.146: INFO: Deleting pod "simpletest.rc-s2q4g" in namespace "gc-7514"
    Sep  4 17:25:38.210: INFO: Deleting pod "simpletest.rc-s82n7" in namespace "gc-7514"
    Sep  4 17:25:38.227: INFO: Deleting pod "simpletest.rc-sptm6" in namespace "gc-7514"
    Sep  4 17:25:38.241: INFO: Deleting pod "simpletest.rc-svdnf" in namespace "gc-7514"
    Sep  4 17:25:38.300: INFO: Deleting pod "simpletest.rc-t2bm2" in namespace "gc-7514"
    Sep  4 17:25:38.377: INFO: Deleting pod "simpletest.rc-t65w7" in namespace "gc-7514"
    Sep  4 17:25:38.421: INFO: Deleting pod "simpletest.rc-tdwkf" in namespace "gc-7514"
    Sep  4 17:25:38.479: INFO: Deleting pod "simpletest.rc-tftps" in namespace "gc-7514"
    Sep  4 17:25:38.551: INFO: Deleting pod "simpletest.rc-txt9c" in namespace "gc-7514"
    Sep  4 17:25:38.638: INFO: Deleting pod "simpletest.rc-v2bfk" in namespace "gc-7514"
    Sep  4 17:25:38.678: INFO: Deleting pod "simpletest.rc-v95k6" in namespace "gc-7514"
    Sep  4 17:25:38.708: INFO: Deleting pod "simpletest.rc-wcm6g" in namespace "gc-7514"
    Sep  4 17:25:38.742: INFO: Deleting pod "simpletest.rc-wdkng" in namespace "gc-7514"
    Sep  4 17:25:38.769: INFO: Deleting pod "simpletest.rc-wr25r" in namespace "gc-7514"
    Sep  4 17:25:38.824: INFO: Deleting pod "simpletest.rc-xcgp2" in namespace "gc-7514"
    Sep  4 17:25:38.893: INFO: Deleting pod "simpletest.rc-xdnh8" in namespace "gc-7514"
    Sep  4 17:25:39.008: INFO: Deleting pod "simpletest.rc-xgf7w" in namespace "gc-7514"
    Sep  4 17:25:39.026: INFO: Deleting pod "simpletest.rc-xn4vj" in namespace "gc-7514"
    Sep  4 17:25:39.048: INFO: Deleting pod "simpletest.rc-z4h62" in namespace "gc-7514"
    Sep  4 17:25:39.134: INFO: Deleting pod "simpletest.rc-z88t6" in namespace "gc-7514"
    Sep  4 17:25:39.191: INFO: Deleting pod "simpletest.rc-zczdq" in namespace "gc-7514"
    Sep  4 17:25:39.231: INFO: Deleting pod "simpletest.rc-zppcc" in namespace "gc-7514"
    Sep  4 17:25:39.485: INFO: Deleting pod "simpletest.rc-zrb55" in namespace "gc-7514"
    Sep  4 17:25:39.510: INFO: Deleting pod "simpletest.rc-zxpvr" in namespace "gc-7514"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:25:39.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7514" for this suite. 09/04/23 17:25:39.53
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:25:39.542
Sep  4 17:25:39.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename subpath 09/04/23 17:25:39.549
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:25:39.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:25:39.577
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/04/23 17:25:39.58
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-fp5g 09/04/23 17:25:39.599
STEP: Creating a pod to test atomic-volume-subpath 09/04/23 17:25:39.599
Sep  4 17:25:39.613: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fp5g" in namespace "subpath-8869" to be "Succeeded or Failed"
Sep  4 17:25:39.621: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Pending", Reason="", readiness=false. Elapsed: 6.898685ms
Sep  4 17:25:41.626: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011930548s
Sep  4 17:25:43.626: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011503088s
Sep  4 17:25:45.626: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011192036s
Sep  4 17:25:47.680: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065941281s
Sep  4 17:25:49.629: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Pending", Reason="", readiness=false. Elapsed: 10.014586918s
Sep  4 17:25:51.650: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035184331s
Sep  4 17:25:53.639: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 14.024513009s
Sep  4 17:25:55.626: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 16.011265811s
Sep  4 17:25:57.625: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 18.010650063s
Sep  4 17:25:59.628: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 20.01322364s
Sep  4 17:26:01.625: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 22.010880321s
Sep  4 17:26:03.629: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 24.01444154s
Sep  4 17:26:05.626: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 26.011749988s
Sep  4 17:26:07.629: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 28.015003599s
Sep  4 17:26:09.628: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 30.013753117s
Sep  4 17:26:11.625: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 32.010907431s
Sep  4 17:26:13.626: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=false. Elapsed: 34.011053678s
Sep  4 17:26:15.626: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.011760784s
STEP: Saw pod success 09/04/23 17:26:15.627
Sep  4 17:26:15.627: INFO: Pod "pod-subpath-test-downwardapi-fp5g" satisfied condition "Succeeded or Failed"
Sep  4 17:26:15.632: INFO: Trying to get logs from node tenant-000001 pod pod-subpath-test-downwardapi-fp5g container test-container-subpath-downwardapi-fp5g: <nil>
STEP: delete the pod 09/04/23 17:26:15.643
Sep  4 17:26:15.658: INFO: Waiting for pod pod-subpath-test-downwardapi-fp5g to disappear
Sep  4 17:26:15.661: INFO: Pod pod-subpath-test-downwardapi-fp5g no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fp5g 09/04/23 17:26:15.662
Sep  4 17:26:15.662: INFO: Deleting pod "pod-subpath-test-downwardapi-fp5g" in namespace "subpath-8869"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:15.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8869" for this suite. 09/04/23 17:26:15.671
------------------------------
â€¢ [SLOW TEST] [36.137 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:25:39.542
    Sep  4 17:25:39.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename subpath 09/04/23 17:25:39.549
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:25:39.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:25:39.577
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/04/23 17:25:39.58
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-fp5g 09/04/23 17:25:39.599
    STEP: Creating a pod to test atomic-volume-subpath 09/04/23 17:25:39.599
    Sep  4 17:25:39.613: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fp5g" in namespace "subpath-8869" to be "Succeeded or Failed"
    Sep  4 17:25:39.621: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Pending", Reason="", readiness=false. Elapsed: 6.898685ms
    Sep  4 17:25:41.626: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011930548s
    Sep  4 17:25:43.626: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011503088s
    Sep  4 17:25:45.626: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011192036s
    Sep  4 17:25:47.680: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065941281s
    Sep  4 17:25:49.629: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Pending", Reason="", readiness=false. Elapsed: 10.014586918s
    Sep  4 17:25:51.650: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035184331s
    Sep  4 17:25:53.639: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 14.024513009s
    Sep  4 17:25:55.626: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 16.011265811s
    Sep  4 17:25:57.625: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 18.010650063s
    Sep  4 17:25:59.628: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 20.01322364s
    Sep  4 17:26:01.625: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 22.010880321s
    Sep  4 17:26:03.629: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 24.01444154s
    Sep  4 17:26:05.626: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 26.011749988s
    Sep  4 17:26:07.629: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 28.015003599s
    Sep  4 17:26:09.628: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 30.013753117s
    Sep  4 17:26:11.625: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=true. Elapsed: 32.010907431s
    Sep  4 17:26:13.626: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Running", Reason="", readiness=false. Elapsed: 34.011053678s
    Sep  4 17:26:15.626: INFO: Pod "pod-subpath-test-downwardapi-fp5g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.011760784s
    STEP: Saw pod success 09/04/23 17:26:15.627
    Sep  4 17:26:15.627: INFO: Pod "pod-subpath-test-downwardapi-fp5g" satisfied condition "Succeeded or Failed"
    Sep  4 17:26:15.632: INFO: Trying to get logs from node tenant-000001 pod pod-subpath-test-downwardapi-fp5g container test-container-subpath-downwardapi-fp5g: <nil>
    STEP: delete the pod 09/04/23 17:26:15.643
    Sep  4 17:26:15.658: INFO: Waiting for pod pod-subpath-test-downwardapi-fp5g to disappear
    Sep  4 17:26:15.661: INFO: Pod pod-subpath-test-downwardapi-fp5g no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-fp5g 09/04/23 17:26:15.662
    Sep  4 17:26:15.662: INFO: Deleting pod "pod-subpath-test-downwardapi-fp5g" in namespace "subpath-8869"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:15.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8869" for this suite. 09/04/23 17:26:15.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:15.68
Sep  4 17:26:15.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 17:26:15.681
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:15.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:15.702
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 09/04/23 17:26:15.705
Sep  4 17:26:15.706: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4111 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 09/04/23 17:26:15.762
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:15.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4111" for this suite. 09/04/23 17:26:15.776
------------------------------
â€¢ [0.111 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:15.68
    Sep  4 17:26:15.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 17:26:15.681
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:15.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:15.702
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 09/04/23 17:26:15.705
    Sep  4 17:26:15.706: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4111 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 09/04/23 17:26:15.762
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:15.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4111" for this suite. 09/04/23 17:26:15.776
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:15.792
Sep  4 17:26:15.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 17:26:15.793
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:15.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:15.823
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-3d68b900-c9e4-4695-98f8-c666b23b59c4 09/04/23 17:26:15.826
STEP: Creating a pod to test consume configMaps 09/04/23 17:26:15.832
Sep  4 17:26:15.841: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228" in namespace "projected-6216" to be "Succeeded or Failed"
Sep  4 17:26:15.858: INFO: Pod "pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228": Phase="Pending", Reason="", readiness=false. Elapsed: 16.780029ms
Sep  4 17:26:17.863: INFO: Pod "pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021060088s
Sep  4 17:26:19.864: INFO: Pod "pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02192929s
STEP: Saw pod success 09/04/23 17:26:19.864
Sep  4 17:26:19.864: INFO: Pod "pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228" satisfied condition "Succeeded or Failed"
Sep  4 17:26:19.867: INFO: Trying to get logs from node tenant-000001 pod pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 17:26:19.88
Sep  4 17:26:19.903: INFO: Waiting for pod pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228 to disappear
Sep  4 17:26:19.906: INFO: Pod pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:19.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6216" for this suite. 09/04/23 17:26:19.919
------------------------------
â€¢ [4.138 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:15.792
    Sep  4 17:26:15.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 17:26:15.793
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:15.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:15.823
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-3d68b900-c9e4-4695-98f8-c666b23b59c4 09/04/23 17:26:15.826
    STEP: Creating a pod to test consume configMaps 09/04/23 17:26:15.832
    Sep  4 17:26:15.841: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228" in namespace "projected-6216" to be "Succeeded or Failed"
    Sep  4 17:26:15.858: INFO: Pod "pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228": Phase="Pending", Reason="", readiness=false. Elapsed: 16.780029ms
    Sep  4 17:26:17.863: INFO: Pod "pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021060088s
    Sep  4 17:26:19.864: INFO: Pod "pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02192929s
    STEP: Saw pod success 09/04/23 17:26:19.864
    Sep  4 17:26:19.864: INFO: Pod "pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228" satisfied condition "Succeeded or Failed"
    Sep  4 17:26:19.867: INFO: Trying to get logs from node tenant-000001 pod pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 17:26:19.88
    Sep  4 17:26:19.903: INFO: Waiting for pod pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228 to disappear
    Sep  4 17:26:19.906: INFO: Pod pod-projected-configmaps-7b723e65-518d-4d8e-a1f3-34d7ca53d228 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:19.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6216" for this suite. 09/04/23 17:26:19.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:19.936
Sep  4 17:26:19.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 17:26:19.938
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:19.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:19.961
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 09/04/23 17:26:19.964
Sep  4 17:26:19.964: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep  4 17:26:19.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 create -f -'
Sep  4 17:26:20.759: INFO: stderr: ""
Sep  4 17:26:20.759: INFO: stdout: "service/agnhost-replica created\n"
Sep  4 17:26:20.759: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep  4 17:26:20.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 create -f -'
Sep  4 17:26:21.144: INFO: stderr: ""
Sep  4 17:26:21.144: INFO: stdout: "service/agnhost-primary created\n"
Sep  4 17:26:21.144: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  4 17:26:21.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 create -f -'
Sep  4 17:26:21.973: INFO: stderr: ""
Sep  4 17:26:21.973: INFO: stdout: "service/frontend created\n"
Sep  4 17:26:21.973: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep  4 17:26:21.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 create -f -'
Sep  4 17:26:22.974: INFO: stderr: ""
Sep  4 17:26:22.974: INFO: stdout: "deployment.apps/frontend created\n"
Sep  4 17:26:22.974: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  4 17:26:22.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 create -f -'
Sep  4 17:26:23.313: INFO: stderr: ""
Sep  4 17:26:23.313: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep  4 17:26:23.313: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  4 17:26:23.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 create -f -'
Sep  4 17:26:23.769: INFO: stderr: ""
Sep  4 17:26:23.769: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 09/04/23 17:26:23.769
Sep  4 17:26:23.769: INFO: Waiting for all frontend pods to be Running.
Sep  4 17:26:28.852: INFO: Waiting for frontend to serve content.
Sep  4 17:26:28.873: INFO: Trying to add a new entry to the guestbook.
Sep  4 17:26:28.898: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 09/04/23 17:26:28.912
Sep  4 17:26:28.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 delete --grace-period=0 --force -f -'
Sep  4 17:26:29.028: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 17:26:29.028: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 09/04/23 17:26:29.028
Sep  4 17:26:29.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 delete --grace-period=0 --force -f -'
Sep  4 17:26:29.205: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 17:26:29.205: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/04/23 17:26:29.205
Sep  4 17:26:29.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 delete --grace-period=0 --force -f -'
Sep  4 17:26:29.329: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 17:26:29.330: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/04/23 17:26:29.33
Sep  4 17:26:29.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 delete --grace-period=0 --force -f -'
Sep  4 17:26:29.418: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 17:26:29.418: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/04/23 17:26:29.419
Sep  4 17:26:29.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 delete --grace-period=0 --force -f -'
Sep  4 17:26:29.634: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 17:26:29.634: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/04/23 17:26:29.634
Sep  4 17:26:29.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 delete --grace-period=0 --force -f -'
Sep  4 17:26:29.842: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 17:26:29.842: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:29.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4225" for this suite. 09/04/23 17:26:29.847
------------------------------
â€¢ [SLOW TEST] [9.926 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:19.936
    Sep  4 17:26:19.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 17:26:19.938
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:19.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:19.961
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 09/04/23 17:26:19.964
    Sep  4 17:26:19.964: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Sep  4 17:26:19.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 create -f -'
    Sep  4 17:26:20.759: INFO: stderr: ""
    Sep  4 17:26:20.759: INFO: stdout: "service/agnhost-replica created\n"
    Sep  4 17:26:20.759: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Sep  4 17:26:20.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 create -f -'
    Sep  4 17:26:21.144: INFO: stderr: ""
    Sep  4 17:26:21.144: INFO: stdout: "service/agnhost-primary created\n"
    Sep  4 17:26:21.144: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Sep  4 17:26:21.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 create -f -'
    Sep  4 17:26:21.973: INFO: stderr: ""
    Sep  4 17:26:21.973: INFO: stdout: "service/frontend created\n"
    Sep  4 17:26:21.973: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Sep  4 17:26:21.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 create -f -'
    Sep  4 17:26:22.974: INFO: stderr: ""
    Sep  4 17:26:22.974: INFO: stdout: "deployment.apps/frontend created\n"
    Sep  4 17:26:22.974: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep  4 17:26:22.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 create -f -'
    Sep  4 17:26:23.313: INFO: stderr: ""
    Sep  4 17:26:23.313: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Sep  4 17:26:23.313: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep  4 17:26:23.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 create -f -'
    Sep  4 17:26:23.769: INFO: stderr: ""
    Sep  4 17:26:23.769: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 09/04/23 17:26:23.769
    Sep  4 17:26:23.769: INFO: Waiting for all frontend pods to be Running.
    Sep  4 17:26:28.852: INFO: Waiting for frontend to serve content.
    Sep  4 17:26:28.873: INFO: Trying to add a new entry to the guestbook.
    Sep  4 17:26:28.898: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 09/04/23 17:26:28.912
    Sep  4 17:26:28.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 delete --grace-period=0 --force -f -'
    Sep  4 17:26:29.028: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 17:26:29.028: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 09/04/23 17:26:29.028
    Sep  4 17:26:29.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 delete --grace-period=0 --force -f -'
    Sep  4 17:26:29.205: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 17:26:29.205: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/04/23 17:26:29.205
    Sep  4 17:26:29.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 delete --grace-period=0 --force -f -'
    Sep  4 17:26:29.329: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 17:26:29.330: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/04/23 17:26:29.33
    Sep  4 17:26:29.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 delete --grace-period=0 --force -f -'
    Sep  4 17:26:29.418: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 17:26:29.418: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/04/23 17:26:29.419
    Sep  4 17:26:29.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 delete --grace-period=0 --force -f -'
    Sep  4 17:26:29.634: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 17:26:29.634: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/04/23 17:26:29.634
    Sep  4 17:26:29.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4225 delete --grace-period=0 --force -f -'
    Sep  4 17:26:29.842: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 17:26:29.842: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:29.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4225" for this suite. 09/04/23 17:26:29.847
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:29.862
Sep  4 17:26:29.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 17:26:29.863
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:29.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:29.95
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9729 09/04/23 17:26:29.955
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/04/23 17:26:29.979
STEP: creating service externalsvc in namespace services-9729 09/04/23 17:26:29.98
STEP: creating replication controller externalsvc in namespace services-9729 09/04/23 17:26:30.034
I0904 17:26:30.050534      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9729, replica count: 2
I0904 17:26:33.190288      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 09/04/23 17:26:33.194
Sep  4 17:26:33.219: INFO: Creating new exec pod
Sep  4 17:26:33.233: INFO: Waiting up to 5m0s for pod "execpodt8wr5" in namespace "services-9729" to be "running"
Sep  4 17:26:33.260: INFO: Pod "execpodt8wr5": Phase="Pending", Reason="", readiness=false. Elapsed: 25.9755ms
Sep  4 17:26:35.266: INFO: Pod "execpodt8wr5": Phase="Running", Reason="", readiness=true. Elapsed: 2.032524506s
Sep  4 17:26:35.266: INFO: Pod "execpodt8wr5" satisfied condition "running"
Sep  4 17:26:35.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-9729 exec execpodt8wr5 -- /bin/sh -x -c nslookup nodeport-service.services-9729.svc.cluster.local'
Sep  4 17:26:35.515: INFO: stderr: "+ nslookup nodeport-service.services-9729.svc.cluster.local\n"
Sep  4 17:26:35.515: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-9729.svc.cluster.local\tcanonical name = externalsvc.services-9729.svc.cluster.local.\nName:\texternalsvc.services-9729.svc.cluster.local\nAddress: 10.96.119.55\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9729, will wait for the garbage collector to delete the pods 09/04/23 17:26:35.515
Sep  4 17:26:35.579: INFO: Deleting ReplicationController externalsvc took: 9.309609ms
Sep  4 17:26:35.680: INFO: Terminating ReplicationController externalsvc pods took: 101.663053ms
Sep  4 17:26:38.209: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:38.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9729" for this suite. 09/04/23 17:26:38.238
------------------------------
â€¢ [SLOW TEST] [8.384 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:29.862
    Sep  4 17:26:29.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 17:26:29.863
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:29.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:29.95
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-9729 09/04/23 17:26:29.955
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/04/23 17:26:29.979
    STEP: creating service externalsvc in namespace services-9729 09/04/23 17:26:29.98
    STEP: creating replication controller externalsvc in namespace services-9729 09/04/23 17:26:30.034
    I0904 17:26:30.050534      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9729, replica count: 2
    I0904 17:26:33.190288      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 09/04/23 17:26:33.194
    Sep  4 17:26:33.219: INFO: Creating new exec pod
    Sep  4 17:26:33.233: INFO: Waiting up to 5m0s for pod "execpodt8wr5" in namespace "services-9729" to be "running"
    Sep  4 17:26:33.260: INFO: Pod "execpodt8wr5": Phase="Pending", Reason="", readiness=false. Elapsed: 25.9755ms
    Sep  4 17:26:35.266: INFO: Pod "execpodt8wr5": Phase="Running", Reason="", readiness=true. Elapsed: 2.032524506s
    Sep  4 17:26:35.266: INFO: Pod "execpodt8wr5" satisfied condition "running"
    Sep  4 17:26:35.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-9729 exec execpodt8wr5 -- /bin/sh -x -c nslookup nodeport-service.services-9729.svc.cluster.local'
    Sep  4 17:26:35.515: INFO: stderr: "+ nslookup nodeport-service.services-9729.svc.cluster.local\n"
    Sep  4 17:26:35.515: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-9729.svc.cluster.local\tcanonical name = externalsvc.services-9729.svc.cluster.local.\nName:\texternalsvc.services-9729.svc.cluster.local\nAddress: 10.96.119.55\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9729, will wait for the garbage collector to delete the pods 09/04/23 17:26:35.515
    Sep  4 17:26:35.579: INFO: Deleting ReplicationController externalsvc took: 9.309609ms
    Sep  4 17:26:35.680: INFO: Terminating ReplicationController externalsvc pods took: 101.663053ms
    Sep  4 17:26:38.209: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:38.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9729" for this suite. 09/04/23 17:26:38.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:38.258
Sep  4 17:26:38.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename secrets 09/04/23 17:26:38.259
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:38.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:38.279
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-7576/secret-test-e9b79070-a93f-408b-9cde-0604603629db 09/04/23 17:26:38.281
STEP: Creating a pod to test consume secrets 09/04/23 17:26:38.286
Sep  4 17:26:38.314: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb" in namespace "secrets-7576" to be "Succeeded or Failed"
Sep  4 17:26:38.320: INFO: Pod "pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.329561ms
Sep  4 17:26:40.323: INFO: Pod "pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009145272s
Sep  4 17:26:42.327: INFO: Pod "pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01291798s
STEP: Saw pod success 09/04/23 17:26:42.327
Sep  4 17:26:42.327: INFO: Pod "pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb" satisfied condition "Succeeded or Failed"
Sep  4 17:26:42.330: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb container env-test: <nil>
STEP: delete the pod 09/04/23 17:26:42.35
Sep  4 17:26:42.375: INFO: Waiting for pod pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb to disappear
Sep  4 17:26:42.377: INFO: Pod pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:42.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7576" for this suite. 09/04/23 17:26:42.381
------------------------------
â€¢ [4.132 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:38.258
    Sep  4 17:26:38.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename secrets 09/04/23 17:26:38.259
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:38.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:38.279
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-7576/secret-test-e9b79070-a93f-408b-9cde-0604603629db 09/04/23 17:26:38.281
    STEP: Creating a pod to test consume secrets 09/04/23 17:26:38.286
    Sep  4 17:26:38.314: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb" in namespace "secrets-7576" to be "Succeeded or Failed"
    Sep  4 17:26:38.320: INFO: Pod "pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.329561ms
    Sep  4 17:26:40.323: INFO: Pod "pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009145272s
    Sep  4 17:26:42.327: INFO: Pod "pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01291798s
    STEP: Saw pod success 09/04/23 17:26:42.327
    Sep  4 17:26:42.327: INFO: Pod "pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb" satisfied condition "Succeeded or Failed"
    Sep  4 17:26:42.330: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb container env-test: <nil>
    STEP: delete the pod 09/04/23 17:26:42.35
    Sep  4 17:26:42.375: INFO: Waiting for pod pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb to disappear
    Sep  4 17:26:42.377: INFO: Pod pod-configmaps-ce77f248-d333-43d0-be1e-9ab512670dcb no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:42.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7576" for this suite. 09/04/23 17:26:42.381
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:42.395
Sep  4 17:26:42.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 17:26:42.397
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:42.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:42.42
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-dfd0aa05-1120-4f2c-9f4a-7c069df80de9 09/04/23 17:26:42.431
STEP: Creating the pod 09/04/23 17:26:42.441
Sep  4 17:26:42.453: INFO: Waiting up to 5m0s for pod "pod-configmaps-426ae5e8-b744-4e1e-84b0-0d1c39bcf2b7" in namespace "configmap-7658" to be "running"
Sep  4 17:26:42.466: INFO: Pod "pod-configmaps-426ae5e8-b744-4e1e-84b0-0d1c39bcf2b7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.154491ms
Sep  4 17:26:44.473: INFO: Pod "pod-configmaps-426ae5e8-b744-4e1e-84b0-0d1c39bcf2b7": Phase="Running", Reason="", readiness=false. Elapsed: 2.019690086s
Sep  4 17:26:44.473: INFO: Pod "pod-configmaps-426ae5e8-b744-4e1e-84b0-0d1c39bcf2b7" satisfied condition "running"
STEP: Waiting for pod with text data 09/04/23 17:26:44.473
STEP: Waiting for pod with binary data 09/04/23 17:26:44.483
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:44.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7658" for this suite. 09/04/23 17:26:44.495
------------------------------
â€¢ [2.108 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:42.395
    Sep  4 17:26:42.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 17:26:42.397
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:42.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:42.42
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-dfd0aa05-1120-4f2c-9f4a-7c069df80de9 09/04/23 17:26:42.431
    STEP: Creating the pod 09/04/23 17:26:42.441
    Sep  4 17:26:42.453: INFO: Waiting up to 5m0s for pod "pod-configmaps-426ae5e8-b744-4e1e-84b0-0d1c39bcf2b7" in namespace "configmap-7658" to be "running"
    Sep  4 17:26:42.466: INFO: Pod "pod-configmaps-426ae5e8-b744-4e1e-84b0-0d1c39bcf2b7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.154491ms
    Sep  4 17:26:44.473: INFO: Pod "pod-configmaps-426ae5e8-b744-4e1e-84b0-0d1c39bcf2b7": Phase="Running", Reason="", readiness=false. Elapsed: 2.019690086s
    Sep  4 17:26:44.473: INFO: Pod "pod-configmaps-426ae5e8-b744-4e1e-84b0-0d1c39bcf2b7" satisfied condition "running"
    STEP: Waiting for pod with text data 09/04/23 17:26:44.473
    STEP: Waiting for pod with binary data 09/04/23 17:26:44.483
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:44.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7658" for this suite. 09/04/23 17:26:44.495
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:44.507
Sep  4 17:26:44.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename watch 09/04/23 17:26:44.509
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:44.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:44.531
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 09/04/23 17:26:44.534
STEP: creating a new configmap 09/04/23 17:26:44.535
STEP: modifying the configmap once 09/04/23 17:26:44.542
STEP: closing the watch once it receives two notifications 09/04/23 17:26:44.551
Sep  4 17:26:44.551: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1651  2e10c76a-b28f-442e-beea-8d498234bb59 6126 0 2023-09-04 17:26:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 17:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 17:26:44.552: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1651  2e10c76a-b28f-442e-beea-8d498234bb59 6127 0 2023-09-04 17:26:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 17:26:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 09/04/23 17:26:44.552
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/04/23 17:26:44.562
STEP: deleting the configmap 09/04/23 17:26:44.563
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/04/23 17:26:44.572
Sep  4 17:26:44.572: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1651  2e10c76a-b28f-442e-beea-8d498234bb59 6128 0 2023-09-04 17:26:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 17:26:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 17:26:44.572: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1651  2e10c76a-b28f-442e-beea-8d498234bb59 6129 0 2023-09-04 17:26:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 17:26:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:44.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1651" for this suite. 09/04/23 17:26:44.578
------------------------------
â€¢ [0.079 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:44.507
    Sep  4 17:26:44.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename watch 09/04/23 17:26:44.509
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:44.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:44.531
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 09/04/23 17:26:44.534
    STEP: creating a new configmap 09/04/23 17:26:44.535
    STEP: modifying the configmap once 09/04/23 17:26:44.542
    STEP: closing the watch once it receives two notifications 09/04/23 17:26:44.551
    Sep  4 17:26:44.551: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1651  2e10c76a-b28f-442e-beea-8d498234bb59 6126 0 2023-09-04 17:26:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 17:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 17:26:44.552: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1651  2e10c76a-b28f-442e-beea-8d498234bb59 6127 0 2023-09-04 17:26:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 17:26:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 09/04/23 17:26:44.552
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/04/23 17:26:44.562
    STEP: deleting the configmap 09/04/23 17:26:44.563
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/04/23 17:26:44.572
    Sep  4 17:26:44.572: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1651  2e10c76a-b28f-442e-beea-8d498234bb59 6128 0 2023-09-04 17:26:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 17:26:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 17:26:44.572: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1651  2e10c76a-b28f-442e-beea-8d498234bb59 6129 0 2023-09-04 17:26:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 17:26:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:44.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1651" for this suite. 09/04/23 17:26:44.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:44.597
Sep  4 17:26:44.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename podtemplate 09/04/23 17:26:44.598
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:44.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:44.621
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 09/04/23 17:26:44.624
Sep  4 17:26:44.632: INFO: created test-podtemplate-1
Sep  4 17:26:44.638: INFO: created test-podtemplate-2
Sep  4 17:26:44.645: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 09/04/23 17:26:44.646
STEP: delete collection of pod templates 09/04/23 17:26:44.653
Sep  4 17:26:44.653: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 09/04/23 17:26:44.686
Sep  4 17:26:44.686: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:44.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6331" for this suite. 09/04/23 17:26:44.697
------------------------------
â€¢ [0.108 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:44.597
    Sep  4 17:26:44.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename podtemplate 09/04/23 17:26:44.598
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:44.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:44.621
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 09/04/23 17:26:44.624
    Sep  4 17:26:44.632: INFO: created test-podtemplate-1
    Sep  4 17:26:44.638: INFO: created test-podtemplate-2
    Sep  4 17:26:44.645: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 09/04/23 17:26:44.646
    STEP: delete collection of pod templates 09/04/23 17:26:44.653
    Sep  4 17:26:44.653: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 09/04/23 17:26:44.686
    Sep  4 17:26:44.686: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:44.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6331" for this suite. 09/04/23 17:26:44.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:44.719
Sep  4 17:26:44.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 17:26:44.72
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:44.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:44.75
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 17:26:44.766
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:26:45.346
STEP: Deploying the webhook pod 09/04/23 17:26:45.363
STEP: Wait for the deployment to be ready 09/04/23 17:26:45.39
Sep  4 17:26:45.406: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 17:26:47.416
STEP: Verifying the service has paired with the endpoint 09/04/23 17:26:47.44
Sep  4 17:26:48.441: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 09/04/23 17:26:48.55
STEP: Creating a configMap that should be mutated 09/04/23 17:26:48.574
STEP: Deleting the collection of validation webhooks 09/04/23 17:26:48.632
STEP: Creating a configMap that should not be mutated 09/04/23 17:26:48.721
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:48.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9780" for this suite. 09/04/23 17:26:48.813
STEP: Destroying namespace "webhook-9780-markers" for this suite. 09/04/23 17:26:48.831
------------------------------
â€¢ [4.126 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:44.719
    Sep  4 17:26:44.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 17:26:44.72
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:44.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:44.75
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 17:26:44.766
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:26:45.346
    STEP: Deploying the webhook pod 09/04/23 17:26:45.363
    STEP: Wait for the deployment to be ready 09/04/23 17:26:45.39
    Sep  4 17:26:45.406: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 17:26:47.416
    STEP: Verifying the service has paired with the endpoint 09/04/23 17:26:47.44
    Sep  4 17:26:48.441: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 09/04/23 17:26:48.55
    STEP: Creating a configMap that should be mutated 09/04/23 17:26:48.574
    STEP: Deleting the collection of validation webhooks 09/04/23 17:26:48.632
    STEP: Creating a configMap that should not be mutated 09/04/23 17:26:48.721
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:48.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9780" for this suite. 09/04/23 17:26:48.813
    STEP: Destroying namespace "webhook-9780-markers" for this suite. 09/04/23 17:26:48.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:48.856
Sep  4 17:26:48.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 17:26:48.857
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:48.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:48.881
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-48b2c2f9-0703-4aad-a672-8927e6fd3c5e 09/04/23 17:26:48.884
STEP: Creating a pod to test consume secrets 09/04/23 17:26:48.89
Sep  4 17:26:48.905: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3" in namespace "projected-2829" to be "Succeeded or Failed"
Sep  4 17:26:48.915: INFO: Pod "pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.115275ms
Sep  4 17:26:50.919: INFO: Pod "pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014603155s
Sep  4 17:26:52.920: INFO: Pod "pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014743464s
STEP: Saw pod success 09/04/23 17:26:52.92
Sep  4 17:26:52.920: INFO: Pod "pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3" satisfied condition "Succeeded or Failed"
Sep  4 17:26:52.926: INFO: Trying to get logs from node tenant-000003 pod pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/04/23 17:26:52.965
Sep  4 17:26:52.980: INFO: Waiting for pod pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3 to disappear
Sep  4 17:26:52.983: INFO: Pod pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:52.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2829" for this suite. 09/04/23 17:26:52.986
------------------------------
â€¢ [4.140 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:48.856
    Sep  4 17:26:48.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 17:26:48.857
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:48.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:48.881
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-48b2c2f9-0703-4aad-a672-8927e6fd3c5e 09/04/23 17:26:48.884
    STEP: Creating a pod to test consume secrets 09/04/23 17:26:48.89
    Sep  4 17:26:48.905: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3" in namespace "projected-2829" to be "Succeeded or Failed"
    Sep  4 17:26:48.915: INFO: Pod "pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.115275ms
    Sep  4 17:26:50.919: INFO: Pod "pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014603155s
    Sep  4 17:26:52.920: INFO: Pod "pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014743464s
    STEP: Saw pod success 09/04/23 17:26:52.92
    Sep  4 17:26:52.920: INFO: Pod "pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3" satisfied condition "Succeeded or Failed"
    Sep  4 17:26:52.926: INFO: Trying to get logs from node tenant-000003 pod pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 17:26:52.965
    Sep  4 17:26:52.980: INFO: Waiting for pod pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3 to disappear
    Sep  4 17:26:52.983: INFO: Pod pod-projected-secrets-90ce09ec-5262-48ca-abbb-3ede9804efe3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:52.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2829" for this suite. 09/04/23 17:26:52.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:53
Sep  4 17:26:53.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename runtimeclass 09/04/23 17:26:53.002
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:53.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:53.027
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 09/04/23 17:26:53.03
STEP: getting /apis/node.k8s.io 09/04/23 17:26:53.032
STEP: getting /apis/node.k8s.io/v1 09/04/23 17:26:53.033
STEP: creating 09/04/23 17:26:53.034
STEP: watching 09/04/23 17:26:53.055
Sep  4 17:26:53.055: INFO: starting watch
STEP: getting 09/04/23 17:26:53.063
STEP: listing 09/04/23 17:26:53.066
STEP: patching 09/04/23 17:26:53.071
STEP: updating 09/04/23 17:26:53.077
Sep  4 17:26:53.086: INFO: waiting for watch events with expected annotations
STEP: deleting 09/04/23 17:26:53.086
STEP: deleting a collection 09/04/23 17:26:53.104
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:53.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6467" for this suite. 09/04/23 17:26:53.131
------------------------------
â€¢ [0.140 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:53
    Sep  4 17:26:53.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename runtimeclass 09/04/23 17:26:53.002
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:53.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:53.027
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 09/04/23 17:26:53.03
    STEP: getting /apis/node.k8s.io 09/04/23 17:26:53.032
    STEP: getting /apis/node.k8s.io/v1 09/04/23 17:26:53.033
    STEP: creating 09/04/23 17:26:53.034
    STEP: watching 09/04/23 17:26:53.055
    Sep  4 17:26:53.055: INFO: starting watch
    STEP: getting 09/04/23 17:26:53.063
    STEP: listing 09/04/23 17:26:53.066
    STEP: patching 09/04/23 17:26:53.071
    STEP: updating 09/04/23 17:26:53.077
    Sep  4 17:26:53.086: INFO: waiting for watch events with expected annotations
    STEP: deleting 09/04/23 17:26:53.086
    STEP: deleting a collection 09/04/23 17:26:53.104
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:53.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6467" for this suite. 09/04/23 17:26:53.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:53.148
Sep  4 17:26:53.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 17:26:53.149
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:53.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:53.172
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 09/04/23 17:26:53.175
Sep  4 17:26:53.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-5365 api-versions'
Sep  4 17:26:53.272: INFO: stderr: ""
Sep  4 17:26:53.272: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:53.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5365" for this suite. 09/04/23 17:26:53.278
------------------------------
â€¢ [0.140 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:53.148
    Sep  4 17:26:53.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 17:26:53.149
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:53.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:53.172
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 09/04/23 17:26:53.175
    Sep  4 17:26:53.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-5365 api-versions'
    Sep  4 17:26:53.272: INFO: stderr: ""
    Sep  4 17:26:53.272: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:53.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5365" for this suite. 09/04/23 17:26:53.278
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:53.287
Sep  4 17:26:53.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename sysctl 09/04/23 17:26:53.288
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:53.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:53.32
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/04/23 17:26:53.323
STEP: Watching for error events or started pod 09/04/23 17:26:53.333
STEP: Waiting for pod completion 09/04/23 17:26:55.339
Sep  4 17:26:55.339: INFO: Waiting up to 3m0s for pod "sysctl-d45737a4-2ac8-4575-9656-25a4ee918517" in namespace "sysctl-3971" to be "completed"
Sep  4 17:26:55.343: INFO: Pod "sysctl-d45737a4-2ac8-4575-9656-25a4ee918517": Phase="Pending", Reason="", readiness=false. Elapsed: 3.164517ms
Sep  4 17:26:57.348: INFO: Pod "sysctl-d45737a4-2ac8-4575-9656-25a4ee918517": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007963096s
Sep  4 17:26:57.348: INFO: Pod "sysctl-d45737a4-2ac8-4575-9656-25a4ee918517" satisfied condition "completed"
STEP: Checking that the pod succeeded 09/04/23 17:26:57.356
STEP: Getting logs from the pod 09/04/23 17:26:57.356
STEP: Checking that the sysctl is actually updated 09/04/23 17:26:57.364
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:57.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-3971" for this suite. 09/04/23 17:26:57.37
------------------------------
â€¢ [4.100 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:53.287
    Sep  4 17:26:53.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename sysctl 09/04/23 17:26:53.288
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:53.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:53.32
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/04/23 17:26:53.323
    STEP: Watching for error events or started pod 09/04/23 17:26:53.333
    STEP: Waiting for pod completion 09/04/23 17:26:55.339
    Sep  4 17:26:55.339: INFO: Waiting up to 3m0s for pod "sysctl-d45737a4-2ac8-4575-9656-25a4ee918517" in namespace "sysctl-3971" to be "completed"
    Sep  4 17:26:55.343: INFO: Pod "sysctl-d45737a4-2ac8-4575-9656-25a4ee918517": Phase="Pending", Reason="", readiness=false. Elapsed: 3.164517ms
    Sep  4 17:26:57.348: INFO: Pod "sysctl-d45737a4-2ac8-4575-9656-25a4ee918517": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007963096s
    Sep  4 17:26:57.348: INFO: Pod "sysctl-d45737a4-2ac8-4575-9656-25a4ee918517" satisfied condition "completed"
    STEP: Checking that the pod succeeded 09/04/23 17:26:57.356
    STEP: Getting logs from the pod 09/04/23 17:26:57.356
    STEP: Checking that the sysctl is actually updated 09/04/23 17:26:57.364
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:57.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-3971" for this suite. 09/04/23 17:26:57.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:57.395
Sep  4 17:26:57.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubelet-test 09/04/23 17:26:57.396
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:57.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:57.417
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Sep  4 17:26:57.429: INFO: Waiting up to 5m0s for pod "busybox-scheduling-544ccaa8-6675-4d7a-bffb-261bbc1fb45a" in namespace "kubelet-test-4422" to be "running and ready"
Sep  4 17:26:57.433: INFO: Pod "busybox-scheduling-544ccaa8-6675-4d7a-bffb-261bbc1fb45a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.076912ms
Sep  4 17:26:57.433: INFO: The phase of Pod busybox-scheduling-544ccaa8-6675-4d7a-bffb-261bbc1fb45a is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:26:59.438: INFO: Pod "busybox-scheduling-544ccaa8-6675-4d7a-bffb-261bbc1fb45a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008394826s
Sep  4 17:26:59.438: INFO: The phase of Pod busybox-scheduling-544ccaa8-6675-4d7a-bffb-261bbc1fb45a is Running (Ready = true)
Sep  4 17:26:59.438: INFO: Pod "busybox-scheduling-544ccaa8-6675-4d7a-bffb-261bbc1fb45a" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  4 17:26:59.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4422" for this suite. 09/04/23 17:26:59.462
------------------------------
â€¢ [2.076 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:57.395
    Sep  4 17:26:57.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubelet-test 09/04/23 17:26:57.396
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:57.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:57.417
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Sep  4 17:26:57.429: INFO: Waiting up to 5m0s for pod "busybox-scheduling-544ccaa8-6675-4d7a-bffb-261bbc1fb45a" in namespace "kubelet-test-4422" to be "running and ready"
    Sep  4 17:26:57.433: INFO: Pod "busybox-scheduling-544ccaa8-6675-4d7a-bffb-261bbc1fb45a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.076912ms
    Sep  4 17:26:57.433: INFO: The phase of Pod busybox-scheduling-544ccaa8-6675-4d7a-bffb-261bbc1fb45a is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:26:59.438: INFO: Pod "busybox-scheduling-544ccaa8-6675-4d7a-bffb-261bbc1fb45a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008394826s
    Sep  4 17:26:59.438: INFO: The phase of Pod busybox-scheduling-544ccaa8-6675-4d7a-bffb-261bbc1fb45a is Running (Ready = true)
    Sep  4 17:26:59.438: INFO: Pod "busybox-scheduling-544ccaa8-6675-4d7a-bffb-261bbc1fb45a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:26:59.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4422" for this suite. 09/04/23 17:26:59.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:26:59.475
Sep  4 17:26:59.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 17:26:59.477
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:59.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:59.499
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 17:26:59.519
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:27:00.107
STEP: Deploying the webhook pod 09/04/23 17:27:00.122
STEP: Wait for the deployment to be ready 09/04/23 17:27:00.142
Sep  4 17:27:00.168: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 17:27:02.181
STEP: Verifying the service has paired with the endpoint 09/04/23 17:27:02.202
Sep  4 17:27:03.202: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 09/04/23 17:27:03.209
STEP: Creating a custom resource definition that should be denied by the webhook 09/04/23 17:27:03.232
Sep  4 17:27:03.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:27:03.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9486" for this suite. 09/04/23 17:27:03.345
STEP: Destroying namespace "webhook-9486-markers" for this suite. 09/04/23 17:27:03.357
------------------------------
â€¢ [3.910 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:26:59.475
    Sep  4 17:26:59.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 17:26:59.477
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:26:59.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:26:59.499
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 17:26:59.519
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:27:00.107
    STEP: Deploying the webhook pod 09/04/23 17:27:00.122
    STEP: Wait for the deployment to be ready 09/04/23 17:27:00.142
    Sep  4 17:27:00.168: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 17:27:02.181
    STEP: Verifying the service has paired with the endpoint 09/04/23 17:27:02.202
    Sep  4 17:27:03.202: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 09/04/23 17:27:03.209
    STEP: Creating a custom resource definition that should be denied by the webhook 09/04/23 17:27:03.232
    Sep  4 17:27:03.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:27:03.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9486" for this suite. 09/04/23 17:27:03.345
    STEP: Destroying namespace "webhook-9486-markers" for this suite. 09/04/23 17:27:03.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:27:03.386
Sep  4 17:27:03.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename cronjob 09/04/23 17:27:03.387
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:27:03.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:27:03.437
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 09/04/23 17:27:03.44
STEP: Ensuring more than one job is running at a time 09/04/23 17:27:03.452
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/04/23 17:29:01.466
STEP: Removing cronjob 09/04/23 17:29:01.48
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  4 17:29:01.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6332" for this suite. 09/04/23 17:29:01.495
------------------------------
â€¢ [SLOW TEST] [118.123 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:27:03.386
    Sep  4 17:27:03.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename cronjob 09/04/23 17:27:03.387
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:27:03.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:27:03.437
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 09/04/23 17:27:03.44
    STEP: Ensuring more than one job is running at a time 09/04/23 17:27:03.452
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/04/23 17:29:01.466
    STEP: Removing cronjob 09/04/23 17:29:01.48
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:29:01.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6332" for this suite. 09/04/23 17:29:01.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:29:01.514
Sep  4 17:29:01.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 17:29:01.516
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:29:01.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:29:01.589
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Sep  4 17:29:01.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/04/23 17:29:03.522
Sep  4 17:29:03.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 create -f -'
Sep  4 17:29:04.188: INFO: stderr: ""
Sep  4 17:29:04.188: INFO: stdout: "e2e-test-crd-publish-openapi-6079-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  4 17:29:04.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 delete e2e-test-crd-publish-openapi-6079-crds test-foo'
Sep  4 17:29:04.303: INFO: stderr: ""
Sep  4 17:29:04.303: INFO: stdout: "e2e-test-crd-publish-openapi-6079-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep  4 17:29:04.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 apply -f -'
Sep  4 17:29:04.911: INFO: stderr: ""
Sep  4 17:29:04.911: INFO: stdout: "e2e-test-crd-publish-openapi-6079-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  4 17:29:04.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 delete e2e-test-crd-publish-openapi-6079-crds test-foo'
Sep  4 17:29:05.011: INFO: stderr: ""
Sep  4 17:29:05.011: INFO: stdout: "e2e-test-crd-publish-openapi-6079-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/04/23 17:29:05.011
Sep  4 17:29:05.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 create -f -'
Sep  4 17:29:05.242: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/04/23 17:29:05.242
Sep  4 17:29:05.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 create -f -'
Sep  4 17:29:05.474: INFO: rc: 1
Sep  4 17:29:05.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 apply -f -'
Sep  4 17:29:05.701: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/04/23 17:29:05.701
Sep  4 17:29:05.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 create -f -'
Sep  4 17:29:06.243: INFO: rc: 1
Sep  4 17:29:06.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 apply -f -'
Sep  4 17:29:06.486: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 09/04/23 17:29:06.486
Sep  4 17:29:06.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 explain e2e-test-crd-publish-openapi-6079-crds'
Sep  4 17:29:06.748: INFO: stderr: ""
Sep  4 17:29:06.748: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6079-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 09/04/23 17:29:06.748
Sep  4 17:29:06.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 explain e2e-test-crd-publish-openapi-6079-crds.metadata'
Sep  4 17:29:06.981: INFO: stderr: ""
Sep  4 17:29:06.981: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6079-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep  4 17:29:06.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 explain e2e-test-crd-publish-openapi-6079-crds.spec'
Sep  4 17:29:07.212: INFO: stderr: ""
Sep  4 17:29:07.212: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6079-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep  4 17:29:07.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 explain e2e-test-crd-publish-openapi-6079-crds.spec.bars'
Sep  4 17:29:07.471: INFO: stderr: ""
Sep  4 17:29:07.471: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6079-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/04/23 17:29:07.471
Sep  4 17:29:07.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 explain e2e-test-crd-publish-openapi-6079-crds.spec.bars2'
Sep  4 17:29:07.710: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:29:09.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1598" for this suite. 09/04/23 17:29:09.666
------------------------------
â€¢ [SLOW TEST] [8.164 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:29:01.514
    Sep  4 17:29:01.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 17:29:01.516
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:29:01.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:29:01.589
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Sep  4 17:29:01.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/04/23 17:29:03.522
    Sep  4 17:29:03.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 create -f -'
    Sep  4 17:29:04.188: INFO: stderr: ""
    Sep  4 17:29:04.188: INFO: stdout: "e2e-test-crd-publish-openapi-6079-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep  4 17:29:04.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 delete e2e-test-crd-publish-openapi-6079-crds test-foo'
    Sep  4 17:29:04.303: INFO: stderr: ""
    Sep  4 17:29:04.303: INFO: stdout: "e2e-test-crd-publish-openapi-6079-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Sep  4 17:29:04.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 apply -f -'
    Sep  4 17:29:04.911: INFO: stderr: ""
    Sep  4 17:29:04.911: INFO: stdout: "e2e-test-crd-publish-openapi-6079-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep  4 17:29:04.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 delete e2e-test-crd-publish-openapi-6079-crds test-foo'
    Sep  4 17:29:05.011: INFO: stderr: ""
    Sep  4 17:29:05.011: INFO: stdout: "e2e-test-crd-publish-openapi-6079-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/04/23 17:29:05.011
    Sep  4 17:29:05.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 create -f -'
    Sep  4 17:29:05.242: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/04/23 17:29:05.242
    Sep  4 17:29:05.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 create -f -'
    Sep  4 17:29:05.474: INFO: rc: 1
    Sep  4 17:29:05.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 apply -f -'
    Sep  4 17:29:05.701: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/04/23 17:29:05.701
    Sep  4 17:29:05.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 create -f -'
    Sep  4 17:29:06.243: INFO: rc: 1
    Sep  4 17:29:06.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 --namespace=crd-publish-openapi-1598 apply -f -'
    Sep  4 17:29:06.486: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 09/04/23 17:29:06.486
    Sep  4 17:29:06.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 explain e2e-test-crd-publish-openapi-6079-crds'
    Sep  4 17:29:06.748: INFO: stderr: ""
    Sep  4 17:29:06.748: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6079-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 09/04/23 17:29:06.748
    Sep  4 17:29:06.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 explain e2e-test-crd-publish-openapi-6079-crds.metadata'
    Sep  4 17:29:06.981: INFO: stderr: ""
    Sep  4 17:29:06.981: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6079-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Sep  4 17:29:06.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 explain e2e-test-crd-publish-openapi-6079-crds.spec'
    Sep  4 17:29:07.212: INFO: stderr: ""
    Sep  4 17:29:07.212: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6079-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Sep  4 17:29:07.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 explain e2e-test-crd-publish-openapi-6079-crds.spec.bars'
    Sep  4 17:29:07.471: INFO: stderr: ""
    Sep  4 17:29:07.471: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6079-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/04/23 17:29:07.471
    Sep  4 17:29:07.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-1598 explain e2e-test-crd-publish-openapi-6079-crds.spec.bars2'
    Sep  4 17:29:07.710: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:29:09.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1598" for this suite. 09/04/23 17:29:09.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:29:09.684
Sep  4 17:29:09.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 17:29:09.686
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:29:09.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:29:09.72
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 09/04/23 17:29:09.728
Sep  4 17:29:09.754: INFO: Waiting up to 5m0s for pod "downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7" in namespace "downward-api-1140" to be "Succeeded or Failed"
Sep  4 17:29:09.763: INFO: Pod "downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.605546ms
Sep  4 17:29:11.772: INFO: Pod "downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01692726s
Sep  4 17:29:13.770: INFO: Pod "downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015665667s
STEP: Saw pod success 09/04/23 17:29:13.77
Sep  4 17:29:13.770: INFO: Pod "downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7" satisfied condition "Succeeded or Failed"
Sep  4 17:29:13.776: INFO: Trying to get logs from node tenant-000003 pod downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7 container dapi-container: <nil>
STEP: delete the pod 09/04/23 17:29:13.809
Sep  4 17:29:13.828: INFO: Waiting for pod downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7 to disappear
Sep  4 17:29:13.834: INFO: Pod downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  4 17:29:13.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1140" for this suite. 09/04/23 17:29:13.844
------------------------------
â€¢ [4.172 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:29:09.684
    Sep  4 17:29:09.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 17:29:09.686
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:29:09.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:29:09.72
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 09/04/23 17:29:09.728
    Sep  4 17:29:09.754: INFO: Waiting up to 5m0s for pod "downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7" in namespace "downward-api-1140" to be "Succeeded or Failed"
    Sep  4 17:29:09.763: INFO: Pod "downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.605546ms
    Sep  4 17:29:11.772: INFO: Pod "downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01692726s
    Sep  4 17:29:13.770: INFO: Pod "downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015665667s
    STEP: Saw pod success 09/04/23 17:29:13.77
    Sep  4 17:29:13.770: INFO: Pod "downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7" satisfied condition "Succeeded or Failed"
    Sep  4 17:29:13.776: INFO: Trying to get logs from node tenant-000003 pod downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7 container dapi-container: <nil>
    STEP: delete the pod 09/04/23 17:29:13.809
    Sep  4 17:29:13.828: INFO: Waiting for pod downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7 to disappear
    Sep  4 17:29:13.834: INFO: Pod downward-api-cf3add04-2220-479f-b06c-3bcc13185cb7 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:29:13.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1140" for this suite. 09/04/23 17:29:13.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:29:13.861
Sep  4 17:29:13.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename replication-controller 09/04/23 17:29:13.862
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:29:13.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:29:13.892
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 09/04/23 17:29:13.899
Sep  4 17:29:13.913: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2513" to be "running and ready"
Sep  4 17:29:13.921: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 8.267725ms
Sep  4 17:29:13.922: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:29:15.930: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.017318086s
Sep  4 17:29:15.930: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Sep  4 17:29:15.931: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 09/04/23 17:29:15.936
STEP: Then the orphan pod is adopted 09/04/23 17:29:15.946
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  4 17:29:16.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2513" for this suite. 09/04/23 17:29:16.97
------------------------------
â€¢ [3.122 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:29:13.861
    Sep  4 17:29:13.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename replication-controller 09/04/23 17:29:13.862
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:29:13.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:29:13.892
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 09/04/23 17:29:13.899
    Sep  4 17:29:13.913: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2513" to be "running and ready"
    Sep  4 17:29:13.921: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 8.267725ms
    Sep  4 17:29:13.922: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:29:15.930: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.017318086s
    Sep  4 17:29:15.930: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Sep  4 17:29:15.931: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 09/04/23 17:29:15.936
    STEP: Then the orphan pod is adopted 09/04/23 17:29:15.946
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:29:16.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2513" for this suite. 09/04/23 17:29:16.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:29:16.988
Sep  4 17:29:16.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename security-context 09/04/23 17:29:16.989
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:29:17.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:29:17.018
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/04/23 17:29:17.025
Sep  4 17:29:17.038: INFO: Waiting up to 5m0s for pod "security-context-b04bb794-faad-45ac-a3cf-34514b82375e" in namespace "security-context-4378" to be "Succeeded or Failed"
Sep  4 17:29:17.044: INFO: Pod "security-context-b04bb794-faad-45ac-a3cf-34514b82375e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.59583ms
Sep  4 17:29:19.053: INFO: Pod "security-context-b04bb794-faad-45ac-a3cf-34514b82375e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014747573s
Sep  4 17:29:21.051: INFO: Pod "security-context-b04bb794-faad-45ac-a3cf-34514b82375e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012147424s
STEP: Saw pod success 09/04/23 17:29:21.051
Sep  4 17:29:21.051: INFO: Pod "security-context-b04bb794-faad-45ac-a3cf-34514b82375e" satisfied condition "Succeeded or Failed"
Sep  4 17:29:21.057: INFO: Trying to get logs from node tenant-000001 pod security-context-b04bb794-faad-45ac-a3cf-34514b82375e container test-container: <nil>
STEP: delete the pod 09/04/23 17:29:21.104
Sep  4 17:29:21.136: INFO: Waiting for pod security-context-b04bb794-faad-45ac-a3cf-34514b82375e to disappear
Sep  4 17:29:21.142: INFO: Pod security-context-b04bb794-faad-45ac-a3cf-34514b82375e no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  4 17:29:21.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4378" for this suite. 09/04/23 17:29:21.149
------------------------------
â€¢ [4.171 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:29:16.988
    Sep  4 17:29:16.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename security-context 09/04/23 17:29:16.989
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:29:17.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:29:17.018
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/04/23 17:29:17.025
    Sep  4 17:29:17.038: INFO: Waiting up to 5m0s for pod "security-context-b04bb794-faad-45ac-a3cf-34514b82375e" in namespace "security-context-4378" to be "Succeeded or Failed"
    Sep  4 17:29:17.044: INFO: Pod "security-context-b04bb794-faad-45ac-a3cf-34514b82375e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.59583ms
    Sep  4 17:29:19.053: INFO: Pod "security-context-b04bb794-faad-45ac-a3cf-34514b82375e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014747573s
    Sep  4 17:29:21.051: INFO: Pod "security-context-b04bb794-faad-45ac-a3cf-34514b82375e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012147424s
    STEP: Saw pod success 09/04/23 17:29:21.051
    Sep  4 17:29:21.051: INFO: Pod "security-context-b04bb794-faad-45ac-a3cf-34514b82375e" satisfied condition "Succeeded or Failed"
    Sep  4 17:29:21.057: INFO: Trying to get logs from node tenant-000001 pod security-context-b04bb794-faad-45ac-a3cf-34514b82375e container test-container: <nil>
    STEP: delete the pod 09/04/23 17:29:21.104
    Sep  4 17:29:21.136: INFO: Waiting for pod security-context-b04bb794-faad-45ac-a3cf-34514b82375e to disappear
    Sep  4 17:29:21.142: INFO: Pod security-context-b04bb794-faad-45ac-a3cf-34514b82375e no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:29:21.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4378" for this suite. 09/04/23 17:29:21.149
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:29:21.161
Sep  4 17:29:21.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename statefulset 09/04/23 17:29:21.162
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:29:21.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:29:21.19
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8777 09/04/23 17:29:21.198
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-8777 09/04/23 17:29:21.208
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8777 09/04/23 17:29:21.222
Sep  4 17:29:21.229: INFO: Found 0 stateful pods, waiting for 1
Sep  4 17:29:31.238: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/04/23 17:29:31.239
Sep  4 17:29:31.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-8777 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 17:29:31.454: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 17:29:31.454: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 17:29:31.454: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 17:29:31.463: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  4 17:29:41.469: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 17:29:41.469: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 17:29:41.504: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Sep  4 17:29:41.504: INFO: ss-0  tenant-000001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:21 +0000 UTC  }]
Sep  4 17:29:41.504: INFO: ss-1                 Pending         []
Sep  4 17:29:41.504: INFO: 
Sep  4 17:29:41.504: INFO: StatefulSet ss has not reached scale 3, at 2
Sep  4 17:29:42.511: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986719471s
Sep  4 17:29:43.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979114628s
Sep  4 17:29:44.527: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9702609s
Sep  4 17:29:45.537: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.963274699s
Sep  4 17:29:46.546: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.954063047s
Sep  4 17:29:47.554: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.944357822s
Sep  4 17:29:48.561: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.937200751s
Sep  4 17:29:49.568: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.929756118s
Sep  4 17:29:50.575: INFO: Verifying statefulset ss doesn't scale past 3 for another 922.50753ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8777 09/04/23 17:29:51.576
Sep  4 17:29:51.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-8777 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 17:29:51.772: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  4 17:29:51.772: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 17:29:51.772: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  4 17:29:51.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-8777 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 17:29:51.974: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  4 17:29:51.974: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 17:29:51.974: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  4 17:29:51.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-8777 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 17:29:52.182: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  4 17:29:52.182: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 17:29:52.182: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  4 17:29:52.190: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Sep  4 17:30:02.201: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 17:30:02.201: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 17:30:02.202: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 09/04/23 17:30:02.202
Sep  4 17:30:02.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-8777 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 17:30:02.384: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 17:30:02.384: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 17:30:02.384: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 17:30:02.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-8777 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 17:30:02.557: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 17:30:02.557: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 17:30:02.557: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 17:30:02.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-8777 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 17:30:02.913: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 17:30:02.913: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 17:30:02.913: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 17:30:02.913: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 17:30:02.919: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Sep  4 17:30:12.966: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 17:30:12.966: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 17:30:12.966: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 17:30:13.129: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Sep  4 17:30:13.141: INFO: ss-0  tenant-000001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:21 +0000 UTC  }]
Sep  4 17:30:13.141: INFO: ss-1  tenant-000003  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:41 +0000 UTC  }]
Sep  4 17:30:13.146: INFO: ss-2  tenant-000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:41 +0000 UTC  }]
Sep  4 17:30:13.148: INFO: 
Sep  4 17:30:13.149: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  4 17:30:14.155: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Sep  4 17:30:14.155: INFO: ss-0  tenant-000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:21 +0000 UTC  }]
Sep  4 17:30:14.155: INFO: ss-2  tenant-000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:41 +0000 UTC  }]
Sep  4 17:30:14.155: INFO: 
Sep  4 17:30:14.155: INFO: StatefulSet ss has not reached scale 0, at 2
Sep  4 17:30:15.161: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.881965564s
Sep  4 17:30:16.168: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.875705017s
Sep  4 17:30:17.178: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.868508823s
Sep  4 17:30:18.184: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.85943937s
Sep  4 17:30:19.191: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.853222675s
Sep  4 17:30:20.197: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.846223832s
Sep  4 17:30:21.204: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.839680792s
Sep  4 17:30:22.213: INFO: Verifying statefulset ss doesn't scale past 0 for another 833.044536ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8777 09/04/23 17:30:23.213
Sep  4 17:30:23.220: INFO: Scaling statefulset ss to 0
Sep  4 17:30:23.237: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 17:30:23.245: INFO: Deleting all statefulset in ns statefulset-8777
Sep  4 17:30:23.249: INFO: Scaling statefulset ss to 0
Sep  4 17:30:23.273: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 17:30:23.278: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 17:30:23.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8777" for this suite. 09/04/23 17:30:23.312
------------------------------
â€¢ [SLOW TEST] [62.164 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:29:21.161
    Sep  4 17:29:21.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename statefulset 09/04/23 17:29:21.162
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:29:21.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:29:21.19
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8777 09/04/23 17:29:21.198
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-8777 09/04/23 17:29:21.208
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8777 09/04/23 17:29:21.222
    Sep  4 17:29:21.229: INFO: Found 0 stateful pods, waiting for 1
    Sep  4 17:29:31.238: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/04/23 17:29:31.239
    Sep  4 17:29:31.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-8777 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 17:29:31.454: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 17:29:31.454: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 17:29:31.454: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 17:29:31.463: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep  4 17:29:41.469: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 17:29:41.469: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 17:29:41.504: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Sep  4 17:29:41.504: INFO: ss-0  tenant-000001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:21 +0000 UTC  }]
    Sep  4 17:29:41.504: INFO: ss-1                 Pending         []
    Sep  4 17:29:41.504: INFO: 
    Sep  4 17:29:41.504: INFO: StatefulSet ss has not reached scale 3, at 2
    Sep  4 17:29:42.511: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986719471s
    Sep  4 17:29:43.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979114628s
    Sep  4 17:29:44.527: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9702609s
    Sep  4 17:29:45.537: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.963274699s
    Sep  4 17:29:46.546: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.954063047s
    Sep  4 17:29:47.554: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.944357822s
    Sep  4 17:29:48.561: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.937200751s
    Sep  4 17:29:49.568: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.929756118s
    Sep  4 17:29:50.575: INFO: Verifying statefulset ss doesn't scale past 3 for another 922.50753ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8777 09/04/23 17:29:51.576
    Sep  4 17:29:51.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-8777 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 17:29:51.772: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  4 17:29:51.772: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 17:29:51.772: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  4 17:29:51.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-8777 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 17:29:51.974: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep  4 17:29:51.974: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 17:29:51.974: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  4 17:29:51.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-8777 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 17:29:52.182: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep  4 17:29:52.182: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 17:29:52.182: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  4 17:29:52.190: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Sep  4 17:30:02.201: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 17:30:02.201: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 17:30:02.202: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 09/04/23 17:30:02.202
    Sep  4 17:30:02.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-8777 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 17:30:02.384: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 17:30:02.384: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 17:30:02.384: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 17:30:02.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-8777 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 17:30:02.557: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 17:30:02.557: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 17:30:02.557: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 17:30:02.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-8777 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 17:30:02.913: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 17:30:02.913: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 17:30:02.913: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 17:30:02.913: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 17:30:02.919: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Sep  4 17:30:12.966: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 17:30:12.966: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 17:30:12.966: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 17:30:13.129: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Sep  4 17:30:13.141: INFO: ss-0  tenant-000001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:21 +0000 UTC  }]
    Sep  4 17:30:13.141: INFO: ss-1  tenant-000003  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:41 +0000 UTC  }]
    Sep  4 17:30:13.146: INFO: ss-2  tenant-000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:41 +0000 UTC  }]
    Sep  4 17:30:13.148: INFO: 
    Sep  4 17:30:13.149: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep  4 17:30:14.155: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Sep  4 17:30:14.155: INFO: ss-0  tenant-000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:21 +0000 UTC  }]
    Sep  4 17:30:14.155: INFO: ss-2  tenant-000001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:29:41 +0000 UTC  }]
    Sep  4 17:30:14.155: INFO: 
    Sep  4 17:30:14.155: INFO: StatefulSet ss has not reached scale 0, at 2
    Sep  4 17:30:15.161: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.881965564s
    Sep  4 17:30:16.168: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.875705017s
    Sep  4 17:30:17.178: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.868508823s
    Sep  4 17:30:18.184: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.85943937s
    Sep  4 17:30:19.191: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.853222675s
    Sep  4 17:30:20.197: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.846223832s
    Sep  4 17:30:21.204: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.839680792s
    Sep  4 17:30:22.213: INFO: Verifying statefulset ss doesn't scale past 0 for another 833.044536ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8777 09/04/23 17:30:23.213
    Sep  4 17:30:23.220: INFO: Scaling statefulset ss to 0
    Sep  4 17:30:23.237: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 17:30:23.245: INFO: Deleting all statefulset in ns statefulset-8777
    Sep  4 17:30:23.249: INFO: Scaling statefulset ss to 0
    Sep  4 17:30:23.273: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 17:30:23.278: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:30:23.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8777" for this suite. 09/04/23 17:30:23.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:30:23.333
Sep  4 17:30:23.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename svcaccounts 09/04/23 17:30:23.334
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:30:23.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:30:23.372
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Sep  4 17:30:23.418: INFO: created pod pod-service-account-defaultsa
Sep  4 17:30:23.418: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  4 17:30:23.435: INFO: created pod pod-service-account-mountsa
Sep  4 17:30:23.435: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  4 17:30:23.452: INFO: created pod pod-service-account-nomountsa
Sep  4 17:30:23.452: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  4 17:30:23.464: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  4 17:30:23.464: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  4 17:30:23.473: INFO: created pod pod-service-account-mountsa-mountspec
Sep  4 17:30:23.473: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  4 17:30:23.506: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  4 17:30:23.506: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  4 17:30:23.537: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  4 17:30:23.537: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  4 17:30:23.547: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  4 17:30:23.547: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  4 17:30:23.563: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  4 17:30:23.563: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  4 17:30:23.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4828" for this suite. 09/04/23 17:30:23.576
------------------------------
â€¢ [0.253 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:30:23.333
    Sep  4 17:30:23.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename svcaccounts 09/04/23 17:30:23.334
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:30:23.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:30:23.372
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Sep  4 17:30:23.418: INFO: created pod pod-service-account-defaultsa
    Sep  4 17:30:23.418: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Sep  4 17:30:23.435: INFO: created pod pod-service-account-mountsa
    Sep  4 17:30:23.435: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Sep  4 17:30:23.452: INFO: created pod pod-service-account-nomountsa
    Sep  4 17:30:23.452: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Sep  4 17:30:23.464: INFO: created pod pod-service-account-defaultsa-mountspec
    Sep  4 17:30:23.464: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Sep  4 17:30:23.473: INFO: created pod pod-service-account-mountsa-mountspec
    Sep  4 17:30:23.473: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Sep  4 17:30:23.506: INFO: created pod pod-service-account-nomountsa-mountspec
    Sep  4 17:30:23.506: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Sep  4 17:30:23.537: INFO: created pod pod-service-account-defaultsa-nomountspec
    Sep  4 17:30:23.537: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Sep  4 17:30:23.547: INFO: created pod pod-service-account-mountsa-nomountspec
    Sep  4 17:30:23.547: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Sep  4 17:30:23.563: INFO: created pod pod-service-account-nomountsa-nomountspec
    Sep  4 17:30:23.563: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:30:23.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4828" for this suite. 09/04/23 17:30:23.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:30:23.6
Sep  4 17:30:23.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename namespaces 09/04/23 17:30:23.601
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:30:23.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:30:23.63
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 09/04/23 17:30:23.637
STEP: patching the Namespace 09/04/23 17:30:23.657
STEP: get the Namespace and ensuring it has the label 09/04/23 17:30:23.67
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:30:23.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7296" for this suite. 09/04/23 17:30:23.681
STEP: Destroying namespace "nspatchtest-08e27616-c87b-48a8-9916-290bb4aad1eb-5267" for this suite. 09/04/23 17:30:23.695
------------------------------
â€¢ [0.111 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:30:23.6
    Sep  4 17:30:23.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename namespaces 09/04/23 17:30:23.601
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:30:23.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:30:23.63
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 09/04/23 17:30:23.637
    STEP: patching the Namespace 09/04/23 17:30:23.657
    STEP: get the Namespace and ensuring it has the label 09/04/23 17:30:23.67
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:30:23.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7296" for this suite. 09/04/23 17:30:23.681
    STEP: Destroying namespace "nspatchtest-08e27616-c87b-48a8-9916-290bb4aad1eb-5267" for this suite. 09/04/23 17:30:23.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:30:23.719
Sep  4 17:30:23.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pods 09/04/23 17:30:23.722
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:30:23.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:30:23.754
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 09/04/23 17:30:23.768
STEP: watching for Pod to be ready 09/04/23 17:30:23.787
Sep  4 17:30:23.791: INFO: observed Pod pod-test in namespace pods-1230 in phase Pending with labels: map[test-pod-static:true] & conditions []
Sep  4 17:30:23.804: INFO: observed Pod pod-test in namespace pods-1230 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC  }]
Sep  4 17:30:25.569: INFO: observed Pod pod-test in namespace pods-1230 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC  }]
Sep  4 17:30:25.852: INFO: observed Pod pod-test in namespace pods-1230 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC  }]
Sep  4 17:30:27.449: INFO: Found Pod pod-test in namespace pods-1230 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 09/04/23 17:30:27.455
STEP: getting the Pod and ensuring that it's patched 09/04/23 17:30:27.479
STEP: replacing the Pod's status Ready condition to False 09/04/23 17:30:27.491
STEP: check the Pod again to ensure its Ready conditions are False 09/04/23 17:30:27.514
STEP: deleting the Pod via a Collection with a LabelSelector 09/04/23 17:30:27.514
STEP: watching for the Pod to be deleted 09/04/23 17:30:27.578
Sep  4 17:30:27.596: INFO: observed event type MODIFIED
Sep  4 17:30:28.696: INFO: observed event type MODIFIED
Sep  4 17:30:29.026: INFO: observed event type MODIFIED
Sep  4 17:30:30.243: INFO: observed event type MODIFIED
Sep  4 17:30:30.436: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 17:30:30.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1230" for this suite. 09/04/23 17:30:30.453
------------------------------
â€¢ [SLOW TEST] [6.745 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:30:23.719
    Sep  4 17:30:23.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pods 09/04/23 17:30:23.722
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:30:23.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:30:23.754
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 09/04/23 17:30:23.768
    STEP: watching for Pod to be ready 09/04/23 17:30:23.787
    Sep  4 17:30:23.791: INFO: observed Pod pod-test in namespace pods-1230 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Sep  4 17:30:23.804: INFO: observed Pod pod-test in namespace pods-1230 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC  }]
    Sep  4 17:30:25.569: INFO: observed Pod pod-test in namespace pods-1230 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC  }]
    Sep  4 17:30:25.852: INFO: observed Pod pod-test in namespace pods-1230 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC  }]
    Sep  4 17:30:27.449: INFO: Found Pod pod-test in namespace pods-1230 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:30:23 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 09/04/23 17:30:27.455
    STEP: getting the Pod and ensuring that it's patched 09/04/23 17:30:27.479
    STEP: replacing the Pod's status Ready condition to False 09/04/23 17:30:27.491
    STEP: check the Pod again to ensure its Ready conditions are False 09/04/23 17:30:27.514
    STEP: deleting the Pod via a Collection with a LabelSelector 09/04/23 17:30:27.514
    STEP: watching for the Pod to be deleted 09/04/23 17:30:27.578
    Sep  4 17:30:27.596: INFO: observed event type MODIFIED
    Sep  4 17:30:28.696: INFO: observed event type MODIFIED
    Sep  4 17:30:29.026: INFO: observed event type MODIFIED
    Sep  4 17:30:30.243: INFO: observed event type MODIFIED
    Sep  4 17:30:30.436: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:30:30.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1230" for this suite. 09/04/23 17:30:30.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:30:30.489
Sep  4 17:30:30.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 17:30:30.49
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:30:30.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:30:30.517
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 09/04/23 17:30:30.525
Sep  4 17:30:30.540: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667" in namespace "projected-2062" to be "Succeeded or Failed"
Sep  4 17:30:30.554: INFO: Pod "downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667": Phase="Pending", Reason="", readiness=false. Elapsed: 13.032895ms
Sep  4 17:30:32.567: INFO: Pod "downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026835771s
Sep  4 17:30:34.561: INFO: Pod "downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020091294s
STEP: Saw pod success 09/04/23 17:30:34.561
Sep  4 17:30:34.561: INFO: Pod "downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667" satisfied condition "Succeeded or Failed"
Sep  4 17:30:34.569: INFO: Trying to get logs from node tenant-000003 pod downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667 container client-container: <nil>
STEP: delete the pod 09/04/23 17:30:34.581
Sep  4 17:30:34.605: INFO: Waiting for pod downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667 to disappear
Sep  4 17:30:34.613: INFO: Pod downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 17:30:34.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2062" for this suite. 09/04/23 17:30:34.619
------------------------------
â€¢ [4.143 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:30:30.489
    Sep  4 17:30:30.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 17:30:30.49
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:30:30.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:30:30.517
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 09/04/23 17:30:30.525
    Sep  4 17:30:30.540: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667" in namespace "projected-2062" to be "Succeeded or Failed"
    Sep  4 17:30:30.554: INFO: Pod "downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667": Phase="Pending", Reason="", readiness=false. Elapsed: 13.032895ms
    Sep  4 17:30:32.567: INFO: Pod "downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026835771s
    Sep  4 17:30:34.561: INFO: Pod "downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020091294s
    STEP: Saw pod success 09/04/23 17:30:34.561
    Sep  4 17:30:34.561: INFO: Pod "downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667" satisfied condition "Succeeded or Failed"
    Sep  4 17:30:34.569: INFO: Trying to get logs from node tenant-000003 pod downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667 container client-container: <nil>
    STEP: delete the pod 09/04/23 17:30:34.581
    Sep  4 17:30:34.605: INFO: Waiting for pod downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667 to disappear
    Sep  4 17:30:34.613: INFO: Pod downwardapi-volume-16e8fbcf-2138-4b5e-b06e-261112436667 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:30:34.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2062" for this suite. 09/04/23 17:30:34.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:30:34.639
Sep  4 17:30:34.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-runtime 09/04/23 17:30:34.64
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:30:34.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:30:34.668
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 09/04/23 17:30:34.677
STEP: wait for the container to reach Failed 09/04/23 17:30:34.696
STEP: get the container status 09/04/23 17:30:38.736
STEP: the container should be terminated 09/04/23 17:30:38.744
STEP: the termination message should be set 09/04/23 17:30:38.744
Sep  4 17:30:38.744: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/04/23 17:30:38.744
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  4 17:30:38.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1591" for this suite. 09/04/23 17:30:38.773
------------------------------
â€¢ [4.145 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:30:34.639
    Sep  4 17:30:34.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-runtime 09/04/23 17:30:34.64
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:30:34.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:30:34.668
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 09/04/23 17:30:34.677
    STEP: wait for the container to reach Failed 09/04/23 17:30:34.696
    STEP: get the container status 09/04/23 17:30:38.736
    STEP: the container should be terminated 09/04/23 17:30:38.744
    STEP: the termination message should be set 09/04/23 17:30:38.744
    Sep  4 17:30:38.744: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/04/23 17:30:38.744
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:30:38.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1591" for this suite. 09/04/23 17:30:38.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:30:38.794
Sep  4 17:30:38.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename job 09/04/23 17:30:38.795
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:30:38.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:30:38.82
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 09/04/23 17:30:38.828
STEP: Ensuring job reaches completions 09/04/23 17:30:38.837
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  4 17:30:52.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7825" for this suite. 09/04/23 17:30:52.892
------------------------------
â€¢ [SLOW TEST] [14.120 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:30:38.794
    Sep  4 17:30:38.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename job 09/04/23 17:30:38.795
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:30:38.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:30:38.82
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 09/04/23 17:30:38.828
    STEP: Ensuring job reaches completions 09/04/23 17:30:38.837
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:30:52.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7825" for this suite. 09/04/23 17:30:52.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:30:52.931
Sep  4 17:30:52.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 17:30:52.932
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:30:52.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:30:52.963
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-7336 09/04/23 17:30:52.971
STEP: creating service affinity-nodeport-transition in namespace services-7336 09/04/23 17:30:52.971
STEP: creating replication controller affinity-nodeport-transition in namespace services-7336 09/04/23 17:30:53
I0904 17:30:53.013650      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7336, replica count: 3
I0904 17:30:56.065417      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 17:30:56.084: INFO: Creating new exec pod
Sep  4 17:30:56.094: INFO: Waiting up to 5m0s for pod "execpod-affinityvprkp" in namespace "services-7336" to be "running"
Sep  4 17:30:56.103: INFO: Pod "execpod-affinityvprkp": Phase="Pending", Reason="", readiness=false. Elapsed: 9.15954ms
Sep  4 17:30:58.110: INFO: Pod "execpod-affinityvprkp": Phase="Running", Reason="", readiness=true. Elapsed: 2.01628185s
Sep  4 17:30:58.110: INFO: Pod "execpod-affinityvprkp" satisfied condition "running"
Sep  4 17:30:59.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7336 exec execpod-affinityvprkp -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Sep  4 17:30:59.310: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep  4 17:30:59.311: INFO: stdout: ""
Sep  4 17:30:59.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7336 exec execpod-affinityvprkp -- /bin/sh -x -c nc -v -z -w 2 10.96.229.56 80'
Sep  4 17:30:59.458: INFO: stderr: "+ nc -v -z -w 2 10.96.229.56 80\nConnection to 10.96.229.56 80 port [tcp/http] succeeded!\n"
Sep  4 17:30:59.458: INFO: stdout: ""
Sep  4 17:30:59.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7336 exec execpod-affinityvprkp -- /bin/sh -x -c nc -v -z -w 2 10.225.0.5 31357'
Sep  4 17:30:59.624: INFO: stderr: "+ nc -v -z -w 2 10.225.0.5 31357\nConnection to 10.225.0.5 31357 port [tcp/*] succeeded!\n"
Sep  4 17:30:59.624: INFO: stdout: ""
Sep  4 17:30:59.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7336 exec execpod-affinityvprkp -- /bin/sh -x -c nc -v -z -w 2 10.225.0.7 31357'
Sep  4 17:30:59.798: INFO: stderr: "+ nc -v -z -w 2 10.225.0.7 31357\nConnection to 10.225.0.7 31357 port [tcp/*] succeeded!\n"
Sep  4 17:30:59.798: INFO: stdout: ""
Sep  4 17:30:59.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7336 exec execpod-affinityvprkp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.225.0.5:31357/ ; done'
Sep  4 17:31:00.208: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n"
Sep  4 17:31:00.208: INFO: stdout: "\naffinity-nodeport-transition-vc5vz\naffinity-nodeport-transition-stxhb\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-vc5vz\naffinity-nodeport-transition-stxhb\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-stxhb\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-vc5vz\naffinity-nodeport-transition-stxhb\naffinity-nodeport-transition-stxhb\naffinity-nodeport-transition-vc5vz"
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-vc5vz
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-stxhb
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-vc5vz
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-stxhb
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-stxhb
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-vc5vz
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-stxhb
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-stxhb
Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-vc5vz
Sep  4 17:31:00.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7336 exec execpod-affinityvprkp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.225.0.5:31357/ ; done'
Sep  4 17:31:00.613: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n"
Sep  4 17:31:00.613: INFO: stdout: "\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv"
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
Sep  4 17:31:00.613: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7336, will wait for the garbage collector to delete the pods 09/04/23 17:31:00.636
Sep  4 17:31:00.704: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.306127ms
Sep  4 17:31:00.805: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.356084ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 17:31:03.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7336" for this suite. 09/04/23 17:31:03.556
------------------------------
â€¢ [SLOW TEST] [10.638 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:30:52.931
    Sep  4 17:30:52.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 17:30:52.932
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:30:52.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:30:52.963
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-7336 09/04/23 17:30:52.971
    STEP: creating service affinity-nodeport-transition in namespace services-7336 09/04/23 17:30:52.971
    STEP: creating replication controller affinity-nodeport-transition in namespace services-7336 09/04/23 17:30:53
    I0904 17:30:53.013650      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7336, replica count: 3
    I0904 17:30:56.065417      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 17:30:56.084: INFO: Creating new exec pod
    Sep  4 17:30:56.094: INFO: Waiting up to 5m0s for pod "execpod-affinityvprkp" in namespace "services-7336" to be "running"
    Sep  4 17:30:56.103: INFO: Pod "execpod-affinityvprkp": Phase="Pending", Reason="", readiness=false. Elapsed: 9.15954ms
    Sep  4 17:30:58.110: INFO: Pod "execpod-affinityvprkp": Phase="Running", Reason="", readiness=true. Elapsed: 2.01628185s
    Sep  4 17:30:58.110: INFO: Pod "execpod-affinityvprkp" satisfied condition "running"
    Sep  4 17:30:59.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7336 exec execpod-affinityvprkp -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Sep  4 17:30:59.310: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Sep  4 17:30:59.311: INFO: stdout: ""
    Sep  4 17:30:59.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7336 exec execpod-affinityvprkp -- /bin/sh -x -c nc -v -z -w 2 10.96.229.56 80'
    Sep  4 17:30:59.458: INFO: stderr: "+ nc -v -z -w 2 10.96.229.56 80\nConnection to 10.96.229.56 80 port [tcp/http] succeeded!\n"
    Sep  4 17:30:59.458: INFO: stdout: ""
    Sep  4 17:30:59.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7336 exec execpod-affinityvprkp -- /bin/sh -x -c nc -v -z -w 2 10.225.0.5 31357'
    Sep  4 17:30:59.624: INFO: stderr: "+ nc -v -z -w 2 10.225.0.5 31357\nConnection to 10.225.0.5 31357 port [tcp/*] succeeded!\n"
    Sep  4 17:30:59.624: INFO: stdout: ""
    Sep  4 17:30:59.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7336 exec execpod-affinityvprkp -- /bin/sh -x -c nc -v -z -w 2 10.225.0.7 31357'
    Sep  4 17:30:59.798: INFO: stderr: "+ nc -v -z -w 2 10.225.0.7 31357\nConnection to 10.225.0.7 31357 port [tcp/*] succeeded!\n"
    Sep  4 17:30:59.798: INFO: stdout: ""
    Sep  4 17:30:59.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7336 exec execpod-affinityvprkp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.225.0.5:31357/ ; done'
    Sep  4 17:31:00.208: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n"
    Sep  4 17:31:00.208: INFO: stdout: "\naffinity-nodeport-transition-vc5vz\naffinity-nodeport-transition-stxhb\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-vc5vz\naffinity-nodeport-transition-stxhb\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-stxhb\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-vc5vz\naffinity-nodeport-transition-stxhb\naffinity-nodeport-transition-stxhb\naffinity-nodeport-transition-vc5vz"
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-vc5vz
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-stxhb
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-vc5vz
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-stxhb
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-stxhb
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-vc5vz
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-stxhb
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-stxhb
    Sep  4 17:31:00.208: INFO: Received response from host: affinity-nodeport-transition-vc5vz
    Sep  4 17:31:00.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7336 exec execpod-affinityvprkp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.225.0.5:31357/ ; done'
    Sep  4 17:31:00.613: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:31357/\n"
    Sep  4 17:31:00.613: INFO: stdout: "\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv\naffinity-nodeport-transition-tgltv"
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Received response from host: affinity-nodeport-transition-tgltv
    Sep  4 17:31:00.613: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7336, will wait for the garbage collector to delete the pods 09/04/23 17:31:00.636
    Sep  4 17:31:00.704: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.306127ms
    Sep  4 17:31:00.805: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.356084ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:31:03.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7336" for this suite. 09/04/23 17:31:03.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:31:03.575
Sep  4 17:31:03.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename certificates 09/04/23 17:31:03.577
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:03.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:03.606
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 09/04/23 17:31:04.299
STEP: getting /apis/certificates.k8s.io 09/04/23 17:31:04.307
STEP: getting /apis/certificates.k8s.io/v1 09/04/23 17:31:04.31
STEP: creating 09/04/23 17:31:04.314
STEP: getting 09/04/23 17:31:04.346
STEP: listing 09/04/23 17:31:04.353
STEP: watching 09/04/23 17:31:04.358
Sep  4 17:31:04.358: INFO: starting watch
STEP: patching 09/04/23 17:31:04.361
STEP: updating 09/04/23 17:31:04.37
Sep  4 17:31:04.382: INFO: waiting for watch events with expected annotations
Sep  4 17:31:04.383: INFO: saw patched and updated annotations
STEP: getting /approval 09/04/23 17:31:04.383
STEP: patching /approval 09/04/23 17:31:04.389
STEP: updating /approval 09/04/23 17:31:04.397
STEP: getting /status 09/04/23 17:31:04.411
STEP: patching /status 09/04/23 17:31:04.417
STEP: updating /status 09/04/23 17:31:04.428
STEP: deleting 09/04/23 17:31:04.444
STEP: deleting a collection 09/04/23 17:31:04.47
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:31:04.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-9835" for this suite. 09/04/23 17:31:04.503
------------------------------
â€¢ [0.943 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:31:03.575
    Sep  4 17:31:03.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename certificates 09/04/23 17:31:03.577
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:03.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:03.606
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 09/04/23 17:31:04.299
    STEP: getting /apis/certificates.k8s.io 09/04/23 17:31:04.307
    STEP: getting /apis/certificates.k8s.io/v1 09/04/23 17:31:04.31
    STEP: creating 09/04/23 17:31:04.314
    STEP: getting 09/04/23 17:31:04.346
    STEP: listing 09/04/23 17:31:04.353
    STEP: watching 09/04/23 17:31:04.358
    Sep  4 17:31:04.358: INFO: starting watch
    STEP: patching 09/04/23 17:31:04.361
    STEP: updating 09/04/23 17:31:04.37
    Sep  4 17:31:04.382: INFO: waiting for watch events with expected annotations
    Sep  4 17:31:04.383: INFO: saw patched and updated annotations
    STEP: getting /approval 09/04/23 17:31:04.383
    STEP: patching /approval 09/04/23 17:31:04.389
    STEP: updating /approval 09/04/23 17:31:04.397
    STEP: getting /status 09/04/23 17:31:04.411
    STEP: patching /status 09/04/23 17:31:04.417
    STEP: updating /status 09/04/23 17:31:04.428
    STEP: deleting 09/04/23 17:31:04.444
    STEP: deleting a collection 09/04/23 17:31:04.47
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:31:04.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-9835" for this suite. 09/04/23 17:31:04.503
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:31:04.519
Sep  4 17:31:04.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename resourcequota 09/04/23 17:31:04.521
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:04.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:04.548
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 09/04/23 17:31:04.561
STEP: Ensuring ResourceQuota status is calculated 09/04/23 17:31:04.571
STEP: Creating a ResourceQuota with not terminating scope 09/04/23 17:31:06.578
STEP: Ensuring ResourceQuota status is calculated 09/04/23 17:31:06.589
STEP: Creating a long running pod 09/04/23 17:31:08.596
STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/04/23 17:31:08.627
STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/04/23 17:31:10.634
STEP: Deleting the pod 09/04/23 17:31:12.641
STEP: Ensuring resource quota status released the pod usage 09/04/23 17:31:12.676
STEP: Creating a terminating pod 09/04/23 17:31:14.684
STEP: Ensuring resource quota with terminating scope captures the pod usage 09/04/23 17:31:14.702
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/04/23 17:31:16.712
STEP: Deleting the pod 09/04/23 17:31:18.722
STEP: Ensuring resource quota status released the pod usage 09/04/23 17:31:18.754
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 17:31:20.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9642" for this suite. 09/04/23 17:31:20.767
------------------------------
â€¢ [SLOW TEST] [16.257 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:31:04.519
    Sep  4 17:31:04.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename resourcequota 09/04/23 17:31:04.521
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:04.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:04.548
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 09/04/23 17:31:04.561
    STEP: Ensuring ResourceQuota status is calculated 09/04/23 17:31:04.571
    STEP: Creating a ResourceQuota with not terminating scope 09/04/23 17:31:06.578
    STEP: Ensuring ResourceQuota status is calculated 09/04/23 17:31:06.589
    STEP: Creating a long running pod 09/04/23 17:31:08.596
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/04/23 17:31:08.627
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/04/23 17:31:10.634
    STEP: Deleting the pod 09/04/23 17:31:12.641
    STEP: Ensuring resource quota status released the pod usage 09/04/23 17:31:12.676
    STEP: Creating a terminating pod 09/04/23 17:31:14.684
    STEP: Ensuring resource quota with terminating scope captures the pod usage 09/04/23 17:31:14.702
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/04/23 17:31:16.712
    STEP: Deleting the pod 09/04/23 17:31:18.722
    STEP: Ensuring resource quota status released the pod usage 09/04/23 17:31:18.754
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:31:20.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9642" for this suite. 09/04/23 17:31:20.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:31:20.783
Sep  4 17:31:20.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 17:31:20.785
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:20.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:20.817
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-937e2e4b-6ad8-4ca8-92f4-a00d74a9f3bb 09/04/23 17:31:20.825
STEP: Creating a pod to test consume secrets 09/04/23 17:31:20.832
Sep  4 17:31:20.848: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8" in namespace "projected-1219" to be "Succeeded or Failed"
Sep  4 17:31:20.858: INFO: Pod "pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.147723ms
Sep  4 17:31:22.886: INFO: Pod "pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037838017s
Sep  4 17:31:24.867: INFO: Pod "pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018391156s
STEP: Saw pod success 09/04/23 17:31:24.867
Sep  4 17:31:24.867: INFO: Pod "pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8" satisfied condition "Succeeded or Failed"
Sep  4 17:31:24.872: INFO: Trying to get logs from node tenant-000003 pod pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8 container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 17:31:24.887
Sep  4 17:31:24.904: INFO: Waiting for pod pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8 to disappear
Sep  4 17:31:24.912: INFO: Pod pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  4 17:31:24.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1219" for this suite. 09/04/23 17:31:24.918
------------------------------
â€¢ [4.145 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:31:20.783
    Sep  4 17:31:20.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 17:31:20.785
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:20.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:20.817
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-937e2e4b-6ad8-4ca8-92f4-a00d74a9f3bb 09/04/23 17:31:20.825
    STEP: Creating a pod to test consume secrets 09/04/23 17:31:20.832
    Sep  4 17:31:20.848: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8" in namespace "projected-1219" to be "Succeeded or Failed"
    Sep  4 17:31:20.858: INFO: Pod "pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.147723ms
    Sep  4 17:31:22.886: INFO: Pod "pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037838017s
    Sep  4 17:31:24.867: INFO: Pod "pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018391156s
    STEP: Saw pod success 09/04/23 17:31:24.867
    Sep  4 17:31:24.867: INFO: Pod "pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8" satisfied condition "Succeeded or Failed"
    Sep  4 17:31:24.872: INFO: Trying to get logs from node tenant-000003 pod pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8 container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 17:31:24.887
    Sep  4 17:31:24.904: INFO: Waiting for pod pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8 to disappear
    Sep  4 17:31:24.912: INFO: Pod pod-projected-secrets-42cf790c-c997-4aa9-a272-f230e3ad3dd8 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:31:24.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1219" for this suite. 09/04/23 17:31:24.918
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:31:24.932
Sep  4 17:31:24.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename svc-latency 09/04/23 17:31:24.935
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:24.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:24.982
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Sep  4 17:31:24.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3911 09/04/23 17:31:24.991
I0904 17:31:25.000469      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3911, replica count: 1
I0904 17:31:26.052427      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0904 17:31:27.053094      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0904 17:31:28.053807      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 17:31:28.175: INFO: Created: latency-svc-d57kg
Sep  4 17:31:28.184: INFO: Got endpoints: latency-svc-d57kg [29.341536ms]
Sep  4 17:31:28.212: INFO: Created: latency-svc-gfd7m
Sep  4 17:31:28.212: INFO: Got endpoints: latency-svc-gfd7m [27.716339ms]
Sep  4 17:31:28.224: INFO: Created: latency-svc-ht8jd
Sep  4 17:31:28.229: INFO: Got endpoints: latency-svc-ht8jd [44.599138ms]
Sep  4 17:31:28.235: INFO: Created: latency-svc-24cm7
Sep  4 17:31:28.259: INFO: Got endpoints: latency-svc-24cm7 [74.870728ms]
Sep  4 17:31:28.259: INFO: Created: latency-svc-zzxsj
Sep  4 17:31:28.261: INFO: Got endpoints: latency-svc-zzxsj [77.397378ms]
Sep  4 17:31:28.273: INFO: Created: latency-svc-p7gb5
Sep  4 17:31:28.289: INFO: Got endpoints: latency-svc-p7gb5 [105.173721ms]
Sep  4 17:31:28.292: INFO: Created: latency-svc-jpgnt
Sep  4 17:31:28.304: INFO: Got endpoints: latency-svc-jpgnt [119.420063ms]
Sep  4 17:31:28.311: INFO: Created: latency-svc-xff8c
Sep  4 17:31:28.316: INFO: Got endpoints: latency-svc-xff8c [131.53478ms]
Sep  4 17:31:28.327: INFO: Created: latency-svc-lzr6z
Sep  4 17:31:28.331: INFO: Got endpoints: latency-svc-lzr6z [146.323655ms]
Sep  4 17:31:28.337: INFO: Created: latency-svc-q5vc7
Sep  4 17:31:28.343: INFO: Got endpoints: latency-svc-q5vc7 [158.999204ms]
Sep  4 17:31:28.366: INFO: Created: latency-svc-l4xxq
Sep  4 17:31:28.371: INFO: Got endpoints: latency-svc-l4xxq [186.548134ms]
Sep  4 17:31:28.379: INFO: Created: latency-svc-tk579
Sep  4 17:31:28.388: INFO: Got endpoints: latency-svc-tk579 [203.642145ms]
Sep  4 17:31:28.393: INFO: Created: latency-svc-4tf4z
Sep  4 17:31:28.406: INFO: Got endpoints: latency-svc-4tf4z [220.772258ms]
Sep  4 17:31:28.408: INFO: Created: latency-svc-n5kng
Sep  4 17:31:28.423: INFO: Got endpoints: latency-svc-n5kng [238.749821ms]
Sep  4 17:31:28.432: INFO: Created: latency-svc-82pk2
Sep  4 17:31:28.445: INFO: Created: latency-svc-5q4jn
Sep  4 17:31:28.446: INFO: Got endpoints: latency-svc-82pk2 [261.411261ms]
Sep  4 17:31:28.456: INFO: Got endpoints: latency-svc-5q4jn [271.060733ms]
Sep  4 17:31:28.463: INFO: Created: latency-svc-8s9nj
Sep  4 17:31:28.471: INFO: Got endpoints: latency-svc-8s9nj [259.065623ms]
Sep  4 17:31:28.485: INFO: Created: latency-svc-lnbpb
Sep  4 17:31:28.491: INFO: Got endpoints: latency-svc-lnbpb [262.164506ms]
Sep  4 17:31:28.493: INFO: Created: latency-svc-25zxs
Sep  4 17:31:28.506: INFO: Got endpoints: latency-svc-25zxs [246.631187ms]
Sep  4 17:31:28.528: INFO: Created: latency-svc-2rb8n
Sep  4 17:31:28.533: INFO: Got endpoints: latency-svc-2rb8n [271.964986ms]
Sep  4 17:31:28.541: INFO: Created: latency-svc-mp9l9
Sep  4 17:31:28.554: INFO: Got endpoints: latency-svc-mp9l9 [264.780762ms]
Sep  4 17:31:28.555: INFO: Created: latency-svc-bn2hx
Sep  4 17:31:28.566: INFO: Got endpoints: latency-svc-bn2hx [262.142505ms]
Sep  4 17:31:28.581: INFO: Created: latency-svc-7bhrq
Sep  4 17:31:28.582: INFO: Got endpoints: latency-svc-7bhrq [265.663814ms]
Sep  4 17:31:28.588: INFO: Created: latency-svc-zqh7h
Sep  4 17:31:28.599: INFO: Got endpoints: latency-svc-zqh7h [268.13026ms]
Sep  4 17:31:28.603: INFO: Created: latency-svc-rbfs4
Sep  4 17:31:28.613: INFO: Got endpoints: latency-svc-rbfs4 [269.418236ms]
Sep  4 17:31:28.624: INFO: Created: latency-svc-wtxmg
Sep  4 17:31:28.631: INFO: Got endpoints: latency-svc-wtxmg [259.309338ms]
Sep  4 17:31:28.640: INFO: Created: latency-svc-5g7kg
Sep  4 17:31:28.644: INFO: Got endpoints: latency-svc-5g7kg [255.334802ms]
Sep  4 17:31:28.649: INFO: Created: latency-svc-cd7lm
Sep  4 17:31:28.662: INFO: Got endpoints: latency-svc-cd7lm [255.711725ms]
Sep  4 17:31:28.674: INFO: Created: latency-svc-2qkb8
Sep  4 17:31:28.686: INFO: Got endpoints: latency-svc-2qkb8 [262.472925ms]
Sep  4 17:31:28.695: INFO: Created: latency-svc-fzftv
Sep  4 17:31:28.696: INFO: Created: latency-svc-cvpzf
Sep  4 17:31:28.703: INFO: Got endpoints: latency-svc-cvpzf [256.774787ms]
Sep  4 17:31:28.711: INFO: Got endpoints: latency-svc-fzftv [255.435909ms]
Sep  4 17:31:28.721: INFO: Created: latency-svc-qv4z9
Sep  4 17:31:28.736: INFO: Got endpoints: latency-svc-qv4z9 [264.415939ms]
Sep  4 17:31:28.754: INFO: Created: latency-svc-g4fcv
Sep  4 17:31:28.761: INFO: Got endpoints: latency-svc-g4fcv [269.432737ms]
Sep  4 17:31:28.767: INFO: Created: latency-svc-9sfng
Sep  4 17:31:28.782: INFO: Got endpoints: latency-svc-9sfng [275.998125ms]
Sep  4 17:31:28.793: INFO: Created: latency-svc-mjn6z
Sep  4 17:31:28.796: INFO: Got endpoints: latency-svc-mjn6z [262.664136ms]
Sep  4 17:31:28.805: INFO: Created: latency-svc-qvh6k
Sep  4 17:31:28.809: INFO: Got endpoints: latency-svc-qvh6k [254.461251ms]
Sep  4 17:31:28.814: INFO: Created: latency-svc-4lq5r
Sep  4 17:31:28.833: INFO: Got endpoints: latency-svc-4lq5r [266.716876ms]
Sep  4 17:31:28.837: INFO: Created: latency-svc-rshpl
Sep  4 17:31:28.846: INFO: Got endpoints: latency-svc-rshpl [263.774201ms]
Sep  4 17:31:28.850: INFO: Created: latency-svc-vv758
Sep  4 17:31:28.863: INFO: Got endpoints: latency-svc-vv758 [263.921711ms]
Sep  4 17:31:28.868: INFO: Created: latency-svc-75v82
Sep  4 17:31:28.883: INFO: Got endpoints: latency-svc-75v82 [269.540942ms]
Sep  4 17:31:28.903: INFO: Created: latency-svc-vl9qf
Sep  4 17:31:28.911: INFO: Got endpoints: latency-svc-vl9qf [280.305279ms]
Sep  4 17:31:28.918: INFO: Created: latency-svc-4jcpp
Sep  4 17:31:28.929: INFO: Created: latency-svc-k65zt
Sep  4 17:31:28.932: INFO: Got endpoints: latency-svc-4jcpp [288.198646ms]
Sep  4 17:31:28.937: INFO: Got endpoints: latency-svc-k65zt [275.488394ms]
Sep  4 17:31:28.954: INFO: Created: latency-svc-kf9r4
Sep  4 17:31:28.959: INFO: Created: latency-svc-nwn74
Sep  4 17:31:28.964: INFO: Got endpoints: latency-svc-kf9r4 [277.302101ms]
Sep  4 17:31:28.967: INFO: Got endpoints: latency-svc-nwn74 [263.254571ms]
Sep  4 17:31:28.971: INFO: Created: latency-svc-vrg8n
Sep  4 17:31:28.984: INFO: Got endpoints: latency-svc-vrg8n [272.638926ms]
Sep  4 17:31:28.987: INFO: Created: latency-svc-ms2m5
Sep  4 17:31:29.000: INFO: Created: latency-svc-zcr52
Sep  4 17:31:29.010: INFO: Created: latency-svc-8n4bq
Sep  4 17:31:29.029: INFO: Created: latency-svc-224dw
Sep  4 17:31:29.036: INFO: Got endpoints: latency-svc-ms2m5 [299.772131ms]
Sep  4 17:31:29.041: INFO: Created: latency-svc-swm78
Sep  4 17:31:29.054: INFO: Created: latency-svc-gnlpd
Sep  4 17:31:29.066: INFO: Created: latency-svc-582cb
Sep  4 17:31:29.085: INFO: Got endpoints: latency-svc-zcr52 [324.044266ms]
Sep  4 17:31:29.086: INFO: Created: latency-svc-dwfb7
Sep  4 17:31:29.087: INFO: Created: latency-svc-22k26
Sep  4 17:31:29.101: INFO: Created: latency-svc-cpwsj
Sep  4 17:31:29.112: INFO: Created: latency-svc-j82jc
Sep  4 17:31:29.122: INFO: Created: latency-svc-cktph
Sep  4 17:31:29.135: INFO: Got endpoints: latency-svc-8n4bq [352.788866ms]
Sep  4 17:31:29.139: INFO: Created: latency-svc-n2vp4
Sep  4 17:31:29.151: INFO: Created: latency-svc-zlz7l
Sep  4 17:31:29.164: INFO: Created: latency-svc-nmhsj
Sep  4 17:31:29.181: INFO: Created: latency-svc-xkkgj
Sep  4 17:31:29.190: INFO: Got endpoints: latency-svc-224dw [393.606581ms]
Sep  4 17:31:29.194: INFO: Created: latency-svc-pkgsz
Sep  4 17:31:29.205: INFO: Created: latency-svc-lx5zv
Sep  4 17:31:29.217: INFO: Created: latency-svc-5lp4f
Sep  4 17:31:29.237: INFO: Got endpoints: latency-svc-swm78 [427.723399ms]
Sep  4 17:31:29.254: INFO: Created: latency-svc-xpz4s
Sep  4 17:31:29.287: INFO: Got endpoints: latency-svc-gnlpd [452.653074ms]
Sep  4 17:31:29.304: INFO: Created: latency-svc-gkmpp
Sep  4 17:31:29.335: INFO: Got endpoints: latency-svc-582cb [489.233637ms]
Sep  4 17:31:29.363: INFO: Created: latency-svc-58jtl
Sep  4 17:31:29.402: INFO: Got endpoints: latency-svc-dwfb7 [538.64746ms]
Sep  4 17:31:29.425: INFO: Created: latency-svc-sd2cb
Sep  4 17:31:29.441: INFO: Got endpoints: latency-svc-22k26 [557.676185ms]
Sep  4 17:31:29.467: INFO: Created: latency-svc-rt6wm
Sep  4 17:31:29.484: INFO: Got endpoints: latency-svc-cpwsj [572.331952ms]
Sep  4 17:31:29.501: INFO: Created: latency-svc-6vz7l
Sep  4 17:31:29.536: INFO: Got endpoints: latency-svc-j82jc [603.125174ms]
Sep  4 17:31:29.554: INFO: Created: latency-svc-tcvc5
Sep  4 17:31:29.582: INFO: Got endpoints: latency-svc-cktph [644.907145ms]
Sep  4 17:31:29.602: INFO: Created: latency-svc-m5lk8
Sep  4 17:31:29.632: INFO: Got endpoints: latency-svc-n2vp4 [668.31483ms]
Sep  4 17:31:29.653: INFO: Created: latency-svc-h8jb2
Sep  4 17:31:29.684: INFO: Got endpoints: latency-svc-zlz7l [717.209721ms]
Sep  4 17:31:29.704: INFO: Created: latency-svc-2zcvc
Sep  4 17:31:29.736: INFO: Got endpoints: latency-svc-nmhsj [751.443646ms]
Sep  4 17:31:29.753: INFO: Created: latency-svc-v5rvt
Sep  4 17:31:29.785: INFO: Got endpoints: latency-svc-xkkgj [748.738186ms]
Sep  4 17:31:29.807: INFO: Created: latency-svc-tsf2n
Sep  4 17:31:29.833: INFO: Got endpoints: latency-svc-pkgsz [746.999283ms]
Sep  4 17:31:29.854: INFO: Created: latency-svc-xkx7r
Sep  4 17:31:29.886: INFO: Got endpoints: latency-svc-lx5zv [750.6587ms]
Sep  4 17:31:29.909: INFO: Created: latency-svc-x578q
Sep  4 17:31:29.934: INFO: Got endpoints: latency-svc-5lp4f [743.642785ms]
Sep  4 17:31:29.955: INFO: Created: latency-svc-kbkt7
Sep  4 17:31:29.985: INFO: Got endpoints: latency-svc-xpz4s [747.723827ms]
Sep  4 17:31:30.002: INFO: Created: latency-svc-kgpfg
Sep  4 17:31:30.037: INFO: Got endpoints: latency-svc-gkmpp [749.576536ms]
Sep  4 17:31:30.058: INFO: Created: latency-svc-rl7c4
Sep  4 17:31:30.086: INFO: Got endpoints: latency-svc-58jtl [750.32568ms]
Sep  4 17:31:30.105: INFO: Created: latency-svc-2jkxr
Sep  4 17:31:30.137: INFO: Got endpoints: latency-svc-sd2cb [734.553647ms]
Sep  4 17:31:30.162: INFO: Created: latency-svc-5dh75
Sep  4 17:31:30.195: INFO: Got endpoints: latency-svc-rt6wm [754.317416ms]
Sep  4 17:31:30.223: INFO: Created: latency-svc-wmq6g
Sep  4 17:31:30.237: INFO: Got endpoints: latency-svc-6vz7l [753.079143ms]
Sep  4 17:31:30.257: INFO: Created: latency-svc-wfr2c
Sep  4 17:31:30.285: INFO: Got endpoints: latency-svc-tcvc5 [748.485571ms]
Sep  4 17:31:30.308: INFO: Created: latency-svc-vwbzd
Sep  4 17:31:30.335: INFO: Got endpoints: latency-svc-m5lk8 [751.518251ms]
Sep  4 17:31:30.352: INFO: Created: latency-svc-l4lgc
Sep  4 17:31:30.386: INFO: Got endpoints: latency-svc-h8jb2 [753.383961ms]
Sep  4 17:31:30.411: INFO: Created: latency-svc-2hwbx
Sep  4 17:31:30.436: INFO: Got endpoints: latency-svc-2zcvc [751.291938ms]
Sep  4 17:31:30.457: INFO: Created: latency-svc-7r2kj
Sep  4 17:31:30.482: INFO: Got endpoints: latency-svc-v5rvt [746.026526ms]
Sep  4 17:31:30.500: INFO: Created: latency-svc-6vvrq
Sep  4 17:31:30.534: INFO: Got endpoints: latency-svc-tsf2n [748.670682ms]
Sep  4 17:31:30.560: INFO: Created: latency-svc-494tb
Sep  4 17:31:30.586: INFO: Got endpoints: latency-svc-xkx7r [753.333358ms]
Sep  4 17:31:30.604: INFO: Created: latency-svc-mzrm2
Sep  4 17:31:30.635: INFO: Got endpoints: latency-svc-x578q [749.120709ms]
Sep  4 17:31:30.658: INFO: Created: latency-svc-xttjh
Sep  4 17:31:30.684: INFO: Got endpoints: latency-svc-kbkt7 [750.515491ms]
Sep  4 17:31:30.706: INFO: Created: latency-svc-6bkz8
Sep  4 17:31:30.734: INFO: Got endpoints: latency-svc-kgpfg [748.649981ms]
Sep  4 17:31:30.757: INFO: Created: latency-svc-mb8w5
Sep  4 17:31:30.782: INFO: Got endpoints: latency-svc-rl7c4 [745.499795ms]
Sep  4 17:31:30.801: INFO: Created: latency-svc-5q5kc
Sep  4 17:31:30.838: INFO: Got endpoints: latency-svc-2jkxr [751.620657ms]
Sep  4 17:31:30.855: INFO: Created: latency-svc-xx85x
Sep  4 17:31:30.892: INFO: Got endpoints: latency-svc-5dh75 [755.303575ms]
Sep  4 17:31:30.916: INFO: Created: latency-svc-8r52t
Sep  4 17:31:30.935: INFO: Got endpoints: latency-svc-wmq6g [739.32813ms]
Sep  4 17:31:30.952: INFO: Created: latency-svc-s7jfg
Sep  4 17:31:30.983: INFO: Got endpoints: latency-svc-wfr2c [745.330884ms]
Sep  4 17:31:31.003: INFO: Created: latency-svc-cqcrf
Sep  4 17:31:31.031: INFO: Got endpoints: latency-svc-vwbzd [746.041627ms]
Sep  4 17:31:31.049: INFO: Created: latency-svc-h25g5
Sep  4 17:31:31.086: INFO: Got endpoints: latency-svc-l4lgc [750.433586ms]
Sep  4 17:31:31.104: INFO: Created: latency-svc-jf9k6
Sep  4 17:31:31.135: INFO: Got endpoints: latency-svc-2hwbx [748.971601ms]
Sep  4 17:31:31.156: INFO: Created: latency-svc-xx9jj
Sep  4 17:31:31.185: INFO: Got endpoints: latency-svc-7r2kj [749.400325ms]
Sep  4 17:31:31.203: INFO: Created: latency-svc-2r458
Sep  4 17:31:31.238: INFO: Got endpoints: latency-svc-6vvrq [755.503186ms]
Sep  4 17:31:31.261: INFO: Created: latency-svc-mdk8p
Sep  4 17:31:31.282: INFO: Got endpoints: latency-svc-494tb [747.796931ms]
Sep  4 17:31:31.299: INFO: Created: latency-svc-44mnr
Sep  4 17:31:31.334: INFO: Got endpoints: latency-svc-mzrm2 [747.938439ms]
Sep  4 17:31:31.355: INFO: Created: latency-svc-thz92
Sep  4 17:31:31.386: INFO: Got endpoints: latency-svc-xttjh [750.560295ms]
Sep  4 17:31:31.405: INFO: Created: latency-svc-58x4d
Sep  4 17:31:31.434: INFO: Got endpoints: latency-svc-6bkz8 [748.665382ms]
Sep  4 17:31:31.452: INFO: Created: latency-svc-m9468
Sep  4 17:31:31.481: INFO: Got endpoints: latency-svc-mb8w5 [747.464111ms]
Sep  4 17:31:31.502: INFO: Created: latency-svc-5r9n9
Sep  4 17:31:31.532: INFO: Got endpoints: latency-svc-5q5kc [748.985601ms]
Sep  4 17:31:31.553: INFO: Created: latency-svc-wmvhb
Sep  4 17:31:31.582: INFO: Got endpoints: latency-svc-xx85x [743.466574ms]
Sep  4 17:31:31.599: INFO: Created: latency-svc-6ctvg
Sep  4 17:31:31.634: INFO: Got endpoints: latency-svc-8r52t [741.379452ms]
Sep  4 17:31:31.653: INFO: Created: latency-svc-ppsv4
Sep  4 17:31:31.687: INFO: Got endpoints: latency-svc-s7jfg [751.617857ms]
Sep  4 17:31:31.716: INFO: Created: latency-svc-n9smd
Sep  4 17:31:31.732: INFO: Got endpoints: latency-svc-cqcrf [748.323261ms]
Sep  4 17:31:31.758: INFO: Created: latency-svc-rgnrh
Sep  4 17:31:31.788: INFO: Got endpoints: latency-svc-h25g5 [757.08558ms]
Sep  4 17:31:31.807: INFO: Created: latency-svc-4ctpw
Sep  4 17:31:31.834: INFO: Got endpoints: latency-svc-jf9k6 [748.419767ms]
Sep  4 17:31:31.852: INFO: Created: latency-svc-kznzh
Sep  4 17:31:31.884: INFO: Got endpoints: latency-svc-xx9jj [748.380965ms]
Sep  4 17:31:31.902: INFO: Created: latency-svc-kdl7p
Sep  4 17:31:31.932: INFO: Got endpoints: latency-svc-2r458 [746.740668ms]
Sep  4 17:31:31.954: INFO: Created: latency-svc-mv22x
Sep  4 17:31:31.984: INFO: Got endpoints: latency-svc-mdk8p [745.165875ms]
Sep  4 17:31:32.006: INFO: Created: latency-svc-7rc4d
Sep  4 17:31:32.037: INFO: Got endpoints: latency-svc-44mnr [754.933752ms]
Sep  4 17:31:32.100: INFO: Got endpoints: latency-svc-thz92 [765.688689ms]
Sep  4 17:31:32.101: INFO: Created: latency-svc-54q4q
Sep  4 17:31:32.126: INFO: Created: latency-svc-z79fb
Sep  4 17:31:32.134: INFO: Got endpoints: latency-svc-58x4d [747.593419ms]
Sep  4 17:31:32.153: INFO: Created: latency-svc-5c7ns
Sep  4 17:31:32.184: INFO: Got endpoints: latency-svc-m9468 [749.697643ms]
Sep  4 17:31:32.205: INFO: Created: latency-svc-mn6hj
Sep  4 17:31:32.238: INFO: Got endpoints: latency-svc-5r9n9 [756.295834ms]
Sep  4 17:31:32.266: INFO: Created: latency-svc-6nfbn
Sep  4 17:31:32.283: INFO: Got endpoints: latency-svc-wmvhb [750.756906ms]
Sep  4 17:31:32.301: INFO: Created: latency-svc-rgqdq
Sep  4 17:31:32.335: INFO: Got endpoints: latency-svc-6ctvg [752.51791ms]
Sep  4 17:31:32.364: INFO: Created: latency-svc-s55ml
Sep  4 17:31:32.389: INFO: Got endpoints: latency-svc-ppsv4 [754.312516ms]
Sep  4 17:31:32.447: INFO: Created: latency-svc-mspdw
Sep  4 17:31:32.473: INFO: Got endpoints: latency-svc-n9smd [786.137898ms]
Sep  4 17:31:32.489: INFO: Got endpoints: latency-svc-rgnrh [757.77192ms]
Sep  4 17:31:32.496: INFO: Created: latency-svc-mmgdn
Sep  4 17:31:32.509: INFO: Created: latency-svc-lrnpp
Sep  4 17:31:32.534: INFO: Got endpoints: latency-svc-4ctpw [745.555298ms]
Sep  4 17:31:32.554: INFO: Created: latency-svc-fxxdx
Sep  4 17:31:32.586: INFO: Got endpoints: latency-svc-kznzh [750.912015ms]
Sep  4 17:31:32.605: INFO: Created: latency-svc-r654p
Sep  4 17:31:32.632: INFO: Got endpoints: latency-svc-kdl7p [748.055146ms]
Sep  4 17:31:32.652: INFO: Created: latency-svc-5zcj4
Sep  4 17:31:32.685: INFO: Got endpoints: latency-svc-mv22x [752.711322ms]
Sep  4 17:31:32.706: INFO: Created: latency-svc-4nhts
Sep  4 17:31:32.746: INFO: Got endpoints: latency-svc-7rc4d [761.781658ms]
Sep  4 17:31:32.782: INFO: Created: latency-svc-gw5nj
Sep  4 17:31:32.793: INFO: Got endpoints: latency-svc-54q4q [755.817005ms]
Sep  4 17:31:32.826: INFO: Created: latency-svc-4mpwg
Sep  4 17:31:32.850: INFO: Got endpoints: latency-svc-z79fb [748.993202ms]
Sep  4 17:31:32.878: INFO: Created: latency-svc-2qbrb
Sep  4 17:31:32.893: INFO: Got endpoints: latency-svc-5c7ns [758.629871ms]
Sep  4 17:31:32.921: INFO: Created: latency-svc-qn477
Sep  4 17:31:32.933: INFO: Got endpoints: latency-svc-mn6hj [749.384125ms]
Sep  4 17:31:32.953: INFO: Created: latency-svc-b4zj7
Sep  4 17:31:32.982: INFO: Got endpoints: latency-svc-6nfbn [743.904601ms]
Sep  4 17:31:32.998: INFO: Created: latency-svc-k6knp
Sep  4 17:31:33.035: INFO: Got endpoints: latency-svc-rgqdq [751.895874ms]
Sep  4 17:31:33.052: INFO: Created: latency-svc-zz9qx
Sep  4 17:31:33.088: INFO: Got endpoints: latency-svc-s55ml [752.714625ms]
Sep  4 17:31:33.113: INFO: Created: latency-svc-2jdwz
Sep  4 17:31:33.136: INFO: Got endpoints: latency-svc-mspdw [746.315628ms]
Sep  4 17:31:33.155: INFO: Created: latency-svc-jrh5t
Sep  4 17:31:33.185: INFO: Got endpoints: latency-svc-mmgdn [711.896261ms]
Sep  4 17:31:33.207: INFO: Created: latency-svc-5znlf
Sep  4 17:31:33.234: INFO: Got endpoints: latency-svc-lrnpp [744.385859ms]
Sep  4 17:31:33.258: INFO: Created: latency-svc-bkpr6
Sep  4 17:31:33.283: INFO: Got endpoints: latency-svc-fxxdx [748.36287ms]
Sep  4 17:31:33.301: INFO: Created: latency-svc-59kvk
Sep  4 17:31:33.342: INFO: Got endpoints: latency-svc-r654p [755.833522ms]
Sep  4 17:31:33.361: INFO: Created: latency-svc-4kvfk
Sep  4 17:31:33.393: INFO: Got endpoints: latency-svc-5zcj4 [759.895723ms]
Sep  4 17:31:33.418: INFO: Created: latency-svc-bh5wk
Sep  4 17:31:33.433: INFO: Got endpoints: latency-svc-4nhts [747.585731ms]
Sep  4 17:31:33.451: INFO: Created: latency-svc-j5btm
Sep  4 17:31:33.486: INFO: Got endpoints: latency-svc-gw5nj [739.861624ms]
Sep  4 17:31:33.504: INFO: Created: latency-svc-g8kjz
Sep  4 17:31:33.532: INFO: Got endpoints: latency-svc-4mpwg [738.765852ms]
Sep  4 17:31:33.552: INFO: Created: latency-svc-ngpgq
Sep  4 17:31:33.584: INFO: Got endpoints: latency-svc-2qbrb [733.502998ms]
Sep  4 17:31:33.611: INFO: Created: latency-svc-m4rmz
Sep  4 17:31:33.634: INFO: Got endpoints: latency-svc-qn477 [737.575105ms]
Sep  4 17:31:33.651: INFO: Created: latency-svc-wsr5x
Sep  4 17:31:33.690: INFO: Got endpoints: latency-svc-b4zj7 [756.200238ms]
Sep  4 17:31:33.714: INFO: Created: latency-svc-vd2pc
Sep  4 17:31:33.742: INFO: Got endpoints: latency-svc-k6knp [759.825405ms]
Sep  4 17:31:33.784: INFO: Got endpoints: latency-svc-zz9qx [748.208539ms]
Sep  4 17:31:33.786: INFO: Created: latency-svc-g4hdx
Sep  4 17:31:33.805: INFO: Created: latency-svc-tzmbq
Sep  4 17:31:33.836: INFO: Got endpoints: latency-svc-2jdwz [748.324698ms]
Sep  4 17:31:33.856: INFO: Created: latency-svc-gtrtd
Sep  4 17:31:33.882: INFO: Got endpoints: latency-svc-jrh5t [746.107282ms]
Sep  4 17:31:33.903: INFO: Created: latency-svc-phfbz
Sep  4 17:31:33.936: INFO: Got endpoints: latency-svc-5znlf [750.293002ms]
Sep  4 17:31:33.954: INFO: Created: latency-svc-gg74j
Sep  4 17:31:33.985: INFO: Got endpoints: latency-svc-bkpr6 [750.272601ms]
Sep  4 17:31:34.006: INFO: Created: latency-svc-fss6z
Sep  4 17:31:34.033: INFO: Got endpoints: latency-svc-59kvk [749.155843ms]
Sep  4 17:31:34.057: INFO: Created: latency-svc-xlg8x
Sep  4 17:31:34.087: INFO: Got endpoints: latency-svc-4kvfk [744.487297ms]
Sep  4 17:31:34.108: INFO: Created: latency-svc-gfgnm
Sep  4 17:31:34.136: INFO: Got endpoints: latency-svc-bh5wk [742.277581ms]
Sep  4 17:31:34.153: INFO: Created: latency-svc-sqsh7
Sep  4 17:31:34.188: INFO: Got endpoints: latency-svc-j5btm [754.849841ms]
Sep  4 17:31:34.218: INFO: Created: latency-svc-mnhw7
Sep  4 17:31:34.236: INFO: Got endpoints: latency-svc-g8kjz [749.973385ms]
Sep  4 17:31:34.253: INFO: Created: latency-svc-zvp5s
Sep  4 17:31:34.294: INFO: Got endpoints: latency-svc-ngpgq [761.390785ms]
Sep  4 17:31:34.312: INFO: Created: latency-svc-qc4p7
Sep  4 17:31:34.338: INFO: Got endpoints: latency-svc-m4rmz [753.673879ms]
Sep  4 17:31:34.357: INFO: Created: latency-svc-8dg4l
Sep  4 17:31:34.382: INFO: Got endpoints: latency-svc-wsr5x [747.00913ms]
Sep  4 17:31:34.400: INFO: Created: latency-svc-fh84m
Sep  4 17:31:34.436: INFO: Got endpoints: latency-svc-vd2pc [745.793266ms]
Sep  4 17:31:34.463: INFO: Created: latency-svc-bx2h2
Sep  4 17:31:34.485: INFO: Got endpoints: latency-svc-g4hdx [742.307682ms]
Sep  4 17:31:34.501: INFO: Created: latency-svc-wmvx5
Sep  4 17:31:34.532: INFO: Got endpoints: latency-svc-tzmbq [747.511856ms]
Sep  4 17:31:34.551: INFO: Created: latency-svc-hs75d
Sep  4 17:31:34.581: INFO: Got endpoints: latency-svc-gtrtd [744.627905ms]
Sep  4 17:31:34.601: INFO: Created: latency-svc-krkw4
Sep  4 17:31:34.638: INFO: Got endpoints: latency-svc-phfbz [755.678185ms]
Sep  4 17:31:34.657: INFO: Created: latency-svc-hcnzz
Sep  4 17:31:34.685: INFO: Got endpoints: latency-svc-gg74j [748.797924ms]
Sep  4 17:31:34.711: INFO: Created: latency-svc-79ww2
Sep  4 17:31:34.736: INFO: Got endpoints: latency-svc-fss6z [750.289802ms]
Sep  4 17:31:34.754: INFO: Created: latency-svc-p4f5t
Sep  4 17:31:34.782: INFO: Got endpoints: latency-svc-xlg8x [749.319651ms]
Sep  4 17:31:34.800: INFO: Created: latency-svc-twtjj
Sep  4 17:31:34.836: INFO: Got endpoints: latency-svc-gfgnm [748.813824ms]
Sep  4 17:31:34.856: INFO: Created: latency-svc-wzn5f
Sep  4 17:31:34.883: INFO: Got endpoints: latency-svc-sqsh7 [746.589307ms]
Sep  4 17:31:34.901: INFO: Created: latency-svc-8s5vd
Sep  4 17:31:34.936: INFO: Got endpoints: latency-svc-mnhw7 [747.766469ms]
Sep  4 17:31:34.964: INFO: Created: latency-svc-brfgk
Sep  4 17:31:34.982: INFO: Got endpoints: latency-svc-zvp5s [746.479902ms]
Sep  4 17:31:35.000: INFO: Created: latency-svc-gqksf
Sep  4 17:31:35.032: INFO: Got endpoints: latency-svc-qc4p7 [737.426526ms]
Sep  4 17:31:35.066: INFO: Created: latency-svc-lrk6w
Sep  4 17:31:35.081: INFO: Got endpoints: latency-svc-8dg4l [742.433889ms]
Sep  4 17:31:35.109: INFO: Created: latency-svc-z22hb
Sep  4 17:31:35.137: INFO: Got endpoints: latency-svc-fh84m [754.619429ms]
Sep  4 17:31:35.156: INFO: Created: latency-svc-ngdqg
Sep  4 17:31:35.184: INFO: Got endpoints: latency-svc-bx2h2 [747.97048ms]
Sep  4 17:31:35.216: INFO: Created: latency-svc-mcn6z
Sep  4 17:31:35.236: INFO: Got endpoints: latency-svc-wmvx5 [751.632972ms]
Sep  4 17:31:35.256: INFO: Created: latency-svc-wblq4
Sep  4 17:31:35.286: INFO: Got endpoints: latency-svc-hs75d [753.459468ms]
Sep  4 17:31:35.306: INFO: Created: latency-svc-gv52k
Sep  4 17:31:35.332: INFO: Got endpoints: latency-svc-krkw4 [750.394307ms]
Sep  4 17:31:35.354: INFO: Created: latency-svc-ff2g7
Sep  4 17:31:35.393: INFO: Got endpoints: latency-svc-hcnzz [754.275711ms]
Sep  4 17:31:35.410: INFO: Created: latency-svc-jks9w
Sep  4 17:31:35.435: INFO: Got endpoints: latency-svc-79ww2 [748.720819ms]
Sep  4 17:31:35.450: INFO: Created: latency-svc-4rw2l
Sep  4 17:31:35.483: INFO: Got endpoints: latency-svc-p4f5t [747.536757ms]
Sep  4 17:31:35.503: INFO: Created: latency-svc-m98fw
Sep  4 17:31:35.531: INFO: Got endpoints: latency-svc-twtjj [748.111188ms]
Sep  4 17:31:35.552: INFO: Created: latency-svc-d7g6b
Sep  4 17:31:35.584: INFO: Got endpoints: latency-svc-wzn5f [747.364748ms]
Sep  4 17:31:35.602: INFO: Created: latency-svc-4h4cn
Sep  4 17:31:35.636: INFO: Got endpoints: latency-svc-8s5vd [753.000044ms]
Sep  4 17:31:35.659: INFO: Created: latency-svc-xf2qc
Sep  4 17:31:35.686: INFO: Got endpoints: latency-svc-brfgk [749.778875ms]
Sep  4 17:31:35.713: INFO: Created: latency-svc-nc879
Sep  4 17:31:35.732: INFO: Got endpoints: latency-svc-gqksf [749.061438ms]
Sep  4 17:31:35.757: INFO: Created: latency-svc-2ltxq
Sep  4 17:31:35.782: INFO: Got endpoints: latency-svc-lrk6w [749.937083ms]
Sep  4 17:31:35.806: INFO: Created: latency-svc-h9q8h
Sep  4 17:31:35.830: INFO: Got endpoints: latency-svc-z22hb [749.48396ms]
Sep  4 17:31:35.851: INFO: Created: latency-svc-hp7zh
Sep  4 17:31:35.887: INFO: Got endpoints: latency-svc-ngdqg [750.021587ms]
Sep  4 17:31:35.907: INFO: Created: latency-svc-f2kpz
Sep  4 17:31:35.933: INFO: Got endpoints: latency-svc-mcn6z [748.706619ms]
Sep  4 17:31:35.951: INFO: Created: latency-svc-fmqhz
Sep  4 17:31:35.980: INFO: Got endpoints: latency-svc-wblq4 [743.468844ms]
Sep  4 17:31:35.999: INFO: Created: latency-svc-dk4lv
Sep  4 17:31:36.037: INFO: Got endpoints: latency-svc-gv52k [750.659321ms]
Sep  4 17:31:36.086: INFO: Got endpoints: latency-svc-ff2g7 [752.861637ms]
Sep  4 17:31:36.133: INFO: Got endpoints: latency-svc-jks9w [740.226574ms]
Sep  4 17:31:36.185: INFO: Got endpoints: latency-svc-4rw2l [750.908935ms]
Sep  4 17:31:36.239: INFO: Got endpoints: latency-svc-m98fw [755.271363ms]
Sep  4 17:31:36.287: INFO: Got endpoints: latency-svc-d7g6b [755.309865ms]
Sep  4 17:31:36.332: INFO: Got endpoints: latency-svc-4h4cn [748.338399ms]
Sep  4 17:31:36.385: INFO: Got endpoints: latency-svc-xf2qc [748.413504ms]
Sep  4 17:31:36.433: INFO: Got endpoints: latency-svc-nc879 [746.533704ms]
Sep  4 17:31:36.487: INFO: Got endpoints: latency-svc-2ltxq [755.083154ms]
Sep  4 17:31:36.541: INFO: Got endpoints: latency-svc-h9q8h [758.925255ms]
Sep  4 17:31:36.583: INFO: Got endpoints: latency-svc-hp7zh [752.172301ms]
Sep  4 17:31:36.651: INFO: Got endpoints: latency-svc-f2kpz [764.008122ms]
Sep  4 17:31:36.683: INFO: Got endpoints: latency-svc-fmqhz [749.424656ms]
Sep  4 17:31:36.735: INFO: Got endpoints: latency-svc-dk4lv [755.029351ms]
Sep  4 17:31:36.735: INFO: Latencies: [27.716339ms 44.599138ms 74.870728ms 77.397378ms 105.173721ms 119.420063ms 131.53478ms 146.323655ms 158.999204ms 186.548134ms 203.642145ms 220.772258ms 238.749821ms 246.631187ms 254.461251ms 255.334802ms 255.435909ms 255.711725ms 256.774787ms 259.065623ms 259.309338ms 261.411261ms 262.142505ms 262.164506ms 262.472925ms 262.664136ms 263.254571ms 263.774201ms 263.921711ms 264.415939ms 264.780762ms 265.663814ms 266.716876ms 268.13026ms 269.418236ms 269.432737ms 269.540942ms 271.060733ms 271.964986ms 272.638926ms 275.488394ms 275.998125ms 277.302101ms 280.305279ms 288.198646ms 299.772131ms 324.044266ms 352.788866ms 393.606581ms 427.723399ms 452.653074ms 489.233637ms 538.64746ms 557.676185ms 572.331952ms 603.125174ms 644.907145ms 668.31483ms 711.896261ms 717.209721ms 733.502998ms 734.553647ms 737.426526ms 737.575105ms 738.765852ms 739.32813ms 739.861624ms 740.226574ms 741.379452ms 742.277581ms 742.307682ms 742.433889ms 743.466574ms 743.468844ms 743.642785ms 743.904601ms 744.385859ms 744.487297ms 744.627905ms 745.165875ms 745.330884ms 745.499795ms 745.555298ms 745.793266ms 746.026526ms 746.041627ms 746.107282ms 746.315628ms 746.479902ms 746.533704ms 746.589307ms 746.740668ms 746.999283ms 747.00913ms 747.364748ms 747.464111ms 747.511856ms 747.536757ms 747.585731ms 747.593419ms 747.723827ms 747.766469ms 747.796931ms 747.938439ms 747.97048ms 748.055146ms 748.111188ms 748.208539ms 748.323261ms 748.324698ms 748.338399ms 748.36287ms 748.380965ms 748.413504ms 748.419767ms 748.485571ms 748.649981ms 748.665382ms 748.670682ms 748.706619ms 748.720819ms 748.738186ms 748.797924ms 748.813824ms 748.971601ms 748.985601ms 748.993202ms 749.061438ms 749.120709ms 749.155843ms 749.319651ms 749.384125ms 749.400325ms 749.424656ms 749.48396ms 749.576536ms 749.697643ms 749.778875ms 749.937083ms 749.973385ms 750.021587ms 750.272601ms 750.289802ms 750.293002ms 750.32568ms 750.394307ms 750.433586ms 750.515491ms 750.560295ms 750.6587ms 750.659321ms 750.756906ms 750.908935ms 750.912015ms 751.291938ms 751.443646ms 751.518251ms 751.617857ms 751.620657ms 751.632972ms 751.895874ms 752.172301ms 752.51791ms 752.711322ms 752.714625ms 752.861637ms 753.000044ms 753.079143ms 753.333358ms 753.383961ms 753.459468ms 753.673879ms 754.275711ms 754.312516ms 754.317416ms 754.619429ms 754.849841ms 754.933752ms 755.029351ms 755.083154ms 755.271363ms 755.303575ms 755.309865ms 755.503186ms 755.678185ms 755.817005ms 755.833522ms 756.200238ms 756.295834ms 757.08558ms 757.77192ms 758.629871ms 758.925255ms 759.825405ms 759.895723ms 761.390785ms 761.781658ms 764.008122ms 765.688689ms 786.137898ms]
Sep  4 17:31:36.736: INFO: 50 %ile: 747.723827ms
Sep  4 17:31:36.737: INFO: 90 %ile: 755.271363ms
Sep  4 17:31:36.737: INFO: 99 %ile: 765.688689ms
Sep  4 17:31:36.738: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Sep  4 17:31:36.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-3911" for this suite. 09/04/23 17:31:36.75
------------------------------
â€¢ [SLOW TEST] [11.832 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:31:24.932
    Sep  4 17:31:24.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename svc-latency 09/04/23 17:31:24.935
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:24.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:24.982
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Sep  4 17:31:24.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-3911 09/04/23 17:31:24.991
    I0904 17:31:25.000469      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3911, replica count: 1
    I0904 17:31:26.052427      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0904 17:31:27.053094      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0904 17:31:28.053807      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 17:31:28.175: INFO: Created: latency-svc-d57kg
    Sep  4 17:31:28.184: INFO: Got endpoints: latency-svc-d57kg [29.341536ms]
    Sep  4 17:31:28.212: INFO: Created: latency-svc-gfd7m
    Sep  4 17:31:28.212: INFO: Got endpoints: latency-svc-gfd7m [27.716339ms]
    Sep  4 17:31:28.224: INFO: Created: latency-svc-ht8jd
    Sep  4 17:31:28.229: INFO: Got endpoints: latency-svc-ht8jd [44.599138ms]
    Sep  4 17:31:28.235: INFO: Created: latency-svc-24cm7
    Sep  4 17:31:28.259: INFO: Got endpoints: latency-svc-24cm7 [74.870728ms]
    Sep  4 17:31:28.259: INFO: Created: latency-svc-zzxsj
    Sep  4 17:31:28.261: INFO: Got endpoints: latency-svc-zzxsj [77.397378ms]
    Sep  4 17:31:28.273: INFO: Created: latency-svc-p7gb5
    Sep  4 17:31:28.289: INFO: Got endpoints: latency-svc-p7gb5 [105.173721ms]
    Sep  4 17:31:28.292: INFO: Created: latency-svc-jpgnt
    Sep  4 17:31:28.304: INFO: Got endpoints: latency-svc-jpgnt [119.420063ms]
    Sep  4 17:31:28.311: INFO: Created: latency-svc-xff8c
    Sep  4 17:31:28.316: INFO: Got endpoints: latency-svc-xff8c [131.53478ms]
    Sep  4 17:31:28.327: INFO: Created: latency-svc-lzr6z
    Sep  4 17:31:28.331: INFO: Got endpoints: latency-svc-lzr6z [146.323655ms]
    Sep  4 17:31:28.337: INFO: Created: latency-svc-q5vc7
    Sep  4 17:31:28.343: INFO: Got endpoints: latency-svc-q5vc7 [158.999204ms]
    Sep  4 17:31:28.366: INFO: Created: latency-svc-l4xxq
    Sep  4 17:31:28.371: INFO: Got endpoints: latency-svc-l4xxq [186.548134ms]
    Sep  4 17:31:28.379: INFO: Created: latency-svc-tk579
    Sep  4 17:31:28.388: INFO: Got endpoints: latency-svc-tk579 [203.642145ms]
    Sep  4 17:31:28.393: INFO: Created: latency-svc-4tf4z
    Sep  4 17:31:28.406: INFO: Got endpoints: latency-svc-4tf4z [220.772258ms]
    Sep  4 17:31:28.408: INFO: Created: latency-svc-n5kng
    Sep  4 17:31:28.423: INFO: Got endpoints: latency-svc-n5kng [238.749821ms]
    Sep  4 17:31:28.432: INFO: Created: latency-svc-82pk2
    Sep  4 17:31:28.445: INFO: Created: latency-svc-5q4jn
    Sep  4 17:31:28.446: INFO: Got endpoints: latency-svc-82pk2 [261.411261ms]
    Sep  4 17:31:28.456: INFO: Got endpoints: latency-svc-5q4jn [271.060733ms]
    Sep  4 17:31:28.463: INFO: Created: latency-svc-8s9nj
    Sep  4 17:31:28.471: INFO: Got endpoints: latency-svc-8s9nj [259.065623ms]
    Sep  4 17:31:28.485: INFO: Created: latency-svc-lnbpb
    Sep  4 17:31:28.491: INFO: Got endpoints: latency-svc-lnbpb [262.164506ms]
    Sep  4 17:31:28.493: INFO: Created: latency-svc-25zxs
    Sep  4 17:31:28.506: INFO: Got endpoints: latency-svc-25zxs [246.631187ms]
    Sep  4 17:31:28.528: INFO: Created: latency-svc-2rb8n
    Sep  4 17:31:28.533: INFO: Got endpoints: latency-svc-2rb8n [271.964986ms]
    Sep  4 17:31:28.541: INFO: Created: latency-svc-mp9l9
    Sep  4 17:31:28.554: INFO: Got endpoints: latency-svc-mp9l9 [264.780762ms]
    Sep  4 17:31:28.555: INFO: Created: latency-svc-bn2hx
    Sep  4 17:31:28.566: INFO: Got endpoints: latency-svc-bn2hx [262.142505ms]
    Sep  4 17:31:28.581: INFO: Created: latency-svc-7bhrq
    Sep  4 17:31:28.582: INFO: Got endpoints: latency-svc-7bhrq [265.663814ms]
    Sep  4 17:31:28.588: INFO: Created: latency-svc-zqh7h
    Sep  4 17:31:28.599: INFO: Got endpoints: latency-svc-zqh7h [268.13026ms]
    Sep  4 17:31:28.603: INFO: Created: latency-svc-rbfs4
    Sep  4 17:31:28.613: INFO: Got endpoints: latency-svc-rbfs4 [269.418236ms]
    Sep  4 17:31:28.624: INFO: Created: latency-svc-wtxmg
    Sep  4 17:31:28.631: INFO: Got endpoints: latency-svc-wtxmg [259.309338ms]
    Sep  4 17:31:28.640: INFO: Created: latency-svc-5g7kg
    Sep  4 17:31:28.644: INFO: Got endpoints: latency-svc-5g7kg [255.334802ms]
    Sep  4 17:31:28.649: INFO: Created: latency-svc-cd7lm
    Sep  4 17:31:28.662: INFO: Got endpoints: latency-svc-cd7lm [255.711725ms]
    Sep  4 17:31:28.674: INFO: Created: latency-svc-2qkb8
    Sep  4 17:31:28.686: INFO: Got endpoints: latency-svc-2qkb8 [262.472925ms]
    Sep  4 17:31:28.695: INFO: Created: latency-svc-fzftv
    Sep  4 17:31:28.696: INFO: Created: latency-svc-cvpzf
    Sep  4 17:31:28.703: INFO: Got endpoints: latency-svc-cvpzf [256.774787ms]
    Sep  4 17:31:28.711: INFO: Got endpoints: latency-svc-fzftv [255.435909ms]
    Sep  4 17:31:28.721: INFO: Created: latency-svc-qv4z9
    Sep  4 17:31:28.736: INFO: Got endpoints: latency-svc-qv4z9 [264.415939ms]
    Sep  4 17:31:28.754: INFO: Created: latency-svc-g4fcv
    Sep  4 17:31:28.761: INFO: Got endpoints: latency-svc-g4fcv [269.432737ms]
    Sep  4 17:31:28.767: INFO: Created: latency-svc-9sfng
    Sep  4 17:31:28.782: INFO: Got endpoints: latency-svc-9sfng [275.998125ms]
    Sep  4 17:31:28.793: INFO: Created: latency-svc-mjn6z
    Sep  4 17:31:28.796: INFO: Got endpoints: latency-svc-mjn6z [262.664136ms]
    Sep  4 17:31:28.805: INFO: Created: latency-svc-qvh6k
    Sep  4 17:31:28.809: INFO: Got endpoints: latency-svc-qvh6k [254.461251ms]
    Sep  4 17:31:28.814: INFO: Created: latency-svc-4lq5r
    Sep  4 17:31:28.833: INFO: Got endpoints: latency-svc-4lq5r [266.716876ms]
    Sep  4 17:31:28.837: INFO: Created: latency-svc-rshpl
    Sep  4 17:31:28.846: INFO: Got endpoints: latency-svc-rshpl [263.774201ms]
    Sep  4 17:31:28.850: INFO: Created: latency-svc-vv758
    Sep  4 17:31:28.863: INFO: Got endpoints: latency-svc-vv758 [263.921711ms]
    Sep  4 17:31:28.868: INFO: Created: latency-svc-75v82
    Sep  4 17:31:28.883: INFO: Got endpoints: latency-svc-75v82 [269.540942ms]
    Sep  4 17:31:28.903: INFO: Created: latency-svc-vl9qf
    Sep  4 17:31:28.911: INFO: Got endpoints: latency-svc-vl9qf [280.305279ms]
    Sep  4 17:31:28.918: INFO: Created: latency-svc-4jcpp
    Sep  4 17:31:28.929: INFO: Created: latency-svc-k65zt
    Sep  4 17:31:28.932: INFO: Got endpoints: latency-svc-4jcpp [288.198646ms]
    Sep  4 17:31:28.937: INFO: Got endpoints: latency-svc-k65zt [275.488394ms]
    Sep  4 17:31:28.954: INFO: Created: latency-svc-kf9r4
    Sep  4 17:31:28.959: INFO: Created: latency-svc-nwn74
    Sep  4 17:31:28.964: INFO: Got endpoints: latency-svc-kf9r4 [277.302101ms]
    Sep  4 17:31:28.967: INFO: Got endpoints: latency-svc-nwn74 [263.254571ms]
    Sep  4 17:31:28.971: INFO: Created: latency-svc-vrg8n
    Sep  4 17:31:28.984: INFO: Got endpoints: latency-svc-vrg8n [272.638926ms]
    Sep  4 17:31:28.987: INFO: Created: latency-svc-ms2m5
    Sep  4 17:31:29.000: INFO: Created: latency-svc-zcr52
    Sep  4 17:31:29.010: INFO: Created: latency-svc-8n4bq
    Sep  4 17:31:29.029: INFO: Created: latency-svc-224dw
    Sep  4 17:31:29.036: INFO: Got endpoints: latency-svc-ms2m5 [299.772131ms]
    Sep  4 17:31:29.041: INFO: Created: latency-svc-swm78
    Sep  4 17:31:29.054: INFO: Created: latency-svc-gnlpd
    Sep  4 17:31:29.066: INFO: Created: latency-svc-582cb
    Sep  4 17:31:29.085: INFO: Got endpoints: latency-svc-zcr52 [324.044266ms]
    Sep  4 17:31:29.086: INFO: Created: latency-svc-dwfb7
    Sep  4 17:31:29.087: INFO: Created: latency-svc-22k26
    Sep  4 17:31:29.101: INFO: Created: latency-svc-cpwsj
    Sep  4 17:31:29.112: INFO: Created: latency-svc-j82jc
    Sep  4 17:31:29.122: INFO: Created: latency-svc-cktph
    Sep  4 17:31:29.135: INFO: Got endpoints: latency-svc-8n4bq [352.788866ms]
    Sep  4 17:31:29.139: INFO: Created: latency-svc-n2vp4
    Sep  4 17:31:29.151: INFO: Created: latency-svc-zlz7l
    Sep  4 17:31:29.164: INFO: Created: latency-svc-nmhsj
    Sep  4 17:31:29.181: INFO: Created: latency-svc-xkkgj
    Sep  4 17:31:29.190: INFO: Got endpoints: latency-svc-224dw [393.606581ms]
    Sep  4 17:31:29.194: INFO: Created: latency-svc-pkgsz
    Sep  4 17:31:29.205: INFO: Created: latency-svc-lx5zv
    Sep  4 17:31:29.217: INFO: Created: latency-svc-5lp4f
    Sep  4 17:31:29.237: INFO: Got endpoints: latency-svc-swm78 [427.723399ms]
    Sep  4 17:31:29.254: INFO: Created: latency-svc-xpz4s
    Sep  4 17:31:29.287: INFO: Got endpoints: latency-svc-gnlpd [452.653074ms]
    Sep  4 17:31:29.304: INFO: Created: latency-svc-gkmpp
    Sep  4 17:31:29.335: INFO: Got endpoints: latency-svc-582cb [489.233637ms]
    Sep  4 17:31:29.363: INFO: Created: latency-svc-58jtl
    Sep  4 17:31:29.402: INFO: Got endpoints: latency-svc-dwfb7 [538.64746ms]
    Sep  4 17:31:29.425: INFO: Created: latency-svc-sd2cb
    Sep  4 17:31:29.441: INFO: Got endpoints: latency-svc-22k26 [557.676185ms]
    Sep  4 17:31:29.467: INFO: Created: latency-svc-rt6wm
    Sep  4 17:31:29.484: INFO: Got endpoints: latency-svc-cpwsj [572.331952ms]
    Sep  4 17:31:29.501: INFO: Created: latency-svc-6vz7l
    Sep  4 17:31:29.536: INFO: Got endpoints: latency-svc-j82jc [603.125174ms]
    Sep  4 17:31:29.554: INFO: Created: latency-svc-tcvc5
    Sep  4 17:31:29.582: INFO: Got endpoints: latency-svc-cktph [644.907145ms]
    Sep  4 17:31:29.602: INFO: Created: latency-svc-m5lk8
    Sep  4 17:31:29.632: INFO: Got endpoints: latency-svc-n2vp4 [668.31483ms]
    Sep  4 17:31:29.653: INFO: Created: latency-svc-h8jb2
    Sep  4 17:31:29.684: INFO: Got endpoints: latency-svc-zlz7l [717.209721ms]
    Sep  4 17:31:29.704: INFO: Created: latency-svc-2zcvc
    Sep  4 17:31:29.736: INFO: Got endpoints: latency-svc-nmhsj [751.443646ms]
    Sep  4 17:31:29.753: INFO: Created: latency-svc-v5rvt
    Sep  4 17:31:29.785: INFO: Got endpoints: latency-svc-xkkgj [748.738186ms]
    Sep  4 17:31:29.807: INFO: Created: latency-svc-tsf2n
    Sep  4 17:31:29.833: INFO: Got endpoints: latency-svc-pkgsz [746.999283ms]
    Sep  4 17:31:29.854: INFO: Created: latency-svc-xkx7r
    Sep  4 17:31:29.886: INFO: Got endpoints: latency-svc-lx5zv [750.6587ms]
    Sep  4 17:31:29.909: INFO: Created: latency-svc-x578q
    Sep  4 17:31:29.934: INFO: Got endpoints: latency-svc-5lp4f [743.642785ms]
    Sep  4 17:31:29.955: INFO: Created: latency-svc-kbkt7
    Sep  4 17:31:29.985: INFO: Got endpoints: latency-svc-xpz4s [747.723827ms]
    Sep  4 17:31:30.002: INFO: Created: latency-svc-kgpfg
    Sep  4 17:31:30.037: INFO: Got endpoints: latency-svc-gkmpp [749.576536ms]
    Sep  4 17:31:30.058: INFO: Created: latency-svc-rl7c4
    Sep  4 17:31:30.086: INFO: Got endpoints: latency-svc-58jtl [750.32568ms]
    Sep  4 17:31:30.105: INFO: Created: latency-svc-2jkxr
    Sep  4 17:31:30.137: INFO: Got endpoints: latency-svc-sd2cb [734.553647ms]
    Sep  4 17:31:30.162: INFO: Created: latency-svc-5dh75
    Sep  4 17:31:30.195: INFO: Got endpoints: latency-svc-rt6wm [754.317416ms]
    Sep  4 17:31:30.223: INFO: Created: latency-svc-wmq6g
    Sep  4 17:31:30.237: INFO: Got endpoints: latency-svc-6vz7l [753.079143ms]
    Sep  4 17:31:30.257: INFO: Created: latency-svc-wfr2c
    Sep  4 17:31:30.285: INFO: Got endpoints: latency-svc-tcvc5 [748.485571ms]
    Sep  4 17:31:30.308: INFO: Created: latency-svc-vwbzd
    Sep  4 17:31:30.335: INFO: Got endpoints: latency-svc-m5lk8 [751.518251ms]
    Sep  4 17:31:30.352: INFO: Created: latency-svc-l4lgc
    Sep  4 17:31:30.386: INFO: Got endpoints: latency-svc-h8jb2 [753.383961ms]
    Sep  4 17:31:30.411: INFO: Created: latency-svc-2hwbx
    Sep  4 17:31:30.436: INFO: Got endpoints: latency-svc-2zcvc [751.291938ms]
    Sep  4 17:31:30.457: INFO: Created: latency-svc-7r2kj
    Sep  4 17:31:30.482: INFO: Got endpoints: latency-svc-v5rvt [746.026526ms]
    Sep  4 17:31:30.500: INFO: Created: latency-svc-6vvrq
    Sep  4 17:31:30.534: INFO: Got endpoints: latency-svc-tsf2n [748.670682ms]
    Sep  4 17:31:30.560: INFO: Created: latency-svc-494tb
    Sep  4 17:31:30.586: INFO: Got endpoints: latency-svc-xkx7r [753.333358ms]
    Sep  4 17:31:30.604: INFO: Created: latency-svc-mzrm2
    Sep  4 17:31:30.635: INFO: Got endpoints: latency-svc-x578q [749.120709ms]
    Sep  4 17:31:30.658: INFO: Created: latency-svc-xttjh
    Sep  4 17:31:30.684: INFO: Got endpoints: latency-svc-kbkt7 [750.515491ms]
    Sep  4 17:31:30.706: INFO: Created: latency-svc-6bkz8
    Sep  4 17:31:30.734: INFO: Got endpoints: latency-svc-kgpfg [748.649981ms]
    Sep  4 17:31:30.757: INFO: Created: latency-svc-mb8w5
    Sep  4 17:31:30.782: INFO: Got endpoints: latency-svc-rl7c4 [745.499795ms]
    Sep  4 17:31:30.801: INFO: Created: latency-svc-5q5kc
    Sep  4 17:31:30.838: INFO: Got endpoints: latency-svc-2jkxr [751.620657ms]
    Sep  4 17:31:30.855: INFO: Created: latency-svc-xx85x
    Sep  4 17:31:30.892: INFO: Got endpoints: latency-svc-5dh75 [755.303575ms]
    Sep  4 17:31:30.916: INFO: Created: latency-svc-8r52t
    Sep  4 17:31:30.935: INFO: Got endpoints: latency-svc-wmq6g [739.32813ms]
    Sep  4 17:31:30.952: INFO: Created: latency-svc-s7jfg
    Sep  4 17:31:30.983: INFO: Got endpoints: latency-svc-wfr2c [745.330884ms]
    Sep  4 17:31:31.003: INFO: Created: latency-svc-cqcrf
    Sep  4 17:31:31.031: INFO: Got endpoints: latency-svc-vwbzd [746.041627ms]
    Sep  4 17:31:31.049: INFO: Created: latency-svc-h25g5
    Sep  4 17:31:31.086: INFO: Got endpoints: latency-svc-l4lgc [750.433586ms]
    Sep  4 17:31:31.104: INFO: Created: latency-svc-jf9k6
    Sep  4 17:31:31.135: INFO: Got endpoints: latency-svc-2hwbx [748.971601ms]
    Sep  4 17:31:31.156: INFO: Created: latency-svc-xx9jj
    Sep  4 17:31:31.185: INFO: Got endpoints: latency-svc-7r2kj [749.400325ms]
    Sep  4 17:31:31.203: INFO: Created: latency-svc-2r458
    Sep  4 17:31:31.238: INFO: Got endpoints: latency-svc-6vvrq [755.503186ms]
    Sep  4 17:31:31.261: INFO: Created: latency-svc-mdk8p
    Sep  4 17:31:31.282: INFO: Got endpoints: latency-svc-494tb [747.796931ms]
    Sep  4 17:31:31.299: INFO: Created: latency-svc-44mnr
    Sep  4 17:31:31.334: INFO: Got endpoints: latency-svc-mzrm2 [747.938439ms]
    Sep  4 17:31:31.355: INFO: Created: latency-svc-thz92
    Sep  4 17:31:31.386: INFO: Got endpoints: latency-svc-xttjh [750.560295ms]
    Sep  4 17:31:31.405: INFO: Created: latency-svc-58x4d
    Sep  4 17:31:31.434: INFO: Got endpoints: latency-svc-6bkz8 [748.665382ms]
    Sep  4 17:31:31.452: INFO: Created: latency-svc-m9468
    Sep  4 17:31:31.481: INFO: Got endpoints: latency-svc-mb8w5 [747.464111ms]
    Sep  4 17:31:31.502: INFO: Created: latency-svc-5r9n9
    Sep  4 17:31:31.532: INFO: Got endpoints: latency-svc-5q5kc [748.985601ms]
    Sep  4 17:31:31.553: INFO: Created: latency-svc-wmvhb
    Sep  4 17:31:31.582: INFO: Got endpoints: latency-svc-xx85x [743.466574ms]
    Sep  4 17:31:31.599: INFO: Created: latency-svc-6ctvg
    Sep  4 17:31:31.634: INFO: Got endpoints: latency-svc-8r52t [741.379452ms]
    Sep  4 17:31:31.653: INFO: Created: latency-svc-ppsv4
    Sep  4 17:31:31.687: INFO: Got endpoints: latency-svc-s7jfg [751.617857ms]
    Sep  4 17:31:31.716: INFO: Created: latency-svc-n9smd
    Sep  4 17:31:31.732: INFO: Got endpoints: latency-svc-cqcrf [748.323261ms]
    Sep  4 17:31:31.758: INFO: Created: latency-svc-rgnrh
    Sep  4 17:31:31.788: INFO: Got endpoints: latency-svc-h25g5 [757.08558ms]
    Sep  4 17:31:31.807: INFO: Created: latency-svc-4ctpw
    Sep  4 17:31:31.834: INFO: Got endpoints: latency-svc-jf9k6 [748.419767ms]
    Sep  4 17:31:31.852: INFO: Created: latency-svc-kznzh
    Sep  4 17:31:31.884: INFO: Got endpoints: latency-svc-xx9jj [748.380965ms]
    Sep  4 17:31:31.902: INFO: Created: latency-svc-kdl7p
    Sep  4 17:31:31.932: INFO: Got endpoints: latency-svc-2r458 [746.740668ms]
    Sep  4 17:31:31.954: INFO: Created: latency-svc-mv22x
    Sep  4 17:31:31.984: INFO: Got endpoints: latency-svc-mdk8p [745.165875ms]
    Sep  4 17:31:32.006: INFO: Created: latency-svc-7rc4d
    Sep  4 17:31:32.037: INFO: Got endpoints: latency-svc-44mnr [754.933752ms]
    Sep  4 17:31:32.100: INFO: Got endpoints: latency-svc-thz92 [765.688689ms]
    Sep  4 17:31:32.101: INFO: Created: latency-svc-54q4q
    Sep  4 17:31:32.126: INFO: Created: latency-svc-z79fb
    Sep  4 17:31:32.134: INFO: Got endpoints: latency-svc-58x4d [747.593419ms]
    Sep  4 17:31:32.153: INFO: Created: latency-svc-5c7ns
    Sep  4 17:31:32.184: INFO: Got endpoints: latency-svc-m9468 [749.697643ms]
    Sep  4 17:31:32.205: INFO: Created: latency-svc-mn6hj
    Sep  4 17:31:32.238: INFO: Got endpoints: latency-svc-5r9n9 [756.295834ms]
    Sep  4 17:31:32.266: INFO: Created: latency-svc-6nfbn
    Sep  4 17:31:32.283: INFO: Got endpoints: latency-svc-wmvhb [750.756906ms]
    Sep  4 17:31:32.301: INFO: Created: latency-svc-rgqdq
    Sep  4 17:31:32.335: INFO: Got endpoints: latency-svc-6ctvg [752.51791ms]
    Sep  4 17:31:32.364: INFO: Created: latency-svc-s55ml
    Sep  4 17:31:32.389: INFO: Got endpoints: latency-svc-ppsv4 [754.312516ms]
    Sep  4 17:31:32.447: INFO: Created: latency-svc-mspdw
    Sep  4 17:31:32.473: INFO: Got endpoints: latency-svc-n9smd [786.137898ms]
    Sep  4 17:31:32.489: INFO: Got endpoints: latency-svc-rgnrh [757.77192ms]
    Sep  4 17:31:32.496: INFO: Created: latency-svc-mmgdn
    Sep  4 17:31:32.509: INFO: Created: latency-svc-lrnpp
    Sep  4 17:31:32.534: INFO: Got endpoints: latency-svc-4ctpw [745.555298ms]
    Sep  4 17:31:32.554: INFO: Created: latency-svc-fxxdx
    Sep  4 17:31:32.586: INFO: Got endpoints: latency-svc-kznzh [750.912015ms]
    Sep  4 17:31:32.605: INFO: Created: latency-svc-r654p
    Sep  4 17:31:32.632: INFO: Got endpoints: latency-svc-kdl7p [748.055146ms]
    Sep  4 17:31:32.652: INFO: Created: latency-svc-5zcj4
    Sep  4 17:31:32.685: INFO: Got endpoints: latency-svc-mv22x [752.711322ms]
    Sep  4 17:31:32.706: INFO: Created: latency-svc-4nhts
    Sep  4 17:31:32.746: INFO: Got endpoints: latency-svc-7rc4d [761.781658ms]
    Sep  4 17:31:32.782: INFO: Created: latency-svc-gw5nj
    Sep  4 17:31:32.793: INFO: Got endpoints: latency-svc-54q4q [755.817005ms]
    Sep  4 17:31:32.826: INFO: Created: latency-svc-4mpwg
    Sep  4 17:31:32.850: INFO: Got endpoints: latency-svc-z79fb [748.993202ms]
    Sep  4 17:31:32.878: INFO: Created: latency-svc-2qbrb
    Sep  4 17:31:32.893: INFO: Got endpoints: latency-svc-5c7ns [758.629871ms]
    Sep  4 17:31:32.921: INFO: Created: latency-svc-qn477
    Sep  4 17:31:32.933: INFO: Got endpoints: latency-svc-mn6hj [749.384125ms]
    Sep  4 17:31:32.953: INFO: Created: latency-svc-b4zj7
    Sep  4 17:31:32.982: INFO: Got endpoints: latency-svc-6nfbn [743.904601ms]
    Sep  4 17:31:32.998: INFO: Created: latency-svc-k6knp
    Sep  4 17:31:33.035: INFO: Got endpoints: latency-svc-rgqdq [751.895874ms]
    Sep  4 17:31:33.052: INFO: Created: latency-svc-zz9qx
    Sep  4 17:31:33.088: INFO: Got endpoints: latency-svc-s55ml [752.714625ms]
    Sep  4 17:31:33.113: INFO: Created: latency-svc-2jdwz
    Sep  4 17:31:33.136: INFO: Got endpoints: latency-svc-mspdw [746.315628ms]
    Sep  4 17:31:33.155: INFO: Created: latency-svc-jrh5t
    Sep  4 17:31:33.185: INFO: Got endpoints: latency-svc-mmgdn [711.896261ms]
    Sep  4 17:31:33.207: INFO: Created: latency-svc-5znlf
    Sep  4 17:31:33.234: INFO: Got endpoints: latency-svc-lrnpp [744.385859ms]
    Sep  4 17:31:33.258: INFO: Created: latency-svc-bkpr6
    Sep  4 17:31:33.283: INFO: Got endpoints: latency-svc-fxxdx [748.36287ms]
    Sep  4 17:31:33.301: INFO: Created: latency-svc-59kvk
    Sep  4 17:31:33.342: INFO: Got endpoints: latency-svc-r654p [755.833522ms]
    Sep  4 17:31:33.361: INFO: Created: latency-svc-4kvfk
    Sep  4 17:31:33.393: INFO: Got endpoints: latency-svc-5zcj4 [759.895723ms]
    Sep  4 17:31:33.418: INFO: Created: latency-svc-bh5wk
    Sep  4 17:31:33.433: INFO: Got endpoints: latency-svc-4nhts [747.585731ms]
    Sep  4 17:31:33.451: INFO: Created: latency-svc-j5btm
    Sep  4 17:31:33.486: INFO: Got endpoints: latency-svc-gw5nj [739.861624ms]
    Sep  4 17:31:33.504: INFO: Created: latency-svc-g8kjz
    Sep  4 17:31:33.532: INFO: Got endpoints: latency-svc-4mpwg [738.765852ms]
    Sep  4 17:31:33.552: INFO: Created: latency-svc-ngpgq
    Sep  4 17:31:33.584: INFO: Got endpoints: latency-svc-2qbrb [733.502998ms]
    Sep  4 17:31:33.611: INFO: Created: latency-svc-m4rmz
    Sep  4 17:31:33.634: INFO: Got endpoints: latency-svc-qn477 [737.575105ms]
    Sep  4 17:31:33.651: INFO: Created: latency-svc-wsr5x
    Sep  4 17:31:33.690: INFO: Got endpoints: latency-svc-b4zj7 [756.200238ms]
    Sep  4 17:31:33.714: INFO: Created: latency-svc-vd2pc
    Sep  4 17:31:33.742: INFO: Got endpoints: latency-svc-k6knp [759.825405ms]
    Sep  4 17:31:33.784: INFO: Got endpoints: latency-svc-zz9qx [748.208539ms]
    Sep  4 17:31:33.786: INFO: Created: latency-svc-g4hdx
    Sep  4 17:31:33.805: INFO: Created: latency-svc-tzmbq
    Sep  4 17:31:33.836: INFO: Got endpoints: latency-svc-2jdwz [748.324698ms]
    Sep  4 17:31:33.856: INFO: Created: latency-svc-gtrtd
    Sep  4 17:31:33.882: INFO: Got endpoints: latency-svc-jrh5t [746.107282ms]
    Sep  4 17:31:33.903: INFO: Created: latency-svc-phfbz
    Sep  4 17:31:33.936: INFO: Got endpoints: latency-svc-5znlf [750.293002ms]
    Sep  4 17:31:33.954: INFO: Created: latency-svc-gg74j
    Sep  4 17:31:33.985: INFO: Got endpoints: latency-svc-bkpr6 [750.272601ms]
    Sep  4 17:31:34.006: INFO: Created: latency-svc-fss6z
    Sep  4 17:31:34.033: INFO: Got endpoints: latency-svc-59kvk [749.155843ms]
    Sep  4 17:31:34.057: INFO: Created: latency-svc-xlg8x
    Sep  4 17:31:34.087: INFO: Got endpoints: latency-svc-4kvfk [744.487297ms]
    Sep  4 17:31:34.108: INFO: Created: latency-svc-gfgnm
    Sep  4 17:31:34.136: INFO: Got endpoints: latency-svc-bh5wk [742.277581ms]
    Sep  4 17:31:34.153: INFO: Created: latency-svc-sqsh7
    Sep  4 17:31:34.188: INFO: Got endpoints: latency-svc-j5btm [754.849841ms]
    Sep  4 17:31:34.218: INFO: Created: latency-svc-mnhw7
    Sep  4 17:31:34.236: INFO: Got endpoints: latency-svc-g8kjz [749.973385ms]
    Sep  4 17:31:34.253: INFO: Created: latency-svc-zvp5s
    Sep  4 17:31:34.294: INFO: Got endpoints: latency-svc-ngpgq [761.390785ms]
    Sep  4 17:31:34.312: INFO: Created: latency-svc-qc4p7
    Sep  4 17:31:34.338: INFO: Got endpoints: latency-svc-m4rmz [753.673879ms]
    Sep  4 17:31:34.357: INFO: Created: latency-svc-8dg4l
    Sep  4 17:31:34.382: INFO: Got endpoints: latency-svc-wsr5x [747.00913ms]
    Sep  4 17:31:34.400: INFO: Created: latency-svc-fh84m
    Sep  4 17:31:34.436: INFO: Got endpoints: latency-svc-vd2pc [745.793266ms]
    Sep  4 17:31:34.463: INFO: Created: latency-svc-bx2h2
    Sep  4 17:31:34.485: INFO: Got endpoints: latency-svc-g4hdx [742.307682ms]
    Sep  4 17:31:34.501: INFO: Created: latency-svc-wmvx5
    Sep  4 17:31:34.532: INFO: Got endpoints: latency-svc-tzmbq [747.511856ms]
    Sep  4 17:31:34.551: INFO: Created: latency-svc-hs75d
    Sep  4 17:31:34.581: INFO: Got endpoints: latency-svc-gtrtd [744.627905ms]
    Sep  4 17:31:34.601: INFO: Created: latency-svc-krkw4
    Sep  4 17:31:34.638: INFO: Got endpoints: latency-svc-phfbz [755.678185ms]
    Sep  4 17:31:34.657: INFO: Created: latency-svc-hcnzz
    Sep  4 17:31:34.685: INFO: Got endpoints: latency-svc-gg74j [748.797924ms]
    Sep  4 17:31:34.711: INFO: Created: latency-svc-79ww2
    Sep  4 17:31:34.736: INFO: Got endpoints: latency-svc-fss6z [750.289802ms]
    Sep  4 17:31:34.754: INFO: Created: latency-svc-p4f5t
    Sep  4 17:31:34.782: INFO: Got endpoints: latency-svc-xlg8x [749.319651ms]
    Sep  4 17:31:34.800: INFO: Created: latency-svc-twtjj
    Sep  4 17:31:34.836: INFO: Got endpoints: latency-svc-gfgnm [748.813824ms]
    Sep  4 17:31:34.856: INFO: Created: latency-svc-wzn5f
    Sep  4 17:31:34.883: INFO: Got endpoints: latency-svc-sqsh7 [746.589307ms]
    Sep  4 17:31:34.901: INFO: Created: latency-svc-8s5vd
    Sep  4 17:31:34.936: INFO: Got endpoints: latency-svc-mnhw7 [747.766469ms]
    Sep  4 17:31:34.964: INFO: Created: latency-svc-brfgk
    Sep  4 17:31:34.982: INFO: Got endpoints: latency-svc-zvp5s [746.479902ms]
    Sep  4 17:31:35.000: INFO: Created: latency-svc-gqksf
    Sep  4 17:31:35.032: INFO: Got endpoints: latency-svc-qc4p7 [737.426526ms]
    Sep  4 17:31:35.066: INFO: Created: latency-svc-lrk6w
    Sep  4 17:31:35.081: INFO: Got endpoints: latency-svc-8dg4l [742.433889ms]
    Sep  4 17:31:35.109: INFO: Created: latency-svc-z22hb
    Sep  4 17:31:35.137: INFO: Got endpoints: latency-svc-fh84m [754.619429ms]
    Sep  4 17:31:35.156: INFO: Created: latency-svc-ngdqg
    Sep  4 17:31:35.184: INFO: Got endpoints: latency-svc-bx2h2 [747.97048ms]
    Sep  4 17:31:35.216: INFO: Created: latency-svc-mcn6z
    Sep  4 17:31:35.236: INFO: Got endpoints: latency-svc-wmvx5 [751.632972ms]
    Sep  4 17:31:35.256: INFO: Created: latency-svc-wblq4
    Sep  4 17:31:35.286: INFO: Got endpoints: latency-svc-hs75d [753.459468ms]
    Sep  4 17:31:35.306: INFO: Created: latency-svc-gv52k
    Sep  4 17:31:35.332: INFO: Got endpoints: latency-svc-krkw4 [750.394307ms]
    Sep  4 17:31:35.354: INFO: Created: latency-svc-ff2g7
    Sep  4 17:31:35.393: INFO: Got endpoints: latency-svc-hcnzz [754.275711ms]
    Sep  4 17:31:35.410: INFO: Created: latency-svc-jks9w
    Sep  4 17:31:35.435: INFO: Got endpoints: latency-svc-79ww2 [748.720819ms]
    Sep  4 17:31:35.450: INFO: Created: latency-svc-4rw2l
    Sep  4 17:31:35.483: INFO: Got endpoints: latency-svc-p4f5t [747.536757ms]
    Sep  4 17:31:35.503: INFO: Created: latency-svc-m98fw
    Sep  4 17:31:35.531: INFO: Got endpoints: latency-svc-twtjj [748.111188ms]
    Sep  4 17:31:35.552: INFO: Created: latency-svc-d7g6b
    Sep  4 17:31:35.584: INFO: Got endpoints: latency-svc-wzn5f [747.364748ms]
    Sep  4 17:31:35.602: INFO: Created: latency-svc-4h4cn
    Sep  4 17:31:35.636: INFO: Got endpoints: latency-svc-8s5vd [753.000044ms]
    Sep  4 17:31:35.659: INFO: Created: latency-svc-xf2qc
    Sep  4 17:31:35.686: INFO: Got endpoints: latency-svc-brfgk [749.778875ms]
    Sep  4 17:31:35.713: INFO: Created: latency-svc-nc879
    Sep  4 17:31:35.732: INFO: Got endpoints: latency-svc-gqksf [749.061438ms]
    Sep  4 17:31:35.757: INFO: Created: latency-svc-2ltxq
    Sep  4 17:31:35.782: INFO: Got endpoints: latency-svc-lrk6w [749.937083ms]
    Sep  4 17:31:35.806: INFO: Created: latency-svc-h9q8h
    Sep  4 17:31:35.830: INFO: Got endpoints: latency-svc-z22hb [749.48396ms]
    Sep  4 17:31:35.851: INFO: Created: latency-svc-hp7zh
    Sep  4 17:31:35.887: INFO: Got endpoints: latency-svc-ngdqg [750.021587ms]
    Sep  4 17:31:35.907: INFO: Created: latency-svc-f2kpz
    Sep  4 17:31:35.933: INFO: Got endpoints: latency-svc-mcn6z [748.706619ms]
    Sep  4 17:31:35.951: INFO: Created: latency-svc-fmqhz
    Sep  4 17:31:35.980: INFO: Got endpoints: latency-svc-wblq4 [743.468844ms]
    Sep  4 17:31:35.999: INFO: Created: latency-svc-dk4lv
    Sep  4 17:31:36.037: INFO: Got endpoints: latency-svc-gv52k [750.659321ms]
    Sep  4 17:31:36.086: INFO: Got endpoints: latency-svc-ff2g7 [752.861637ms]
    Sep  4 17:31:36.133: INFO: Got endpoints: latency-svc-jks9w [740.226574ms]
    Sep  4 17:31:36.185: INFO: Got endpoints: latency-svc-4rw2l [750.908935ms]
    Sep  4 17:31:36.239: INFO: Got endpoints: latency-svc-m98fw [755.271363ms]
    Sep  4 17:31:36.287: INFO: Got endpoints: latency-svc-d7g6b [755.309865ms]
    Sep  4 17:31:36.332: INFO: Got endpoints: latency-svc-4h4cn [748.338399ms]
    Sep  4 17:31:36.385: INFO: Got endpoints: latency-svc-xf2qc [748.413504ms]
    Sep  4 17:31:36.433: INFO: Got endpoints: latency-svc-nc879 [746.533704ms]
    Sep  4 17:31:36.487: INFO: Got endpoints: latency-svc-2ltxq [755.083154ms]
    Sep  4 17:31:36.541: INFO: Got endpoints: latency-svc-h9q8h [758.925255ms]
    Sep  4 17:31:36.583: INFO: Got endpoints: latency-svc-hp7zh [752.172301ms]
    Sep  4 17:31:36.651: INFO: Got endpoints: latency-svc-f2kpz [764.008122ms]
    Sep  4 17:31:36.683: INFO: Got endpoints: latency-svc-fmqhz [749.424656ms]
    Sep  4 17:31:36.735: INFO: Got endpoints: latency-svc-dk4lv [755.029351ms]
    Sep  4 17:31:36.735: INFO: Latencies: [27.716339ms 44.599138ms 74.870728ms 77.397378ms 105.173721ms 119.420063ms 131.53478ms 146.323655ms 158.999204ms 186.548134ms 203.642145ms 220.772258ms 238.749821ms 246.631187ms 254.461251ms 255.334802ms 255.435909ms 255.711725ms 256.774787ms 259.065623ms 259.309338ms 261.411261ms 262.142505ms 262.164506ms 262.472925ms 262.664136ms 263.254571ms 263.774201ms 263.921711ms 264.415939ms 264.780762ms 265.663814ms 266.716876ms 268.13026ms 269.418236ms 269.432737ms 269.540942ms 271.060733ms 271.964986ms 272.638926ms 275.488394ms 275.998125ms 277.302101ms 280.305279ms 288.198646ms 299.772131ms 324.044266ms 352.788866ms 393.606581ms 427.723399ms 452.653074ms 489.233637ms 538.64746ms 557.676185ms 572.331952ms 603.125174ms 644.907145ms 668.31483ms 711.896261ms 717.209721ms 733.502998ms 734.553647ms 737.426526ms 737.575105ms 738.765852ms 739.32813ms 739.861624ms 740.226574ms 741.379452ms 742.277581ms 742.307682ms 742.433889ms 743.466574ms 743.468844ms 743.642785ms 743.904601ms 744.385859ms 744.487297ms 744.627905ms 745.165875ms 745.330884ms 745.499795ms 745.555298ms 745.793266ms 746.026526ms 746.041627ms 746.107282ms 746.315628ms 746.479902ms 746.533704ms 746.589307ms 746.740668ms 746.999283ms 747.00913ms 747.364748ms 747.464111ms 747.511856ms 747.536757ms 747.585731ms 747.593419ms 747.723827ms 747.766469ms 747.796931ms 747.938439ms 747.97048ms 748.055146ms 748.111188ms 748.208539ms 748.323261ms 748.324698ms 748.338399ms 748.36287ms 748.380965ms 748.413504ms 748.419767ms 748.485571ms 748.649981ms 748.665382ms 748.670682ms 748.706619ms 748.720819ms 748.738186ms 748.797924ms 748.813824ms 748.971601ms 748.985601ms 748.993202ms 749.061438ms 749.120709ms 749.155843ms 749.319651ms 749.384125ms 749.400325ms 749.424656ms 749.48396ms 749.576536ms 749.697643ms 749.778875ms 749.937083ms 749.973385ms 750.021587ms 750.272601ms 750.289802ms 750.293002ms 750.32568ms 750.394307ms 750.433586ms 750.515491ms 750.560295ms 750.6587ms 750.659321ms 750.756906ms 750.908935ms 750.912015ms 751.291938ms 751.443646ms 751.518251ms 751.617857ms 751.620657ms 751.632972ms 751.895874ms 752.172301ms 752.51791ms 752.711322ms 752.714625ms 752.861637ms 753.000044ms 753.079143ms 753.333358ms 753.383961ms 753.459468ms 753.673879ms 754.275711ms 754.312516ms 754.317416ms 754.619429ms 754.849841ms 754.933752ms 755.029351ms 755.083154ms 755.271363ms 755.303575ms 755.309865ms 755.503186ms 755.678185ms 755.817005ms 755.833522ms 756.200238ms 756.295834ms 757.08558ms 757.77192ms 758.629871ms 758.925255ms 759.825405ms 759.895723ms 761.390785ms 761.781658ms 764.008122ms 765.688689ms 786.137898ms]
    Sep  4 17:31:36.736: INFO: 50 %ile: 747.723827ms
    Sep  4 17:31:36.737: INFO: 90 %ile: 755.271363ms
    Sep  4 17:31:36.737: INFO: 99 %ile: 765.688689ms
    Sep  4 17:31:36.738: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:31:36.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-3911" for this suite. 09/04/23 17:31:36.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:31:36.771
Sep  4 17:31:36.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 17:31:36.772
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:36.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:36.797
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Sep  4 17:31:36.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:31:43.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1279" for this suite. 09/04/23 17:31:43.341
------------------------------
â€¢ [SLOW TEST] [6.584 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:31:36.771
    Sep  4 17:31:36.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 17:31:36.772
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:36.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:36.797
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Sep  4 17:31:36.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:31:43.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1279" for this suite. 09/04/23 17:31:43.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:31:43.364
Sep  4 17:31:43.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename disruption 09/04/23 17:31:43.365
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:43.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:43.412
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 09/04/23 17:31:43.422
STEP: Waiting for the pdb to be processed 09/04/23 17:31:43.431
STEP: First trying to evict a pod which shouldn't be evictable 09/04/23 17:31:43.467
STEP: Waiting for all pods to be running 09/04/23 17:31:43.468
Sep  4 17:31:43.481: INFO: pods: 0 < 3
Sep  4 17:31:45.510: INFO: running pods: 2 < 3
STEP: locating a running pod 09/04/23 17:31:47.493
STEP: Updating the pdb to allow a pod to be evicted 09/04/23 17:31:47.506
STEP: Waiting for the pdb to be processed 09/04/23 17:31:47.529
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/04/23 17:31:47.536
STEP: Waiting for all pods to be running 09/04/23 17:31:47.536
STEP: Waiting for the pdb to observed all healthy pods 09/04/23 17:31:47.545
STEP: Patching the pdb to disallow a pod to be evicted 09/04/23 17:31:47.595
STEP: Waiting for the pdb to be processed 09/04/23 17:31:47.643
STEP: Waiting for all pods to be running 09/04/23 17:31:47.651
Sep  4 17:31:47.674: INFO: running pods: 2 < 3
STEP: locating a running pod 09/04/23 17:31:49.684
STEP: Deleting the pdb to allow a pod to be evicted 09/04/23 17:31:49.701
STEP: Waiting for the pdb to be deleted 09/04/23 17:31:49.715
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/04/23 17:31:49.72
STEP: Waiting for all pods to be running 09/04/23 17:31:49.72
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  4 17:31:49.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1282" for this suite. 09/04/23 17:31:49.796
------------------------------
â€¢ [SLOW TEST] [6.450 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:31:43.364
    Sep  4 17:31:43.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename disruption 09/04/23 17:31:43.365
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:43.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:43.412
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 09/04/23 17:31:43.422
    STEP: Waiting for the pdb to be processed 09/04/23 17:31:43.431
    STEP: First trying to evict a pod which shouldn't be evictable 09/04/23 17:31:43.467
    STEP: Waiting for all pods to be running 09/04/23 17:31:43.468
    Sep  4 17:31:43.481: INFO: pods: 0 < 3
    Sep  4 17:31:45.510: INFO: running pods: 2 < 3
    STEP: locating a running pod 09/04/23 17:31:47.493
    STEP: Updating the pdb to allow a pod to be evicted 09/04/23 17:31:47.506
    STEP: Waiting for the pdb to be processed 09/04/23 17:31:47.529
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/04/23 17:31:47.536
    STEP: Waiting for all pods to be running 09/04/23 17:31:47.536
    STEP: Waiting for the pdb to observed all healthy pods 09/04/23 17:31:47.545
    STEP: Patching the pdb to disallow a pod to be evicted 09/04/23 17:31:47.595
    STEP: Waiting for the pdb to be processed 09/04/23 17:31:47.643
    STEP: Waiting for all pods to be running 09/04/23 17:31:47.651
    Sep  4 17:31:47.674: INFO: running pods: 2 < 3
    STEP: locating a running pod 09/04/23 17:31:49.684
    STEP: Deleting the pdb to allow a pod to be evicted 09/04/23 17:31:49.701
    STEP: Waiting for the pdb to be deleted 09/04/23 17:31:49.715
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/04/23 17:31:49.72
    STEP: Waiting for all pods to be running 09/04/23 17:31:49.72
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:31:49.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1282" for this suite. 09/04/23 17:31:49.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:31:49.824
Sep  4 17:31:49.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename ingress 09/04/23 17:31:49.825
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:49.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:49.848
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 09/04/23 17:31:49.855
STEP: getting /apis/networking.k8s.io 09/04/23 17:31:49.864
STEP: getting /apis/networking.k8s.iov1 09/04/23 17:31:49.869
STEP: creating 09/04/23 17:31:49.874
STEP: getting 09/04/23 17:31:49.91
STEP: listing 09/04/23 17:31:49.917
STEP: watching 09/04/23 17:31:49.924
Sep  4 17:31:49.924: INFO: starting watch
STEP: cluster-wide listing 09/04/23 17:31:49.927
STEP: cluster-wide watching 09/04/23 17:31:49.932
Sep  4 17:31:49.933: INFO: starting watch
STEP: patching 09/04/23 17:31:49.936
STEP: updating 09/04/23 17:31:49.95
Sep  4 17:31:49.970: INFO: waiting for watch events with expected annotations
Sep  4 17:31:49.970: INFO: saw patched and updated annotations
STEP: patching /status 09/04/23 17:31:49.97
STEP: updating /status 09/04/23 17:31:49.983
STEP: get /status 09/04/23 17:31:49.999
STEP: deleting 09/04/23 17:31:50.007
STEP: deleting a collection 09/04/23 17:31:50.031
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Sep  4 17:31:50.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-4444" for this suite. 09/04/23 17:31:50.07
------------------------------
â€¢ [0.256 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:31:49.824
    Sep  4 17:31:49.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename ingress 09/04/23 17:31:49.825
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:49.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:49.848
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 09/04/23 17:31:49.855
    STEP: getting /apis/networking.k8s.io 09/04/23 17:31:49.864
    STEP: getting /apis/networking.k8s.iov1 09/04/23 17:31:49.869
    STEP: creating 09/04/23 17:31:49.874
    STEP: getting 09/04/23 17:31:49.91
    STEP: listing 09/04/23 17:31:49.917
    STEP: watching 09/04/23 17:31:49.924
    Sep  4 17:31:49.924: INFO: starting watch
    STEP: cluster-wide listing 09/04/23 17:31:49.927
    STEP: cluster-wide watching 09/04/23 17:31:49.932
    Sep  4 17:31:49.933: INFO: starting watch
    STEP: patching 09/04/23 17:31:49.936
    STEP: updating 09/04/23 17:31:49.95
    Sep  4 17:31:49.970: INFO: waiting for watch events with expected annotations
    Sep  4 17:31:49.970: INFO: saw patched and updated annotations
    STEP: patching /status 09/04/23 17:31:49.97
    STEP: updating /status 09/04/23 17:31:49.983
    STEP: get /status 09/04/23 17:31:49.999
    STEP: deleting 09/04/23 17:31:50.007
    STEP: deleting a collection 09/04/23 17:31:50.031
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:31:50.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-4444" for this suite. 09/04/23 17:31:50.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:31:50.084
Sep  4 17:31:50.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 17:31:50.085
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:50.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:50.116
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 09/04/23 17:31:50.123
Sep  4 17:31:50.138: INFO: Waiting up to 5m0s for pod "downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39" in namespace "projected-9995" to be "Succeeded or Failed"
Sep  4 17:31:50.149: INFO: Pod "downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.36811ms
Sep  4 17:31:52.158: INFO: Pod "downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019909394s
Sep  4 17:31:54.156: INFO: Pod "downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017365766s
STEP: Saw pod success 09/04/23 17:31:54.156
Sep  4 17:31:54.156: INFO: Pod "downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39" satisfied condition "Succeeded or Failed"
Sep  4 17:31:54.164: INFO: Trying to get logs from node tenant-000003 pod downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39 container client-container: <nil>
STEP: delete the pod 09/04/23 17:31:54.18
Sep  4 17:31:54.199: INFO: Waiting for pod downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39 to disappear
Sep  4 17:31:54.205: INFO: Pod downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 17:31:54.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9995" for this suite. 09/04/23 17:31:54.211
------------------------------
â€¢ [4.144 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:31:50.084
    Sep  4 17:31:50.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 17:31:50.085
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:50.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:50.116
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 09/04/23 17:31:50.123
    Sep  4 17:31:50.138: INFO: Waiting up to 5m0s for pod "downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39" in namespace "projected-9995" to be "Succeeded or Failed"
    Sep  4 17:31:50.149: INFO: Pod "downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.36811ms
    Sep  4 17:31:52.158: INFO: Pod "downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019909394s
    Sep  4 17:31:54.156: INFO: Pod "downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017365766s
    STEP: Saw pod success 09/04/23 17:31:54.156
    Sep  4 17:31:54.156: INFO: Pod "downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39" satisfied condition "Succeeded or Failed"
    Sep  4 17:31:54.164: INFO: Trying to get logs from node tenant-000003 pod downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39 container client-container: <nil>
    STEP: delete the pod 09/04/23 17:31:54.18
    Sep  4 17:31:54.199: INFO: Waiting for pod downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39 to disappear
    Sep  4 17:31:54.205: INFO: Pod downwardapi-volume-367828c1-292b-447d-a90f-7db04eb09e39 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:31:54.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9995" for this suite. 09/04/23 17:31:54.211
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:31:54.232
Sep  4 17:31:54.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 17:31:54.234
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:54.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:54.265
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 17:31:54.304
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:31:54.723
STEP: Deploying the webhook pod 09/04/23 17:31:54.736
STEP: Wait for the deployment to be ready 09/04/23 17:31:54.76
Sep  4 17:31:54.785: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 17:31:56.807
STEP: Verifying the service has paired with the endpoint 09/04/23 17:31:56.836
Sep  4 17:31:57.836: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 09/04/23 17:31:57.843
STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 17:31:57.875
STEP: Updating a validating webhook configuration's rules to not include the create operation 09/04/23 17:31:57.894
STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 17:31:57.912
STEP: Patching a validating webhook configuration's rules to include the create operation 09/04/23 17:31:57.934
STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 17:31:57.945
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:31:57.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5186" for this suite. 09/04/23 17:31:58.057
STEP: Destroying namespace "webhook-5186-markers" for this suite. 09/04/23 17:31:58.069
------------------------------
â€¢ [3.848 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:31:54.232
    Sep  4 17:31:54.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 17:31:54.234
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:54.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:54.265
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 17:31:54.304
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:31:54.723
    STEP: Deploying the webhook pod 09/04/23 17:31:54.736
    STEP: Wait for the deployment to be ready 09/04/23 17:31:54.76
    Sep  4 17:31:54.785: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 17:31:56.807
    STEP: Verifying the service has paired with the endpoint 09/04/23 17:31:56.836
    Sep  4 17:31:57.836: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 09/04/23 17:31:57.843
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 17:31:57.875
    STEP: Updating a validating webhook configuration's rules to not include the create operation 09/04/23 17:31:57.894
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 17:31:57.912
    STEP: Patching a validating webhook configuration's rules to include the create operation 09/04/23 17:31:57.934
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 17:31:57.945
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:31:57.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5186" for this suite. 09/04/23 17:31:58.057
    STEP: Destroying namespace "webhook-5186-markers" for this suite. 09/04/23 17:31:58.069
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:31:58.083
Sep  4 17:31:58.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pod-network-test 09/04/23 17:31:58.086
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:58.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:58.11
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-547 09/04/23 17:31:58.118
STEP: creating a selector 09/04/23 17:31:58.119
STEP: Creating the service pods in kubernetes 09/04/23 17:31:58.119
Sep  4 17:31:58.119: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  4 17:31:58.161: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-547" to be "running and ready"
Sep  4 17:31:58.172: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.333881ms
Sep  4 17:31:58.172: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:32:00.178: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016383068s
Sep  4 17:32:00.178: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 17:32:02.180: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01843583s
Sep  4 17:32:02.180: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 17:32:04.179: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017576328s
Sep  4 17:32:04.180: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 17:32:06.178: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016843231s
Sep  4 17:32:06.179: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 17:32:08.179: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017464394s
Sep  4 17:32:08.179: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 17:32:10.180: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.018276468s
Sep  4 17:32:10.180: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  4 17:32:10.180: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  4 17:32:10.188: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-547" to be "running and ready"
Sep  4 17:32:10.193: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 5.249896ms
Sep  4 17:32:10.194: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  4 17:32:12.201: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.012618139s
Sep  4 17:32:12.201: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  4 17:32:14.204: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.01550253s
Sep  4 17:32:14.204: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  4 17:32:16.202: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.013990996s
Sep  4 17:32:16.202: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  4 17:32:18.203: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.014351058s
Sep  4 17:32:18.203: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep  4 17:32:20.203: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.014991621s
Sep  4 17:32:20.203: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  4 17:32:20.204: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/04/23 17:32:20.209
Sep  4 17:32:20.219: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-547" to be "running"
Sep  4 17:32:20.229: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.470454ms
Sep  4 17:32:22.238: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018275391s
Sep  4 17:32:22.238: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  4 17:32:22.243: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  4 17:32:22.243: INFO: Breadth first check of 10.36.55.108 on host 10.225.0.5...
Sep  4 17:32:22.248: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.36.217.219:9080/dial?request=hostname&protocol=udp&host=10.36.55.108&port=8081&tries=1'] Namespace:pod-network-test-547 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:32:22.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:32:22.249: INFO: ExecWithOptions: Clientset creation
Sep  4 17:32:22.249: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-547/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.36.217.219%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.36.55.108%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  4 17:32:22.358: INFO: Waiting for responses: map[]
Sep  4 17:32:22.358: INFO: reached 10.36.55.108 after 0/1 tries
Sep  4 17:32:22.359: INFO: Breadth first check of 10.36.217.237 on host 10.225.0.7...
Sep  4 17:32:22.365: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.36.217.219:9080/dial?request=hostname&protocol=udp&host=10.36.217.237&port=8081&tries=1'] Namespace:pod-network-test-547 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:32:22.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:32:22.365: INFO: ExecWithOptions: Clientset creation
Sep  4 17:32:22.366: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-547/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.36.217.219%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.36.217.237%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  4 17:32:22.473: INFO: Waiting for responses: map[]
Sep  4 17:32:22.473: INFO: reached 10.36.217.237 after 0/1 tries
Sep  4 17:32:22.473: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  4 17:32:22.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-547" for this suite. 09/04/23 17:32:22.48
------------------------------
â€¢ [SLOW TEST] [24.407 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:31:58.083
    Sep  4 17:31:58.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pod-network-test 09/04/23 17:31:58.086
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:31:58.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:31:58.11
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-547 09/04/23 17:31:58.118
    STEP: creating a selector 09/04/23 17:31:58.119
    STEP: Creating the service pods in kubernetes 09/04/23 17:31:58.119
    Sep  4 17:31:58.119: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  4 17:31:58.161: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-547" to be "running and ready"
    Sep  4 17:31:58.172: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.333881ms
    Sep  4 17:31:58.172: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:32:00.178: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016383068s
    Sep  4 17:32:00.178: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 17:32:02.180: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01843583s
    Sep  4 17:32:02.180: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 17:32:04.179: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017576328s
    Sep  4 17:32:04.180: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 17:32:06.178: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016843231s
    Sep  4 17:32:06.179: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 17:32:08.179: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017464394s
    Sep  4 17:32:08.179: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 17:32:10.180: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.018276468s
    Sep  4 17:32:10.180: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  4 17:32:10.180: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  4 17:32:10.188: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-547" to be "running and ready"
    Sep  4 17:32:10.193: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 5.249896ms
    Sep  4 17:32:10.194: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  4 17:32:12.201: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.012618139s
    Sep  4 17:32:12.201: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  4 17:32:14.204: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.01550253s
    Sep  4 17:32:14.204: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  4 17:32:16.202: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.013990996s
    Sep  4 17:32:16.202: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  4 17:32:18.203: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.014351058s
    Sep  4 17:32:18.203: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep  4 17:32:20.203: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.014991621s
    Sep  4 17:32:20.203: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  4 17:32:20.204: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/04/23 17:32:20.209
    Sep  4 17:32:20.219: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-547" to be "running"
    Sep  4 17:32:20.229: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.470454ms
    Sep  4 17:32:22.238: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018275391s
    Sep  4 17:32:22.238: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  4 17:32:22.243: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  4 17:32:22.243: INFO: Breadth first check of 10.36.55.108 on host 10.225.0.5...
    Sep  4 17:32:22.248: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.36.217.219:9080/dial?request=hostname&protocol=udp&host=10.36.55.108&port=8081&tries=1'] Namespace:pod-network-test-547 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:32:22.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:32:22.249: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:32:22.249: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-547/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.36.217.219%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.36.55.108%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  4 17:32:22.358: INFO: Waiting for responses: map[]
    Sep  4 17:32:22.358: INFO: reached 10.36.55.108 after 0/1 tries
    Sep  4 17:32:22.359: INFO: Breadth first check of 10.36.217.237 on host 10.225.0.7...
    Sep  4 17:32:22.365: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.36.217.219:9080/dial?request=hostname&protocol=udp&host=10.36.217.237&port=8081&tries=1'] Namespace:pod-network-test-547 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:32:22.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:32:22.365: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:32:22.366: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-547/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.36.217.219%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.36.217.237%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  4 17:32:22.473: INFO: Waiting for responses: map[]
    Sep  4 17:32:22.473: INFO: reached 10.36.217.237 after 0/1 tries
    Sep  4 17:32:22.473: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:32:22.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-547" for this suite. 09/04/23 17:32:22.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:32:22.506
Sep  4 17:32:22.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 17:32:22.507
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:32:22.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:32:22.535
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-ca95274f-5ff7-4dd7-93fa-51a4696969d8 09/04/23 17:32:22.543
STEP: Creating a pod to test consume secrets 09/04/23 17:32:22.556
Sep  4 17:32:22.568: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f" in namespace "projected-9442" to be "Succeeded or Failed"
Sep  4 17:32:22.577: INFO: Pod "pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.982422ms
Sep  4 17:32:24.586: INFO: Pod "pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017625365s
Sep  4 17:32:26.584: INFO: Pod "pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015648889s
STEP: Saw pod success 09/04/23 17:32:26.584
Sep  4 17:32:26.584: INFO: Pod "pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f" satisfied condition "Succeeded or Failed"
Sep  4 17:32:26.592: INFO: Trying to get logs from node tenant-000001 pod pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f container projected-secret-volume-test: <nil>
STEP: delete the pod 09/04/23 17:32:26.647
Sep  4 17:32:26.666: INFO: Waiting for pod pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f to disappear
Sep  4 17:32:26.671: INFO: Pod pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  4 17:32:26.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9442" for this suite. 09/04/23 17:32:26.68
------------------------------
â€¢ [4.183 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:32:22.506
    Sep  4 17:32:22.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 17:32:22.507
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:32:22.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:32:22.535
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-ca95274f-5ff7-4dd7-93fa-51a4696969d8 09/04/23 17:32:22.543
    STEP: Creating a pod to test consume secrets 09/04/23 17:32:22.556
    Sep  4 17:32:22.568: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f" in namespace "projected-9442" to be "Succeeded or Failed"
    Sep  4 17:32:22.577: INFO: Pod "pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.982422ms
    Sep  4 17:32:24.586: INFO: Pod "pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017625365s
    Sep  4 17:32:26.584: INFO: Pod "pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015648889s
    STEP: Saw pod success 09/04/23 17:32:26.584
    Sep  4 17:32:26.584: INFO: Pod "pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f" satisfied condition "Succeeded or Failed"
    Sep  4 17:32:26.592: INFO: Trying to get logs from node tenant-000001 pod pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 17:32:26.647
    Sep  4 17:32:26.666: INFO: Waiting for pod pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f to disappear
    Sep  4 17:32:26.671: INFO: Pod pod-projected-secrets-c64a841d-7568-4f10-a4a1-512a4995440f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:32:26.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9442" for this suite. 09/04/23 17:32:26.68
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:32:26.692
Sep  4 17:32:26.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename csistoragecapacity 09/04/23 17:32:26.695
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:32:26.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:32:26.721
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 09/04/23 17:32:26.733
STEP: getting /apis/storage.k8s.io 09/04/23 17:32:26.74
STEP: getting /apis/storage.k8s.io/v1 09/04/23 17:32:26.743
STEP: creating 09/04/23 17:32:26.747
STEP: watching 09/04/23 17:32:26.779
Sep  4 17:32:26.779: INFO: starting watch
STEP: getting 09/04/23 17:32:26.792
STEP: listing in namespace 09/04/23 17:32:26.797
STEP: listing across namespaces 09/04/23 17:32:26.805
STEP: patching 09/04/23 17:32:26.809
STEP: updating 09/04/23 17:32:26.819
Sep  4 17:32:26.831: INFO: waiting for watch events with expected annotations in namespace
Sep  4 17:32:26.831: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 09/04/23 17:32:26.831
STEP: deleting a collection 09/04/23 17:32:26.851
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Sep  4 17:32:26.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-6341" for this suite. 09/04/23 17:32:26.884
------------------------------
â€¢ [0.201 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:32:26.692
    Sep  4 17:32:26.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename csistoragecapacity 09/04/23 17:32:26.695
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:32:26.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:32:26.721
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 09/04/23 17:32:26.733
    STEP: getting /apis/storage.k8s.io 09/04/23 17:32:26.74
    STEP: getting /apis/storage.k8s.io/v1 09/04/23 17:32:26.743
    STEP: creating 09/04/23 17:32:26.747
    STEP: watching 09/04/23 17:32:26.779
    Sep  4 17:32:26.779: INFO: starting watch
    STEP: getting 09/04/23 17:32:26.792
    STEP: listing in namespace 09/04/23 17:32:26.797
    STEP: listing across namespaces 09/04/23 17:32:26.805
    STEP: patching 09/04/23 17:32:26.809
    STEP: updating 09/04/23 17:32:26.819
    Sep  4 17:32:26.831: INFO: waiting for watch events with expected annotations in namespace
    Sep  4 17:32:26.831: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 09/04/23 17:32:26.831
    STEP: deleting a collection 09/04/23 17:32:26.851
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:32:26.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-6341" for this suite. 09/04/23 17:32:26.884
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:32:26.893
Sep  4 17:32:26.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename cronjob 09/04/23 17:32:26.894
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:32:26.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:32:26.927
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 09/04/23 17:32:26.935
STEP: Ensuring a job is scheduled 09/04/23 17:32:26.95
STEP: Ensuring exactly one is scheduled 09/04/23 17:33:00.958
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/04/23 17:33:00.971
STEP: Ensuring no more jobs are scheduled 09/04/23 17:33:00.977
STEP: Removing cronjob 09/04/23 17:38:00.993
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  4 17:38:01.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4480" for this suite. 09/04/23 17:38:01.013
------------------------------
â€¢ [SLOW TEST] [334.145 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:32:26.893
    Sep  4 17:32:26.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename cronjob 09/04/23 17:32:26.894
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:32:26.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:32:26.927
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 09/04/23 17:32:26.935
    STEP: Ensuring a job is scheduled 09/04/23 17:32:26.95
    STEP: Ensuring exactly one is scheduled 09/04/23 17:33:00.958
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/04/23 17:33:00.971
    STEP: Ensuring no more jobs are scheduled 09/04/23 17:33:00.977
    STEP: Removing cronjob 09/04/23 17:38:00.993
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:38:01.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4480" for this suite. 09/04/23 17:38:01.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:38:01.115
Sep  4 17:38:01.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename daemonsets 09/04/23 17:38:01.141
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:38:01.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:38:01.181
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 09/04/23 17:38:01.214
STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 17:38:01.223
Sep  4 17:38:01.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 17:38:01.240: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 17:38:02.251: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 17:38:02.251: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 17:38:03.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 17:38:03.252: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 09/04/23 17:38:03.26
STEP: DeleteCollection of the DaemonSets 09/04/23 17:38:03.266
STEP: Verify that ReplicaSets have been deleted 09/04/23 17:38:03.281
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Sep  4 17:38:03.331: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10885"},"items":null}

Sep  4 17:38:03.337: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10885"},"items":[{"metadata":{"name":"daemon-set-r5qb2","generateName":"daemon-set-","namespace":"daemonsets-8824","uid":"1d2afd06-7665-48e9-a87a-53e3658ee8d7","resourceVersion":"10884","creationTimestamp":"2023-09-04T17:38:01Z","deletionTimestamp":"2023-09-04T17:38:33Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c803be41e7331b363a0156ab92859f7577041921325a25df84b8ff5f3da19f6f","cni.projectcalico.org/podIP":"10.36.217.236/32","cni.projectcalico.org/podIPs":"10.36.217.236/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a09bf9ac-2c75-48e5-ad0f-8c00888d4187","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-04T17:38:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-04T17:38:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a09bf9ac-2c75-48e5-ad0f-8c00888d4187\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-04T17:38:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-brwts","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-brwts","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"tenant-000003","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["tenant-000003"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:01Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:02Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:02Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:01Z"}],"hostIP":"10.225.0.7","podIP":"10.36.217.236","podIPs":[{"ip":"10.36.217.236"}],"startTime":"2023-09-04T17:38:01Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-04T17:38:02Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://8fabe9351e832cf76d5c14d941085fbdcef610b09ea35b14291f84e9a41d8661","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tfl8h","generateName":"daemon-set-","namespace":"daemonsets-8824","uid":"1888e9e8-0c95-42b8-9f1e-8ae4c9e0f657","resourceVersion":"10882","creationTimestamp":"2023-09-04T17:38:01Z","deletionTimestamp":"2023-09-04T17:38:33Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"1087c84b3363ea4f7cc31d27fd34541e2846d2341666dfa84aacaa6eed6f7f82","cni.projectcalico.org/podIP":"10.36.55.84/32","cni.projectcalico.org/podIPs":"10.36.55.84/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a09bf9ac-2c75-48e5-ad0f-8c00888d4187","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-04T17:38:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-04T17:38:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a09bf9ac-2c75-48e5-ad0f-8c00888d4187\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-04T17:38:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xwr9p","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xwr9p","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"tenant-000001","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["tenant-000001"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:01Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:02Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:02Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:01Z"}],"hostIP":"10.225.0.5","podIP":"10.36.55.84","podIPs":[{"ip":"10.36.55.84"}],"startTime":"2023-09-04T17:38:01Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-04T17:38:02Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4e3b91b00871697d59016d79b63dd576cfb013b55ec1fb0078149628a92a6a56","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:38:03.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8824" for this suite. 09/04/23 17:38:03.369
------------------------------
â€¢ [2.271 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:38:01.115
    Sep  4 17:38:01.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename daemonsets 09/04/23 17:38:01.141
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:38:01.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:38:01.181
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 09/04/23 17:38:01.214
    STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 17:38:01.223
    Sep  4 17:38:01.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 17:38:01.240: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 17:38:02.251: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 17:38:02.251: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 17:38:03.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 17:38:03.252: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 09/04/23 17:38:03.26
    STEP: DeleteCollection of the DaemonSets 09/04/23 17:38:03.266
    STEP: Verify that ReplicaSets have been deleted 09/04/23 17:38:03.281
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Sep  4 17:38:03.331: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10885"},"items":null}

    Sep  4 17:38:03.337: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10885"},"items":[{"metadata":{"name":"daemon-set-r5qb2","generateName":"daemon-set-","namespace":"daemonsets-8824","uid":"1d2afd06-7665-48e9-a87a-53e3658ee8d7","resourceVersion":"10884","creationTimestamp":"2023-09-04T17:38:01Z","deletionTimestamp":"2023-09-04T17:38:33Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c803be41e7331b363a0156ab92859f7577041921325a25df84b8ff5f3da19f6f","cni.projectcalico.org/podIP":"10.36.217.236/32","cni.projectcalico.org/podIPs":"10.36.217.236/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a09bf9ac-2c75-48e5-ad0f-8c00888d4187","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-04T17:38:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-04T17:38:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a09bf9ac-2c75-48e5-ad0f-8c00888d4187\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-04T17:38:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-brwts","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-brwts","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"tenant-000003","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["tenant-000003"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:01Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:02Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:02Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:01Z"}],"hostIP":"10.225.0.7","podIP":"10.36.217.236","podIPs":[{"ip":"10.36.217.236"}],"startTime":"2023-09-04T17:38:01Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-04T17:38:02Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://8fabe9351e832cf76d5c14d941085fbdcef610b09ea35b14291f84e9a41d8661","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tfl8h","generateName":"daemon-set-","namespace":"daemonsets-8824","uid":"1888e9e8-0c95-42b8-9f1e-8ae4c9e0f657","resourceVersion":"10882","creationTimestamp":"2023-09-04T17:38:01Z","deletionTimestamp":"2023-09-04T17:38:33Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"1087c84b3363ea4f7cc31d27fd34541e2846d2341666dfa84aacaa6eed6f7f82","cni.projectcalico.org/podIP":"10.36.55.84/32","cni.projectcalico.org/podIPs":"10.36.55.84/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a09bf9ac-2c75-48e5-ad0f-8c00888d4187","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-04T17:38:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-04T17:38:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a09bf9ac-2c75-48e5-ad0f-8c00888d4187\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-04T17:38:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xwr9p","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xwr9p","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"tenant-000001","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["tenant-000001"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:01Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:02Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:02Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T17:38:01Z"}],"hostIP":"10.225.0.5","podIP":"10.36.55.84","podIPs":[{"ip":"10.36.55.84"}],"startTime":"2023-09-04T17:38:01Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-04T17:38:02Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4e3b91b00871697d59016d79b63dd576cfb013b55ec1fb0078149628a92a6a56","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:38:03.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8824" for this suite. 09/04/23 17:38:03.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:38:03.391
Sep  4 17:38:03.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 17:38:03.393
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:38:03.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:38:03.421
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 17:38:03.449
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:38:03.868
STEP: Deploying the webhook pod 09/04/23 17:38:03.884
STEP: Wait for the deployment to be ready 09/04/23 17:38:03.904
Sep  4 17:38:03.924: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 17:38:05.945
STEP: Verifying the service has paired with the endpoint 09/04/23 17:38:05.968
Sep  4 17:38:06.969: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 09/04/23 17:38:06.975
STEP: create a pod that should be denied by the webhook 09/04/23 17:38:07.008
STEP: create a pod that causes the webhook to hang 09/04/23 17:38:07.03
STEP: create a configmap that should be denied by the webhook 09/04/23 17:38:17.047
STEP: create a configmap that should be admitted by the webhook 09/04/23 17:38:17.072
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/04/23 17:38:17.092
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/04/23 17:38:17.111
STEP: create a namespace that bypass the webhook 09/04/23 17:38:17.122
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/04/23 17:38:17.132
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:38:17.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1160" for this suite. 09/04/23 17:38:17.302
STEP: Destroying namespace "webhook-1160-markers" for this suite. 09/04/23 17:38:17.338
------------------------------
â€¢ [SLOW TEST] [13.963 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:38:03.391
    Sep  4 17:38:03.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 17:38:03.393
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:38:03.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:38:03.421
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 17:38:03.449
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:38:03.868
    STEP: Deploying the webhook pod 09/04/23 17:38:03.884
    STEP: Wait for the deployment to be ready 09/04/23 17:38:03.904
    Sep  4 17:38:03.924: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 17:38:05.945
    STEP: Verifying the service has paired with the endpoint 09/04/23 17:38:05.968
    Sep  4 17:38:06.969: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 09/04/23 17:38:06.975
    STEP: create a pod that should be denied by the webhook 09/04/23 17:38:07.008
    STEP: create a pod that causes the webhook to hang 09/04/23 17:38:07.03
    STEP: create a configmap that should be denied by the webhook 09/04/23 17:38:17.047
    STEP: create a configmap that should be admitted by the webhook 09/04/23 17:38:17.072
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/04/23 17:38:17.092
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/04/23 17:38:17.111
    STEP: create a namespace that bypass the webhook 09/04/23 17:38:17.122
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/04/23 17:38:17.132
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:38:17.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1160" for this suite. 09/04/23 17:38:17.302
    STEP: Destroying namespace "webhook-1160-markers" for this suite. 09/04/23 17:38:17.338
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:38:17.355
Sep  4 17:38:17.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pods 09/04/23 17:38:17.356
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:38:17.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:38:17.394
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 09/04/23 17:38:17.403
Sep  4 17:38:17.417: INFO: created test-pod-1
Sep  4 17:38:17.429: INFO: created test-pod-2
Sep  4 17:38:17.439: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 09/04/23 17:38:17.439
Sep  4 17:38:17.439: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4057' to be running and ready
Sep  4 17:38:17.466: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  4 17:38:17.466: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  4 17:38:17.466: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  4 17:38:17.466: INFO: 0 / 3 pods in namespace 'pods-4057' are running and ready (0 seconds elapsed)
Sep  4 17:38:17.466: INFO: expected 0 pod replicas in namespace 'pods-4057', 0 are Running and Ready.
Sep  4 17:38:17.466: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Sep  4 17:38:17.466: INFO: test-pod-1  tenant-000001  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:38:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:38:17 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:38:17 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:38:17 +0000 UTC  }]
Sep  4 17:38:17.466: INFO: test-pod-2  tenant-000001  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:38:17 +0000 UTC  }]
Sep  4 17:38:17.466: INFO: test-pod-3  tenant-000003  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:38:17 +0000 UTC  }]
Sep  4 17:38:17.466: INFO: 
Sep  4 17:38:19.481: INFO: 3 / 3 pods in namespace 'pods-4057' are running and ready (2 seconds elapsed)
Sep  4 17:38:19.482: INFO: expected 0 pod replicas in namespace 'pods-4057', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 09/04/23 17:38:19.53
Sep  4 17:38:19.536: INFO: Pod quantity 3 is different from expected quantity 0
Sep  4 17:38:20.543: INFO: Pod quantity 3 is different from expected quantity 0
Sep  4 17:38:21.543: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 17:38:22.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4057" for this suite. 09/04/23 17:38:22.552
------------------------------
â€¢ [SLOW TEST] [5.209 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:38:17.355
    Sep  4 17:38:17.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pods 09/04/23 17:38:17.356
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:38:17.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:38:17.394
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 09/04/23 17:38:17.403
    Sep  4 17:38:17.417: INFO: created test-pod-1
    Sep  4 17:38:17.429: INFO: created test-pod-2
    Sep  4 17:38:17.439: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 09/04/23 17:38:17.439
    Sep  4 17:38:17.439: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4057' to be running and ready
    Sep  4 17:38:17.466: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  4 17:38:17.466: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  4 17:38:17.466: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  4 17:38:17.466: INFO: 0 / 3 pods in namespace 'pods-4057' are running and ready (0 seconds elapsed)
    Sep  4 17:38:17.466: INFO: expected 0 pod replicas in namespace 'pods-4057', 0 are Running and Ready.
    Sep  4 17:38:17.466: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
    Sep  4 17:38:17.466: INFO: test-pod-1  tenant-000001  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:38:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:38:17 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:38:17 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:38:17 +0000 UTC  }]
    Sep  4 17:38:17.466: INFO: test-pod-2  tenant-000001  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:38:17 +0000 UTC  }]
    Sep  4 17:38:17.466: INFO: test-pod-3  tenant-000003  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 17:38:17 +0000 UTC  }]
    Sep  4 17:38:17.466: INFO: 
    Sep  4 17:38:19.481: INFO: 3 / 3 pods in namespace 'pods-4057' are running and ready (2 seconds elapsed)
    Sep  4 17:38:19.482: INFO: expected 0 pod replicas in namespace 'pods-4057', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 09/04/23 17:38:19.53
    Sep  4 17:38:19.536: INFO: Pod quantity 3 is different from expected quantity 0
    Sep  4 17:38:20.543: INFO: Pod quantity 3 is different from expected quantity 0
    Sep  4 17:38:21.543: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:38:22.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4057" for this suite. 09/04/23 17:38:22.552
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:38:22.567
Sep  4 17:38:22.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename deployment 09/04/23 17:38:22.569
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:38:22.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:38:22.598
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Sep  4 17:38:22.605: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  4 17:38:22.622: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  4 17:38:27.631: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/04/23 17:38:27.632
Sep  4 17:38:27.632: INFO: Creating deployment "test-rolling-update-deployment"
Sep  4 17:38:27.642: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  4 17:38:27.657: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  4 17:38:29.673: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  4 17:38:29.679: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 17:38:29.698: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3659  bd5e74b7-a6d6-4a90-a522-38c7740f86a0 11187 1 2023-09-04 17:38:27 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-09-04 17:38:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:38:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00035f418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-04 17:38:27 +0000 UTC,LastTransitionTime:2023-09-04 17:38:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-09-04 17:38:28 +0000 UTC,LastTransitionTime:2023-09-04 17:38:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  4 17:38:29.706: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-3659  4b40e78f-a751-4bf8-8711-821e047fa4dc 11177 1 2023-09-04 17:38:27 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment bd5e74b7-a6d6-4a90-a522-38c7740f86a0 0xc001380857 0xc001380858}] [] [{kube-controller-manager Update apps/v1 2023-09-04 17:38:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd5e74b7-a6d6-4a90-a522-38c7740f86a0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:38:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0013817f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  4 17:38:29.706: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  4 17:38:29.707: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3659  dffb6cbf-d9cf-49a8-b274-631b8016020a 11186 2 2023-09-04 17:38:22 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment bd5e74b7-a6d6-4a90-a522-38c7740f86a0 0xc001380387 0xc001380388}] [] [{e2e.test Update apps/v1 2023-09-04 17:38:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:38:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd5e74b7-a6d6-4a90-a522-38c7740f86a0\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:38:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001380548 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  4 17:38:29.713: INFO: Pod "test-rolling-update-deployment-7549d9f46d-qqndp" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-qqndp test-rolling-update-deployment-7549d9f46d- deployment-3659  ebaa285c-13b3-4725-b835-20c9549c56c6 11176 0 2023-09-04 17:38:27 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:5c282714275f9225fbffc9bf7f12fcb23f258e0a5bcee8a888ee0b0a22db56e9 cni.projectcalico.org/podIP:10.36.55.99/32 cni.projectcalico.org/podIPs:10.36.55.99/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 4b40e78f-a751-4bf8-8711-821e047fa4dc 0xc000c4e4b7 0xc000c4e4b8}] [] [{kube-controller-manager Update v1 2023-09-04 17:38:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b40e78f-a751-4bf8-8711-821e047fa4dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:38:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:38:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgx9h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgx9h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:38:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:38:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:38:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:38:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.99,StartTime:2023-09-04 17:38:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:38:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://1e6dfcf57793e87f2b9006808d7f128c2d5774a145f966abf5dc7da783c5ea5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 17:38:29.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3659" for this suite. 09/04/23 17:38:29.719
------------------------------
â€¢ [SLOW TEST] [7.165 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:38:22.567
    Sep  4 17:38:22.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename deployment 09/04/23 17:38:22.569
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:38:22.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:38:22.598
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Sep  4 17:38:22.605: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Sep  4 17:38:22.622: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  4 17:38:27.631: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/04/23 17:38:27.632
    Sep  4 17:38:27.632: INFO: Creating deployment "test-rolling-update-deployment"
    Sep  4 17:38:27.642: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Sep  4 17:38:27.657: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Sep  4 17:38:29.673: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Sep  4 17:38:29.679: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 17:38:29.698: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3659  bd5e74b7-a6d6-4a90-a522-38c7740f86a0 11187 1 2023-09-04 17:38:27 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-09-04 17:38:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:38:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00035f418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-04 17:38:27 +0000 UTC,LastTransitionTime:2023-09-04 17:38:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-09-04 17:38:28 +0000 UTC,LastTransitionTime:2023-09-04 17:38:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  4 17:38:29.706: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-3659  4b40e78f-a751-4bf8-8711-821e047fa4dc 11177 1 2023-09-04 17:38:27 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment bd5e74b7-a6d6-4a90-a522-38c7740f86a0 0xc001380857 0xc001380858}] [] [{kube-controller-manager Update apps/v1 2023-09-04 17:38:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd5e74b7-a6d6-4a90-a522-38c7740f86a0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:38:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0013817f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 17:38:29.706: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Sep  4 17:38:29.707: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3659  dffb6cbf-d9cf-49a8-b274-631b8016020a 11186 2 2023-09-04 17:38:22 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment bd5e74b7-a6d6-4a90-a522-38c7740f86a0 0xc001380387 0xc001380388}] [] [{e2e.test Update apps/v1 2023-09-04 17:38:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:38:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd5e74b7-a6d6-4a90-a522-38c7740f86a0\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:38:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001380548 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 17:38:29.713: INFO: Pod "test-rolling-update-deployment-7549d9f46d-qqndp" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-qqndp test-rolling-update-deployment-7549d9f46d- deployment-3659  ebaa285c-13b3-4725-b835-20c9549c56c6 11176 0 2023-09-04 17:38:27 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:5c282714275f9225fbffc9bf7f12fcb23f258e0a5bcee8a888ee0b0a22db56e9 cni.projectcalico.org/podIP:10.36.55.99/32 cni.projectcalico.org/podIPs:10.36.55.99/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 4b40e78f-a751-4bf8-8711-821e047fa4dc 0xc000c4e4b7 0xc000c4e4b8}] [] [{kube-controller-manager Update v1 2023-09-04 17:38:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b40e78f-a751-4bf8-8711-821e047fa4dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:38:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:38:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgx9h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgx9h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:38:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:38:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:38:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:38:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.99,StartTime:2023-09-04 17:38:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:38:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://1e6dfcf57793e87f2b9006808d7f128c2d5774a145f966abf5dc7da783c5ea5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:38:29.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3659" for this suite. 09/04/23 17:38:29.719
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:38:29.736
Sep  4 17:38:29.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir-wrapper 09/04/23 17:38:29.738
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:38:29.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:38:29.778
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 09/04/23 17:38:29.785
STEP: Creating RC which spawns configmap-volume pods 09/04/23 17:38:30.248
Sep  4 17:38:30.266: INFO: Pod name wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a: Found 0 pods out of 5
Sep  4 17:38:35.283: INFO: Pod name wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/04/23 17:38:35.283
Sep  4 17:38:35.284: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:38:35.294: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng": Phase="Pending", Reason="", readiness=false. Elapsed: 10.156548ms
Sep  4 17:38:37.303: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019138191s
Sep  4 17:38:39.302: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018922441s
Sep  4 17:38:41.303: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019602433s
Sep  4 17:38:43.303: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019770338s
Sep  4 17:38:45.305: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng": Phase="Running", Reason="", readiness=true. Elapsed: 10.021270187s
Sep  4 17:38:45.305: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng" satisfied condition "running"
Sep  4 17:38:45.305: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-srmrq" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:38:45.311: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-srmrq": Phase="Running", Reason="", readiness=true. Elapsed: 6.050359ms
Sep  4 17:38:45.311: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-srmrq" satisfied condition "running"
Sep  4 17:38:45.311: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-t254t" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:38:45.324: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-t254t": Phase="Running", Reason="", readiness=true. Elapsed: 12.199426ms
Sep  4 17:38:45.324: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-t254t" satisfied condition "running"
Sep  4 17:38:45.324: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-wtbhv" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:38:45.329: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-wtbhv": Phase="Running", Reason="", readiness=true. Elapsed: 5.488616ms
Sep  4 17:38:45.330: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-wtbhv" satisfied condition "running"
Sep  4 17:38:45.330: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-z6xsp" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:38:45.337: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-z6xsp": Phase="Running", Reason="", readiness=true. Elapsed: 6.575399ms
Sep  4 17:38:45.337: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-z6xsp" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a in namespace emptydir-wrapper-5385, will wait for the garbage collector to delete the pods 09/04/23 17:38:45.337
Sep  4 17:38:45.408: INFO: Deleting ReplicationController wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a took: 14.251681ms
Sep  4 17:38:45.509: INFO: Terminating ReplicationController wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a pods took: 101.22118ms
STEP: Creating RC which spawns configmap-volume pods 09/04/23 17:38:50.215
Sep  4 17:38:50.248: INFO: Pod name wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b: Found 0 pods out of 5
Sep  4 17:38:55.265: INFO: Pod name wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/04/23 17:38:55.265
Sep  4 17:38:55.266: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:38:55.275: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55": Phase="Pending", Reason="", readiness=false. Elapsed: 8.955071ms
Sep  4 17:38:57.285: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018664135s
Sep  4 17:38:59.285: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019191479s
Sep  4 17:39:01.285: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019316609s
Sep  4 17:39:03.382: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116232596s
Sep  4 17:39:05.302: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55": Phase="Running", Reason="", readiness=true. Elapsed: 10.036261262s
Sep  4 17:39:05.303: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55" satisfied condition "running"
Sep  4 17:39:05.303: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-cwfgl" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:39:05.345: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-cwfgl": Phase="Running", Reason="", readiness=true. Elapsed: 41.713437ms
Sep  4 17:39:05.345: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-cwfgl" satisfied condition "running"
Sep  4 17:39:05.345: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-h7lgv" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:39:05.404: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-h7lgv": Phase="Pending", Reason="", readiness=false. Elapsed: 59.418514ms
Sep  4 17:39:07.419: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-h7lgv": Phase="Running", Reason="", readiness=true. Elapsed: 2.073828115s
Sep  4 17:39:07.419: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-h7lgv" satisfied condition "running"
Sep  4 17:39:07.419: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-nrb79" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:39:07.425: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-nrb79": Phase="Running", Reason="", readiness=true. Elapsed: 5.942532ms
Sep  4 17:39:07.425: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-nrb79" satisfied condition "running"
Sep  4 17:39:07.425: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-nt5f4" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:39:07.432: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-nt5f4": Phase="Running", Reason="", readiness=true. Elapsed: 5.979336ms
Sep  4 17:39:07.432: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-nt5f4" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b in namespace emptydir-wrapper-5385, will wait for the garbage collector to delete the pods 09/04/23 17:39:07.432
Sep  4 17:39:07.504: INFO: Deleting ReplicationController wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b took: 13.154358ms
Sep  4 17:39:07.705: INFO: Terminating ReplicationController wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b pods took: 200.673911ms
STEP: Creating RC which spawns configmap-volume pods 09/04/23 17:39:11.316
Sep  4 17:39:11.364: INFO: Pod name wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541: Found 0 pods out of 5
Sep  4 17:39:16.382: INFO: Pod name wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/04/23 17:39:16.382
Sep  4 17:39:16.382: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:39:16.391: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z": Phase="Pending", Reason="", readiness=false. Elapsed: 5.836019ms
Sep  4 17:39:18.404: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018905834s
Sep  4 17:39:20.401: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015732821s
Sep  4 17:39:22.401: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015875046s
Sep  4 17:39:24.425: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039144724s
Sep  4 17:39:26.404: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018263722s
Sep  4 17:39:28.401: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z": Phase="Running", Reason="", readiness=true. Elapsed: 12.015280438s
Sep  4 17:39:28.401: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z" satisfied condition "running"
Sep  4 17:39:28.401: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-hkt2h" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:39:28.407: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-hkt2h": Phase="Running", Reason="", readiness=true. Elapsed: 5.758815ms
Sep  4 17:39:28.407: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-hkt2h" satisfied condition "running"
Sep  4 17:39:28.407: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-lcrwx" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:39:28.417: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-lcrwx": Phase="Running", Reason="", readiness=true. Elapsed: 9.401078ms
Sep  4 17:39:28.417: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-lcrwx" satisfied condition "running"
Sep  4 17:39:28.417: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-scpld" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:39:28.423: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-scpld": Phase="Running", Reason="", readiness=true. Elapsed: 5.606104ms
Sep  4 17:39:28.423: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-scpld" satisfied condition "running"
Sep  4 17:39:28.423: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-xgmvq" in namespace "emptydir-wrapper-5385" to be "running"
Sep  4 17:39:28.429: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-xgmvq": Phase="Running", Reason="", readiness=true. Elapsed: 5.782316ms
Sep  4 17:39:28.429: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-xgmvq" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541 in namespace emptydir-wrapper-5385, will wait for the garbage collector to delete the pods 09/04/23 17:39:28.429
Sep  4 17:39:28.502: INFO: Deleting ReplicationController wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541 took: 16.906819ms
Sep  4 17:39:28.703: INFO: Terminating ReplicationController wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541 pods took: 201.030787ms
STEP: Cleaning up the configMaps 09/04/23 17:39:31.903
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 17:39:32.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-5385" for this suite. 09/04/23 17:39:32.563
------------------------------
â€¢ [SLOW TEST] [62.842 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:38:29.736
    Sep  4 17:38:29.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir-wrapper 09/04/23 17:38:29.738
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:38:29.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:38:29.778
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 09/04/23 17:38:29.785
    STEP: Creating RC which spawns configmap-volume pods 09/04/23 17:38:30.248
    Sep  4 17:38:30.266: INFO: Pod name wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a: Found 0 pods out of 5
    Sep  4 17:38:35.283: INFO: Pod name wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/04/23 17:38:35.283
    Sep  4 17:38:35.284: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:38:35.294: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng": Phase="Pending", Reason="", readiness=false. Elapsed: 10.156548ms
    Sep  4 17:38:37.303: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019138191s
    Sep  4 17:38:39.302: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018922441s
    Sep  4 17:38:41.303: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019602433s
    Sep  4 17:38:43.303: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019770338s
    Sep  4 17:38:45.305: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng": Phase="Running", Reason="", readiness=true. Elapsed: 10.021270187s
    Sep  4 17:38:45.305: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-6zrng" satisfied condition "running"
    Sep  4 17:38:45.305: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-srmrq" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:38:45.311: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-srmrq": Phase="Running", Reason="", readiness=true. Elapsed: 6.050359ms
    Sep  4 17:38:45.311: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-srmrq" satisfied condition "running"
    Sep  4 17:38:45.311: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-t254t" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:38:45.324: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-t254t": Phase="Running", Reason="", readiness=true. Elapsed: 12.199426ms
    Sep  4 17:38:45.324: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-t254t" satisfied condition "running"
    Sep  4 17:38:45.324: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-wtbhv" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:38:45.329: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-wtbhv": Phase="Running", Reason="", readiness=true. Elapsed: 5.488616ms
    Sep  4 17:38:45.330: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-wtbhv" satisfied condition "running"
    Sep  4 17:38:45.330: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-z6xsp" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:38:45.337: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-z6xsp": Phase="Running", Reason="", readiness=true. Elapsed: 6.575399ms
    Sep  4 17:38:45.337: INFO: Pod "wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a-z6xsp" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a in namespace emptydir-wrapper-5385, will wait for the garbage collector to delete the pods 09/04/23 17:38:45.337
    Sep  4 17:38:45.408: INFO: Deleting ReplicationController wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a took: 14.251681ms
    Sep  4 17:38:45.509: INFO: Terminating ReplicationController wrapped-volume-race-95cefc39-ef05-4944-84ba-19a71668a60a pods took: 101.22118ms
    STEP: Creating RC which spawns configmap-volume pods 09/04/23 17:38:50.215
    Sep  4 17:38:50.248: INFO: Pod name wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b: Found 0 pods out of 5
    Sep  4 17:38:55.265: INFO: Pod name wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/04/23 17:38:55.265
    Sep  4 17:38:55.266: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:38:55.275: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55": Phase="Pending", Reason="", readiness=false. Elapsed: 8.955071ms
    Sep  4 17:38:57.285: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018664135s
    Sep  4 17:38:59.285: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019191479s
    Sep  4 17:39:01.285: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019316609s
    Sep  4 17:39:03.382: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116232596s
    Sep  4 17:39:05.302: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55": Phase="Running", Reason="", readiness=true. Elapsed: 10.036261262s
    Sep  4 17:39:05.303: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-6mz55" satisfied condition "running"
    Sep  4 17:39:05.303: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-cwfgl" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:39:05.345: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-cwfgl": Phase="Running", Reason="", readiness=true. Elapsed: 41.713437ms
    Sep  4 17:39:05.345: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-cwfgl" satisfied condition "running"
    Sep  4 17:39:05.345: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-h7lgv" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:39:05.404: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-h7lgv": Phase="Pending", Reason="", readiness=false. Elapsed: 59.418514ms
    Sep  4 17:39:07.419: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-h7lgv": Phase="Running", Reason="", readiness=true. Elapsed: 2.073828115s
    Sep  4 17:39:07.419: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-h7lgv" satisfied condition "running"
    Sep  4 17:39:07.419: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-nrb79" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:39:07.425: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-nrb79": Phase="Running", Reason="", readiness=true. Elapsed: 5.942532ms
    Sep  4 17:39:07.425: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-nrb79" satisfied condition "running"
    Sep  4 17:39:07.425: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-nt5f4" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:39:07.432: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-nt5f4": Phase="Running", Reason="", readiness=true. Elapsed: 5.979336ms
    Sep  4 17:39:07.432: INFO: Pod "wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b-nt5f4" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b in namespace emptydir-wrapper-5385, will wait for the garbage collector to delete the pods 09/04/23 17:39:07.432
    Sep  4 17:39:07.504: INFO: Deleting ReplicationController wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b took: 13.154358ms
    Sep  4 17:39:07.705: INFO: Terminating ReplicationController wrapped-volume-race-0c984a92-1f6e-46ae-a90c-837a7f04874b pods took: 200.673911ms
    STEP: Creating RC which spawns configmap-volume pods 09/04/23 17:39:11.316
    Sep  4 17:39:11.364: INFO: Pod name wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541: Found 0 pods out of 5
    Sep  4 17:39:16.382: INFO: Pod name wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/04/23 17:39:16.382
    Sep  4 17:39:16.382: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:39:16.391: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z": Phase="Pending", Reason="", readiness=false. Elapsed: 5.836019ms
    Sep  4 17:39:18.404: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018905834s
    Sep  4 17:39:20.401: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015732821s
    Sep  4 17:39:22.401: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015875046s
    Sep  4 17:39:24.425: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039144724s
    Sep  4 17:39:26.404: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018263722s
    Sep  4 17:39:28.401: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z": Phase="Running", Reason="", readiness=true. Elapsed: 12.015280438s
    Sep  4 17:39:28.401: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-ff24z" satisfied condition "running"
    Sep  4 17:39:28.401: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-hkt2h" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:39:28.407: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-hkt2h": Phase="Running", Reason="", readiness=true. Elapsed: 5.758815ms
    Sep  4 17:39:28.407: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-hkt2h" satisfied condition "running"
    Sep  4 17:39:28.407: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-lcrwx" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:39:28.417: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-lcrwx": Phase="Running", Reason="", readiness=true. Elapsed: 9.401078ms
    Sep  4 17:39:28.417: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-lcrwx" satisfied condition "running"
    Sep  4 17:39:28.417: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-scpld" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:39:28.423: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-scpld": Phase="Running", Reason="", readiness=true. Elapsed: 5.606104ms
    Sep  4 17:39:28.423: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-scpld" satisfied condition "running"
    Sep  4 17:39:28.423: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-xgmvq" in namespace "emptydir-wrapper-5385" to be "running"
    Sep  4 17:39:28.429: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-xgmvq": Phase="Running", Reason="", readiness=true. Elapsed: 5.782316ms
    Sep  4 17:39:28.429: INFO: Pod "wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541-xgmvq" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541 in namespace emptydir-wrapper-5385, will wait for the garbage collector to delete the pods 09/04/23 17:39:28.429
    Sep  4 17:39:28.502: INFO: Deleting ReplicationController wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541 took: 16.906819ms
    Sep  4 17:39:28.703: INFO: Terminating ReplicationController wrapped-volume-race-1ec4126e-8779-4fc6-bc15-cba124805541 pods took: 201.030787ms
    STEP: Cleaning up the configMaps 09/04/23 17:39:31.903
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:39:32.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-5385" for this suite. 09/04/23 17:39:32.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:39:32.578
Sep  4 17:39:32.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 17:39:32.579
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:39:32.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:39:32.607
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/04/23 17:39:32.615
Sep  4 17:39:32.629: INFO: Waiting up to 5m0s for pod "pod-a5829b8a-593e-4ae8-87c7-e2281454f52b" in namespace "emptydir-2851" to be "Succeeded or Failed"
Sep  4 17:39:32.644: INFO: Pod "pod-a5829b8a-593e-4ae8-87c7-e2281454f52b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.334206ms
Sep  4 17:39:34.651: INFO: Pod "pod-a5829b8a-593e-4ae8-87c7-e2281454f52b": Phase="Running", Reason="", readiness=true. Elapsed: 2.022316737s
Sep  4 17:39:36.650: INFO: Pod "pod-a5829b8a-593e-4ae8-87c7-e2281454f52b": Phase="Running", Reason="", readiness=false. Elapsed: 4.021302144s
Sep  4 17:39:38.651: INFO: Pod "pod-a5829b8a-593e-4ae8-87c7-e2281454f52b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022321599s
STEP: Saw pod success 09/04/23 17:39:38.652
Sep  4 17:39:38.652: INFO: Pod "pod-a5829b8a-593e-4ae8-87c7-e2281454f52b" satisfied condition "Succeeded or Failed"
Sep  4 17:39:38.658: INFO: Trying to get logs from node tenant-000001 pod pod-a5829b8a-593e-4ae8-87c7-e2281454f52b container test-container: <nil>
STEP: delete the pod 09/04/23 17:39:38.695
Sep  4 17:39:38.717: INFO: Waiting for pod pod-a5829b8a-593e-4ae8-87c7-e2281454f52b to disappear
Sep  4 17:39:38.724: INFO: Pod pod-a5829b8a-593e-4ae8-87c7-e2281454f52b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 17:39:38.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2851" for this suite. 09/04/23 17:39:38.732
------------------------------
â€¢ [SLOW TEST] [6.176 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:39:32.578
    Sep  4 17:39:32.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 17:39:32.579
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:39:32.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:39:32.607
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/04/23 17:39:32.615
    Sep  4 17:39:32.629: INFO: Waiting up to 5m0s for pod "pod-a5829b8a-593e-4ae8-87c7-e2281454f52b" in namespace "emptydir-2851" to be "Succeeded or Failed"
    Sep  4 17:39:32.644: INFO: Pod "pod-a5829b8a-593e-4ae8-87c7-e2281454f52b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.334206ms
    Sep  4 17:39:34.651: INFO: Pod "pod-a5829b8a-593e-4ae8-87c7-e2281454f52b": Phase="Running", Reason="", readiness=true. Elapsed: 2.022316737s
    Sep  4 17:39:36.650: INFO: Pod "pod-a5829b8a-593e-4ae8-87c7-e2281454f52b": Phase="Running", Reason="", readiness=false. Elapsed: 4.021302144s
    Sep  4 17:39:38.651: INFO: Pod "pod-a5829b8a-593e-4ae8-87c7-e2281454f52b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022321599s
    STEP: Saw pod success 09/04/23 17:39:38.652
    Sep  4 17:39:38.652: INFO: Pod "pod-a5829b8a-593e-4ae8-87c7-e2281454f52b" satisfied condition "Succeeded or Failed"
    Sep  4 17:39:38.658: INFO: Trying to get logs from node tenant-000001 pod pod-a5829b8a-593e-4ae8-87c7-e2281454f52b container test-container: <nil>
    STEP: delete the pod 09/04/23 17:39:38.695
    Sep  4 17:39:38.717: INFO: Waiting for pod pod-a5829b8a-593e-4ae8-87c7-e2281454f52b to disappear
    Sep  4 17:39:38.724: INFO: Pod pod-a5829b8a-593e-4ae8-87c7-e2281454f52b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:39:38.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2851" for this suite. 09/04/23 17:39:38.732
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:39:38.756
Sep  4 17:39:38.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename deployment 09/04/23 17:39:38.759
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:39:38.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:39:38.79
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Sep  4 17:39:38.818: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  4 17:39:43.829: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/04/23 17:39:43.829
Sep  4 17:39:43.829: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  4 17:39:45.835: INFO: Creating deployment "test-rollover-deployment"
Sep  4 17:39:45.851: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  4 17:39:47.867: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  4 17:39:47.881: INFO: Ensure that both replica sets have 1 created replica
Sep  4 17:39:47.894: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  4 17:39:47.911: INFO: Updating deployment test-rollover-deployment
Sep  4 17:39:47.911: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  4 17:39:49.928: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  4 17:39:49.944: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  4 17:39:49.955: INFO: all replica sets need to contain the pod-template-hash label
Sep  4 17:39:49.955: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 17:39:51.969: INFO: all replica sets need to contain the pod-template-hash label
Sep  4 17:39:51.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 17:39:53.969: INFO: all replica sets need to contain the pod-template-hash label
Sep  4 17:39:53.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 17:39:55.967: INFO: all replica sets need to contain the pod-template-hash label
Sep  4 17:39:55.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 17:39:57.968: INFO: all replica sets need to contain the pod-template-hash label
Sep  4 17:39:57.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 17:39:59.966: INFO: 
Sep  4 17:39:59.966: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 17:39:59.990: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9903  eddd4949-d4b7-417c-9df0-cbfe3781470f 12362 2 2023-09-04 17:39:45 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-04 17:39:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00224f7e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-04 17:39:45 +0000 UTC,LastTransitionTime:2023-09-04 17:39:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-09-04 17:39:59 +0000 UTC,LastTransitionTime:2023-09-04 17:39:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  4 17:39:59.997: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-9903  bceaeee7-2357-45d0-b125-878f4d2f59d9 12352 2 2023-09-04 17:39:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment eddd4949-d4b7-417c-9df0-cbfe3781470f 0xc0024a4317 0xc0024a4318}] [] [{kube-controller-manager Update apps/v1 2023-09-04 17:39:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eddd4949-d4b7-417c-9df0-cbfe3781470f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:39:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0024a43c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  4 17:39:59.997: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  4 17:39:59.997: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9903  740870cc-e303-4050-ab29-f784a3400773 12361 2 2023-09-04 17:39:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment eddd4949-d4b7-417c-9df0-cbfe3781470f 0xc0024a41e7 0xc0024a41e8}] [] [{e2e.test Update apps/v1 2023-09-04 17:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eddd4949-d4b7-417c-9df0-cbfe3781470f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:39:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0024a42a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  4 17:39:59.997: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-9903  72d619b2-7067-40a8-af3c-b8cb8e6d58b6 12304 2 2023-09-04 17:39:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment eddd4949-d4b7-417c-9df0-cbfe3781470f 0xc0024a4437 0xc0024a4438}] [] [{kube-controller-manager Update apps/v1 2023-09-04 17:39:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eddd4949-d4b7-417c-9df0-cbfe3781470f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:39:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0024a44e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  4 17:40:00.003: INFO: Pod "test-rollover-deployment-6c6df9974f-p4rfl" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-p4rfl test-rollover-deployment-6c6df9974f- deployment-9903  fc73090a-164c-4a9b-9289-8135c9fb00e1 12315 0 2023-09-04 17:39:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:91f7edb7fc62960737e43616d8ff31bda2bb07de19cbc115222d22cea24105cf cni.projectcalico.org/podIP:10.36.55.96/32 cni.projectcalico.org/podIPs:10.36.55.96/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f bceaeee7-2357-45d0-b125-878f4d2f59d9 0xc0024a4a57 0xc0024a4a58}] [] [{kube-controller-manager Update v1 2023-09-04 17:39:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bceaeee7-2357-45d0-b125-878f4d2f59d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:39:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:39:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s78rn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s78rn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:39:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:39:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:39:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:39:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.96,StartTime:2023-09-04 17:39:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:39:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://440fb2bdb1f57f52c2951d8b33d9366c982ecc92a229334f686bd5993c79bf9e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 17:40:00.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9903" for this suite. 09/04/23 17:40:00.013
------------------------------
â€¢ [SLOW TEST] [21.273 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:39:38.756
    Sep  4 17:39:38.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename deployment 09/04/23 17:39:38.759
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:39:38.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:39:38.79
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Sep  4 17:39:38.818: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Sep  4 17:39:43.829: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/04/23 17:39:43.829
    Sep  4 17:39:43.829: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Sep  4 17:39:45.835: INFO: Creating deployment "test-rollover-deployment"
    Sep  4 17:39:45.851: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Sep  4 17:39:47.867: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Sep  4 17:39:47.881: INFO: Ensure that both replica sets have 1 created replica
    Sep  4 17:39:47.894: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Sep  4 17:39:47.911: INFO: Updating deployment test-rollover-deployment
    Sep  4 17:39:47.911: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Sep  4 17:39:49.928: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Sep  4 17:39:49.944: INFO: Make sure deployment "test-rollover-deployment" is complete
    Sep  4 17:39:49.955: INFO: all replica sets need to contain the pod-template-hash label
    Sep  4 17:39:49.955: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 17:39:51.969: INFO: all replica sets need to contain the pod-template-hash label
    Sep  4 17:39:51.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 17:39:53.969: INFO: all replica sets need to contain the pod-template-hash label
    Sep  4 17:39:53.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 17:39:55.967: INFO: all replica sets need to contain the pod-template-hash label
    Sep  4 17:39:55.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 17:39:57.968: INFO: all replica sets need to contain the pod-template-hash label
    Sep  4 17:39:57.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 17, 39, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 17, 39, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 17:39:59.966: INFO: 
    Sep  4 17:39:59.966: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 17:39:59.990: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-9903  eddd4949-d4b7-417c-9df0-cbfe3781470f 12362 2 2023-09-04 17:39:45 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-04 17:39:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00224f7e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-04 17:39:45 +0000 UTC,LastTransitionTime:2023-09-04 17:39:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-09-04 17:39:59 +0000 UTC,LastTransitionTime:2023-09-04 17:39:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  4 17:39:59.997: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-9903  bceaeee7-2357-45d0-b125-878f4d2f59d9 12352 2 2023-09-04 17:39:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment eddd4949-d4b7-417c-9df0-cbfe3781470f 0xc0024a4317 0xc0024a4318}] [] [{kube-controller-manager Update apps/v1 2023-09-04 17:39:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eddd4949-d4b7-417c-9df0-cbfe3781470f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:39:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0024a43c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 17:39:59.997: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Sep  4 17:39:59.997: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9903  740870cc-e303-4050-ab29-f784a3400773 12361 2 2023-09-04 17:39:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment eddd4949-d4b7-417c-9df0-cbfe3781470f 0xc0024a41e7 0xc0024a41e8}] [] [{e2e.test Update apps/v1 2023-09-04 17:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eddd4949-d4b7-417c-9df0-cbfe3781470f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:39:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0024a42a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 17:39:59.997: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-9903  72d619b2-7067-40a8-af3c-b8cb8e6d58b6 12304 2 2023-09-04 17:39:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment eddd4949-d4b7-417c-9df0-cbfe3781470f 0xc0024a4437 0xc0024a4438}] [] [{kube-controller-manager Update apps/v1 2023-09-04 17:39:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eddd4949-d4b7-417c-9df0-cbfe3781470f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:39:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0024a44e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 17:40:00.003: INFO: Pod "test-rollover-deployment-6c6df9974f-p4rfl" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-p4rfl test-rollover-deployment-6c6df9974f- deployment-9903  fc73090a-164c-4a9b-9289-8135c9fb00e1 12315 0 2023-09-04 17:39:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:91f7edb7fc62960737e43616d8ff31bda2bb07de19cbc115222d22cea24105cf cni.projectcalico.org/podIP:10.36.55.96/32 cni.projectcalico.org/podIPs:10.36.55.96/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f bceaeee7-2357-45d0-b125-878f4d2f59d9 0xc0024a4a57 0xc0024a4a58}] [] [{kube-controller-manager Update v1 2023-09-04 17:39:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bceaeee7-2357-45d0-b125-878f4d2f59d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:39:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:39:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s78rn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s78rn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:39:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:39:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:39:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:39:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.96,StartTime:2023-09-04 17:39:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:39:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://440fb2bdb1f57f52c2951d8b33d9366c982ecc92a229334f686bd5993c79bf9e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:40:00.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9903" for this suite. 09/04/23 17:40:00.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:40:00.033
Sep  4 17:40:00.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 17:40:00.034
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:40:00.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:40:00.064
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-6235052d-db4c-4a50-99bf-24a70da4f262 09/04/23 17:40:00.078
STEP: Creating secret with name s-test-opt-upd-61191c02-70f8-4729-be7a-571d112dfbc3 09/04/23 17:40:00.088
STEP: Creating the pod 09/04/23 17:40:00.097
Sep  4 17:40:00.111: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-85d80cef-607e-4021-bd2f-66aecfb296eb" in namespace "projected-3889" to be "running and ready"
Sep  4 17:40:00.130: INFO: Pod "pod-projected-secrets-85d80cef-607e-4021-bd2f-66aecfb296eb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.651741ms
Sep  4 17:40:00.130: INFO: The phase of Pod pod-projected-secrets-85d80cef-607e-4021-bd2f-66aecfb296eb is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:40:02.141: INFO: Pod "pod-projected-secrets-85d80cef-607e-4021-bd2f-66aecfb296eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.029454873s
Sep  4 17:40:02.141: INFO: The phase of Pod pod-projected-secrets-85d80cef-607e-4021-bd2f-66aecfb296eb is Running (Ready = true)
Sep  4 17:40:02.141: INFO: Pod "pod-projected-secrets-85d80cef-607e-4021-bd2f-66aecfb296eb" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-6235052d-db4c-4a50-99bf-24a70da4f262 09/04/23 17:40:02.194
STEP: Updating secret s-test-opt-upd-61191c02-70f8-4729-be7a-571d112dfbc3 09/04/23 17:40:02.207
STEP: Creating secret with name s-test-opt-create-3dd1bb68-d4d2-4d65-8d20-2450d8cd98f6 09/04/23 17:40:02.217
STEP: waiting to observe update in volume 09/04/23 17:40:02.23
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  4 17:40:04.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3889" for this suite. 09/04/23 17:40:04.293
------------------------------
â€¢ [4.269 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:40:00.033
    Sep  4 17:40:00.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 17:40:00.034
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:40:00.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:40:00.064
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-6235052d-db4c-4a50-99bf-24a70da4f262 09/04/23 17:40:00.078
    STEP: Creating secret with name s-test-opt-upd-61191c02-70f8-4729-be7a-571d112dfbc3 09/04/23 17:40:00.088
    STEP: Creating the pod 09/04/23 17:40:00.097
    Sep  4 17:40:00.111: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-85d80cef-607e-4021-bd2f-66aecfb296eb" in namespace "projected-3889" to be "running and ready"
    Sep  4 17:40:00.130: INFO: Pod "pod-projected-secrets-85d80cef-607e-4021-bd2f-66aecfb296eb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.651741ms
    Sep  4 17:40:00.130: INFO: The phase of Pod pod-projected-secrets-85d80cef-607e-4021-bd2f-66aecfb296eb is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:40:02.141: INFO: Pod "pod-projected-secrets-85d80cef-607e-4021-bd2f-66aecfb296eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.029454873s
    Sep  4 17:40:02.141: INFO: The phase of Pod pod-projected-secrets-85d80cef-607e-4021-bd2f-66aecfb296eb is Running (Ready = true)
    Sep  4 17:40:02.141: INFO: Pod "pod-projected-secrets-85d80cef-607e-4021-bd2f-66aecfb296eb" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-6235052d-db4c-4a50-99bf-24a70da4f262 09/04/23 17:40:02.194
    STEP: Updating secret s-test-opt-upd-61191c02-70f8-4729-be7a-571d112dfbc3 09/04/23 17:40:02.207
    STEP: Creating secret with name s-test-opt-create-3dd1bb68-d4d2-4d65-8d20-2450d8cd98f6 09/04/23 17:40:02.217
    STEP: waiting to observe update in volume 09/04/23 17:40:02.23
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:40:04.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3889" for this suite. 09/04/23 17:40:04.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:40:04.309
Sep  4 17:40:04.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 17:40:04.311
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:40:04.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:40:04.345
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 09/04/23 17:40:04.357
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/04/23 17:40:04.36
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/04/23 17:40:04.36
STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/04/23 17:40:04.361
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/04/23 17:40:04.364
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/04/23 17:40:04.364
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/04/23 17:40:04.368
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:40:04.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2897" for this suite. 09/04/23 17:40:04.377
------------------------------
â€¢ [0.077 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:40:04.309
    Sep  4 17:40:04.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 17:40:04.311
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:40:04.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:40:04.345
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 09/04/23 17:40:04.357
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/04/23 17:40:04.36
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/04/23 17:40:04.36
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/04/23 17:40:04.361
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/04/23 17:40:04.364
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/04/23 17:40:04.364
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/04/23 17:40:04.368
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:40:04.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2897" for this suite. 09/04/23 17:40:04.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:40:04.392
Sep  4 17:40:04.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename disruption 09/04/23 17:40:04.393
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:40:04.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:40:04.427
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:40:04.435
Sep  4 17:40:04.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename disruption-2 09/04/23 17:40:04.436
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:40:04.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:40:04.475
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 09/04/23 17:40:04.489
STEP: Waiting for the pdb to be processed 09/04/23 17:40:04.513
STEP: Waiting for the pdb to be processed 09/04/23 17:40:04.533
STEP: listing a collection of PDBs across all namespaces 09/04/23 17:40:04.541
STEP: listing a collection of PDBs in namespace disruption-9586 09/04/23 17:40:04.55
STEP: deleting a collection of PDBs 09/04/23 17:40:04.556
STEP: Waiting for the PDB collection to be deleted 09/04/23 17:40:04.576
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Sep  4 17:40:04.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  4 17:40:04.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-3478" for this suite. 09/04/23 17:40:04.594
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9586" for this suite. 09/04/23 17:40:04.605
------------------------------
â€¢ [0.224 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:40:04.392
    Sep  4 17:40:04.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename disruption 09/04/23 17:40:04.393
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:40:04.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:40:04.427
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:40:04.435
    Sep  4 17:40:04.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename disruption-2 09/04/23 17:40:04.436
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:40:04.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:40:04.475
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 09/04/23 17:40:04.489
    STEP: Waiting for the pdb to be processed 09/04/23 17:40:04.513
    STEP: Waiting for the pdb to be processed 09/04/23 17:40:04.533
    STEP: listing a collection of PDBs across all namespaces 09/04/23 17:40:04.541
    STEP: listing a collection of PDBs in namespace disruption-9586 09/04/23 17:40:04.55
    STEP: deleting a collection of PDBs 09/04/23 17:40:04.556
    STEP: Waiting for the PDB collection to be deleted 09/04/23 17:40:04.576
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:40:04.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:40:04.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-3478" for this suite. 09/04/23 17:40:04.594
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9586" for this suite. 09/04/23 17:40:04.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:40:04.62
Sep  4 17:40:04.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 17:40:04.622
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:40:04.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:40:04.652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 17:40:04.659
Sep  4 17:40:04.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9452 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep  4 17:40:04.757: INFO: stderr: ""
Sep  4 17:40:04.757: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 09/04/23 17:40:04.757
STEP: verifying the pod e2e-test-httpd-pod was created 09/04/23 17:40:09.81
Sep  4 17:40:09.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9452 get pod e2e-test-httpd-pod -o json'
Sep  4 17:40:10.020: INFO: stderr: ""
Sep  4 17:40:10.020: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"e0ebf72e9a248ff579127b052effb876c7bfddbcad1962b835437eb524ee2d50\",\n            \"cni.projectcalico.org/podIP\": \"10.36.217.208/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.36.217.208/32\"\n        },\n        \"creationTimestamp\": \"2023-09-04T17:40:04Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9452\",\n        \"resourceVersion\": \"12485\",\n        \"uid\": \"99830b67-723a-46a5-9900-111126d3660f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-6z4qj\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"tenant-000003\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-6z4qj\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T17:40:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T17:40:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T17:40:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T17:40:04Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://d2492344375a0407001ce25a74e1a38eb4a30e01d8e44f9127004b6716e7e3fb\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-09-04T17:40:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.225.0.7\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.36.217.208\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.36.217.208\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-09-04T17:40:04Z\"\n    }\n}\n"
STEP: replace the image in the pod 09/04/23 17:40:10.02
Sep  4 17:40:10.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9452 replace -f -'
Sep  4 17:40:10.886: INFO: stderr: ""
Sep  4 17:40:10.886: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 09/04/23 17:40:10.886
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Sep  4 17:40:10.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9452 delete pods e2e-test-httpd-pod'
Sep  4 17:40:12.654: INFO: stderr: ""
Sep  4 17:40:12.654: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 17:40:12.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9452" for this suite. 09/04/23 17:40:12.669
------------------------------
â€¢ [SLOW TEST] [8.061 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:40:04.62
    Sep  4 17:40:04.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 17:40:04.622
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:40:04.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:40:04.652
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 17:40:04.659
    Sep  4 17:40:04.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9452 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep  4 17:40:04.757: INFO: stderr: ""
    Sep  4 17:40:04.757: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 09/04/23 17:40:04.757
    STEP: verifying the pod e2e-test-httpd-pod was created 09/04/23 17:40:09.81
    Sep  4 17:40:09.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9452 get pod e2e-test-httpd-pod -o json'
    Sep  4 17:40:10.020: INFO: stderr: ""
    Sep  4 17:40:10.020: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"e0ebf72e9a248ff579127b052effb876c7bfddbcad1962b835437eb524ee2d50\",\n            \"cni.projectcalico.org/podIP\": \"10.36.217.208/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.36.217.208/32\"\n        },\n        \"creationTimestamp\": \"2023-09-04T17:40:04Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9452\",\n        \"resourceVersion\": \"12485\",\n        \"uid\": \"99830b67-723a-46a5-9900-111126d3660f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-6z4qj\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"tenant-000003\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-6z4qj\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T17:40:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T17:40:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T17:40:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T17:40:04Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://d2492344375a0407001ce25a74e1a38eb4a30e01d8e44f9127004b6716e7e3fb\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-09-04T17:40:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.225.0.7\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.36.217.208\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.36.217.208\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-09-04T17:40:04Z\"\n    }\n}\n"
    STEP: replace the image in the pod 09/04/23 17:40:10.02
    Sep  4 17:40:10.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9452 replace -f -'
    Sep  4 17:40:10.886: INFO: stderr: ""
    Sep  4 17:40:10.886: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 09/04/23 17:40:10.886
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Sep  4 17:40:10.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9452 delete pods e2e-test-httpd-pod'
    Sep  4 17:40:12.654: INFO: stderr: ""
    Sep  4 17:40:12.654: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:40:12.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9452" for this suite. 09/04/23 17:40:12.669
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:40:12.687
Sep  4 17:40:12.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-probe 09/04/23 17:40:12.688
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:40:12.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:40:12.729
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-b2d8438c-e9e8-454f-b8d5-f61993073762 in namespace container-probe-3143 09/04/23 17:40:12.739
Sep  4 17:40:12.795: INFO: Waiting up to 5m0s for pod "liveness-b2d8438c-e9e8-454f-b8d5-f61993073762" in namespace "container-probe-3143" to be "not pending"
Sep  4 17:40:12.814: INFO: Pod "liveness-b2d8438c-e9e8-454f-b8d5-f61993073762": Phase="Pending", Reason="", readiness=false. Elapsed: 18.653307ms
Sep  4 17:40:14.823: INFO: Pod "liveness-b2d8438c-e9e8-454f-b8d5-f61993073762": Phase="Running", Reason="", readiness=true. Elapsed: 2.027454546s
Sep  4 17:40:14.823: INFO: Pod "liveness-b2d8438c-e9e8-454f-b8d5-f61993073762" satisfied condition "not pending"
Sep  4 17:40:14.823: INFO: Started pod liveness-b2d8438c-e9e8-454f-b8d5-f61993073762 in namespace container-probe-3143
STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 17:40:14.823
Sep  4 17:40:14.828: INFO: Initial restart count of pod liveness-b2d8438c-e9e8-454f-b8d5-f61993073762 is 0
STEP: deleting the pod 09/04/23 17:44:15.746
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 17:44:15.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3143" for this suite. 09/04/23 17:44:15.795
------------------------------
â€¢ [SLOW TEST] [243.167 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:40:12.687
    Sep  4 17:40:12.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-probe 09/04/23 17:40:12.688
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:40:12.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:40:12.729
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-b2d8438c-e9e8-454f-b8d5-f61993073762 in namespace container-probe-3143 09/04/23 17:40:12.739
    Sep  4 17:40:12.795: INFO: Waiting up to 5m0s for pod "liveness-b2d8438c-e9e8-454f-b8d5-f61993073762" in namespace "container-probe-3143" to be "not pending"
    Sep  4 17:40:12.814: INFO: Pod "liveness-b2d8438c-e9e8-454f-b8d5-f61993073762": Phase="Pending", Reason="", readiness=false. Elapsed: 18.653307ms
    Sep  4 17:40:14.823: INFO: Pod "liveness-b2d8438c-e9e8-454f-b8d5-f61993073762": Phase="Running", Reason="", readiness=true. Elapsed: 2.027454546s
    Sep  4 17:40:14.823: INFO: Pod "liveness-b2d8438c-e9e8-454f-b8d5-f61993073762" satisfied condition "not pending"
    Sep  4 17:40:14.823: INFO: Started pod liveness-b2d8438c-e9e8-454f-b8d5-f61993073762 in namespace container-probe-3143
    STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 17:40:14.823
    Sep  4 17:40:14.828: INFO: Initial restart count of pod liveness-b2d8438c-e9e8-454f-b8d5-f61993073762 is 0
    STEP: deleting the pod 09/04/23 17:44:15.746
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:44:15.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3143" for this suite. 09/04/23 17:44:15.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:44:15.902
Sep  4 17:44:15.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 17:44:15.906
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:44:15.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:44:15.949
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 09/04/23 17:44:15.956
Sep  4 17:44:15.973: INFO: Waiting up to 5m0s for pod "pod-ce6c121e-4e82-4da6-b746-821d8a29fb75" in namespace "emptydir-4393" to be "Succeeded or Failed"
Sep  4 17:44:15.981: INFO: Pod "pod-ce6c121e-4e82-4da6-b746-821d8a29fb75": Phase="Pending", Reason="", readiness=false. Elapsed: 7.430604ms
Sep  4 17:44:17.987: INFO: Pod "pod-ce6c121e-4e82-4da6-b746-821d8a29fb75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014023508s
Sep  4 17:44:19.988: INFO: Pod "pod-ce6c121e-4e82-4da6-b746-821d8a29fb75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015362255s
STEP: Saw pod success 09/04/23 17:44:19.989
Sep  4 17:44:19.989: INFO: Pod "pod-ce6c121e-4e82-4da6-b746-821d8a29fb75" satisfied condition "Succeeded or Failed"
Sep  4 17:44:19.996: INFO: Trying to get logs from node tenant-000001 pod pod-ce6c121e-4e82-4da6-b746-821d8a29fb75 container test-container: <nil>
STEP: delete the pod 09/04/23 17:44:20.03
Sep  4 17:44:20.049: INFO: Waiting for pod pod-ce6c121e-4e82-4da6-b746-821d8a29fb75 to disappear
Sep  4 17:44:20.056: INFO: Pod pod-ce6c121e-4e82-4da6-b746-821d8a29fb75 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 17:44:20.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4393" for this suite. 09/04/23 17:44:20.065
------------------------------
â€¢ [4.180 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:44:15.902
    Sep  4 17:44:15.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 17:44:15.906
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:44:15.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:44:15.949
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/04/23 17:44:15.956
    Sep  4 17:44:15.973: INFO: Waiting up to 5m0s for pod "pod-ce6c121e-4e82-4da6-b746-821d8a29fb75" in namespace "emptydir-4393" to be "Succeeded or Failed"
    Sep  4 17:44:15.981: INFO: Pod "pod-ce6c121e-4e82-4da6-b746-821d8a29fb75": Phase="Pending", Reason="", readiness=false. Elapsed: 7.430604ms
    Sep  4 17:44:17.987: INFO: Pod "pod-ce6c121e-4e82-4da6-b746-821d8a29fb75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014023508s
    Sep  4 17:44:19.988: INFO: Pod "pod-ce6c121e-4e82-4da6-b746-821d8a29fb75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015362255s
    STEP: Saw pod success 09/04/23 17:44:19.989
    Sep  4 17:44:19.989: INFO: Pod "pod-ce6c121e-4e82-4da6-b746-821d8a29fb75" satisfied condition "Succeeded or Failed"
    Sep  4 17:44:19.996: INFO: Trying to get logs from node tenant-000001 pod pod-ce6c121e-4e82-4da6-b746-821d8a29fb75 container test-container: <nil>
    STEP: delete the pod 09/04/23 17:44:20.03
    Sep  4 17:44:20.049: INFO: Waiting for pod pod-ce6c121e-4e82-4da6-b746-821d8a29fb75 to disappear
    Sep  4 17:44:20.056: INFO: Pod pod-ce6c121e-4e82-4da6-b746-821d8a29fb75 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:44:20.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4393" for this suite. 09/04/23 17:44:20.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:44:20.086
Sep  4 17:44:20.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename resourcequota 09/04/23 17:44:20.087
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:44:20.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:44:20.117
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 09/04/23 17:44:20.125
STEP: Getting a ResourceQuota 09/04/23 17:44:20.135
STEP: Updating a ResourceQuota 09/04/23 17:44:20.141
STEP: Verifying a ResourceQuota was modified 09/04/23 17:44:20.154
STEP: Deleting a ResourceQuota 09/04/23 17:44:20.161
STEP: Verifying the deleted ResourceQuota 09/04/23 17:44:20.176
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 17:44:20.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1395" for this suite. 09/04/23 17:44:20.188
------------------------------
â€¢ [0.120 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:44:20.086
    Sep  4 17:44:20.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename resourcequota 09/04/23 17:44:20.087
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:44:20.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:44:20.117
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 09/04/23 17:44:20.125
    STEP: Getting a ResourceQuota 09/04/23 17:44:20.135
    STEP: Updating a ResourceQuota 09/04/23 17:44:20.141
    STEP: Verifying a ResourceQuota was modified 09/04/23 17:44:20.154
    STEP: Deleting a ResourceQuota 09/04/23 17:44:20.161
    STEP: Verifying the deleted ResourceQuota 09/04/23 17:44:20.176
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:44:20.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1395" for this suite. 09/04/23 17:44:20.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:44:20.213
Sep  4 17:44:20.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pods 09/04/23 17:44:20.214
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:44:20.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:44:20.241
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 09/04/23 17:44:20.248
STEP: submitting the pod to kubernetes 09/04/23 17:44:20.249
Sep  4 17:44:20.267: INFO: Waiting up to 5m0s for pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86" in namespace "pods-5895" to be "running and ready"
Sep  4 17:44:20.280: INFO: Pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86": Phase="Pending", Reason="", readiness=false. Elapsed: 12.473246ms
Sep  4 17:44:20.280: INFO: The phase of Pod pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:44:22.289: INFO: Pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86": Phase="Running", Reason="", readiness=true. Elapsed: 2.021420706s
Sep  4 17:44:22.289: INFO: The phase of Pod pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86 is Running (Ready = true)
Sep  4 17:44:22.289: INFO: Pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/04/23 17:44:22.294
STEP: updating the pod 09/04/23 17:44:22.3
Sep  4 17:44:22.890: INFO: Successfully updated pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86"
Sep  4 17:44:22.891: INFO: Waiting up to 5m0s for pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86" in namespace "pods-5895" to be "running"
Sep  4 17:44:22.926: INFO: Pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86": Phase="Running", Reason="", readiness=true. Elapsed: 34.991901ms
Sep  4 17:44:22.926: INFO: Pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 09/04/23 17:44:22.926
Sep  4 17:44:22.958: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 17:44:22.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5895" for this suite. 09/04/23 17:44:22.982
------------------------------
â€¢ [2.822 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:44:20.213
    Sep  4 17:44:20.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pods 09/04/23 17:44:20.214
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:44:20.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:44:20.241
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 09/04/23 17:44:20.248
    STEP: submitting the pod to kubernetes 09/04/23 17:44:20.249
    Sep  4 17:44:20.267: INFO: Waiting up to 5m0s for pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86" in namespace "pods-5895" to be "running and ready"
    Sep  4 17:44:20.280: INFO: Pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86": Phase="Pending", Reason="", readiness=false. Elapsed: 12.473246ms
    Sep  4 17:44:20.280: INFO: The phase of Pod pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:44:22.289: INFO: Pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86": Phase="Running", Reason="", readiness=true. Elapsed: 2.021420706s
    Sep  4 17:44:22.289: INFO: The phase of Pod pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86 is Running (Ready = true)
    Sep  4 17:44:22.289: INFO: Pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/04/23 17:44:22.294
    STEP: updating the pod 09/04/23 17:44:22.3
    Sep  4 17:44:22.890: INFO: Successfully updated pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86"
    Sep  4 17:44:22.891: INFO: Waiting up to 5m0s for pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86" in namespace "pods-5895" to be "running"
    Sep  4 17:44:22.926: INFO: Pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86": Phase="Running", Reason="", readiness=true. Elapsed: 34.991901ms
    Sep  4 17:44:22.926: INFO: Pod "pod-update-5ebcb51b-d656-4075-9e1e-8d0adf3deb86" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 09/04/23 17:44:22.926
    Sep  4 17:44:22.958: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:44:22.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5895" for this suite. 09/04/23 17:44:22.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:44:23.131
Sep  4 17:44:23.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename crd-watch 09/04/23 17:44:23.132
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:44:23.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:44:23.169
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Sep  4 17:44:23.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Creating first CR  09/04/23 17:44:25.767
Sep  4 17:44:25.777: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T17:44:25Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T17:44:25Z]] name:name1 resourceVersion:13117 uid:3bd294be-6f25-4814-8bab-91797b40668c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 09/04/23 17:44:35.777
Sep  4 17:44:35.789: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T17:44:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T17:44:35Z]] name:name2 resourceVersion:13153 uid:154fca69-5aca-4463-9b6d-32b5e4abf836] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 09/04/23 17:44:45.79
Sep  4 17:44:45.800: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T17:44:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T17:44:45Z]] name:name1 resourceVersion:13172 uid:3bd294be-6f25-4814-8bab-91797b40668c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 09/04/23 17:44:55.8
Sep  4 17:44:55.811: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T17:44:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T17:44:55Z]] name:name2 resourceVersion:13191 uid:154fca69-5aca-4463-9b6d-32b5e4abf836] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 09/04/23 17:45:05.811
Sep  4 17:45:05.824: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T17:44:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T17:44:45Z]] name:name1 resourceVersion:13210 uid:3bd294be-6f25-4814-8bab-91797b40668c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 09/04/23 17:45:15.825
Sep  4 17:45:15.845: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T17:44:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T17:44:55Z]] name:name2 resourceVersion:13229 uid:154fca69-5aca-4463-9b6d-32b5e4abf836] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:45:26.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-9604" for this suite. 09/04/23 17:45:26.377
------------------------------
â€¢ [SLOW TEST] [63.259 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:44:23.131
    Sep  4 17:44:23.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename crd-watch 09/04/23 17:44:23.132
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:44:23.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:44:23.169
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Sep  4 17:44:23.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Creating first CR  09/04/23 17:44:25.767
    Sep  4 17:44:25.777: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T17:44:25Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T17:44:25Z]] name:name1 resourceVersion:13117 uid:3bd294be-6f25-4814-8bab-91797b40668c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 09/04/23 17:44:35.777
    Sep  4 17:44:35.789: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T17:44:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T17:44:35Z]] name:name2 resourceVersion:13153 uid:154fca69-5aca-4463-9b6d-32b5e4abf836] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 09/04/23 17:44:45.79
    Sep  4 17:44:45.800: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T17:44:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T17:44:45Z]] name:name1 resourceVersion:13172 uid:3bd294be-6f25-4814-8bab-91797b40668c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 09/04/23 17:44:55.8
    Sep  4 17:44:55.811: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T17:44:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T17:44:55Z]] name:name2 resourceVersion:13191 uid:154fca69-5aca-4463-9b6d-32b5e4abf836] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 09/04/23 17:45:05.811
    Sep  4 17:45:05.824: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T17:44:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T17:44:45Z]] name:name1 resourceVersion:13210 uid:3bd294be-6f25-4814-8bab-91797b40668c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 09/04/23 17:45:15.825
    Sep  4 17:45:15.845: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T17:44:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T17:44:55Z]] name:name2 resourceVersion:13229 uid:154fca69-5aca-4463-9b6d-32b5e4abf836] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:45:26.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-9604" for this suite. 09/04/23 17:45:26.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:45:26.401
Sep  4 17:45:26.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 17:45:26.402
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:26.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:26.429
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 09/04/23 17:45:26.437
Sep  4 17:45:26.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8" in namespace "downward-api-1894" to be "Succeeded or Failed"
Sep  4 17:45:26.467: INFO: Pod "downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.208011ms
Sep  4 17:45:28.473: INFO: Pod "downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024088476s
Sep  4 17:45:30.474: INFO: Pod "downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024924005s
STEP: Saw pod success 09/04/23 17:45:30.475
Sep  4 17:45:30.475: INFO: Pod "downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8" satisfied condition "Succeeded or Failed"
Sep  4 17:45:30.483: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8 container client-container: <nil>
STEP: delete the pod 09/04/23 17:45:30.497
Sep  4 17:45:30.515: INFO: Waiting for pod downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8 to disappear
Sep  4 17:45:30.520: INFO: Pod downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 17:45:30.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1894" for this suite. 09/04/23 17:45:30.527
------------------------------
â€¢ [4.137 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:45:26.401
    Sep  4 17:45:26.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 17:45:26.402
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:26.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:26.429
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 09/04/23 17:45:26.437
    Sep  4 17:45:26.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8" in namespace "downward-api-1894" to be "Succeeded or Failed"
    Sep  4 17:45:26.467: INFO: Pod "downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.208011ms
    Sep  4 17:45:28.473: INFO: Pod "downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024088476s
    Sep  4 17:45:30.474: INFO: Pod "downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024924005s
    STEP: Saw pod success 09/04/23 17:45:30.475
    Sep  4 17:45:30.475: INFO: Pod "downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8" satisfied condition "Succeeded or Failed"
    Sep  4 17:45:30.483: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8 container client-container: <nil>
    STEP: delete the pod 09/04/23 17:45:30.497
    Sep  4 17:45:30.515: INFO: Waiting for pod downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8 to disappear
    Sep  4 17:45:30.520: INFO: Pod downwardapi-volume-f989bd50-4629-48ec-91b5-7ef1025a21a8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:45:30.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1894" for this suite. 09/04/23 17:45:30.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:45:30.544
Sep  4 17:45:30.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename job 09/04/23 17:45:30.545
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:30.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:30.57
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 09/04/23 17:45:30.577
STEP: Ensure pods equal to parallelism count is attached to the job 09/04/23 17:45:30.589
STEP: patching /status 09/04/23 17:45:32.6
STEP: updating /status 09/04/23 17:45:32.615
STEP: get /status 09/04/23 17:45:32.63
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  4 17:45:32.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3542" for this suite. 09/04/23 17:45:32.642
------------------------------
â€¢ [2.109 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:45:30.544
    Sep  4 17:45:30.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename job 09/04/23 17:45:30.545
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:30.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:30.57
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 09/04/23 17:45:30.577
    STEP: Ensure pods equal to parallelism count is attached to the job 09/04/23 17:45:30.589
    STEP: patching /status 09/04/23 17:45:32.6
    STEP: updating /status 09/04/23 17:45:32.615
    STEP: get /status 09/04/23 17:45:32.63
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:45:32.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3542" for this suite. 09/04/23 17:45:32.642
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:45:32.656
Sep  4 17:45:32.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 17:45:32.658
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:32.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:32.691
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 09/04/23 17:45:32.699
Sep  4 17:45:32.719: INFO: Waiting up to 5m0s for pod "downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80" in namespace "downward-api-8627" to be "Succeeded or Failed"
Sep  4 17:45:32.730: INFO: Pod "downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80": Phase="Pending", Reason="", readiness=false. Elapsed: 11.12644ms
Sep  4 17:45:34.738: INFO: Pod "downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018849425s
Sep  4 17:45:36.739: INFO: Pod "downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020624791s
STEP: Saw pod success 09/04/23 17:45:36.739
Sep  4 17:45:36.740: INFO: Pod "downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80" satisfied condition "Succeeded or Failed"
Sep  4 17:45:36.744: INFO: Trying to get logs from node tenant-000003 pod downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80 container dapi-container: <nil>
STEP: delete the pod 09/04/23 17:45:36.782
Sep  4 17:45:36.811: INFO: Waiting for pod downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80 to disappear
Sep  4 17:45:36.816: INFO: Pod downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  4 17:45:36.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8627" for this suite. 09/04/23 17:45:36.823
------------------------------
â€¢ [4.185 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:45:32.656
    Sep  4 17:45:32.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 17:45:32.658
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:32.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:32.691
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 09/04/23 17:45:32.699
    Sep  4 17:45:32.719: INFO: Waiting up to 5m0s for pod "downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80" in namespace "downward-api-8627" to be "Succeeded or Failed"
    Sep  4 17:45:32.730: INFO: Pod "downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80": Phase="Pending", Reason="", readiness=false. Elapsed: 11.12644ms
    Sep  4 17:45:34.738: INFO: Pod "downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018849425s
    Sep  4 17:45:36.739: INFO: Pod "downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020624791s
    STEP: Saw pod success 09/04/23 17:45:36.739
    Sep  4 17:45:36.740: INFO: Pod "downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80" satisfied condition "Succeeded or Failed"
    Sep  4 17:45:36.744: INFO: Trying to get logs from node tenant-000003 pod downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80 container dapi-container: <nil>
    STEP: delete the pod 09/04/23 17:45:36.782
    Sep  4 17:45:36.811: INFO: Waiting for pod downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80 to disappear
    Sep  4 17:45:36.816: INFO: Pod downward-api-8b2488d0-6b4e-41e6-b233-c2cc73fb2e80 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:45:36.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8627" for this suite. 09/04/23 17:45:36.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:45:36.849
Sep  4 17:45:36.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename namespaces 09/04/23 17:45:36.851
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:36.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:36.877
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 09/04/23 17:45:36.884
Sep  4 17:45:36.896: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 09/04/23 17:45:36.896
Sep  4 17:45:36.906: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 09/04/23 17:45:36.906
Sep  4 17:45:36.923: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:45:36.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2853" for this suite. 09/04/23 17:45:36.933
------------------------------
â€¢ [0.095 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:45:36.849
    Sep  4 17:45:36.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename namespaces 09/04/23 17:45:36.851
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:36.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:36.877
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 09/04/23 17:45:36.884
    Sep  4 17:45:36.896: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 09/04/23 17:45:36.896
    Sep  4 17:45:36.906: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 09/04/23 17:45:36.906
    Sep  4 17:45:36.923: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:45:36.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2853" for this suite. 09/04/23 17:45:36.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:45:36.952
Sep  4 17:45:36.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename namespaces 09/04/23 17:45:36.953
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:36.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:36.991
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 09/04/23 17:45:36.999
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:37.019
STEP: Creating a pod in the namespace 09/04/23 17:45:37.027
STEP: Waiting for the pod to have running status 09/04/23 17:45:37.038
Sep  4 17:45:37.038: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6774" to be "running"
Sep  4 17:45:37.045: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.675041ms
Sep  4 17:45:39.052: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013298228s
Sep  4 17:45:39.052: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 09/04/23 17:45:39.052
STEP: Waiting for the namespace to be removed. 09/04/23 17:45:39.064
STEP: Recreating the namespace 09/04/23 17:45:50.071
STEP: Verifying there are no pods in the namespace 09/04/23 17:45:50.101
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:45:50.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4160" for this suite. 09/04/23 17:45:50.117
STEP: Destroying namespace "nsdeletetest-6774" for this suite. 09/04/23 17:45:50.131
Sep  4 17:45:50.139: INFO: Namespace nsdeletetest-6774 was already deleted
STEP: Destroying namespace "nsdeletetest-2620" for this suite. 09/04/23 17:45:50.139
------------------------------
â€¢ [SLOW TEST] [13.200 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:45:36.952
    Sep  4 17:45:36.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename namespaces 09/04/23 17:45:36.953
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:36.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:36.991
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 09/04/23 17:45:36.999
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:37.019
    STEP: Creating a pod in the namespace 09/04/23 17:45:37.027
    STEP: Waiting for the pod to have running status 09/04/23 17:45:37.038
    Sep  4 17:45:37.038: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6774" to be "running"
    Sep  4 17:45:37.045: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.675041ms
    Sep  4 17:45:39.052: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013298228s
    Sep  4 17:45:39.052: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 09/04/23 17:45:39.052
    STEP: Waiting for the namespace to be removed. 09/04/23 17:45:39.064
    STEP: Recreating the namespace 09/04/23 17:45:50.071
    STEP: Verifying there are no pods in the namespace 09/04/23 17:45:50.101
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:45:50.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4160" for this suite. 09/04/23 17:45:50.117
    STEP: Destroying namespace "nsdeletetest-6774" for this suite. 09/04/23 17:45:50.131
    Sep  4 17:45:50.139: INFO: Namespace nsdeletetest-6774 was already deleted
    STEP: Destroying namespace "nsdeletetest-2620" for this suite. 09/04/23 17:45:50.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:45:50.162
Sep  4 17:45:50.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename replicaset 09/04/23 17:45:50.164
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:50.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:50.194
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 09/04/23 17:45:50.201
STEP: Verify that the required pods have come up 09/04/23 17:45:50.212
Sep  4 17:45:50.223: INFO: Pod name sample-pod: Found 0 pods out of 3
Sep  4 17:45:55.230: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 09/04/23 17:45:55.23
Sep  4 17:45:55.235: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 09/04/23 17:45:55.235
STEP: DeleteCollection of the ReplicaSets 09/04/23 17:45:55.24
STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/04/23 17:45:55.254
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  4 17:45:55.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7180" for this suite. 09/04/23 17:45:55.331
------------------------------
â€¢ [SLOW TEST] [5.203 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:45:50.162
    Sep  4 17:45:50.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename replicaset 09/04/23 17:45:50.164
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:50.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:50.194
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 09/04/23 17:45:50.201
    STEP: Verify that the required pods have come up 09/04/23 17:45:50.212
    Sep  4 17:45:50.223: INFO: Pod name sample-pod: Found 0 pods out of 3
    Sep  4 17:45:55.230: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 09/04/23 17:45:55.23
    Sep  4 17:45:55.235: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 09/04/23 17:45:55.235
    STEP: DeleteCollection of the ReplicaSets 09/04/23 17:45:55.24
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/04/23 17:45:55.254
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:45:55.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7180" for this suite. 09/04/23 17:45:55.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:45:55.369
Sep  4 17:45:55.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename init-container 09/04/23 17:45:55.37
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:55.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:55.414
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 09/04/23 17:45:55.422
Sep  4 17:45:55.422: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:45:59.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1551" for this suite. 09/04/23 17:45:59.385
------------------------------
â€¢ [4.033 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:45:55.369
    Sep  4 17:45:55.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename init-container 09/04/23 17:45:55.37
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:55.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:55.414
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 09/04/23 17:45:55.422
    Sep  4 17:45:55.422: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:45:59.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1551" for this suite. 09/04/23 17:45:59.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:45:59.403
Sep  4 17:45:59.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 17:45:59.404
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:59.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:59.521
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 09/04/23 17:45:59.529
STEP: fetching the ConfigMap 09/04/23 17:45:59.536
STEP: patching the ConfigMap 09/04/23 17:45:59.541
STEP: listing all ConfigMaps in all namespaces with a label selector 09/04/23 17:45:59.551
STEP: deleting the ConfigMap by collection with a label selector 09/04/23 17:45:59.556
STEP: listing all ConfigMaps in test namespace 09/04/23 17:45:59.567
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 17:45:59.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6402" for this suite. 09/04/23 17:45:59.578
------------------------------
â€¢ [0.186 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:45:59.403
    Sep  4 17:45:59.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 17:45:59.404
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:59.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:59.521
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 09/04/23 17:45:59.529
    STEP: fetching the ConfigMap 09/04/23 17:45:59.536
    STEP: patching the ConfigMap 09/04/23 17:45:59.541
    STEP: listing all ConfigMaps in all namespaces with a label selector 09/04/23 17:45:59.551
    STEP: deleting the ConfigMap by collection with a label selector 09/04/23 17:45:59.556
    STEP: listing all ConfigMaps in test namespace 09/04/23 17:45:59.567
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:45:59.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6402" for this suite. 09/04/23 17:45:59.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:45:59.596
Sep  4 17:45:59.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename disruption 09/04/23 17:45:59.597
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:59.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:59.626
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 09/04/23 17:45:59.64
STEP: Waiting for all pods to be running 09/04/23 17:45:59.71
Sep  4 17:45:59.724: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  4 17:46:01.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7349" for this suite. 09/04/23 17:46:01.757
------------------------------
â€¢ [2.185 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:45:59.596
    Sep  4 17:45:59.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename disruption 09/04/23 17:45:59.597
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:45:59.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:45:59.626
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 09/04/23 17:45:59.64
    STEP: Waiting for all pods to be running 09/04/23 17:45:59.71
    Sep  4 17:45:59.724: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:46:01.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7349" for this suite. 09/04/23 17:46:01.757
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:46:01.781
Sep  4 17:46:01.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename daemonsets 09/04/23 17:46:01.781
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:01.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:01.82
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Sep  4 17:46:01.855: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 17:46:01.863
Sep  4 17:46:01.885: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 17:46:01.885: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 17:46:02.929: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 17:46:02.934: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 17:46:03.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 17:46:03.900: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 09/04/23 17:46:03.924
STEP: Check that daemon pods images are updated. 09/04/23 17:46:03.949
Sep  4 17:46:03.956: INFO: Wrong image for pod: daemon-set-dm25r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  4 17:46:03.956: INFO: Wrong image for pod: daemon-set-xdkqp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  4 17:46:04.981: INFO: Wrong image for pod: daemon-set-dm25r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  4 17:46:05.991: INFO: Wrong image for pod: daemon-set-dm25r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  4 17:46:07.127: INFO: Pod daemon-set-2qngj is not available
Sep  4 17:46:07.127: INFO: Wrong image for pod: daemon-set-dm25r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  4 17:46:08.109: INFO: Pod daemon-set-2qngj is not available
Sep  4 17:46:08.113: INFO: Wrong image for pod: daemon-set-dm25r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  4 17:46:09.980: INFO: Pod daemon-set-n8bsh is not available
STEP: Check that daemon pods are still running on every node of the cluster. 09/04/23 17:46:09.988
Sep  4 17:46:10.004: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 17:46:10.005: INFO: Node tenant-000003 is running 0 daemon pod, expected 1
Sep  4 17:46:11.025: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 17:46:11.026: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/04/23 17:46:11.065
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5340, will wait for the garbage collector to delete the pods 09/04/23 17:46:11.066
Sep  4 17:46:11.134: INFO: Deleting DaemonSet.extensions daemon-set took: 13.002053ms
Sep  4 17:46:11.235: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.292943ms
Sep  4 17:46:13.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 17:46:13.442: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  4 17:46:13.450: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13878"},"items":null}

Sep  4 17:46:13.458: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13878"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:46:13.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5340" for this suite. 09/04/23 17:46:13.488
------------------------------
â€¢ [SLOW TEST] [11.717 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:46:01.781
    Sep  4 17:46:01.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename daemonsets 09/04/23 17:46:01.781
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:01.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:01.82
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Sep  4 17:46:01.855: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 17:46:01.863
    Sep  4 17:46:01.885: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 17:46:01.885: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 17:46:02.929: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 17:46:02.934: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 17:46:03.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 17:46:03.900: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 09/04/23 17:46:03.924
    STEP: Check that daemon pods images are updated. 09/04/23 17:46:03.949
    Sep  4 17:46:03.956: INFO: Wrong image for pod: daemon-set-dm25r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  4 17:46:03.956: INFO: Wrong image for pod: daemon-set-xdkqp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  4 17:46:04.981: INFO: Wrong image for pod: daemon-set-dm25r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  4 17:46:05.991: INFO: Wrong image for pod: daemon-set-dm25r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  4 17:46:07.127: INFO: Pod daemon-set-2qngj is not available
    Sep  4 17:46:07.127: INFO: Wrong image for pod: daemon-set-dm25r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  4 17:46:08.109: INFO: Pod daemon-set-2qngj is not available
    Sep  4 17:46:08.113: INFO: Wrong image for pod: daemon-set-dm25r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  4 17:46:09.980: INFO: Pod daemon-set-n8bsh is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 09/04/23 17:46:09.988
    Sep  4 17:46:10.004: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 17:46:10.005: INFO: Node tenant-000003 is running 0 daemon pod, expected 1
    Sep  4 17:46:11.025: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 17:46:11.026: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/04/23 17:46:11.065
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5340, will wait for the garbage collector to delete the pods 09/04/23 17:46:11.066
    Sep  4 17:46:11.134: INFO: Deleting DaemonSet.extensions daemon-set took: 13.002053ms
    Sep  4 17:46:11.235: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.292943ms
    Sep  4 17:46:13.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 17:46:13.442: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  4 17:46:13.450: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13878"},"items":null}

    Sep  4 17:46:13.458: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13878"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:46:13.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5340" for this suite. 09/04/23 17:46:13.488
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:46:13.501
Sep  4 17:46:13.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 17:46:13.503
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:13.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:13.545
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 17:46:13.575
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:46:14.106
STEP: Deploying the webhook pod 09/04/23 17:46:14.123
STEP: Wait for the deployment to be ready 09/04/23 17:46:14.147
Sep  4 17:46:14.167: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 17:46:16.185
STEP: Verifying the service has paired with the endpoint 09/04/23 17:46:16.207
Sep  4 17:46:17.208: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 09/04/23 17:46:17.357
STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 17:46:17.442
STEP: Deleting the collection of validation webhooks 09/04/23 17:46:17.535
STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 17:46:17.605
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:46:17.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3557" for this suite. 09/04/23 17:46:17.697
STEP: Destroying namespace "webhook-3557-markers" for this suite. 09/04/23 17:46:17.737
------------------------------
â€¢ [4.257 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:46:13.501
    Sep  4 17:46:13.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 17:46:13.503
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:13.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:13.545
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 17:46:13.575
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:46:14.106
    STEP: Deploying the webhook pod 09/04/23 17:46:14.123
    STEP: Wait for the deployment to be ready 09/04/23 17:46:14.147
    Sep  4 17:46:14.167: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 17:46:16.185
    STEP: Verifying the service has paired with the endpoint 09/04/23 17:46:16.207
    Sep  4 17:46:17.208: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 09/04/23 17:46:17.357
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 17:46:17.442
    STEP: Deleting the collection of validation webhooks 09/04/23 17:46:17.535
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 17:46:17.605
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:46:17.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3557" for this suite. 09/04/23 17:46:17.697
    STEP: Destroying namespace "webhook-3557-markers" for this suite. 09/04/23 17:46:17.737
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:46:17.761
Sep  4 17:46:17.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename secrets 09/04/23 17:46:17.763
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:17.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:17.791
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-a97c7209-a636-4987-a3dd-27ccc0931ca7 09/04/23 17:46:17.839
STEP: Creating a pod to test consume secrets 09/04/23 17:46:17.847
Sep  4 17:46:17.862: INFO: Waiting up to 5m0s for pod "pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e" in namespace "secrets-3748" to be "Succeeded or Failed"
Sep  4 17:46:17.881: INFO: Pod "pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.854605ms
Sep  4 17:46:19.888: INFO: Pod "pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026032292s
Sep  4 17:46:21.888: INFO: Pod "pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025276772s
STEP: Saw pod success 09/04/23 17:46:21.888
Sep  4 17:46:21.888: INFO: Pod "pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e" satisfied condition "Succeeded or Failed"
Sep  4 17:46:21.895: INFO: Trying to get logs from node tenant-000001 pod pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 17:46:21.908
Sep  4 17:46:21.929: INFO: Waiting for pod pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e to disappear
Sep  4 17:46:21.934: INFO: Pod pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 17:46:21.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3748" for this suite. 09/04/23 17:46:21.944
STEP: Destroying namespace "secret-namespace-9638" for this suite. 09/04/23 17:46:21.952
------------------------------
â€¢ [4.205 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:46:17.761
    Sep  4 17:46:17.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename secrets 09/04/23 17:46:17.763
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:17.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:17.791
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-a97c7209-a636-4987-a3dd-27ccc0931ca7 09/04/23 17:46:17.839
    STEP: Creating a pod to test consume secrets 09/04/23 17:46:17.847
    Sep  4 17:46:17.862: INFO: Waiting up to 5m0s for pod "pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e" in namespace "secrets-3748" to be "Succeeded or Failed"
    Sep  4 17:46:17.881: INFO: Pod "pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.854605ms
    Sep  4 17:46:19.888: INFO: Pod "pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026032292s
    Sep  4 17:46:21.888: INFO: Pod "pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025276772s
    STEP: Saw pod success 09/04/23 17:46:21.888
    Sep  4 17:46:21.888: INFO: Pod "pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e" satisfied condition "Succeeded or Failed"
    Sep  4 17:46:21.895: INFO: Trying to get logs from node tenant-000001 pod pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 17:46:21.908
    Sep  4 17:46:21.929: INFO: Waiting for pod pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e to disappear
    Sep  4 17:46:21.934: INFO: Pod pod-secrets-92cf0fca-1d13-4a9f-9236-48693702936e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:46:21.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3748" for this suite. 09/04/23 17:46:21.944
    STEP: Destroying namespace "secret-namespace-9638" for this suite. 09/04/23 17:46:21.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:46:21.966
Sep  4 17:46:21.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 17:46:21.967
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:21.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:22.002
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 09/04/23 17:46:22.009
Sep  4 17:46:22.025: INFO: Waiting up to 5m0s for pod "pod-38deda2d-eeeb-457e-a53a-e47aa731712b" in namespace "emptydir-8634" to be "Succeeded or Failed"
Sep  4 17:46:22.043: INFO: Pod "pod-38deda2d-eeeb-457e-a53a-e47aa731712b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.793838ms
Sep  4 17:46:24.050: INFO: Pod "pod-38deda2d-eeeb-457e-a53a-e47aa731712b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024285027s
Sep  4 17:46:26.053: INFO: Pod "pod-38deda2d-eeeb-457e-a53a-e47aa731712b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027730433s
STEP: Saw pod success 09/04/23 17:46:26.054
Sep  4 17:46:26.054: INFO: Pod "pod-38deda2d-eeeb-457e-a53a-e47aa731712b" satisfied condition "Succeeded or Failed"
Sep  4 17:46:26.059: INFO: Trying to get logs from node tenant-000001 pod pod-38deda2d-eeeb-457e-a53a-e47aa731712b container test-container: <nil>
STEP: delete the pod 09/04/23 17:46:26.069
Sep  4 17:46:26.092: INFO: Waiting for pod pod-38deda2d-eeeb-457e-a53a-e47aa731712b to disappear
Sep  4 17:46:26.098: INFO: Pod pod-38deda2d-eeeb-457e-a53a-e47aa731712b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 17:46:26.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8634" for this suite. 09/04/23 17:46:26.104
------------------------------
â€¢ [4.153 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:46:21.966
    Sep  4 17:46:21.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 17:46:21.967
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:21.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:22.002
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/04/23 17:46:22.009
    Sep  4 17:46:22.025: INFO: Waiting up to 5m0s for pod "pod-38deda2d-eeeb-457e-a53a-e47aa731712b" in namespace "emptydir-8634" to be "Succeeded or Failed"
    Sep  4 17:46:22.043: INFO: Pod "pod-38deda2d-eeeb-457e-a53a-e47aa731712b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.793838ms
    Sep  4 17:46:24.050: INFO: Pod "pod-38deda2d-eeeb-457e-a53a-e47aa731712b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024285027s
    Sep  4 17:46:26.053: INFO: Pod "pod-38deda2d-eeeb-457e-a53a-e47aa731712b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027730433s
    STEP: Saw pod success 09/04/23 17:46:26.054
    Sep  4 17:46:26.054: INFO: Pod "pod-38deda2d-eeeb-457e-a53a-e47aa731712b" satisfied condition "Succeeded or Failed"
    Sep  4 17:46:26.059: INFO: Trying to get logs from node tenant-000001 pod pod-38deda2d-eeeb-457e-a53a-e47aa731712b container test-container: <nil>
    STEP: delete the pod 09/04/23 17:46:26.069
    Sep  4 17:46:26.092: INFO: Waiting for pod pod-38deda2d-eeeb-457e-a53a-e47aa731712b to disappear
    Sep  4 17:46:26.098: INFO: Pod pod-38deda2d-eeeb-457e-a53a-e47aa731712b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:46:26.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8634" for this suite. 09/04/23 17:46:26.104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:46:26.12
Sep  4 17:46:26.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 17:46:26.124
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:26.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:26.161
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-4692/configmap-test-ac91b5e6-be7c-4976-b86f-322c41bdeefb 09/04/23 17:46:26.169
STEP: Creating a pod to test consume configMaps 09/04/23 17:46:26.177
Sep  4 17:46:26.192: INFO: Waiting up to 5m0s for pod "pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f" in namespace "configmap-4692" to be "Succeeded or Failed"
Sep  4 17:46:26.200: INFO: Pod "pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.037667ms
Sep  4 17:46:28.207: INFO: Pod "pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014845585s
Sep  4 17:46:30.207: INFO: Pod "pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014483733s
STEP: Saw pod success 09/04/23 17:46:30.207
Sep  4 17:46:30.207: INFO: Pod "pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f" satisfied condition "Succeeded or Failed"
Sep  4 17:46:30.214: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f container env-test: <nil>
STEP: delete the pod 09/04/23 17:46:30.226
Sep  4 17:46:30.244: INFO: Waiting for pod pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f to disappear
Sep  4 17:46:30.249: INFO: Pod pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 17:46:30.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4692" for this suite. 09/04/23 17:46:30.256
------------------------------
â€¢ [4.146 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:46:26.12
    Sep  4 17:46:26.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 17:46:26.124
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:26.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:26.161
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-4692/configmap-test-ac91b5e6-be7c-4976-b86f-322c41bdeefb 09/04/23 17:46:26.169
    STEP: Creating a pod to test consume configMaps 09/04/23 17:46:26.177
    Sep  4 17:46:26.192: INFO: Waiting up to 5m0s for pod "pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f" in namespace "configmap-4692" to be "Succeeded or Failed"
    Sep  4 17:46:26.200: INFO: Pod "pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.037667ms
    Sep  4 17:46:28.207: INFO: Pod "pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014845585s
    Sep  4 17:46:30.207: INFO: Pod "pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014483733s
    STEP: Saw pod success 09/04/23 17:46:30.207
    Sep  4 17:46:30.207: INFO: Pod "pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f" satisfied condition "Succeeded or Failed"
    Sep  4 17:46:30.214: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f container env-test: <nil>
    STEP: delete the pod 09/04/23 17:46:30.226
    Sep  4 17:46:30.244: INFO: Waiting for pod pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f to disappear
    Sep  4 17:46:30.249: INFO: Pod pod-configmaps-ca40e0e3-b607-4bd4-8853-c82dda296b2f no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:46:30.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4692" for this suite. 09/04/23 17:46:30.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:46:30.272
Sep  4 17:46:30.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename gc 09/04/23 17:46:30.274
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:30.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:30.303
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 09/04/23 17:46:30.311
STEP: Wait for the Deployment to create new ReplicaSet 09/04/23 17:46:30.322
STEP: delete the deployment 09/04/23 17:46:30.332
STEP: wait for all rs to be garbage collected 09/04/23 17:46:30.357
STEP: expected 0 pods, got 2 pods 09/04/23 17:46:30.409
STEP: Gathering metrics 09/04/23 17:46:30.928
W0904 17:46:30.944937      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep  4 17:46:30.945: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  4 17:46:30.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2140" for this suite. 09/04/23 17:46:30.953
------------------------------
â€¢ [0.692 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:46:30.272
    Sep  4 17:46:30.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename gc 09/04/23 17:46:30.274
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:30.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:30.303
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 09/04/23 17:46:30.311
    STEP: Wait for the Deployment to create new ReplicaSet 09/04/23 17:46:30.322
    STEP: delete the deployment 09/04/23 17:46:30.332
    STEP: wait for all rs to be garbage collected 09/04/23 17:46:30.357
    STEP: expected 0 pods, got 2 pods 09/04/23 17:46:30.409
    STEP: Gathering metrics 09/04/23 17:46:30.928
    W0904 17:46:30.944937      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep  4 17:46:30.945: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:46:30.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2140" for this suite. 09/04/23 17:46:30.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:46:30.975
Sep  4 17:46:30.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 17:46:30.976
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:30.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:31.002
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-24206abe-bb98-4f12-aadd-3dbfaa0eea81 09/04/23 17:46:31.01
STEP: Creating a pod to test consume configMaps 09/04/23 17:46:31.022
Sep  4 17:46:31.031: INFO: Waiting up to 5m0s for pod "pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345" in namespace "configmap-1226" to be "Succeeded or Failed"
Sep  4 17:46:31.046: INFO: Pod "pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345": Phase="Pending", Reason="", readiness=false. Elapsed: 15.619083ms
Sep  4 17:46:33.055: INFO: Pod "pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02436806s
Sep  4 17:46:35.056: INFO: Pod "pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025023828s
STEP: Saw pod success 09/04/23 17:46:35.056
Sep  4 17:46:35.057: INFO: Pod "pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345" satisfied condition "Succeeded or Failed"
Sep  4 17:46:35.063: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 17:46:35.075
Sep  4 17:46:35.101: INFO: Waiting for pod pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345 to disappear
Sep  4 17:46:35.106: INFO: Pod pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 17:46:35.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1226" for this suite. 09/04/23 17:46:35.113
------------------------------
â€¢ [4.149 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:46:30.975
    Sep  4 17:46:30.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 17:46:30.976
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:30.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:31.002
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-24206abe-bb98-4f12-aadd-3dbfaa0eea81 09/04/23 17:46:31.01
    STEP: Creating a pod to test consume configMaps 09/04/23 17:46:31.022
    Sep  4 17:46:31.031: INFO: Waiting up to 5m0s for pod "pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345" in namespace "configmap-1226" to be "Succeeded or Failed"
    Sep  4 17:46:31.046: INFO: Pod "pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345": Phase="Pending", Reason="", readiness=false. Elapsed: 15.619083ms
    Sep  4 17:46:33.055: INFO: Pod "pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02436806s
    Sep  4 17:46:35.056: INFO: Pod "pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025023828s
    STEP: Saw pod success 09/04/23 17:46:35.056
    Sep  4 17:46:35.057: INFO: Pod "pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345" satisfied condition "Succeeded or Failed"
    Sep  4 17:46:35.063: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 17:46:35.075
    Sep  4 17:46:35.101: INFO: Waiting for pod pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345 to disappear
    Sep  4 17:46:35.106: INFO: Pod pod-configmaps-39b5acfd-8bdb-40db-be26-3311a85b7345 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:46:35.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1226" for this suite. 09/04/23 17:46:35.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:46:35.136
Sep  4 17:46:35.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 17:46:35.138
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:35.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:35.167
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 09/04/23 17:46:35.174
Sep  4 17:46:35.175: INFO: namespace kubectl-9026
Sep  4 17:46:35.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9026 create -f -'
Sep  4 17:46:35.942: INFO: stderr: ""
Sep  4 17:46:35.942: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/04/23 17:46:35.942
Sep  4 17:46:36.950: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 17:46:36.950: INFO: Found 0 / 1
Sep  4 17:46:37.949: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 17:46:37.949: INFO: Found 1 / 1
Sep  4 17:46:37.949: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  4 17:46:37.957: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 17:46:37.957: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  4 17:46:37.957: INFO: wait on agnhost-primary startup in kubectl-9026 
Sep  4 17:46:37.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9026 logs agnhost-primary-5trz7 agnhost-primary'
Sep  4 17:46:38.040: INFO: stderr: ""
Sep  4 17:46:38.040: INFO: stdout: "Paused\n"
STEP: exposing RC 09/04/23 17:46:38.04
Sep  4 17:46:38.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9026 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Sep  4 17:46:38.146: INFO: stderr: ""
Sep  4 17:46:38.146: INFO: stdout: "service/rm2 exposed\n"
Sep  4 17:46:38.154: INFO: Service rm2 in namespace kubectl-9026 found.
STEP: exposing service 09/04/23 17:46:40.166
Sep  4 17:46:40.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9026 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Sep  4 17:46:40.271: INFO: stderr: ""
Sep  4 17:46:40.271: INFO: stdout: "service/rm3 exposed\n"
Sep  4 17:46:40.279: INFO: Service rm3 in namespace kubectl-9026 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 17:46:42.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9026" for this suite. 09/04/23 17:46:42.297
------------------------------
â€¢ [SLOW TEST] [7.176 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:46:35.136
    Sep  4 17:46:35.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 17:46:35.138
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:35.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:35.167
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 09/04/23 17:46:35.174
    Sep  4 17:46:35.175: INFO: namespace kubectl-9026
    Sep  4 17:46:35.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9026 create -f -'
    Sep  4 17:46:35.942: INFO: stderr: ""
    Sep  4 17:46:35.942: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/04/23 17:46:35.942
    Sep  4 17:46:36.950: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 17:46:36.950: INFO: Found 0 / 1
    Sep  4 17:46:37.949: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 17:46:37.949: INFO: Found 1 / 1
    Sep  4 17:46:37.949: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep  4 17:46:37.957: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 17:46:37.957: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  4 17:46:37.957: INFO: wait on agnhost-primary startup in kubectl-9026 
    Sep  4 17:46:37.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9026 logs agnhost-primary-5trz7 agnhost-primary'
    Sep  4 17:46:38.040: INFO: stderr: ""
    Sep  4 17:46:38.040: INFO: stdout: "Paused\n"
    STEP: exposing RC 09/04/23 17:46:38.04
    Sep  4 17:46:38.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9026 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Sep  4 17:46:38.146: INFO: stderr: ""
    Sep  4 17:46:38.146: INFO: stdout: "service/rm2 exposed\n"
    Sep  4 17:46:38.154: INFO: Service rm2 in namespace kubectl-9026 found.
    STEP: exposing service 09/04/23 17:46:40.166
    Sep  4 17:46:40.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9026 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Sep  4 17:46:40.271: INFO: stderr: ""
    Sep  4 17:46:40.271: INFO: stdout: "service/rm3 exposed\n"
    Sep  4 17:46:40.279: INFO: Service rm3 in namespace kubectl-9026 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:46:42.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9026" for this suite. 09/04/23 17:46:42.297
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:46:42.315
Sep  4 17:46:42.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 17:46:42.317
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:42.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:42.363
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 09/04/23 17:46:42.375
STEP: watching for the Service to be added 09/04/23 17:46:42.398
Sep  4 17:46:42.405: INFO: Found Service test-service-qftgc in namespace services-7918 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Sep  4 17:46:42.405: INFO: Service test-service-qftgc created
STEP: Getting /status 09/04/23 17:46:42.405
Sep  4 17:46:42.412: INFO: Service test-service-qftgc has LoadBalancer: {[]}
STEP: patching the ServiceStatus 09/04/23 17:46:42.412
STEP: watching for the Service to be patched 09/04/23 17:46:42.426
Sep  4 17:46:42.431: INFO: observed Service test-service-qftgc in namespace services-7918 with annotations: map[] & LoadBalancer: {[]}
Sep  4 17:46:42.432: INFO: Found Service test-service-qftgc in namespace services-7918 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Sep  4 17:46:42.432: INFO: Service test-service-qftgc has service status patched
STEP: updating the ServiceStatus 09/04/23 17:46:42.432
Sep  4 17:46:42.453: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 09/04/23 17:46:42.454
Sep  4 17:46:42.460: INFO: Observed Service test-service-qftgc in namespace services-7918 with annotations: map[] & Conditions: {[]}
Sep  4 17:46:42.460: INFO: Observed event: &Service{ObjectMeta:{test-service-qftgc  services-7918  fc8a4d7c-94cb-44c4-b4e6-3f010c9d3fba 14263 0 2023-09-04 17:46:42 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-04 17:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-04 17:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.109.58,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.109.58],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Sep  4 17:46:42.461: INFO: Found Service test-service-qftgc in namespace services-7918 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  4 17:46:42.461: INFO: Service test-service-qftgc has service status updated
STEP: patching the service 09/04/23 17:46:42.461
STEP: watching for the Service to be patched 09/04/23 17:46:42.476
Sep  4 17:46:42.481: INFO: observed Service test-service-qftgc in namespace services-7918 with labels: map[test-service-static:true]
Sep  4 17:46:42.481: INFO: observed Service test-service-qftgc in namespace services-7918 with labels: map[test-service-static:true]
Sep  4 17:46:42.481: INFO: observed Service test-service-qftgc in namespace services-7918 with labels: map[test-service-static:true]
Sep  4 17:46:42.482: INFO: Found Service test-service-qftgc in namespace services-7918 with labels: map[test-service:patched test-service-static:true]
Sep  4 17:46:42.482: INFO: Service test-service-qftgc patched
STEP: deleting the service 09/04/23 17:46:42.482
STEP: watching for the Service to be deleted 09/04/23 17:46:42.508
Sep  4 17:46:42.514: INFO: Observed event: ADDED
Sep  4 17:46:42.514: INFO: Observed event: MODIFIED
Sep  4 17:46:42.514: INFO: Observed event: MODIFIED
Sep  4 17:46:42.514: INFO: Observed event: MODIFIED
Sep  4 17:46:42.515: INFO: Found Service test-service-qftgc in namespace services-7918 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Sep  4 17:46:42.515: INFO: Service test-service-qftgc deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 17:46:42.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7918" for this suite. 09/04/23 17:46:42.523
------------------------------
â€¢ [0.221 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:46:42.315
    Sep  4 17:46:42.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 17:46:42.317
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:42.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:42.363
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 09/04/23 17:46:42.375
    STEP: watching for the Service to be added 09/04/23 17:46:42.398
    Sep  4 17:46:42.405: INFO: Found Service test-service-qftgc in namespace services-7918 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Sep  4 17:46:42.405: INFO: Service test-service-qftgc created
    STEP: Getting /status 09/04/23 17:46:42.405
    Sep  4 17:46:42.412: INFO: Service test-service-qftgc has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 09/04/23 17:46:42.412
    STEP: watching for the Service to be patched 09/04/23 17:46:42.426
    Sep  4 17:46:42.431: INFO: observed Service test-service-qftgc in namespace services-7918 with annotations: map[] & LoadBalancer: {[]}
    Sep  4 17:46:42.432: INFO: Found Service test-service-qftgc in namespace services-7918 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Sep  4 17:46:42.432: INFO: Service test-service-qftgc has service status patched
    STEP: updating the ServiceStatus 09/04/23 17:46:42.432
    Sep  4 17:46:42.453: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 09/04/23 17:46:42.454
    Sep  4 17:46:42.460: INFO: Observed Service test-service-qftgc in namespace services-7918 with annotations: map[] & Conditions: {[]}
    Sep  4 17:46:42.460: INFO: Observed event: &Service{ObjectMeta:{test-service-qftgc  services-7918  fc8a4d7c-94cb-44c4-b4e6-3f010c9d3fba 14263 0 2023-09-04 17:46:42 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-04 17:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-04 17:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.109.58,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.109.58],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Sep  4 17:46:42.461: INFO: Found Service test-service-qftgc in namespace services-7918 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  4 17:46:42.461: INFO: Service test-service-qftgc has service status updated
    STEP: patching the service 09/04/23 17:46:42.461
    STEP: watching for the Service to be patched 09/04/23 17:46:42.476
    Sep  4 17:46:42.481: INFO: observed Service test-service-qftgc in namespace services-7918 with labels: map[test-service-static:true]
    Sep  4 17:46:42.481: INFO: observed Service test-service-qftgc in namespace services-7918 with labels: map[test-service-static:true]
    Sep  4 17:46:42.481: INFO: observed Service test-service-qftgc in namespace services-7918 with labels: map[test-service-static:true]
    Sep  4 17:46:42.482: INFO: Found Service test-service-qftgc in namespace services-7918 with labels: map[test-service:patched test-service-static:true]
    Sep  4 17:46:42.482: INFO: Service test-service-qftgc patched
    STEP: deleting the service 09/04/23 17:46:42.482
    STEP: watching for the Service to be deleted 09/04/23 17:46:42.508
    Sep  4 17:46:42.514: INFO: Observed event: ADDED
    Sep  4 17:46:42.514: INFO: Observed event: MODIFIED
    Sep  4 17:46:42.514: INFO: Observed event: MODIFIED
    Sep  4 17:46:42.514: INFO: Observed event: MODIFIED
    Sep  4 17:46:42.515: INFO: Found Service test-service-qftgc in namespace services-7918 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Sep  4 17:46:42.515: INFO: Service test-service-qftgc deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:46:42.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7918" for this suite. 09/04/23 17:46:42.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:46:42.556
Sep  4 17:46:42.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename secrets 09/04/23 17:46:42.557
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:42.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:42.603
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-572935a7-7699-47e8-b6ff-815ca3cf0c1b 09/04/23 17:46:42.611
STEP: Creating a pod to test consume secrets 09/04/23 17:46:42.622
Sep  4 17:46:42.637: INFO: Waiting up to 5m0s for pod "pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6" in namespace "secrets-5055" to be "Succeeded or Failed"
Sep  4 17:46:42.653: INFO: Pod "pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.893862ms
Sep  4 17:46:44.660: INFO: Pod "pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6": Phase="Running", Reason="", readiness=false. Elapsed: 2.022599764s
Sep  4 17:46:46.663: INFO: Pod "pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025267625s
STEP: Saw pod success 09/04/23 17:46:46.663
Sep  4 17:46:46.663: INFO: Pod "pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6" satisfied condition "Succeeded or Failed"
Sep  4 17:46:46.670: INFO: Trying to get logs from node tenant-000001 pod pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6 container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 17:46:46.68
Sep  4 17:46:46.701: INFO: Waiting for pod pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6 to disappear
Sep  4 17:46:46.706: INFO: Pod pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 17:46:46.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5055" for this suite. 09/04/23 17:46:46.714
------------------------------
â€¢ [4.171 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:46:42.556
    Sep  4 17:46:42.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename secrets 09/04/23 17:46:42.557
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:42.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:42.603
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-572935a7-7699-47e8-b6ff-815ca3cf0c1b 09/04/23 17:46:42.611
    STEP: Creating a pod to test consume secrets 09/04/23 17:46:42.622
    Sep  4 17:46:42.637: INFO: Waiting up to 5m0s for pod "pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6" in namespace "secrets-5055" to be "Succeeded or Failed"
    Sep  4 17:46:42.653: INFO: Pod "pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.893862ms
    Sep  4 17:46:44.660: INFO: Pod "pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6": Phase="Running", Reason="", readiness=false. Elapsed: 2.022599764s
    Sep  4 17:46:46.663: INFO: Pod "pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025267625s
    STEP: Saw pod success 09/04/23 17:46:46.663
    Sep  4 17:46:46.663: INFO: Pod "pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6" satisfied condition "Succeeded or Failed"
    Sep  4 17:46:46.670: INFO: Trying to get logs from node tenant-000001 pod pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6 container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 17:46:46.68
    Sep  4 17:46:46.701: INFO: Waiting for pod pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6 to disappear
    Sep  4 17:46:46.706: INFO: Pod pod-secrets-6e98d08a-25e8-4763-98b6-530244edd0d6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:46:46.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5055" for this suite. 09/04/23 17:46:46.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:46:46.737
Sep  4 17:46:46.737: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 17:46:46.744
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:46.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:46.777
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 17:46:46.84
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:46:47.504
STEP: Deploying the webhook pod 09/04/23 17:46:47.523
STEP: Wait for the deployment to be ready 09/04/23 17:46:47.552
Sep  4 17:46:47.576: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 17:46:49.598
STEP: Verifying the service has paired with the endpoint 09/04/23 17:46:49.621
Sep  4 17:46:50.621: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Sep  4 17:46:50.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1742-crds.webhook.example.com via the AdmissionRegistration API 09/04/23 17:46:51.148
STEP: Creating a custom resource that should be mutated by the webhook 09/04/23 17:46:51.187
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:46:53.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8822" for this suite. 09/04/23 17:46:53.919
STEP: Destroying namespace "webhook-8822-markers" for this suite. 09/04/23 17:46:53.948
------------------------------
â€¢ [SLOW TEST] [7.231 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:46:46.737
    Sep  4 17:46:46.737: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 17:46:46.744
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:46.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:46.777
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 17:46:46.84
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:46:47.504
    STEP: Deploying the webhook pod 09/04/23 17:46:47.523
    STEP: Wait for the deployment to be ready 09/04/23 17:46:47.552
    Sep  4 17:46:47.576: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 17:46:49.598
    STEP: Verifying the service has paired with the endpoint 09/04/23 17:46:49.621
    Sep  4 17:46:50.621: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Sep  4 17:46:50.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1742-crds.webhook.example.com via the AdmissionRegistration API 09/04/23 17:46:51.148
    STEP: Creating a custom resource that should be mutated by the webhook 09/04/23 17:46:51.187
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:46:53.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8822" for this suite. 09/04/23 17:46:53.919
    STEP: Destroying namespace "webhook-8822-markers" for this suite. 09/04/23 17:46:53.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:46:53.973
Sep  4 17:46:53.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 17:46:53.983
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:54.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:54.035
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-63a00570-5d77-47d2-b362-f87fccf4cad1 09/04/23 17:46:54.06
STEP: Creating a pod to test consume configMaps 09/04/23 17:46:54.082
Sep  4 17:46:54.114: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c" in namespace "projected-6798" to be "Succeeded or Failed"
Sep  4 17:46:54.122: INFO: Pod "pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.153338ms
Sep  4 17:46:56.128: INFO: Pod "pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01340266s
Sep  4 17:46:58.131: INFO: Pod "pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016094948s
STEP: Saw pod success 09/04/23 17:46:58.131
Sep  4 17:46:58.131: INFO: Pod "pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c" satisfied condition "Succeeded or Failed"
Sep  4 17:46:58.136: INFO: Trying to get logs from node tenant-000001 pod pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c container agnhost-container: <nil>
STEP: delete the pod 09/04/23 17:46:58.149
Sep  4 17:46:58.174: INFO: Waiting for pod pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c to disappear
Sep  4 17:46:58.178: INFO: Pod pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 17:46:58.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6798" for this suite. 09/04/23 17:46:58.184
------------------------------
â€¢ [4.223 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:46:53.973
    Sep  4 17:46:53.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 17:46:53.983
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:54.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:54.035
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-63a00570-5d77-47d2-b362-f87fccf4cad1 09/04/23 17:46:54.06
    STEP: Creating a pod to test consume configMaps 09/04/23 17:46:54.082
    Sep  4 17:46:54.114: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c" in namespace "projected-6798" to be "Succeeded or Failed"
    Sep  4 17:46:54.122: INFO: Pod "pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.153338ms
    Sep  4 17:46:56.128: INFO: Pod "pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01340266s
    Sep  4 17:46:58.131: INFO: Pod "pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016094948s
    STEP: Saw pod success 09/04/23 17:46:58.131
    Sep  4 17:46:58.131: INFO: Pod "pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c" satisfied condition "Succeeded or Failed"
    Sep  4 17:46:58.136: INFO: Trying to get logs from node tenant-000001 pod pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 17:46:58.149
    Sep  4 17:46:58.174: INFO: Waiting for pod pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c to disappear
    Sep  4 17:46:58.178: INFO: Pod pod-projected-configmaps-f10c02f5-e060-4aaa-8d5b-847f07963c7c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:46:58.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6798" for this suite. 09/04/23 17:46:58.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:46:58.212
Sep  4 17:46:58.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 17:46:58.213
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:58.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:58.239
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/04/23 17:46:58.246
Sep  4 17:46:58.256: INFO: Waiting up to 5m0s for pod "pod-6b018106-0e8e-4443-ab7d-07c6827c969a" in namespace "emptydir-6778" to be "Succeeded or Failed"
Sep  4 17:46:58.270: INFO: Pod "pod-6b018106-0e8e-4443-ab7d-07c6827c969a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.692535ms
Sep  4 17:47:00.276: INFO: Pod "pod-6b018106-0e8e-4443-ab7d-07c6827c969a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020129826s
Sep  4 17:47:02.277: INFO: Pod "pod-6b018106-0e8e-4443-ab7d-07c6827c969a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020551349s
STEP: Saw pod success 09/04/23 17:47:02.277
Sep  4 17:47:02.278: INFO: Pod "pod-6b018106-0e8e-4443-ab7d-07c6827c969a" satisfied condition "Succeeded or Failed"
Sep  4 17:47:02.286: INFO: Trying to get logs from node tenant-000001 pod pod-6b018106-0e8e-4443-ab7d-07c6827c969a container test-container: <nil>
STEP: delete the pod 09/04/23 17:47:02.296
Sep  4 17:47:02.330: INFO: Waiting for pod pod-6b018106-0e8e-4443-ab7d-07c6827c969a to disappear
Sep  4 17:47:02.336: INFO: Pod pod-6b018106-0e8e-4443-ab7d-07c6827c969a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 17:47:02.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6778" for this suite. 09/04/23 17:47:02.347
------------------------------
â€¢ [4.152 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:46:58.212
    Sep  4 17:46:58.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 17:46:58.213
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:46:58.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:46:58.239
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/04/23 17:46:58.246
    Sep  4 17:46:58.256: INFO: Waiting up to 5m0s for pod "pod-6b018106-0e8e-4443-ab7d-07c6827c969a" in namespace "emptydir-6778" to be "Succeeded or Failed"
    Sep  4 17:46:58.270: INFO: Pod "pod-6b018106-0e8e-4443-ab7d-07c6827c969a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.692535ms
    Sep  4 17:47:00.276: INFO: Pod "pod-6b018106-0e8e-4443-ab7d-07c6827c969a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020129826s
    Sep  4 17:47:02.277: INFO: Pod "pod-6b018106-0e8e-4443-ab7d-07c6827c969a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020551349s
    STEP: Saw pod success 09/04/23 17:47:02.277
    Sep  4 17:47:02.278: INFO: Pod "pod-6b018106-0e8e-4443-ab7d-07c6827c969a" satisfied condition "Succeeded or Failed"
    Sep  4 17:47:02.286: INFO: Trying to get logs from node tenant-000001 pod pod-6b018106-0e8e-4443-ab7d-07c6827c969a container test-container: <nil>
    STEP: delete the pod 09/04/23 17:47:02.296
    Sep  4 17:47:02.330: INFO: Waiting for pod pod-6b018106-0e8e-4443-ab7d-07c6827c969a to disappear
    Sep  4 17:47:02.336: INFO: Pod pod-6b018106-0e8e-4443-ab7d-07c6827c969a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:47:02.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6778" for this suite. 09/04/23 17:47:02.347
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:47:02.369
Sep  4 17:47:02.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 17:47:02.37
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:47:02.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:47:02.397
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 09/04/23 17:47:02.404
Sep  4 17:47:02.418: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b" in namespace "projected-9105" to be "Succeeded or Failed"
Sep  4 17:47:02.441: INFO: Pod "downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.657707ms
Sep  4 17:47:04.451: INFO: Pod "downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b": Phase="Running", Reason="", readiness=true. Elapsed: 2.033083088s
Sep  4 17:47:06.449: INFO: Pod "downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b": Phase="Running", Reason="", readiness=false. Elapsed: 4.030565607s
Sep  4 17:47:08.448: INFO: Pod "downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029551227s
STEP: Saw pod success 09/04/23 17:47:08.448
Sep  4 17:47:08.448: INFO: Pod "downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b" satisfied condition "Succeeded or Failed"
Sep  4 17:47:08.456: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b container client-container: <nil>
STEP: delete the pod 09/04/23 17:47:08.466
Sep  4 17:47:08.481: INFO: Waiting for pod downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b to disappear
Sep  4 17:47:08.487: INFO: Pod downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 17:47:08.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9105" for this suite. 09/04/23 17:47:08.496
------------------------------
â€¢ [SLOW TEST] [6.138 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:47:02.369
    Sep  4 17:47:02.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 17:47:02.37
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:47:02.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:47:02.397
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 09/04/23 17:47:02.404
    Sep  4 17:47:02.418: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b" in namespace "projected-9105" to be "Succeeded or Failed"
    Sep  4 17:47:02.441: INFO: Pod "downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.657707ms
    Sep  4 17:47:04.451: INFO: Pod "downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b": Phase="Running", Reason="", readiness=true. Elapsed: 2.033083088s
    Sep  4 17:47:06.449: INFO: Pod "downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b": Phase="Running", Reason="", readiness=false. Elapsed: 4.030565607s
    Sep  4 17:47:08.448: INFO: Pod "downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029551227s
    STEP: Saw pod success 09/04/23 17:47:08.448
    Sep  4 17:47:08.448: INFO: Pod "downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b" satisfied condition "Succeeded or Failed"
    Sep  4 17:47:08.456: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b container client-container: <nil>
    STEP: delete the pod 09/04/23 17:47:08.466
    Sep  4 17:47:08.481: INFO: Waiting for pod downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b to disappear
    Sep  4 17:47:08.487: INFO: Pod downwardapi-volume-b25bb7d8-5331-49ec-a451-c58fac8fdf2b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:47:08.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9105" for this suite. 09/04/23 17:47:08.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:47:08.512
Sep  4 17:47:08.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename secrets 09/04/23 17:47:08.513
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:47:08.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:47:08.541
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 17:47:08.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8117" for this suite. 09/04/23 17:47:08.628
------------------------------
â€¢ [0.130 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:47:08.512
    Sep  4 17:47:08.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename secrets 09/04/23 17:47:08.513
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:47:08.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:47:08.541
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:47:08.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8117" for this suite. 09/04/23 17:47:08.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:47:08.649
Sep  4 17:47:08.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename sched-pred 09/04/23 17:47:08.651
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:47:08.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:47:08.68
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  4 17:47:08.688: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  4 17:47:08.699: INFO: Waiting for terminating namespaces to be deleted...
Sep  4 17:47:08.707: INFO: 
Logging pods the apiserver thinks is on node tenant-000001 before test
Sep  4 17:47:08.721: INFO: calico-node-dj2mb from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
Sep  4 17:47:08.721: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 17:47:08.721: INFO: konnectivity-agent-cc5gh from kube-system started at 2023-09-04 17:16:08 +0000 UTC (1 container statuses recorded)
Sep  4 17:47:08.721: INFO: 	Container konnectivity-agent ready: true, restart count 0
Sep  4 17:47:08.721: INFO: kube-proxy-959c5 from kube-system started at 2023-09-04 17:14:03 +0000 UTC (1 container statuses recorded)
Sep  4 17:47:08.721: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 17:47:08.722: INFO: sonobuoy from sonobuoy started at 2023-09-04 17:16:08 +0000 UTC (1 container statuses recorded)
Sep  4 17:47:08.722: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  4 17:47:08.722: INFO: sonobuoy-e2e-job-8da28a692bc241e2 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
Sep  4 17:47:08.722: INFO: 	Container e2e ready: true, restart count 0
Sep  4 17:47:08.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  4 17:47:08.722: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-64p86 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
Sep  4 17:47:08.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  4 17:47:08.722: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  4 17:47:08.722: INFO: 
Logging pods the apiserver thinks is on node tenant-000003 before test
Sep  4 17:47:08.731: INFO: calico-kube-controllers-5f94594857-4wpdt from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 17:47:08.731: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  4 17:47:08.731: INFO: calico-node-wlgx4 from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
Sep  4 17:47:08.731: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 17:47:08.731: INFO: coredns-6bfc4dc876-rnzz9 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 17:47:08.731: INFO: 	Container coredns ready: true, restart count 0
Sep  4 17:47:08.731: INFO: coredns-6bfc4dc876-xp5ff from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 17:47:08.731: INFO: 	Container coredns ready: true, restart count 0
Sep  4 17:47:08.731: INFO: konnectivity-agent-j62r4 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 17:47:08.731: INFO: 	Container konnectivity-agent ready: true, restart count 0
Sep  4 17:47:08.731: INFO: kube-proxy-z2kxt from kube-system started at 2023-09-04 17:15:09 +0000 UTC (1 container statuses recorded)
Sep  4 17:47:08.731: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 17:47:08.731: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-7wr4k from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
Sep  4 17:47:08.731: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  4 17:47:08.731: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 09/04/23 17:47:08.731
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1781c34351b6006c], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 09/04/23 17:47:08.776
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:47:09.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9742" for this suite. 09/04/23 17:47:09.792
------------------------------
â€¢ [1.160 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:47:08.649
    Sep  4 17:47:08.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename sched-pred 09/04/23 17:47:08.651
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:47:08.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:47:08.68
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  4 17:47:08.688: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  4 17:47:08.699: INFO: Waiting for terminating namespaces to be deleted...
    Sep  4 17:47:08.707: INFO: 
    Logging pods the apiserver thinks is on node tenant-000001 before test
    Sep  4 17:47:08.721: INFO: calico-node-dj2mb from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
    Sep  4 17:47:08.721: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 17:47:08.721: INFO: konnectivity-agent-cc5gh from kube-system started at 2023-09-04 17:16:08 +0000 UTC (1 container statuses recorded)
    Sep  4 17:47:08.721: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Sep  4 17:47:08.721: INFO: kube-proxy-959c5 from kube-system started at 2023-09-04 17:14:03 +0000 UTC (1 container statuses recorded)
    Sep  4 17:47:08.721: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 17:47:08.722: INFO: sonobuoy from sonobuoy started at 2023-09-04 17:16:08 +0000 UTC (1 container statuses recorded)
    Sep  4 17:47:08.722: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  4 17:47:08.722: INFO: sonobuoy-e2e-job-8da28a692bc241e2 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
    Sep  4 17:47:08.722: INFO: 	Container e2e ready: true, restart count 0
    Sep  4 17:47:08.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  4 17:47:08.722: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-64p86 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
    Sep  4 17:47:08.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  4 17:47:08.722: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  4 17:47:08.722: INFO: 
    Logging pods the apiserver thinks is on node tenant-000003 before test
    Sep  4 17:47:08.731: INFO: calico-kube-controllers-5f94594857-4wpdt from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 17:47:08.731: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  4 17:47:08.731: INFO: calico-node-wlgx4 from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
    Sep  4 17:47:08.731: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 17:47:08.731: INFO: coredns-6bfc4dc876-rnzz9 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 17:47:08.731: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 17:47:08.731: INFO: coredns-6bfc4dc876-xp5ff from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 17:47:08.731: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 17:47:08.731: INFO: konnectivity-agent-j62r4 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 17:47:08.731: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Sep  4 17:47:08.731: INFO: kube-proxy-z2kxt from kube-system started at 2023-09-04 17:15:09 +0000 UTC (1 container statuses recorded)
    Sep  4 17:47:08.731: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 17:47:08.731: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-7wr4k from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
    Sep  4 17:47:08.731: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  4 17:47:08.731: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 09/04/23 17:47:08.731
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1781c34351b6006c], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 09/04/23 17:47:08.776
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:47:09.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9742" for this suite. 09/04/23 17:47:09.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:47:09.815
Sep  4 17:47:09.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename csiinlinevolumes 09/04/23 17:47:09.816
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:47:09.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:47:09.852
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 09/04/23 17:47:09.859
STEP: getting 09/04/23 17:47:09.895
STEP: listing in namespace 09/04/23 17:47:09.904
STEP: patching 09/04/23 17:47:09.913
STEP: deleting 09/04/23 17:47:09.925
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Sep  4 17:47:09.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-4018" for this suite. 09/04/23 17:47:09.952
------------------------------
â€¢ [0.148 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:47:09.815
    Sep  4 17:47:09.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename csiinlinevolumes 09/04/23 17:47:09.816
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:47:09.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:47:09.852
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 09/04/23 17:47:09.859
    STEP: getting 09/04/23 17:47:09.895
    STEP: listing in namespace 09/04/23 17:47:09.904
    STEP: patching 09/04/23 17:47:09.913
    STEP: deleting 09/04/23 17:47:09.925
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:47:09.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-4018" for this suite. 09/04/23 17:47:09.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:47:09.963
Sep  4 17:47:09.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename sched-preemption 09/04/23 17:47:09.964
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:47:09.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:47:09.992
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  4 17:47:10.025: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  4 17:48:10.069: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 09/04/23 17:48:10.075
Sep  4 17:48:10.156: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep  4 17:48:10.168: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep  4 17:48:10.198: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep  4 17:48:10.210: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/04/23 17:48:10.21
Sep  4 17:48:10.211: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3510" to be "running"
Sep  4 17:48:10.219: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.462504ms
Sep  4 17:48:12.226: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015666098s
Sep  4 17:48:12.227: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep  4 17:48:12.227: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3510" to be "running"
Sep  4 17:48:12.235: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.819265ms
Sep  4 17:48:12.235: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  4 17:48:12.235: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3510" to be "running"
Sep  4 17:48:12.240: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.926493ms
Sep  4 17:48:12.240: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  4 17:48:12.240: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3510" to be "running"
Sep  4 17:48:12.246: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.427822ms
Sep  4 17:48:12.246: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/04/23 17:48:12.246
Sep  4 17:48:12.262: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3510" to be "running"
Sep  4 17:48:12.285: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.057212ms
Sep  4 17:48:14.294: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031071314s
Sep  4 17:48:16.322: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059771266s
Sep  4 17:48:18.292: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.028999563s
Sep  4 17:48:18.292: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:48:18.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3510" for this suite. 09/04/23 17:48:18.404
------------------------------
â€¢ [SLOW TEST] [68.456 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:47:09.963
    Sep  4 17:47:09.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename sched-preemption 09/04/23 17:47:09.964
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:47:09.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:47:09.992
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  4 17:47:10.025: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  4 17:48:10.069: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 09/04/23 17:48:10.075
    Sep  4 17:48:10.156: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep  4 17:48:10.168: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep  4 17:48:10.198: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep  4 17:48:10.210: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/04/23 17:48:10.21
    Sep  4 17:48:10.211: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3510" to be "running"
    Sep  4 17:48:10.219: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.462504ms
    Sep  4 17:48:12.226: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015666098s
    Sep  4 17:48:12.227: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep  4 17:48:12.227: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3510" to be "running"
    Sep  4 17:48:12.235: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.819265ms
    Sep  4 17:48:12.235: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  4 17:48:12.235: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3510" to be "running"
    Sep  4 17:48:12.240: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.926493ms
    Sep  4 17:48:12.240: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  4 17:48:12.240: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3510" to be "running"
    Sep  4 17:48:12.246: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.427822ms
    Sep  4 17:48:12.246: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/04/23 17:48:12.246
    Sep  4 17:48:12.262: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3510" to be "running"
    Sep  4 17:48:12.285: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.057212ms
    Sep  4 17:48:14.294: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031071314s
    Sep  4 17:48:16.322: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059771266s
    Sep  4 17:48:18.292: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.028999563s
    Sep  4 17:48:18.292: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:48:18.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3510" for this suite. 09/04/23 17:48:18.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:48:18.421
Sep  4 17:48:18.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 17:48:18.422
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:18.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:18.449
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-ca4bfdf8-47b9-415d-aa0a-f2a08e58e5fa 09/04/23 17:48:18.457
STEP: Creating a pod to test consume configMaps 09/04/23 17:48:18.467
Sep  4 17:48:18.480: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637" in namespace "projected-6266" to be "Succeeded or Failed"
Sep  4 17:48:18.495: INFO: Pod "pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637": Phase="Pending", Reason="", readiness=false. Elapsed: 14.77838ms
Sep  4 17:48:20.503: INFO: Pod "pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022242314s
Sep  4 17:48:22.504: INFO: Pod "pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023796582s
STEP: Saw pod success 09/04/23 17:48:22.505
Sep  4 17:48:22.505: INFO: Pod "pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637" satisfied condition "Succeeded or Failed"
Sep  4 17:48:22.511: INFO: Trying to get logs from node tenant-000001 pod pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 17:48:22.522
Sep  4 17:48:22.544: INFO: Waiting for pod pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637 to disappear
Sep  4 17:48:22.551: INFO: Pod pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 17:48:22.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6266" for this suite. 09/04/23 17:48:22.561
------------------------------
â€¢ [4.154 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:48:18.421
    Sep  4 17:48:18.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 17:48:18.422
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:18.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:18.449
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-ca4bfdf8-47b9-415d-aa0a-f2a08e58e5fa 09/04/23 17:48:18.457
    STEP: Creating a pod to test consume configMaps 09/04/23 17:48:18.467
    Sep  4 17:48:18.480: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637" in namespace "projected-6266" to be "Succeeded or Failed"
    Sep  4 17:48:18.495: INFO: Pod "pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637": Phase="Pending", Reason="", readiness=false. Elapsed: 14.77838ms
    Sep  4 17:48:20.503: INFO: Pod "pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022242314s
    Sep  4 17:48:22.504: INFO: Pod "pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023796582s
    STEP: Saw pod success 09/04/23 17:48:22.505
    Sep  4 17:48:22.505: INFO: Pod "pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637" satisfied condition "Succeeded or Failed"
    Sep  4 17:48:22.511: INFO: Trying to get logs from node tenant-000001 pod pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 17:48:22.522
    Sep  4 17:48:22.544: INFO: Waiting for pod pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637 to disappear
    Sep  4 17:48:22.551: INFO: Pod pod-projected-configmaps-65369c25-f8e1-4d1d-860e-bb6f1e972637 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:48:22.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6266" for this suite. 09/04/23 17:48:22.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:48:22.583
Sep  4 17:48:22.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 17:48:22.584
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:22.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:22.611
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/04/23 17:48:22.619
Sep  4 17:48:22.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:48:24.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:48:32.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1786" for this suite. 09/04/23 17:48:32.923
------------------------------
â€¢ [SLOW TEST] [10.351 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:48:22.583
    Sep  4 17:48:22.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 17:48:22.584
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:22.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:22.611
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/04/23 17:48:22.619
    Sep  4 17:48:22.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:48:24.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:48:32.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1786" for this suite. 09/04/23 17:48:32.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:48:32.94
Sep  4 17:48:32.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir-wrapper 09/04/23 17:48:32.942
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:32.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:32.971
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Sep  4 17:48:33.003: INFO: Waiting up to 5m0s for pod "pod-secrets-c51dc7ce-6f51-4894-8d82-8e9a78b3229c" in namespace "emptydir-wrapper-7358" to be "running and ready"
Sep  4 17:48:33.021: INFO: Pod "pod-secrets-c51dc7ce-6f51-4894-8d82-8e9a78b3229c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.405413ms
Sep  4 17:48:33.021: INFO: The phase of Pod pod-secrets-c51dc7ce-6f51-4894-8d82-8e9a78b3229c is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:48:35.031: INFO: Pod "pod-secrets-c51dc7ce-6f51-4894-8d82-8e9a78b3229c": Phase="Running", Reason="", readiness=true. Elapsed: 2.027877937s
Sep  4 17:48:35.032: INFO: The phase of Pod pod-secrets-c51dc7ce-6f51-4894-8d82-8e9a78b3229c is Running (Ready = true)
Sep  4 17:48:35.032: INFO: Pod "pod-secrets-c51dc7ce-6f51-4894-8d82-8e9a78b3229c" satisfied condition "running and ready"
STEP: Cleaning up the secret 09/04/23 17:48:35.036
STEP: Cleaning up the configmap 09/04/23 17:48:35.046
STEP: Cleaning up the pod 09/04/23 17:48:35.058
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 17:48:35.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-7358" for this suite. 09/04/23 17:48:35.115
------------------------------
â€¢ [2.190 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:48:32.94
    Sep  4 17:48:32.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir-wrapper 09/04/23 17:48:32.942
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:32.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:32.971
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Sep  4 17:48:33.003: INFO: Waiting up to 5m0s for pod "pod-secrets-c51dc7ce-6f51-4894-8d82-8e9a78b3229c" in namespace "emptydir-wrapper-7358" to be "running and ready"
    Sep  4 17:48:33.021: INFO: Pod "pod-secrets-c51dc7ce-6f51-4894-8d82-8e9a78b3229c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.405413ms
    Sep  4 17:48:33.021: INFO: The phase of Pod pod-secrets-c51dc7ce-6f51-4894-8d82-8e9a78b3229c is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:48:35.031: INFO: Pod "pod-secrets-c51dc7ce-6f51-4894-8d82-8e9a78b3229c": Phase="Running", Reason="", readiness=true. Elapsed: 2.027877937s
    Sep  4 17:48:35.032: INFO: The phase of Pod pod-secrets-c51dc7ce-6f51-4894-8d82-8e9a78b3229c is Running (Ready = true)
    Sep  4 17:48:35.032: INFO: Pod "pod-secrets-c51dc7ce-6f51-4894-8d82-8e9a78b3229c" satisfied condition "running and ready"
    STEP: Cleaning up the secret 09/04/23 17:48:35.036
    STEP: Cleaning up the configmap 09/04/23 17:48:35.046
    STEP: Cleaning up the pod 09/04/23 17:48:35.058
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:48:35.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-7358" for this suite. 09/04/23 17:48:35.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:48:35.135
Sep  4 17:48:35.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 17:48:35.137
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:35.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:35.164
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 09/04/23 17:48:35.169
Sep  4 17:48:35.183: INFO: Waiting up to 5m0s for pod "pod-603ba317-b28f-45b2-9259-b9c28d6e3db3" in namespace "emptydir-1050" to be "Succeeded or Failed"
Sep  4 17:48:35.197: INFO: Pod "pod-603ba317-b28f-45b2-9259-b9c28d6e3db3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.198168ms
Sep  4 17:48:37.205: INFO: Pod "pod-603ba317-b28f-45b2-9259-b9c28d6e3db3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021187148s
Sep  4 17:48:39.203: INFO: Pod "pod-603ba317-b28f-45b2-9259-b9c28d6e3db3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01907799s
STEP: Saw pod success 09/04/23 17:48:39.203
Sep  4 17:48:39.203: INFO: Pod "pod-603ba317-b28f-45b2-9259-b9c28d6e3db3" satisfied condition "Succeeded or Failed"
Sep  4 17:48:39.211: INFO: Trying to get logs from node tenant-000001 pod pod-603ba317-b28f-45b2-9259-b9c28d6e3db3 container test-container: <nil>
STEP: delete the pod 09/04/23 17:48:39.25
Sep  4 17:48:39.285: INFO: Waiting for pod pod-603ba317-b28f-45b2-9259-b9c28d6e3db3 to disappear
Sep  4 17:48:39.290: INFO: Pod pod-603ba317-b28f-45b2-9259-b9c28d6e3db3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 17:48:39.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1050" for this suite. 09/04/23 17:48:39.299
------------------------------
â€¢ [4.174 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:48:35.135
    Sep  4 17:48:35.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 17:48:35.137
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:35.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:35.164
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/04/23 17:48:35.169
    Sep  4 17:48:35.183: INFO: Waiting up to 5m0s for pod "pod-603ba317-b28f-45b2-9259-b9c28d6e3db3" in namespace "emptydir-1050" to be "Succeeded or Failed"
    Sep  4 17:48:35.197: INFO: Pod "pod-603ba317-b28f-45b2-9259-b9c28d6e3db3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.198168ms
    Sep  4 17:48:37.205: INFO: Pod "pod-603ba317-b28f-45b2-9259-b9c28d6e3db3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021187148s
    Sep  4 17:48:39.203: INFO: Pod "pod-603ba317-b28f-45b2-9259-b9c28d6e3db3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01907799s
    STEP: Saw pod success 09/04/23 17:48:39.203
    Sep  4 17:48:39.203: INFO: Pod "pod-603ba317-b28f-45b2-9259-b9c28d6e3db3" satisfied condition "Succeeded or Failed"
    Sep  4 17:48:39.211: INFO: Trying to get logs from node tenant-000001 pod pod-603ba317-b28f-45b2-9259-b9c28d6e3db3 container test-container: <nil>
    STEP: delete the pod 09/04/23 17:48:39.25
    Sep  4 17:48:39.285: INFO: Waiting for pod pod-603ba317-b28f-45b2-9259-b9c28d6e3db3 to disappear
    Sep  4 17:48:39.290: INFO: Pod pod-603ba317-b28f-45b2-9259-b9c28d6e3db3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:48:39.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1050" for this suite. 09/04/23 17:48:39.299
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:48:39.314
Sep  4 17:48:39.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 17:48:39.316
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:39.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:39.347
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 17:48:39.373
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:48:39.566
STEP: Deploying the webhook pod 09/04/23 17:48:39.581
STEP: Wait for the deployment to be ready 09/04/23 17:48:39.602
Sep  4 17:48:39.617: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 17:48:41.634
STEP: Verifying the service has paired with the endpoint 09/04/23 17:48:41.654
Sep  4 17:48:42.655: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 09/04/23 17:48:42.666
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/04/23 17:48:42.668
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/04/23 17:48:42.668
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/04/23 17:48:42.669
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/04/23 17:48:42.671
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/04/23 17:48:42.671
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/04/23 17:48:42.674
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:48:42.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-875" for this suite. 09/04/23 17:48:42.933
STEP: Destroying namespace "webhook-875-markers" for this suite. 09/04/23 17:48:42.975
------------------------------
â€¢ [3.674 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:48:39.314
    Sep  4 17:48:39.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 17:48:39.316
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:39.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:39.347
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 17:48:39.373
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:48:39.566
    STEP: Deploying the webhook pod 09/04/23 17:48:39.581
    STEP: Wait for the deployment to be ready 09/04/23 17:48:39.602
    Sep  4 17:48:39.617: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 17:48:41.634
    STEP: Verifying the service has paired with the endpoint 09/04/23 17:48:41.654
    Sep  4 17:48:42.655: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 09/04/23 17:48:42.666
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/04/23 17:48:42.668
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/04/23 17:48:42.668
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/04/23 17:48:42.669
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/04/23 17:48:42.671
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/04/23 17:48:42.671
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/04/23 17:48:42.674
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:48:42.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-875" for this suite. 09/04/23 17:48:42.933
    STEP: Destroying namespace "webhook-875-markers" for this suite. 09/04/23 17:48:42.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:48:42.999
Sep  4 17:48:43.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename resourcequota 09/04/23 17:48:43.001
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:43.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:43.03
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 09/04/23 17:48:43.036
STEP: Creating a ResourceQuota 09/04/23 17:48:48.043
STEP: Ensuring resource quota status is calculated 09/04/23 17:48:48.053
STEP: Creating a ReplicaSet 09/04/23 17:48:50.061
STEP: Ensuring resource quota status captures replicaset creation 09/04/23 17:48:50.085
STEP: Deleting a ReplicaSet 09/04/23 17:48:52.092
STEP: Ensuring resource quota status released usage 09/04/23 17:48:52.104
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 17:48:54.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-414" for this suite. 09/04/23 17:48:54.12
------------------------------
â€¢ [SLOW TEST] [11.131 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:48:42.999
    Sep  4 17:48:43.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename resourcequota 09/04/23 17:48:43.001
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:43.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:43.03
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 09/04/23 17:48:43.036
    STEP: Creating a ResourceQuota 09/04/23 17:48:48.043
    STEP: Ensuring resource quota status is calculated 09/04/23 17:48:48.053
    STEP: Creating a ReplicaSet 09/04/23 17:48:50.061
    STEP: Ensuring resource quota status captures replicaset creation 09/04/23 17:48:50.085
    STEP: Deleting a ReplicaSet 09/04/23 17:48:52.092
    STEP: Ensuring resource quota status released usage 09/04/23 17:48:52.104
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:48:54.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-414" for this suite. 09/04/23 17:48:54.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:48:54.131
Sep  4 17:48:54.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 17:48:54.133
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:54.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:54.165
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-e1147856-c891-4864-8660-f058ef692bf3 09/04/23 17:48:54.17
STEP: Creating a pod to test consume secrets 09/04/23 17:48:54.177
Sep  4 17:48:54.191: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c" in namespace "projected-1171" to be "Succeeded or Failed"
Sep  4 17:48:54.199: INFO: Pod "pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.987617ms
Sep  4 17:48:56.209: INFO: Pod "pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016881793s
Sep  4 17:48:58.209: INFO: Pod "pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017066703s
STEP: Saw pod success 09/04/23 17:48:58.209
Sep  4 17:48:58.209: INFO: Pod "pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c" satisfied condition "Succeeded or Failed"
Sep  4 17:48:58.214: INFO: Trying to get logs from node tenant-000001 pod pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c container projected-secret-volume-test: <nil>
STEP: delete the pod 09/04/23 17:48:58.226
Sep  4 17:48:58.250: INFO: Waiting for pod pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c to disappear
Sep  4 17:48:58.255: INFO: Pod pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  4 17:48:58.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1171" for this suite. 09/04/23 17:48:58.261
------------------------------
â€¢ [4.139 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:48:54.131
    Sep  4 17:48:54.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 17:48:54.133
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:54.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:54.165
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-e1147856-c891-4864-8660-f058ef692bf3 09/04/23 17:48:54.17
    STEP: Creating a pod to test consume secrets 09/04/23 17:48:54.177
    Sep  4 17:48:54.191: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c" in namespace "projected-1171" to be "Succeeded or Failed"
    Sep  4 17:48:54.199: INFO: Pod "pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.987617ms
    Sep  4 17:48:56.209: INFO: Pod "pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016881793s
    Sep  4 17:48:58.209: INFO: Pod "pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017066703s
    STEP: Saw pod success 09/04/23 17:48:58.209
    Sep  4 17:48:58.209: INFO: Pod "pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c" satisfied condition "Succeeded or Failed"
    Sep  4 17:48:58.214: INFO: Trying to get logs from node tenant-000001 pod pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 17:48:58.226
    Sep  4 17:48:58.250: INFO: Waiting for pod pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c to disappear
    Sep  4 17:48:58.255: INFO: Pod pod-projected-secrets-24a59137-6313-4247-b395-9c7f76a3575c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:48:58.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1171" for this suite. 09/04/23 17:48:58.261
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:48:58.274
Sep  4 17:48:58.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename crd-webhook 09/04/23 17:48:58.276
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:58.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:58.303
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/04/23 17:48:58.308
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/04/23 17:48:58.609
STEP: Deploying the custom resource conversion webhook pod 09/04/23 17:48:58.617
STEP: Wait for the deployment to be ready 09/04/23 17:48:58.638
Sep  4 17:48:58.657: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 17:49:00.674
STEP: Verifying the service has paired with the endpoint 09/04/23 17:49:00.697
Sep  4 17:49:01.698: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Sep  4 17:49:01.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Creating a v1 custom resource 09/04/23 17:49:04.319
STEP: v2 custom resource should be converted 09/04/23 17:49:04.328
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:49:04.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-3944" for this suite. 09/04/23 17:49:04.946
------------------------------
â€¢ [SLOW TEST] [6.716 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:48:58.274
    Sep  4 17:48:58.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename crd-webhook 09/04/23 17:48:58.276
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:48:58.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:48:58.303
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/04/23 17:48:58.308
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/04/23 17:48:58.609
    STEP: Deploying the custom resource conversion webhook pod 09/04/23 17:48:58.617
    STEP: Wait for the deployment to be ready 09/04/23 17:48:58.638
    Sep  4 17:48:58.657: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 17:49:00.674
    STEP: Verifying the service has paired with the endpoint 09/04/23 17:49:00.697
    Sep  4 17:49:01.698: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Sep  4 17:49:01.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Creating a v1 custom resource 09/04/23 17:49:04.319
    STEP: v2 custom resource should be converted 09/04/23 17:49:04.328
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:49:04.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-3944" for this suite. 09/04/23 17:49:04.946
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:49:05.035
Sep  4 17:49:05.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/04/23 17:49:05.042
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:49:05.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:49:05.082
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 09/04/23 17:49:05.105
STEP: Creating hostNetwork=false pod 09/04/23 17:49:05.105
Sep  4 17:49:05.128: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8646" to be "running and ready"
Sep  4 17:49:05.134: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.319207ms
Sep  4 17:49:05.134: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:49:07.141: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012329319s
Sep  4 17:49:07.141: INFO: The phase of Pod test-pod is Running (Ready = true)
Sep  4 17:49:07.142: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 09/04/23 17:49:07.148
Sep  4 17:49:07.160: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8646" to be "running and ready"
Sep  4 17:49:07.165: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.636567ms
Sep  4 17:49:07.165: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:49:09.171: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010924238s
Sep  4 17:49:09.171: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Sep  4 17:49:09.171: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 09/04/23 17:49:09.177
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/04/23 17:49:09.177
Sep  4 17:49:09.178: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:49:09.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:49:09.179: INFO: ExecWithOptions: Clientset creation
Sep  4 17:49:09.179: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  4 17:49:09.295: INFO: Exec stderr: ""
Sep  4 17:49:09.295: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:49:09.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:49:09.296: INFO: ExecWithOptions: Clientset creation
Sep  4 17:49:09.296: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  4 17:49:09.378: INFO: Exec stderr: ""
Sep  4 17:49:09.378: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:49:09.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:49:09.379: INFO: ExecWithOptions: Clientset creation
Sep  4 17:49:09.379: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  4 17:49:09.452: INFO: Exec stderr: ""
Sep  4 17:49:09.452: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:49:09.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:49:09.453: INFO: ExecWithOptions: Clientset creation
Sep  4 17:49:09.454: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  4 17:49:09.532: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/04/23 17:49:09.532
Sep  4 17:49:09.532: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:49:09.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:49:09.536: INFO: ExecWithOptions: Clientset creation
Sep  4 17:49:09.536: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep  4 17:49:09.619: INFO: Exec stderr: ""
Sep  4 17:49:09.619: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:49:09.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:49:09.620: INFO: ExecWithOptions: Clientset creation
Sep  4 17:49:09.621: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep  4 17:49:09.711: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/04/23 17:49:09.712
Sep  4 17:49:09.712: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:49:09.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:49:09.713: INFO: ExecWithOptions: Clientset creation
Sep  4 17:49:09.713: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  4 17:49:09.812: INFO: Exec stderr: ""
Sep  4 17:49:09.817: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:49:09.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:49:09.817: INFO: ExecWithOptions: Clientset creation
Sep  4 17:49:09.818: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  4 17:49:09.926: INFO: Exec stderr: ""
Sep  4 17:49:09.926: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:49:09.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:49:09.927: INFO: ExecWithOptions: Clientset creation
Sep  4 17:49:09.928: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  4 17:49:10.015: INFO: Exec stderr: ""
Sep  4 17:49:10.015: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 17:49:10.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 17:49:10.015: INFO: ExecWithOptions: Clientset creation
Sep  4 17:49:10.015: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  4 17:49:10.082: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Sep  4 17:49:10.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8646" for this suite. 09/04/23 17:49:10.089
------------------------------
â€¢ [SLOW TEST] [5.066 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:49:05.035
    Sep  4 17:49:05.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/04/23 17:49:05.042
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:49:05.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:49:05.082
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 09/04/23 17:49:05.105
    STEP: Creating hostNetwork=false pod 09/04/23 17:49:05.105
    Sep  4 17:49:05.128: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8646" to be "running and ready"
    Sep  4 17:49:05.134: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.319207ms
    Sep  4 17:49:05.134: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:49:07.141: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012329319s
    Sep  4 17:49:07.141: INFO: The phase of Pod test-pod is Running (Ready = true)
    Sep  4 17:49:07.142: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 09/04/23 17:49:07.148
    Sep  4 17:49:07.160: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8646" to be "running and ready"
    Sep  4 17:49:07.165: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.636567ms
    Sep  4 17:49:07.165: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:49:09.171: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010924238s
    Sep  4 17:49:09.171: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Sep  4 17:49:09.171: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 09/04/23 17:49:09.177
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/04/23 17:49:09.177
    Sep  4 17:49:09.178: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:49:09.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:49:09.179: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:49:09.179: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  4 17:49:09.295: INFO: Exec stderr: ""
    Sep  4 17:49:09.295: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:49:09.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:49:09.296: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:49:09.296: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  4 17:49:09.378: INFO: Exec stderr: ""
    Sep  4 17:49:09.378: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:49:09.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:49:09.379: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:49:09.379: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  4 17:49:09.452: INFO: Exec stderr: ""
    Sep  4 17:49:09.452: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:49:09.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:49:09.453: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:49:09.454: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  4 17:49:09.532: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/04/23 17:49:09.532
    Sep  4 17:49:09.532: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:49:09.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:49:09.536: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:49:09.536: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep  4 17:49:09.619: INFO: Exec stderr: ""
    Sep  4 17:49:09.619: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:49:09.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:49:09.620: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:49:09.621: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep  4 17:49:09.711: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/04/23 17:49:09.712
    Sep  4 17:49:09.712: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:49:09.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:49:09.713: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:49:09.713: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  4 17:49:09.812: INFO: Exec stderr: ""
    Sep  4 17:49:09.817: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:49:09.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:49:09.817: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:49:09.818: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  4 17:49:09.926: INFO: Exec stderr: ""
    Sep  4 17:49:09.926: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:49:09.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:49:09.927: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:49:09.928: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  4 17:49:10.015: INFO: Exec stderr: ""
    Sep  4 17:49:10.015: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8646 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 17:49:10.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 17:49:10.015: INFO: ExecWithOptions: Clientset creation
    Sep  4 17:49:10.015: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8646/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  4 17:49:10.082: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:49:10.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-8646" for this suite. 09/04/23 17:49:10.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:49:10.107
Sep  4 17:49:10.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename prestop 09/04/23 17:49:10.109
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:49:10.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:49:10.142
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9968 09/04/23 17:49:10.147
STEP: Waiting for pods to come up. 09/04/23 17:49:10.16
Sep  4 17:49:10.160: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9968" to be "running"
Sep  4 17:49:10.166: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.064592ms
Sep  4 17:49:12.172: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.011644535s
Sep  4 17:49:12.172: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9968 09/04/23 17:49:12.179
Sep  4 17:49:12.190: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9968" to be "running"
Sep  4 17:49:12.204: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 13.934202ms
Sep  4 17:49:14.210: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.020441254s
Sep  4 17:49:14.211: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 09/04/23 17:49:14.211
Sep  4 17:49:19.259: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 09/04/23 17:49:19.259
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Sep  4 17:49:19.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-9968" for this suite. 09/04/23 17:49:19.29
------------------------------
â€¢ [SLOW TEST] [9.193 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:49:10.107
    Sep  4 17:49:10.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename prestop 09/04/23 17:49:10.109
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:49:10.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:49:10.142
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9968 09/04/23 17:49:10.147
    STEP: Waiting for pods to come up. 09/04/23 17:49:10.16
    Sep  4 17:49:10.160: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9968" to be "running"
    Sep  4 17:49:10.166: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.064592ms
    Sep  4 17:49:12.172: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.011644535s
    Sep  4 17:49:12.172: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9968 09/04/23 17:49:12.179
    Sep  4 17:49:12.190: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9968" to be "running"
    Sep  4 17:49:12.204: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 13.934202ms
    Sep  4 17:49:14.210: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.020441254s
    Sep  4 17:49:14.211: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 09/04/23 17:49:14.211
    Sep  4 17:49:19.259: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 09/04/23 17:49:19.259
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:49:19.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-9968" for this suite. 09/04/23 17:49:19.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:49:19.301
Sep  4 17:49:19.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 17:49:19.302
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:49:19.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:49:19.362
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3641 09/04/23 17:49:19.372
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/04/23 17:49:19.397
STEP: creating service externalsvc in namespace services-3641 09/04/23 17:49:19.398
STEP: creating replication controller externalsvc in namespace services-3641 09/04/23 17:49:19.416
I0904 17:49:19.427892      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3641, replica count: 2
I0904 17:49:22.479396      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 09/04/23 17:49:22.485
Sep  4 17:49:22.512: INFO: Creating new exec pod
Sep  4 17:49:22.528: INFO: Waiting up to 5m0s for pod "execpodrrz8x" in namespace "services-3641" to be "running"
Sep  4 17:49:22.543: INFO: Pod "execpodrrz8x": Phase="Pending", Reason="", readiness=false. Elapsed: 15.527651ms
Sep  4 17:49:24.550: INFO: Pod "execpodrrz8x": Phase="Running", Reason="", readiness=true. Elapsed: 2.022427714s
Sep  4 17:49:24.550: INFO: Pod "execpodrrz8x" satisfied condition "running"
Sep  4 17:49:24.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-3641 exec execpodrrz8x -- /bin/sh -x -c nslookup clusterip-service.services-3641.svc.cluster.local'
Sep  4 17:49:24.784: INFO: stderr: "+ nslookup clusterip-service.services-3641.svc.cluster.local\n"
Sep  4 17:49:24.784: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3641.svc.cluster.local\tcanonical name = externalsvc.services-3641.svc.cluster.local.\nName:\texternalsvc.services-3641.svc.cluster.local\nAddress: 10.96.157.142\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3641, will wait for the garbage collector to delete the pods 09/04/23 17:49:24.784
Sep  4 17:49:24.850: INFO: Deleting ReplicationController externalsvc took: 10.699386ms
Sep  4 17:49:24.953: INFO: Terminating ReplicationController externalsvc pods took: 103.233056ms
Sep  4 17:49:27.190: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 17:49:27.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3641" for this suite. 09/04/23 17:49:27.233
------------------------------
â€¢ [SLOW TEST] [7.943 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:49:19.301
    Sep  4 17:49:19.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 17:49:19.302
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:49:19.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:49:19.362
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3641 09/04/23 17:49:19.372
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/04/23 17:49:19.397
    STEP: creating service externalsvc in namespace services-3641 09/04/23 17:49:19.398
    STEP: creating replication controller externalsvc in namespace services-3641 09/04/23 17:49:19.416
    I0904 17:49:19.427892      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3641, replica count: 2
    I0904 17:49:22.479396      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 09/04/23 17:49:22.485
    Sep  4 17:49:22.512: INFO: Creating new exec pod
    Sep  4 17:49:22.528: INFO: Waiting up to 5m0s for pod "execpodrrz8x" in namespace "services-3641" to be "running"
    Sep  4 17:49:22.543: INFO: Pod "execpodrrz8x": Phase="Pending", Reason="", readiness=false. Elapsed: 15.527651ms
    Sep  4 17:49:24.550: INFO: Pod "execpodrrz8x": Phase="Running", Reason="", readiness=true. Elapsed: 2.022427714s
    Sep  4 17:49:24.550: INFO: Pod "execpodrrz8x" satisfied condition "running"
    Sep  4 17:49:24.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-3641 exec execpodrrz8x -- /bin/sh -x -c nslookup clusterip-service.services-3641.svc.cluster.local'
    Sep  4 17:49:24.784: INFO: stderr: "+ nslookup clusterip-service.services-3641.svc.cluster.local\n"
    Sep  4 17:49:24.784: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3641.svc.cluster.local\tcanonical name = externalsvc.services-3641.svc.cluster.local.\nName:\texternalsvc.services-3641.svc.cluster.local\nAddress: 10.96.157.142\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3641, will wait for the garbage collector to delete the pods 09/04/23 17:49:24.784
    Sep  4 17:49:24.850: INFO: Deleting ReplicationController externalsvc took: 10.699386ms
    Sep  4 17:49:24.953: INFO: Terminating ReplicationController externalsvc pods took: 103.233056ms
    Sep  4 17:49:27.190: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:49:27.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3641" for this suite. 09/04/23 17:49:27.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:49:27.268
Sep  4 17:49:27.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename gc 09/04/23 17:49:27.269
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:49:27.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:49:27.298
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 09/04/23 17:49:27.308
STEP: create the rc2 09/04/23 17:49:27.316
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/04/23 17:49:32.384
STEP: delete the rc simpletest-rc-to-be-deleted 09/04/23 17:49:34.436
STEP: wait for the rc to be deleted 09/04/23 17:49:34.567
Sep  4 17:49:39.727: INFO: 68 pods remaining
Sep  4 17:49:39.753: INFO: 68 pods has nil DeletionTimestamp
Sep  4 17:49:39.753: INFO: 
STEP: Gathering metrics 09/04/23 17:49:44.675
W0904 17:49:44.728779      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep  4 17:49:44.728: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  4 17:49:44.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-2cflh" in namespace "gc-9493"
Sep  4 17:49:44.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gvvj" in namespace "gc-9493"
Sep  4 17:49:44.788: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hmrz" in namespace "gc-9493"
Sep  4 17:49:44.855: INFO: Deleting pod "simpletest-rc-to-be-deleted-2j4qp" in namespace "gc-9493"
Sep  4 17:49:45.226: INFO: Deleting pod "simpletest-rc-to-be-deleted-2kbv8" in namespace "gc-9493"
Sep  4 17:49:45.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vc9g" in namespace "gc-9493"
Sep  4 17:49:45.628: INFO: Deleting pod "simpletest-rc-to-be-deleted-47xdx" in namespace "gc-9493"
Sep  4 17:49:45.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jbsn" in namespace "gc-9493"
Sep  4 17:49:46.134: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zb5d" in namespace "gc-9493"
Sep  4 17:49:46.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-54x6n" in namespace "gc-9493"
Sep  4 17:49:46.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-58gqs" in namespace "gc-9493"
Sep  4 17:49:46.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-5br4l" in namespace "gc-9493"
Sep  4 17:49:46.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jtqd" in namespace "gc-9493"
Sep  4 17:49:46.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-5plql" in namespace "gc-9493"
Sep  4 17:49:46.829: INFO: Deleting pod "simpletest-rc-to-be-deleted-62wt7" in namespace "gc-9493"
Sep  4 17:49:46.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-66m5s" in namespace "gc-9493"
Sep  4 17:49:47.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dstk" in namespace "gc-9493"
Sep  4 17:49:47.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zrqk" in namespace "gc-9493"
Sep  4 17:49:47.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-757xf" in namespace "gc-9493"
Sep  4 17:49:47.507: INFO: Deleting pod "simpletest-rc-to-be-deleted-77qkw" in namespace "gc-9493"
Sep  4 17:49:47.554: INFO: Deleting pod "simpletest-rc-to-be-deleted-7c7gw" in namespace "gc-9493"
Sep  4 17:49:47.678: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p2d8" in namespace "gc-9493"
Sep  4 17:49:47.749: INFO: Deleting pod "simpletest-rc-to-be-deleted-7sf4r" in namespace "gc-9493"
Sep  4 17:49:47.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vlx2" in namespace "gc-9493"
Sep  4 17:49:48.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-82rl2" in namespace "gc-9493"
Sep  4 17:49:48.164: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jf54" in namespace "gc-9493"
Sep  4 17:49:48.199: INFO: Deleting pod "simpletest-rc-to-be-deleted-9j4zf" in namespace "gc-9493"
Sep  4 17:49:48.516: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vn8x" in namespace "gc-9493"
Sep  4 17:49:48.561: INFO: Deleting pod "simpletest-rc-to-be-deleted-bptmd" in namespace "gc-9493"
Sep  4 17:49:48.817: INFO: Deleting pod "simpletest-rc-to-be-deleted-c8skg" in namespace "gc-9493"
Sep  4 17:49:49.155: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgswj" in namespace "gc-9493"
Sep  4 17:49:49.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-cq5hj" in namespace "gc-9493"
Sep  4 17:49:49.483: INFO: Deleting pod "simpletest-rc-to-be-deleted-cq8h7" in namespace "gc-9493"
Sep  4 17:49:49.531: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5bpv" in namespace "gc-9493"
Sep  4 17:49:49.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8gmc" in namespace "gc-9493"
Sep  4 17:49:49.625: INFO: Deleting pod "simpletest-rc-to-be-deleted-fcgvc" in namespace "gc-9493"
Sep  4 17:49:49.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgbrk" in namespace "gc-9493"
Sep  4 17:49:49.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkwjk" in namespace "gc-9493"
Sep  4 17:49:50.100: INFO: Deleting pod "simpletest-rc-to-be-deleted-fphnd" in namespace "gc-9493"
Sep  4 17:49:50.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx879" in namespace "gc-9493"
Sep  4 17:49:50.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfmrv" in namespace "gc-9493"
Sep  4 17:49:50.460: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggv6t" in namespace "gc-9493"
Sep  4 17:49:50.506: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmbs5" in namespace "gc-9493"
Sep  4 17:49:50.752: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6rjw" in namespace "gc-9493"
Sep  4 17:49:51.072: INFO: Deleting pod "simpletest-rc-to-be-deleted-hfhjm" in namespace "gc-9493"
Sep  4 17:49:51.322: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpp26" in namespace "gc-9493"
Sep  4 17:49:51.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvp4d" in namespace "gc-9493"
Sep  4 17:49:51.830: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxjmc" in namespace "gc-9493"
Sep  4 17:49:52.154: INFO: Deleting pod "simpletest-rc-to-be-deleted-j4w2z" in namespace "gc-9493"
Sep  4 17:49:52.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-j58rd" in namespace "gc-9493"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  4 17:49:52.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9493" for this suite. 09/04/23 17:49:52.304
------------------------------
â€¢ [SLOW TEST] [25.132 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:49:27.268
    Sep  4 17:49:27.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename gc 09/04/23 17:49:27.269
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:49:27.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:49:27.298
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 09/04/23 17:49:27.308
    STEP: create the rc2 09/04/23 17:49:27.316
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/04/23 17:49:32.384
    STEP: delete the rc simpletest-rc-to-be-deleted 09/04/23 17:49:34.436
    STEP: wait for the rc to be deleted 09/04/23 17:49:34.567
    Sep  4 17:49:39.727: INFO: 68 pods remaining
    Sep  4 17:49:39.753: INFO: 68 pods has nil DeletionTimestamp
    Sep  4 17:49:39.753: INFO: 
    STEP: Gathering metrics 09/04/23 17:49:44.675
    W0904 17:49:44.728779      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep  4 17:49:44.728: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep  4 17:49:44.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-2cflh" in namespace "gc-9493"
    Sep  4 17:49:44.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gvvj" in namespace "gc-9493"
    Sep  4 17:49:44.788: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hmrz" in namespace "gc-9493"
    Sep  4 17:49:44.855: INFO: Deleting pod "simpletest-rc-to-be-deleted-2j4qp" in namespace "gc-9493"
    Sep  4 17:49:45.226: INFO: Deleting pod "simpletest-rc-to-be-deleted-2kbv8" in namespace "gc-9493"
    Sep  4 17:49:45.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vc9g" in namespace "gc-9493"
    Sep  4 17:49:45.628: INFO: Deleting pod "simpletest-rc-to-be-deleted-47xdx" in namespace "gc-9493"
    Sep  4 17:49:45.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jbsn" in namespace "gc-9493"
    Sep  4 17:49:46.134: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zb5d" in namespace "gc-9493"
    Sep  4 17:49:46.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-54x6n" in namespace "gc-9493"
    Sep  4 17:49:46.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-58gqs" in namespace "gc-9493"
    Sep  4 17:49:46.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-5br4l" in namespace "gc-9493"
    Sep  4 17:49:46.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jtqd" in namespace "gc-9493"
    Sep  4 17:49:46.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-5plql" in namespace "gc-9493"
    Sep  4 17:49:46.829: INFO: Deleting pod "simpletest-rc-to-be-deleted-62wt7" in namespace "gc-9493"
    Sep  4 17:49:46.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-66m5s" in namespace "gc-9493"
    Sep  4 17:49:47.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dstk" in namespace "gc-9493"
    Sep  4 17:49:47.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zrqk" in namespace "gc-9493"
    Sep  4 17:49:47.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-757xf" in namespace "gc-9493"
    Sep  4 17:49:47.507: INFO: Deleting pod "simpletest-rc-to-be-deleted-77qkw" in namespace "gc-9493"
    Sep  4 17:49:47.554: INFO: Deleting pod "simpletest-rc-to-be-deleted-7c7gw" in namespace "gc-9493"
    Sep  4 17:49:47.678: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p2d8" in namespace "gc-9493"
    Sep  4 17:49:47.749: INFO: Deleting pod "simpletest-rc-to-be-deleted-7sf4r" in namespace "gc-9493"
    Sep  4 17:49:47.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vlx2" in namespace "gc-9493"
    Sep  4 17:49:48.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-82rl2" in namespace "gc-9493"
    Sep  4 17:49:48.164: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jf54" in namespace "gc-9493"
    Sep  4 17:49:48.199: INFO: Deleting pod "simpletest-rc-to-be-deleted-9j4zf" in namespace "gc-9493"
    Sep  4 17:49:48.516: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vn8x" in namespace "gc-9493"
    Sep  4 17:49:48.561: INFO: Deleting pod "simpletest-rc-to-be-deleted-bptmd" in namespace "gc-9493"
    Sep  4 17:49:48.817: INFO: Deleting pod "simpletest-rc-to-be-deleted-c8skg" in namespace "gc-9493"
    Sep  4 17:49:49.155: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgswj" in namespace "gc-9493"
    Sep  4 17:49:49.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-cq5hj" in namespace "gc-9493"
    Sep  4 17:49:49.483: INFO: Deleting pod "simpletest-rc-to-be-deleted-cq8h7" in namespace "gc-9493"
    Sep  4 17:49:49.531: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5bpv" in namespace "gc-9493"
    Sep  4 17:49:49.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8gmc" in namespace "gc-9493"
    Sep  4 17:49:49.625: INFO: Deleting pod "simpletest-rc-to-be-deleted-fcgvc" in namespace "gc-9493"
    Sep  4 17:49:49.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgbrk" in namespace "gc-9493"
    Sep  4 17:49:49.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkwjk" in namespace "gc-9493"
    Sep  4 17:49:50.100: INFO: Deleting pod "simpletest-rc-to-be-deleted-fphnd" in namespace "gc-9493"
    Sep  4 17:49:50.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx879" in namespace "gc-9493"
    Sep  4 17:49:50.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfmrv" in namespace "gc-9493"
    Sep  4 17:49:50.460: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggv6t" in namespace "gc-9493"
    Sep  4 17:49:50.506: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmbs5" in namespace "gc-9493"
    Sep  4 17:49:50.752: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6rjw" in namespace "gc-9493"
    Sep  4 17:49:51.072: INFO: Deleting pod "simpletest-rc-to-be-deleted-hfhjm" in namespace "gc-9493"
    Sep  4 17:49:51.322: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpp26" in namespace "gc-9493"
    Sep  4 17:49:51.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvp4d" in namespace "gc-9493"
    Sep  4 17:49:51.830: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxjmc" in namespace "gc-9493"
    Sep  4 17:49:52.154: INFO: Deleting pod "simpletest-rc-to-be-deleted-j4w2z" in namespace "gc-9493"
    Sep  4 17:49:52.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-j58rd" in namespace "gc-9493"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:49:52.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9493" for this suite. 09/04/23 17:49:52.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:49:52.449
Sep  4 17:49:52.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename sched-preemption 09/04/23 17:49:52.56
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:49:52.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:49:52.806
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  4 17:49:52.987: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  4 17:50:53.304: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 09/04/23 17:50:53.309
Sep  4 17:50:53.339: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep  4 17:50:53.350: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep  4 17:50:53.387: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep  4 17:50:53.395: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/04/23 17:50:53.395
Sep  4 17:50:53.396: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1255" to be "running"
Sep  4 17:50:53.403: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.110993ms
Sep  4 17:50:55.410: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.014231881s
Sep  4 17:50:55.410: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep  4 17:50:55.410: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1255" to be "running"
Sep  4 17:50:55.416: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.367087ms
Sep  4 17:50:55.416: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  4 17:50:55.416: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1255" to be "running"
Sep  4 17:50:55.421: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.742653ms
Sep  4 17:50:57.428: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.011903841s
Sep  4 17:50:57.429: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  4 17:50:57.429: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1255" to be "running"
Sep  4 17:50:57.434: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.539196ms
Sep  4 17:50:57.435: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 09/04/23 17:50:57.435
Sep  4 17:50:57.463: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Sep  4 17:50:57.489: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 26.148599ms
Sep  4 17:50:59.496: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033243184s
Sep  4 17:51:01.496: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.033166384s
Sep  4 17:51:01.497: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:51:01.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1255" for this suite. 09/04/23 17:51:01.633
------------------------------
â€¢ [SLOW TEST] [69.200 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:49:52.449
    Sep  4 17:49:52.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename sched-preemption 09/04/23 17:49:52.56
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:49:52.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:49:52.806
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  4 17:49:52.987: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  4 17:50:53.304: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 09/04/23 17:50:53.309
    Sep  4 17:50:53.339: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep  4 17:50:53.350: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep  4 17:50:53.387: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep  4 17:50:53.395: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/04/23 17:50:53.395
    Sep  4 17:50:53.396: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1255" to be "running"
    Sep  4 17:50:53.403: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.110993ms
    Sep  4 17:50:55.410: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.014231881s
    Sep  4 17:50:55.410: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep  4 17:50:55.410: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1255" to be "running"
    Sep  4 17:50:55.416: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.367087ms
    Sep  4 17:50:55.416: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  4 17:50:55.416: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1255" to be "running"
    Sep  4 17:50:55.421: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.742653ms
    Sep  4 17:50:57.428: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.011903841s
    Sep  4 17:50:57.429: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  4 17:50:57.429: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1255" to be "running"
    Sep  4 17:50:57.434: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.539196ms
    Sep  4 17:50:57.435: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 09/04/23 17:50:57.435
    Sep  4 17:50:57.463: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Sep  4 17:50:57.489: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 26.148599ms
    Sep  4 17:50:59.496: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033243184s
    Sep  4 17:51:01.496: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.033166384s
    Sep  4 17:51:01.497: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:51:01.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1255" for this suite. 09/04/23 17:51:01.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:51:01.65
Sep  4 17:51:01.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename containers 09/04/23 17:51:01.651
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:51:01.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:51:01.679
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Sep  4 17:51:01.701: INFO: Waiting up to 5m0s for pod "client-containers-c6779634-0f6e-47d7-9c79-8905a44da840" in namespace "containers-318" to be "running"
Sep  4 17:51:01.720: INFO: Pod "client-containers-c6779634-0f6e-47d7-9c79-8905a44da840": Phase="Pending", Reason="", readiness=false. Elapsed: 18.247077ms
Sep  4 17:51:03.727: INFO: Pod "client-containers-c6779634-0f6e-47d7-9c79-8905a44da840": Phase="Running", Reason="", readiness=true. Elapsed: 2.025363451s
Sep  4 17:51:03.727: INFO: Pod "client-containers-c6779634-0f6e-47d7-9c79-8905a44da840" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  4 17:51:03.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-318" for this suite. 09/04/23 17:51:03.778
------------------------------
â€¢ [2.141 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:51:01.65
    Sep  4 17:51:01.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename containers 09/04/23 17:51:01.651
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:51:01.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:51:01.679
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Sep  4 17:51:01.701: INFO: Waiting up to 5m0s for pod "client-containers-c6779634-0f6e-47d7-9c79-8905a44da840" in namespace "containers-318" to be "running"
    Sep  4 17:51:01.720: INFO: Pod "client-containers-c6779634-0f6e-47d7-9c79-8905a44da840": Phase="Pending", Reason="", readiness=false. Elapsed: 18.247077ms
    Sep  4 17:51:03.727: INFO: Pod "client-containers-c6779634-0f6e-47d7-9c79-8905a44da840": Phase="Running", Reason="", readiness=true. Elapsed: 2.025363451s
    Sep  4 17:51:03.727: INFO: Pod "client-containers-c6779634-0f6e-47d7-9c79-8905a44da840" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:51:03.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-318" for this suite. 09/04/23 17:51:03.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:51:03.81
Sep  4 17:51:03.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename security-context-test 09/04/23 17:51:03.811
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:51:03.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:51:03.84
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Sep  4 17:51:03.858: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-399c98f4-f594-4162-87d7-fa2d53a1d142" in namespace "security-context-test-4825" to be "Succeeded or Failed"
Sep  4 17:51:03.864: INFO: Pod "busybox-readonly-false-399c98f4-f594-4162-87d7-fa2d53a1d142": Phase="Pending", Reason="", readiness=false. Elapsed: 6.397557ms
Sep  4 17:51:05.871: INFO: Pod "busybox-readonly-false-399c98f4-f594-4162-87d7-fa2d53a1d142": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013029961s
Sep  4 17:51:07.872: INFO: Pod "busybox-readonly-false-399c98f4-f594-4162-87d7-fa2d53a1d142": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014480576s
Sep  4 17:51:07.872: INFO: Pod "busybox-readonly-false-399c98f4-f594-4162-87d7-fa2d53a1d142" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  4 17:51:07.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4825" for this suite. 09/04/23 17:51:07.879
------------------------------
â€¢ [4.082 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:51:03.81
    Sep  4 17:51:03.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename security-context-test 09/04/23 17:51:03.811
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:51:03.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:51:03.84
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Sep  4 17:51:03.858: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-399c98f4-f594-4162-87d7-fa2d53a1d142" in namespace "security-context-test-4825" to be "Succeeded or Failed"
    Sep  4 17:51:03.864: INFO: Pod "busybox-readonly-false-399c98f4-f594-4162-87d7-fa2d53a1d142": Phase="Pending", Reason="", readiness=false. Elapsed: 6.397557ms
    Sep  4 17:51:05.871: INFO: Pod "busybox-readonly-false-399c98f4-f594-4162-87d7-fa2d53a1d142": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013029961s
    Sep  4 17:51:07.872: INFO: Pod "busybox-readonly-false-399c98f4-f594-4162-87d7-fa2d53a1d142": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014480576s
    Sep  4 17:51:07.872: INFO: Pod "busybox-readonly-false-399c98f4-f594-4162-87d7-fa2d53a1d142" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:51:07.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4825" for this suite. 09/04/23 17:51:07.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:51:07.899
Sep  4 17:51:07.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 17:51:07.9
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:51:07.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:51:07.93
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-1993abf1-fabf-4cdf-a83f-c144e20175b3 09/04/23 17:51:07.941
STEP: Creating configMap with name cm-test-opt-upd-010e0a1c-3b67-496a-a525-98a1cd89d183 09/04/23 17:51:07.948
STEP: Creating the pod 09/04/23 17:51:07.96
Sep  4 17:51:07.972: INFO: Waiting up to 5m0s for pod "pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d" in namespace "configmap-7908" to be "running and ready"
Sep  4 17:51:07.980: INFO: Pod "pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.098751ms
Sep  4 17:51:07.980: INFO: The phase of Pod pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:51:09.986: INFO: Pod "pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014588947s
Sep  4 17:51:09.986: INFO: The phase of Pod pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:51:11.986: INFO: Pod "pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d": Phase="Running", Reason="", readiness=true. Elapsed: 4.014270788s
Sep  4 17:51:11.986: INFO: The phase of Pod pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d is Running (Ready = true)
Sep  4 17:51:11.986: INFO: Pod "pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-1993abf1-fabf-4cdf-a83f-c144e20175b3 09/04/23 17:51:12.025
STEP: Updating configmap cm-test-opt-upd-010e0a1c-3b67-496a-a525-98a1cd89d183 09/04/23 17:51:12.037
STEP: Creating configMap with name cm-test-opt-create-a281ac1d-3145-453a-bce5-0dca3a634dc4 09/04/23 17:51:12.048
STEP: waiting to observe update in volume 09/04/23 17:51:12.069
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 17:52:22.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7908" for this suite. 09/04/23 17:52:22.624
------------------------------
â€¢ [SLOW TEST] [74.737 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:51:07.899
    Sep  4 17:51:07.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 17:51:07.9
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:51:07.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:51:07.93
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-1993abf1-fabf-4cdf-a83f-c144e20175b3 09/04/23 17:51:07.941
    STEP: Creating configMap with name cm-test-opt-upd-010e0a1c-3b67-496a-a525-98a1cd89d183 09/04/23 17:51:07.948
    STEP: Creating the pod 09/04/23 17:51:07.96
    Sep  4 17:51:07.972: INFO: Waiting up to 5m0s for pod "pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d" in namespace "configmap-7908" to be "running and ready"
    Sep  4 17:51:07.980: INFO: Pod "pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.098751ms
    Sep  4 17:51:07.980: INFO: The phase of Pod pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:51:09.986: INFO: Pod "pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014588947s
    Sep  4 17:51:09.986: INFO: The phase of Pod pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:51:11.986: INFO: Pod "pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d": Phase="Running", Reason="", readiness=true. Elapsed: 4.014270788s
    Sep  4 17:51:11.986: INFO: The phase of Pod pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d is Running (Ready = true)
    Sep  4 17:51:11.986: INFO: Pod "pod-configmaps-dfe2843d-1aa1-4e70-a186-22743154566d" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-1993abf1-fabf-4cdf-a83f-c144e20175b3 09/04/23 17:51:12.025
    STEP: Updating configmap cm-test-opt-upd-010e0a1c-3b67-496a-a525-98a1cd89d183 09/04/23 17:51:12.037
    STEP: Creating configMap with name cm-test-opt-create-a281ac1d-3145-453a-bce5-0dca3a634dc4 09/04/23 17:51:12.048
    STEP: waiting to observe update in volume 09/04/23 17:51:12.069
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:52:22.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7908" for this suite. 09/04/23 17:52:22.624
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:52:22.636
Sep  4 17:52:22.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 17:52:22.637
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:52:22.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:52:22.667
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/04/23 17:52:22.673
Sep  4 17:52:22.689: INFO: Waiting up to 5m0s for pod "pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af" in namespace "emptydir-6073" to be "Succeeded or Failed"
Sep  4 17:52:22.702: INFO: Pod "pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af": Phase="Pending", Reason="", readiness=false. Elapsed: 13.311312ms
Sep  4 17:52:24.712: INFO: Pod "pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022633231s
Sep  4 17:52:26.709: INFO: Pod "pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01981028s
STEP: Saw pod success 09/04/23 17:52:26.709
Sep  4 17:52:26.709: INFO: Pod "pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af" satisfied condition "Succeeded or Failed"
Sep  4 17:52:26.715: INFO: Trying to get logs from node tenant-000003 pod pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af container test-container: <nil>
STEP: delete the pod 09/04/23 17:52:26.752
Sep  4 17:52:26.777: INFO: Waiting for pod pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af to disappear
Sep  4 17:52:26.785: INFO: Pod pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 17:52:26.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6073" for this suite. 09/04/23 17:52:26.792
------------------------------
â€¢ [4.166 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:52:22.636
    Sep  4 17:52:22.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 17:52:22.637
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:52:22.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:52:22.667
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/04/23 17:52:22.673
    Sep  4 17:52:22.689: INFO: Waiting up to 5m0s for pod "pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af" in namespace "emptydir-6073" to be "Succeeded or Failed"
    Sep  4 17:52:22.702: INFO: Pod "pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af": Phase="Pending", Reason="", readiness=false. Elapsed: 13.311312ms
    Sep  4 17:52:24.712: INFO: Pod "pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022633231s
    Sep  4 17:52:26.709: INFO: Pod "pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01981028s
    STEP: Saw pod success 09/04/23 17:52:26.709
    Sep  4 17:52:26.709: INFO: Pod "pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af" satisfied condition "Succeeded or Failed"
    Sep  4 17:52:26.715: INFO: Trying to get logs from node tenant-000003 pod pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af container test-container: <nil>
    STEP: delete the pod 09/04/23 17:52:26.752
    Sep  4 17:52:26.777: INFO: Waiting for pod pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af to disappear
    Sep  4 17:52:26.785: INFO: Pod pod-137d9ddf-5f2e-4500-b5e6-c209adc7c4af no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:52:26.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6073" for this suite. 09/04/23 17:52:26.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:52:26.811
Sep  4 17:52:26.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename subpath 09/04/23 17:52:26.812
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:52:26.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:52:26.842
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/04/23 17:52:26.849
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-c6sv 09/04/23 17:52:26.866
STEP: Creating a pod to test atomic-volume-subpath 09/04/23 17:52:26.866
Sep  4 17:52:26.877: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c6sv" in namespace "subpath-7321" to be "Succeeded or Failed"
Sep  4 17:52:26.884: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.405436ms
Sep  4 17:52:28.893: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 2.0152807s
Sep  4 17:52:30.891: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 4.013352042s
Sep  4 17:52:32.891: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 6.013522341s
Sep  4 17:52:34.891: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 8.013577534s
Sep  4 17:52:36.891: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 10.013054196s
Sep  4 17:52:38.890: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 12.012210743s
Sep  4 17:52:40.895: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 14.017491402s
Sep  4 17:52:42.893: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 16.015453269s
Sep  4 17:52:44.892: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 18.014644603s
Sep  4 17:52:46.891: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 20.013060497s
Sep  4 17:52:48.889: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=false. Elapsed: 22.011838851s
Sep  4 17:52:50.891: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=false. Elapsed: 24.01350266s
Sep  4 17:52:52.895: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.017215278s
STEP: Saw pod success 09/04/23 17:52:52.895
Sep  4 17:52:52.895: INFO: Pod "pod-subpath-test-configmap-c6sv" satisfied condition "Succeeded or Failed"
Sep  4 17:52:52.904: INFO: Trying to get logs from node tenant-000003 pod pod-subpath-test-configmap-c6sv container test-container-subpath-configmap-c6sv: <nil>
STEP: delete the pod 09/04/23 17:52:52.916
Sep  4 17:52:52.940: INFO: Waiting for pod pod-subpath-test-configmap-c6sv to disappear
Sep  4 17:52:52.945: INFO: Pod pod-subpath-test-configmap-c6sv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-c6sv 09/04/23 17:52:52.946
Sep  4 17:52:52.946: INFO: Deleting pod "pod-subpath-test-configmap-c6sv" in namespace "subpath-7321"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  4 17:52:52.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7321" for this suite. 09/04/23 17:52:52.958
------------------------------
â€¢ [SLOW TEST] [26.170 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:52:26.811
    Sep  4 17:52:26.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename subpath 09/04/23 17:52:26.812
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:52:26.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:52:26.842
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/04/23 17:52:26.849
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-c6sv 09/04/23 17:52:26.866
    STEP: Creating a pod to test atomic-volume-subpath 09/04/23 17:52:26.866
    Sep  4 17:52:26.877: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c6sv" in namespace "subpath-7321" to be "Succeeded or Failed"
    Sep  4 17:52:26.884: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.405436ms
    Sep  4 17:52:28.893: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 2.0152807s
    Sep  4 17:52:30.891: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 4.013352042s
    Sep  4 17:52:32.891: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 6.013522341s
    Sep  4 17:52:34.891: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 8.013577534s
    Sep  4 17:52:36.891: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 10.013054196s
    Sep  4 17:52:38.890: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 12.012210743s
    Sep  4 17:52:40.895: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 14.017491402s
    Sep  4 17:52:42.893: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 16.015453269s
    Sep  4 17:52:44.892: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 18.014644603s
    Sep  4 17:52:46.891: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=true. Elapsed: 20.013060497s
    Sep  4 17:52:48.889: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=false. Elapsed: 22.011838851s
    Sep  4 17:52:50.891: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Running", Reason="", readiness=false. Elapsed: 24.01350266s
    Sep  4 17:52:52.895: INFO: Pod "pod-subpath-test-configmap-c6sv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.017215278s
    STEP: Saw pod success 09/04/23 17:52:52.895
    Sep  4 17:52:52.895: INFO: Pod "pod-subpath-test-configmap-c6sv" satisfied condition "Succeeded or Failed"
    Sep  4 17:52:52.904: INFO: Trying to get logs from node tenant-000003 pod pod-subpath-test-configmap-c6sv container test-container-subpath-configmap-c6sv: <nil>
    STEP: delete the pod 09/04/23 17:52:52.916
    Sep  4 17:52:52.940: INFO: Waiting for pod pod-subpath-test-configmap-c6sv to disappear
    Sep  4 17:52:52.945: INFO: Pod pod-subpath-test-configmap-c6sv no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-c6sv 09/04/23 17:52:52.946
    Sep  4 17:52:52.946: INFO: Deleting pod "pod-subpath-test-configmap-c6sv" in namespace "subpath-7321"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:52:52.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7321" for this suite. 09/04/23 17:52:52.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:52:52.989
Sep  4 17:52:52.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubelet-test 09/04/23 17:52:52.99
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:52:53.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:52:53.019
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Sep  4 17:52:53.039: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsdc76fd6e-42cf-462a-aca7-fce8baf07c99" in namespace "kubelet-test-3814" to be "running and ready"
Sep  4 17:52:53.057: INFO: Pod "busybox-readonly-fsdc76fd6e-42cf-462a-aca7-fce8baf07c99": Phase="Pending", Reason="", readiness=false. Elapsed: 17.901058ms
Sep  4 17:52:53.057: INFO: The phase of Pod busybox-readonly-fsdc76fd6e-42cf-462a-aca7-fce8baf07c99 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:52:55.064: INFO: Pod "busybox-readonly-fsdc76fd6e-42cf-462a-aca7-fce8baf07c99": Phase="Running", Reason="", readiness=true. Elapsed: 2.025250401s
Sep  4 17:52:55.065: INFO: The phase of Pod busybox-readonly-fsdc76fd6e-42cf-462a-aca7-fce8baf07c99 is Running (Ready = true)
Sep  4 17:52:55.065: INFO: Pod "busybox-readonly-fsdc76fd6e-42cf-462a-aca7-fce8baf07c99" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  4 17:52:55.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3814" for this suite. 09/04/23 17:52:55.092
------------------------------
â€¢ [2.112 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:52:52.989
    Sep  4 17:52:52.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubelet-test 09/04/23 17:52:52.99
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:52:53.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:52:53.019
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Sep  4 17:52:53.039: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsdc76fd6e-42cf-462a-aca7-fce8baf07c99" in namespace "kubelet-test-3814" to be "running and ready"
    Sep  4 17:52:53.057: INFO: Pod "busybox-readonly-fsdc76fd6e-42cf-462a-aca7-fce8baf07c99": Phase="Pending", Reason="", readiness=false. Elapsed: 17.901058ms
    Sep  4 17:52:53.057: INFO: The phase of Pod busybox-readonly-fsdc76fd6e-42cf-462a-aca7-fce8baf07c99 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:52:55.064: INFO: Pod "busybox-readonly-fsdc76fd6e-42cf-462a-aca7-fce8baf07c99": Phase="Running", Reason="", readiness=true. Elapsed: 2.025250401s
    Sep  4 17:52:55.065: INFO: The phase of Pod busybox-readonly-fsdc76fd6e-42cf-462a-aca7-fce8baf07c99 is Running (Ready = true)
    Sep  4 17:52:55.065: INFO: Pod "busybox-readonly-fsdc76fd6e-42cf-462a-aca7-fce8baf07c99" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:52:55.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3814" for this suite. 09/04/23 17:52:55.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:52:55.106
Sep  4 17:52:55.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 17:52:55.108
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:52:55.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:52:55.145
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 17:52:55.17
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:52:55.875
STEP: Deploying the webhook pod 09/04/23 17:52:55.887
STEP: Wait for the deployment to be ready 09/04/23 17:52:55.916
Sep  4 17:52:55.933: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 17:52:57.95
STEP: Verifying the service has paired with the endpoint 09/04/23 17:52:57.972
Sep  4 17:52:58.972: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 09/04/23 17:52:58.977
STEP: create a pod 09/04/23 17:52:59.011
Sep  4 17:52:59.025: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-5124" to be "running"
Sep  4 17:52:59.031: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.269936ms
Sep  4 17:53:01.038: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013102836s
Sep  4 17:53:01.039: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 09/04/23 17:53:01.039
Sep  4 17:53:01.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=webhook-5124 attach --namespace=webhook-5124 to-be-attached-pod -i -c=container1'
Sep  4 17:53:01.146: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:53:01.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5124" for this suite. 09/04/23 17:53:01.238
STEP: Destroying namespace "webhook-5124-markers" for this suite. 09/04/23 17:53:01.271
------------------------------
â€¢ [SLOW TEST] [6.180 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:52:55.106
    Sep  4 17:52:55.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 17:52:55.108
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:52:55.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:52:55.145
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 17:52:55.17
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 17:52:55.875
    STEP: Deploying the webhook pod 09/04/23 17:52:55.887
    STEP: Wait for the deployment to be ready 09/04/23 17:52:55.916
    Sep  4 17:52:55.933: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 17:52:57.95
    STEP: Verifying the service has paired with the endpoint 09/04/23 17:52:57.972
    Sep  4 17:52:58.972: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 09/04/23 17:52:58.977
    STEP: create a pod 09/04/23 17:52:59.011
    Sep  4 17:52:59.025: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-5124" to be "running"
    Sep  4 17:52:59.031: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.269936ms
    Sep  4 17:53:01.038: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013102836s
    Sep  4 17:53:01.039: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 09/04/23 17:53:01.039
    Sep  4 17:53:01.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=webhook-5124 attach --namespace=webhook-5124 to-be-attached-pod -i -c=container1'
    Sep  4 17:53:01.146: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:53:01.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5124" for this suite. 09/04/23 17:53:01.238
    STEP: Destroying namespace "webhook-5124-markers" for this suite. 09/04/23 17:53:01.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:53:01.287
Sep  4 17:53:01.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 17:53:01.288
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:01.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:01.334
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 09/04/23 17:53:01.342
Sep  4 17:53:01.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep  4 17:53:01.630: INFO: stderr: ""
Sep  4 17:53:01.630: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 09/04/23 17:53:01.63
Sep  4 17:53:01.630: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep  4 17:53:01.630: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2759" to be "running and ready, or succeeded"
Sep  4 17:53:01.635: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.354788ms
Sep  4 17:53:01.635: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'tenant-000001' to be 'Running' but was 'Pending'
Sep  4 17:53:03.642: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.012330814s
Sep  4 17:53:03.643: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep  4 17:53:03.643: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 09/04/23 17:53:03.643
Sep  4 17:53:03.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 logs logs-generator logs-generator'
Sep  4 17:53:03.768: INFO: stderr: ""
Sep  4 17:53:03.768: INFO: stdout: "I0904 17:53:02.514418       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/xmc 207\nI0904 17:53:02.717581       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/6z8z 538\nI0904 17:53:02.916533       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/4sc 346\nI0904 17:53:03.115095       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/982 416\nI0904 17:53:03.314773       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/5v62 408\nI0904 17:53:03.515436       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/n6xp 390\nI0904 17:53:03.714790       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/n6c 379\n"
STEP: limiting log lines 09/04/23 17:53:03.768
Sep  4 17:53:03.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 logs logs-generator logs-generator --tail=1'
Sep  4 17:53:03.885: INFO: stderr: ""
Sep  4 17:53:03.885: INFO: stdout: "I0904 17:53:03.714790       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/n6c 379\n"
Sep  4 17:53:03.885: INFO: got output "I0904 17:53:03.714790       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/n6c 379\n"
STEP: limiting log bytes 09/04/23 17:53:03.885
Sep  4 17:53:03.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 logs logs-generator logs-generator --limit-bytes=1'
Sep  4 17:53:03.983: INFO: stderr: ""
Sep  4 17:53:03.983: INFO: stdout: "I"
Sep  4 17:53:03.983: INFO: got output "I"
STEP: exposing timestamps 09/04/23 17:53:03.983
Sep  4 17:53:03.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 logs logs-generator logs-generator --tail=1 --timestamps'
Sep  4 17:53:04.071: INFO: stderr: ""
Sep  4 17:53:04.071: INFO: stdout: "2023-09-04T17:53:03.918214768Z I0904 17:53:03.918075       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/bnzv 340\n"
Sep  4 17:53:04.071: INFO: got output "2023-09-04T17:53:03.918214768Z I0904 17:53:03.918075       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/bnzv 340\n"
STEP: restricting to a time range 09/04/23 17:53:04.071
Sep  4 17:53:06.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 logs logs-generator logs-generator --since=1s'
Sep  4 17:53:06.733: INFO: stderr: ""
Sep  4 17:53:06.733: INFO: stdout: "I0904 17:53:05.915152       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/frzb 546\nI0904 17:53:06.114557       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/2zt 392\nI0904 17:53:06.319797       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/vtz2 373\nI0904 17:53:06.515317       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/hxl4 286\nI0904 17:53:06.714950       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/7tg 448\n"
Sep  4 17:53:06.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 logs logs-generator logs-generator --since=24h'
Sep  4 17:53:06.856: INFO: stderr: ""
Sep  4 17:53:06.856: INFO: stdout: "I0904 17:53:02.514418       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/xmc 207\nI0904 17:53:02.717581       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/6z8z 538\nI0904 17:53:02.916533       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/4sc 346\nI0904 17:53:03.115095       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/982 416\nI0904 17:53:03.314773       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/5v62 408\nI0904 17:53:03.515436       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/n6xp 390\nI0904 17:53:03.714790       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/n6c 379\nI0904 17:53:03.918075       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/bnzv 340\nI0904 17:53:04.114642       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/zll5 435\nI0904 17:53:04.315188       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/rgq 267\nI0904 17:53:04.514851       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/zh9p 585\nI0904 17:53:04.715212       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/2wgj 391\nI0904 17:53:04.914527       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/7jh8 521\nI0904 17:53:05.114891       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/c8xx 474\nI0904 17:53:05.315453       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/bt9d 590\nI0904 17:53:05.515134       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/sb6m 283\nI0904 17:53:05.714519       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/ws5 328\nI0904 17:53:05.915152       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/frzb 546\nI0904 17:53:06.114557       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/2zt 392\nI0904 17:53:06.319797       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/vtz2 373\nI0904 17:53:06.515317       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/hxl4 286\nI0904 17:53:06.714950       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/7tg 448\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Sep  4 17:53:06.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 delete pod logs-generator'
Sep  4 17:53:07.874: INFO: stderr: ""
Sep  4 17:53:07.874: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 17:53:07.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2759" for this suite. 09/04/23 17:53:07.88
------------------------------
â€¢ [SLOW TEST] [6.603 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:53:01.287
    Sep  4 17:53:01.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 17:53:01.288
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:01.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:01.334
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 09/04/23 17:53:01.342
    Sep  4 17:53:01.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Sep  4 17:53:01.630: INFO: stderr: ""
    Sep  4 17:53:01.630: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 09/04/23 17:53:01.63
    Sep  4 17:53:01.630: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Sep  4 17:53:01.630: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2759" to be "running and ready, or succeeded"
    Sep  4 17:53:01.635: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.354788ms
    Sep  4 17:53:01.635: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'tenant-000001' to be 'Running' but was 'Pending'
    Sep  4 17:53:03.642: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.012330814s
    Sep  4 17:53:03.643: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Sep  4 17:53:03.643: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 09/04/23 17:53:03.643
    Sep  4 17:53:03.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 logs logs-generator logs-generator'
    Sep  4 17:53:03.768: INFO: stderr: ""
    Sep  4 17:53:03.768: INFO: stdout: "I0904 17:53:02.514418       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/xmc 207\nI0904 17:53:02.717581       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/6z8z 538\nI0904 17:53:02.916533       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/4sc 346\nI0904 17:53:03.115095       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/982 416\nI0904 17:53:03.314773       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/5v62 408\nI0904 17:53:03.515436       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/n6xp 390\nI0904 17:53:03.714790       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/n6c 379\n"
    STEP: limiting log lines 09/04/23 17:53:03.768
    Sep  4 17:53:03.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 logs logs-generator logs-generator --tail=1'
    Sep  4 17:53:03.885: INFO: stderr: ""
    Sep  4 17:53:03.885: INFO: stdout: "I0904 17:53:03.714790       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/n6c 379\n"
    Sep  4 17:53:03.885: INFO: got output "I0904 17:53:03.714790       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/n6c 379\n"
    STEP: limiting log bytes 09/04/23 17:53:03.885
    Sep  4 17:53:03.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 logs logs-generator logs-generator --limit-bytes=1'
    Sep  4 17:53:03.983: INFO: stderr: ""
    Sep  4 17:53:03.983: INFO: stdout: "I"
    Sep  4 17:53:03.983: INFO: got output "I"
    STEP: exposing timestamps 09/04/23 17:53:03.983
    Sep  4 17:53:03.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 logs logs-generator logs-generator --tail=1 --timestamps'
    Sep  4 17:53:04.071: INFO: stderr: ""
    Sep  4 17:53:04.071: INFO: stdout: "2023-09-04T17:53:03.918214768Z I0904 17:53:03.918075       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/bnzv 340\n"
    Sep  4 17:53:04.071: INFO: got output "2023-09-04T17:53:03.918214768Z I0904 17:53:03.918075       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/bnzv 340\n"
    STEP: restricting to a time range 09/04/23 17:53:04.071
    Sep  4 17:53:06.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 logs logs-generator logs-generator --since=1s'
    Sep  4 17:53:06.733: INFO: stderr: ""
    Sep  4 17:53:06.733: INFO: stdout: "I0904 17:53:05.915152       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/frzb 546\nI0904 17:53:06.114557       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/2zt 392\nI0904 17:53:06.319797       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/vtz2 373\nI0904 17:53:06.515317       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/hxl4 286\nI0904 17:53:06.714950       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/7tg 448\n"
    Sep  4 17:53:06.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 logs logs-generator logs-generator --since=24h'
    Sep  4 17:53:06.856: INFO: stderr: ""
    Sep  4 17:53:06.856: INFO: stdout: "I0904 17:53:02.514418       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/xmc 207\nI0904 17:53:02.717581       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/6z8z 538\nI0904 17:53:02.916533       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/4sc 346\nI0904 17:53:03.115095       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/982 416\nI0904 17:53:03.314773       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/5v62 408\nI0904 17:53:03.515436       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/n6xp 390\nI0904 17:53:03.714790       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/n6c 379\nI0904 17:53:03.918075       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/bnzv 340\nI0904 17:53:04.114642       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/zll5 435\nI0904 17:53:04.315188       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/rgq 267\nI0904 17:53:04.514851       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/zh9p 585\nI0904 17:53:04.715212       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/2wgj 391\nI0904 17:53:04.914527       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/7jh8 521\nI0904 17:53:05.114891       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/c8xx 474\nI0904 17:53:05.315453       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/bt9d 590\nI0904 17:53:05.515134       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/sb6m 283\nI0904 17:53:05.714519       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/ws5 328\nI0904 17:53:05.915152       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/frzb 546\nI0904 17:53:06.114557       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/2zt 392\nI0904 17:53:06.319797       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/vtz2 373\nI0904 17:53:06.515317       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/hxl4 286\nI0904 17:53:06.714950       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/7tg 448\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Sep  4 17:53:06.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-2759 delete pod logs-generator'
    Sep  4 17:53:07.874: INFO: stderr: ""
    Sep  4 17:53:07.874: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:53:07.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2759" for this suite. 09/04/23 17:53:07.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:53:07.89
Sep  4 17:53:07.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename job 09/04/23 17:53:07.893
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:07.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:07.923
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 09/04/23 17:53:07.928
STEP: Ensuring active pods == parallelism 09/04/23 17:53:07.939
STEP: Orphaning one of the Job's Pods 09/04/23 17:53:09.945
Sep  4 17:53:10.481: INFO: Successfully updated pod "adopt-release-8fsdz"
STEP: Checking that the Job readopts the Pod 09/04/23 17:53:10.481
Sep  4 17:53:10.482: INFO: Waiting up to 15m0s for pod "adopt-release-8fsdz" in namespace "job-2598" to be "adopted"
Sep  4 17:53:10.498: INFO: Pod "adopt-release-8fsdz": Phase="Running", Reason="", readiness=true. Elapsed: 16.037152ms
Sep  4 17:53:12.505: INFO: Pod "adopt-release-8fsdz": Phase="Running", Reason="", readiness=true. Elapsed: 2.023550453s
Sep  4 17:53:12.506: INFO: Pod "adopt-release-8fsdz" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 09/04/23 17:53:12.506
Sep  4 17:53:13.041: INFO: Successfully updated pod "adopt-release-8fsdz"
STEP: Checking that the Job releases the Pod 09/04/23 17:53:13.041
Sep  4 17:53:13.041: INFO: Waiting up to 15m0s for pod "adopt-release-8fsdz" in namespace "job-2598" to be "released"
Sep  4 17:53:13.048: INFO: Pod "adopt-release-8fsdz": Phase="Running", Reason="", readiness=true. Elapsed: 6.540749ms
Sep  4 17:53:15.055: INFO: Pod "adopt-release-8fsdz": Phase="Running", Reason="", readiness=true. Elapsed: 2.013694127s
Sep  4 17:53:15.055: INFO: Pod "adopt-release-8fsdz" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  4 17:53:15.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2598" for this suite. 09/04/23 17:53:15.062
------------------------------
â€¢ [SLOW TEST] [7.187 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:53:07.89
    Sep  4 17:53:07.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename job 09/04/23 17:53:07.893
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:07.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:07.923
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 09/04/23 17:53:07.928
    STEP: Ensuring active pods == parallelism 09/04/23 17:53:07.939
    STEP: Orphaning one of the Job's Pods 09/04/23 17:53:09.945
    Sep  4 17:53:10.481: INFO: Successfully updated pod "adopt-release-8fsdz"
    STEP: Checking that the Job readopts the Pod 09/04/23 17:53:10.481
    Sep  4 17:53:10.482: INFO: Waiting up to 15m0s for pod "adopt-release-8fsdz" in namespace "job-2598" to be "adopted"
    Sep  4 17:53:10.498: INFO: Pod "adopt-release-8fsdz": Phase="Running", Reason="", readiness=true. Elapsed: 16.037152ms
    Sep  4 17:53:12.505: INFO: Pod "adopt-release-8fsdz": Phase="Running", Reason="", readiness=true. Elapsed: 2.023550453s
    Sep  4 17:53:12.506: INFO: Pod "adopt-release-8fsdz" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 09/04/23 17:53:12.506
    Sep  4 17:53:13.041: INFO: Successfully updated pod "adopt-release-8fsdz"
    STEP: Checking that the Job releases the Pod 09/04/23 17:53:13.041
    Sep  4 17:53:13.041: INFO: Waiting up to 15m0s for pod "adopt-release-8fsdz" in namespace "job-2598" to be "released"
    Sep  4 17:53:13.048: INFO: Pod "adopt-release-8fsdz": Phase="Running", Reason="", readiness=true. Elapsed: 6.540749ms
    Sep  4 17:53:15.055: INFO: Pod "adopt-release-8fsdz": Phase="Running", Reason="", readiness=true. Elapsed: 2.013694127s
    Sep  4 17:53:15.055: INFO: Pod "adopt-release-8fsdz" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:53:15.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2598" for this suite. 09/04/23 17:53:15.062
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:53:15.08
Sep  4 17:53:15.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 17:53:15.081
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:15.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:15.113
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-d65a2bce-cc4f-4cb3-9db0-f704fb788bfa 09/04/23 17:53:15.118
STEP: Creating a pod to test consume configMaps 09/04/23 17:53:15.126
Sep  4 17:53:15.140: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec" in namespace "projected-56" to be "Succeeded or Failed"
Sep  4 17:53:15.151: INFO: Pod "pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.974033ms
Sep  4 17:53:17.159: INFO: Pod "pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018453581s
Sep  4 17:53:19.157: INFO: Pod "pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016603579s
STEP: Saw pod success 09/04/23 17:53:19.157
Sep  4 17:53:19.157: INFO: Pod "pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec" satisfied condition "Succeeded or Failed"
Sep  4 17:53:19.163: INFO: Trying to get logs from node tenant-000003 pod pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec container agnhost-container: <nil>
STEP: delete the pod 09/04/23 17:53:19.174
Sep  4 17:53:19.193: INFO: Waiting for pod pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec to disappear
Sep  4 17:53:19.197: INFO: Pod pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 17:53:19.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-56" for this suite. 09/04/23 17:53:19.204
------------------------------
â€¢ [4.138 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:53:15.08
    Sep  4 17:53:15.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 17:53:15.081
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:15.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:15.113
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-d65a2bce-cc4f-4cb3-9db0-f704fb788bfa 09/04/23 17:53:15.118
    STEP: Creating a pod to test consume configMaps 09/04/23 17:53:15.126
    Sep  4 17:53:15.140: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec" in namespace "projected-56" to be "Succeeded or Failed"
    Sep  4 17:53:15.151: INFO: Pod "pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.974033ms
    Sep  4 17:53:17.159: INFO: Pod "pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018453581s
    Sep  4 17:53:19.157: INFO: Pod "pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016603579s
    STEP: Saw pod success 09/04/23 17:53:19.157
    Sep  4 17:53:19.157: INFO: Pod "pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec" satisfied condition "Succeeded or Failed"
    Sep  4 17:53:19.163: INFO: Trying to get logs from node tenant-000003 pod pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 17:53:19.174
    Sep  4 17:53:19.193: INFO: Waiting for pod pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec to disappear
    Sep  4 17:53:19.197: INFO: Pod pod-projected-configmaps-e7302180-4919-4cfd-b0cf-bc4ed76dfbec no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:53:19.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-56" for this suite. 09/04/23 17:53:19.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:53:19.222
Sep  4 17:53:19.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pods 09/04/23 17:53:19.226
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:19.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:19.263
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 09/04/23 17:53:19.267
STEP: submitting the pod to kubernetes 09/04/23 17:53:19.268
STEP: verifying QOS class is set on the pod 09/04/23 17:53:19.282
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Sep  4 17:53:19.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1431" for this suite. 09/04/23 17:53:19.294
------------------------------
â€¢ [0.085 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:53:19.222
    Sep  4 17:53:19.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pods 09/04/23 17:53:19.226
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:19.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:19.263
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 09/04/23 17:53:19.267
    STEP: submitting the pod to kubernetes 09/04/23 17:53:19.268
    STEP: verifying QOS class is set on the pod 09/04/23 17:53:19.282
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:53:19.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1431" for this suite. 09/04/23 17:53:19.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:53:19.338
Sep  4 17:53:19.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pods 09/04/23 17:53:19.342
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:19.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:19.371
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Sep  4 17:53:19.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: creating the pod 09/04/23 17:53:19.377
STEP: submitting the pod to kubernetes 09/04/23 17:53:19.377
Sep  4 17:53:19.391: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-d0095f52-5a6f-443f-bb4c-4d2228040e03" in namespace "pods-4857" to be "running and ready"
Sep  4 17:53:19.404: INFO: Pod "pod-exec-websocket-d0095f52-5a6f-443f-bb4c-4d2228040e03": Phase="Pending", Reason="", readiness=false. Elapsed: 12.789009ms
Sep  4 17:53:19.404: INFO: The phase of Pod pod-exec-websocket-d0095f52-5a6f-443f-bb4c-4d2228040e03 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 17:53:21.410: INFO: Pod "pod-exec-websocket-d0095f52-5a6f-443f-bb4c-4d2228040e03": Phase="Running", Reason="", readiness=true. Elapsed: 2.018982908s
Sep  4 17:53:21.410: INFO: The phase of Pod pod-exec-websocket-d0095f52-5a6f-443f-bb4c-4d2228040e03 is Running (Ready = true)
Sep  4 17:53:21.411: INFO: Pod "pod-exec-websocket-d0095f52-5a6f-443f-bb4c-4d2228040e03" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 17:53:21.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4857" for this suite. 09/04/23 17:53:21.519
------------------------------
â€¢ [2.193 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:53:19.338
    Sep  4 17:53:19.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pods 09/04/23 17:53:19.342
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:19.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:19.371
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Sep  4 17:53:19.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: creating the pod 09/04/23 17:53:19.377
    STEP: submitting the pod to kubernetes 09/04/23 17:53:19.377
    Sep  4 17:53:19.391: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-d0095f52-5a6f-443f-bb4c-4d2228040e03" in namespace "pods-4857" to be "running and ready"
    Sep  4 17:53:19.404: INFO: Pod "pod-exec-websocket-d0095f52-5a6f-443f-bb4c-4d2228040e03": Phase="Pending", Reason="", readiness=false. Elapsed: 12.789009ms
    Sep  4 17:53:19.404: INFO: The phase of Pod pod-exec-websocket-d0095f52-5a6f-443f-bb4c-4d2228040e03 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 17:53:21.410: INFO: Pod "pod-exec-websocket-d0095f52-5a6f-443f-bb4c-4d2228040e03": Phase="Running", Reason="", readiness=true. Elapsed: 2.018982908s
    Sep  4 17:53:21.410: INFO: The phase of Pod pod-exec-websocket-d0095f52-5a6f-443f-bb4c-4d2228040e03 is Running (Ready = true)
    Sep  4 17:53:21.411: INFO: Pod "pod-exec-websocket-d0095f52-5a6f-443f-bb4c-4d2228040e03" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:53:21.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4857" for this suite. 09/04/23 17:53:21.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:53:21.537
Sep  4 17:53:21.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 17:53:21.538
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:21.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:21.566
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-3444985d-99b0-4733-8609-f9b4733797ae 09/04/23 17:53:21.571
STEP: Creating a pod to test consume configMaps 09/04/23 17:53:21.578
Sep  4 17:53:21.598: INFO: Waiting up to 5m0s for pod "pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0" in namespace "configmap-5807" to be "Succeeded or Failed"
Sep  4 17:53:21.605: INFO: Pod "pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.100594ms
Sep  4 17:53:23.612: INFO: Pod "pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013906026s
Sep  4 17:53:25.611: INFO: Pod "pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013026933s
STEP: Saw pod success 09/04/23 17:53:25.611
Sep  4 17:53:25.611: INFO: Pod "pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0" satisfied condition "Succeeded or Failed"
Sep  4 17:53:25.616: INFO: Trying to get logs from node tenant-000003 pod pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 17:53:25.628
Sep  4 17:53:25.651: INFO: Waiting for pod pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0 to disappear
Sep  4 17:53:25.656: INFO: Pod pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 17:53:25.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5807" for this suite. 09/04/23 17:53:25.663
------------------------------
â€¢ [4.137 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:53:21.537
    Sep  4 17:53:21.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 17:53:21.538
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:21.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:21.566
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-3444985d-99b0-4733-8609-f9b4733797ae 09/04/23 17:53:21.571
    STEP: Creating a pod to test consume configMaps 09/04/23 17:53:21.578
    Sep  4 17:53:21.598: INFO: Waiting up to 5m0s for pod "pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0" in namespace "configmap-5807" to be "Succeeded or Failed"
    Sep  4 17:53:21.605: INFO: Pod "pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.100594ms
    Sep  4 17:53:23.612: INFO: Pod "pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013906026s
    Sep  4 17:53:25.611: INFO: Pod "pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013026933s
    STEP: Saw pod success 09/04/23 17:53:25.611
    Sep  4 17:53:25.611: INFO: Pod "pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0" satisfied condition "Succeeded or Failed"
    Sep  4 17:53:25.616: INFO: Trying to get logs from node tenant-000003 pod pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 17:53:25.628
    Sep  4 17:53:25.651: INFO: Waiting for pod pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0 to disappear
    Sep  4 17:53:25.656: INFO: Pod pod-configmaps-1689a353-a8bd-4891-b939-0b799bd478d0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:53:25.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5807" for this suite. 09/04/23 17:53:25.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:53:25.687
Sep  4 17:53:25.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename deployment 09/04/23 17:53:25.688
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:25.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:25.725
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Sep  4 17:53:25.731: INFO: Creating deployment "webserver-deployment"
Sep  4 17:53:25.740: INFO: Waiting for observed generation 1
Sep  4 17:53:27.769: INFO: Waiting for all required pods to come up
Sep  4 17:53:27.789: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 09/04/23 17:53:27.789
Sep  4 17:53:27.790: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-xzcn5" in namespace "deployment-2017" to be "running"
Sep  4 17:53:27.790: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5m69k" in namespace "deployment-2017" to be "running"
Sep  4 17:53:27.795: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-cm5mr" in namespace "deployment-2017" to be "running"
Sep  4 17:53:27.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-g7hm6" in namespace "deployment-2017" to be "running"
Sep  4 17:53:27.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-gwklk" in namespace "deployment-2017" to be "running"
Sep  4 17:53:27.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jx4rc" in namespace "deployment-2017" to be "running"
Sep  4 17:53:27.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-lzqvf" in namespace "deployment-2017" to be "running"
Sep  4 17:53:27.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-m27rz" in namespace "deployment-2017" to be "running"
Sep  4 17:53:27.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rkx9k" in namespace "deployment-2017" to be "running"
Sep  4 17:53:27.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vcwp9" in namespace "deployment-2017" to be "running"
Sep  4 17:53:27.803: INFO: Pod "webserver-deployment-7f5969cbc7-xzcn5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.997992ms
Sep  4 17:53:27.803: INFO: Pod "webserver-deployment-7f5969cbc7-5m69k": Phase="Pending", Reason="", readiness=false. Elapsed: 8.195304ms
Sep  4 17:53:27.821: INFO: Pod "webserver-deployment-7f5969cbc7-cm5mr": Phase="Pending", Reason="", readiness=false. Elapsed: 17.9682ms
Sep  4 17:53:27.821: INFO: Pod "webserver-deployment-7f5969cbc7-g7hm6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.88319ms
Sep  4 17:53:27.821: INFO: Pod "webserver-deployment-7f5969cbc7-gwklk": Phase="Pending", Reason="", readiness=false. Elapsed: 17.650861ms
Sep  4 17:53:27.825: INFO: Pod "webserver-deployment-7f5969cbc7-lzqvf": Phase="Pending", Reason="", readiness=false. Elapsed: 21.006872ms
Sep  4 17:53:27.825: INFO: Pod "webserver-deployment-7f5969cbc7-jx4rc": Phase="Pending", Reason="", readiness=false. Elapsed: 21.433025ms
Sep  4 17:53:27.910: INFO: Pod "webserver-deployment-7f5969cbc7-m27rz": Phase="Pending", Reason="", readiness=false. Elapsed: 102.796689ms
Sep  4 17:53:27.991: INFO: Pod "webserver-deployment-7f5969cbc7-vcwp9": Phase="Pending", Reason="", readiness=false. Elapsed: 172.46882ms
Sep  4 17:53:27.991: INFO: Pod "webserver-deployment-7f5969cbc7-rkx9k": Phase="Pending", Reason="", readiness=false. Elapsed: 183.633387ms
Sep  4 17:53:29.828: INFO: Pod "webserver-deployment-7f5969cbc7-xzcn5": Phase="Running", Reason="", readiness=true. Elapsed: 2.038380522s
Sep  4 17:53:29.829: INFO: Pod "webserver-deployment-7f5969cbc7-xzcn5" satisfied condition "running"
Sep  4 17:53:29.829: INFO: Pod "webserver-deployment-7f5969cbc7-cm5mr": Phase="Running", Reason="", readiness=true. Elapsed: 2.02587409s
Sep  4 17:53:29.829: INFO: Pod "webserver-deployment-7f5969cbc7-cm5mr" satisfied condition "running"
Sep  4 17:53:29.830: INFO: Pod "webserver-deployment-7f5969cbc7-g7hm6": Phase="Running", Reason="", readiness=true. Elapsed: 2.026364351s
Sep  4 17:53:29.830: INFO: Pod "webserver-deployment-7f5969cbc7-g7hm6" satisfied condition "running"
Sep  4 17:53:29.830: INFO: Pod "webserver-deployment-7f5969cbc7-5m69k": Phase="Running", Reason="", readiness=true. Elapsed: 2.035305145s
Sep  4 17:53:29.830: INFO: Pod "webserver-deployment-7f5969cbc7-5m69k" satisfied condition "running"
Sep  4 17:53:29.997: INFO: Pod "webserver-deployment-7f5969cbc7-jx4rc": Phase="Running", Reason="", readiness=true. Elapsed: 2.193380403s
Sep  4 17:53:29.998: INFO: Pod "webserver-deployment-7f5969cbc7-jx4rc" satisfied condition "running"
Sep  4 17:53:29.998: INFO: Pod "webserver-deployment-7f5969cbc7-lzqvf": Phase="Running", Reason="", readiness=true. Elapsed: 2.193696142s
Sep  4 17:53:29.998: INFO: Pod "webserver-deployment-7f5969cbc7-lzqvf" satisfied condition "running"
Sep  4 17:53:29.999: INFO: Pod "webserver-deployment-7f5969cbc7-m27rz": Phase="Running", Reason="", readiness=true. Elapsed: 2.191515175s
Sep  4 17:53:29.999: INFO: Pod "webserver-deployment-7f5969cbc7-m27rz" satisfied condition "running"
Sep  4 17:53:29.999: INFO: Pod "webserver-deployment-7f5969cbc7-rkx9k": Phase="Running", Reason="", readiness=true. Elapsed: 2.191685595s
Sep  4 17:53:29.999: INFO: Pod "webserver-deployment-7f5969cbc7-rkx9k" satisfied condition "running"
Sep  4 17:53:30.001: INFO: Pod "webserver-deployment-7f5969cbc7-gwklk": Phase="Running", Reason="", readiness=true. Elapsed: 2.197018849s
Sep  4 17:53:30.001: INFO: Pod "webserver-deployment-7f5969cbc7-gwklk" satisfied condition "running"
Sep  4 17:53:30.001: INFO: Pod "webserver-deployment-7f5969cbc7-vcwp9": Phase="Running", Reason="", readiness=true. Elapsed: 2.182525373s
Sep  4 17:53:30.001: INFO: Pod "webserver-deployment-7f5969cbc7-vcwp9" satisfied condition "running"
Sep  4 17:53:30.001: INFO: Waiting for deployment "webserver-deployment" to complete
Sep  4 17:53:30.012: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep  4 17:53:30.026: INFO: Updating deployment webserver-deployment
Sep  4 17:53:30.026: INFO: Waiting for observed generation 2
Sep  4 17:53:32.047: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  4 17:53:32.071: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  4 17:53:32.083: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  4 17:53:32.118: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  4 17:53:32.118: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  4 17:53:32.122: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  4 17:53:32.132: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep  4 17:53:32.132: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep  4 17:53:32.147: INFO: Updating deployment webserver-deployment
Sep  4 17:53:32.148: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep  4 17:53:32.166: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  4 17:53:34.258: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 17:53:34.363: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2017  68265505-45e8-4dfa-9b89-8e5a31185732 18611 3 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002fce358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-04 17:53:32 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-09-04 17:53:32 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep  4 17:53:34.421: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-2017  d68629d4-1bac-47cf-a9bb-d6b5f56cd449 18607 3 2023-09-04 17:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 68265505-45e8-4dfa-9b89-8e5a31185732 0xc002fcfdd7 0xc002fcfdd8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68265505-45e8-4dfa-9b89-8e5a31185732\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003010008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  4 17:53:34.421: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep  4 17:53:34.421: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-2017  907f1b0e-8d65-45da-acf7-b8228255f6d2 18603 3 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 68265505-45e8-4dfa-9b89-8e5a31185732 0xc002fcfab7 0xc002fcfab8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68265505-45e8-4dfa-9b89-8e5a31185732\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002fcfc78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep  4 17:53:34.480: INFO: Pod "webserver-deployment-7f5969cbc7-5m69k" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5m69k webserver-deployment-7f5969cbc7- deployment-2017  4abf692e-6122-4dda-9bbf-63bc6c9e2464 18390 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:eff7e1819fb1328e0587f72069b653b09ba6984ad10d1b279bdab464e0e78a6c cni.projectcalico.org/podIP:10.36.55.112/32 cni.projectcalico.org/podIPs:10.36.55.112/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc0030112b7 0xc0030112b8}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rgfwn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rgfwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.112,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7bde6d9af2e098d4077d7d6126f742d3056c1f787a27fe5574b6bd2b7e32073b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.480: INFO: Pod "webserver-deployment-7f5969cbc7-82mmj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-82mmj webserver-deployment-7f5969cbc7- deployment-2017  d53c41ec-3cc3-4352-960a-3fe011f36945 18595 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003011cc0 0xc003011cc1}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmfnq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmfnq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.486: INFO: Pod "webserver-deployment-7f5969cbc7-9vbnn" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9vbnn webserver-deployment-7f5969cbc7- deployment-2017  8fbcaff9-fc0c-42ef-b7c0-6dfc2b938967 18658 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e56e446ba6918aa225680037c8efea8d5634585034fd2a6ff3a6d0a9c30d2231 cni.projectcalico.org/podIP:10.36.55.124/32 cni.projectcalico.org/podIPs:10.36.55.124/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003026180 0xc003026181}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 17:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrdfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrdfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:,StartTime:2023-09-04 17:53:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.486: INFO: Pod "webserver-deployment-7f5969cbc7-cm5mr" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cm5mr webserver-deployment-7f5969cbc7- deployment-2017  1e375003-57e9-4ad4-a4bc-93ed78ed4036 18385 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6cc724dd2e061d8982cfc929e43dcc3b626281bbcdd39554baab27d096ec3e4d cni.projectcalico.org/podIP:10.36.55.109/32 cni.projectcalico.org/podIPs:10.36.55.109/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003026830 0xc003026831}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2l56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2l56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.109,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f1c6576b69ba0cfaab245f8a2a387d4fd45743f65ce709fdf99af8e232f4360b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.487: INFO: Pod "webserver-deployment-7f5969cbc7-hvw9m" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hvw9m webserver-deployment-7f5969cbc7- deployment-2017  cb11f8d9-b46f-498c-8588-0ceab06f2c32 18560 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003026e50 0xc003026e51}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8fbz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8fbz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.490: INFO: Pod "webserver-deployment-7f5969cbc7-jx4rc" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jx4rc webserver-deployment-7f5969cbc7- deployment-2017  9f7dcc80-b94a-4957-9185-5113c6c47569 18392 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cf4f99aba7698f9d213fa09cfb9b9397e1648a9ff0eed38d9ca00dd1066b41b8 cni.projectcalico.org/podIP:10.36.217.219/32 cni.projectcalico.org/podIPs:10.36.217.219/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003027390 0xc003027391}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-74gnd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-74gnd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.219,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b58abcd9c2c0ded220099fcb596038168b1d795b80ec5f0d260e51280f7f6eb7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.493: INFO: Pod "webserver-deployment-7f5969cbc7-kxds4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kxds4 webserver-deployment-7f5969cbc7- deployment-2017  517ccca7-315f-4319-8fa2-4701430d1f2d 18678 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7270f86c2076e6c8ab4889deea26ea4b38a832188de80f12c7e2915b7e49b580 cni.projectcalico.org/podIP:10.36.217.228/32 cni.projectcalico.org/podIPs:10.36.217.228/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc0030277d0 0xc0030277d1}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l692d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l692d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.497: INFO: Pod "webserver-deployment-7f5969cbc7-lzqvf" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lzqvf webserver-deployment-7f5969cbc7- deployment-2017  d07049d9-559b-48f0-95fd-f6b132e889ad 18425 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d71db870f514f891884bb4fdeac43fac1c4b6897314e0e4020c7bdc15f2e68bf cni.projectcalico.org/podIP:10.36.217.236/32 cni.projectcalico.org/podIPs:10.36.217.236/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003027c40 0xc003027c41}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4vpqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4vpqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.236,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0107b45c0f80792a8e359d692bd6099bc52ad267c52656749c03de3c78747be3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.236,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.497: INFO: Pod "webserver-deployment-7f5969cbc7-m27rz" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m27rz webserver-deployment-7f5969cbc7- deployment-2017  cb9ef480-1475-4be4-9c17-97732d822253 18419 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f7dcef9e9e7fb8b261af2d09c3f105100a9c79c50a017d66568ac2f1e51da92c cni.projectcalico.org/podIP:10.36.55.117/32 cni.projectcalico.org/podIPs:10.36.55.117/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e520c0 0xc003e520c1}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bxdpx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bxdpx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.117,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://711848def79e06dbc91c01a40258f2f79f23f77d68b922bca5fe05034559a79c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.498: INFO: Pod "webserver-deployment-7f5969cbc7-q9sqr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q9sqr webserver-deployment-7f5969cbc7- deployment-2017  1906046a-6f30-42f4-a0da-95e2c75e4281 18609 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e522d0 0xc003e522d1}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zgvxk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zgvxk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:,StartTime:2023-09-04 17:53:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.498: INFO: Pod "webserver-deployment-7f5969cbc7-qqtf5" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qqtf5 webserver-deployment-7f5969cbc7- deployment-2017  9de2e920-b9bf-4fed-89bd-a457399f1caa 18598 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e52490 0xc003e52491}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bh224,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bh224,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.500: INFO: Pod "webserver-deployment-7f5969cbc7-rcm6k" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rcm6k webserver-deployment-7f5969cbc7- deployment-2017  84049411-ef6e-429d-bf05-a5410c4e07ee 18567 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e525f0 0xc003e525f1}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4f766,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4f766,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.502: INFO: Pod "webserver-deployment-7f5969cbc7-rkx9k" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rkx9k webserver-deployment-7f5969cbc7- deployment-2017  16fa7912-5088-41d0-a78a-fb2392f9f852 18396 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:058027ebe9182e0c2a9fc062ac48fccab4bb1e58e1189df814cb7e277aac9196 cni.projectcalico.org/podIP:10.36.217.237/32 cni.projectcalico.org/podIPs:10.36.217.237/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e52770 0xc003e52771}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d2clm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d2clm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.237,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a1cc085fc0e1803ba5be62dcfe0454211edfbe5eb8a7276db177b88a1524eb1a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.237,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.506: INFO: Pod "webserver-deployment-7f5969cbc7-rrffg" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rrffg webserver-deployment-7f5969cbc7- deployment-2017  ac62dc42-d63e-4aad-bba4-1ce1672b1ad8 18687 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:143873bd93dab6db14b654ed88ddc1090ff62396c2f1239147d2b129f96a80f1 cni.projectcalico.org/podIP:10.36.55.114/32 cni.projectcalico.org/podIPs:10.36.55.114/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e52990 0xc003e52991}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-59ng8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59ng8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.506: INFO: Pod "webserver-deployment-7f5969cbc7-s6mf8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s6mf8 webserver-deployment-7f5969cbc7- deployment-2017  35dc2e4c-ddd4-48b9-a094-7ca671249a69 18594 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e52b10 0xc003e52b11}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrqx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrqx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.506: INFO: Pod "webserver-deployment-7f5969cbc7-tlr5f" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tlr5f webserver-deployment-7f5969cbc7- deployment-2017  b41f2734-2e7a-4642-9d50-5dc1a76dfe91 18697 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a513debc2c1ba650c562f873dc28d189beeaa866a75deaa1494cb4b92957a8bb cni.projectcalico.org/podIP:10.36.217.209/32 cni.projectcalico.org/podIPs:10.36.217.209/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e52c90 0xc003e52c91}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qjw6g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qjw6g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.506: INFO: Pod "webserver-deployment-7f5969cbc7-vcwp9" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vcwp9 webserver-deployment-7f5969cbc7- deployment-2017  bbc2d552-df07-43ae-9bd9-2970efae1a57 18427 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6cfae5216ffe24551aef6fb52de24d456d47342e8ab2b8141694e82d6a89c61e cni.projectcalico.org/podIP:10.36.217.234/32 cni.projectcalico.org/podIPs:10.36.217.234/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e52e30 0xc003e52e31}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8gvn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8gvn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.234,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2d2ccc7fac0547b7d93d2829d744df8eb69266ee7f6f38b87bc3dfad095d48c3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.234,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.507: INFO: Pod "webserver-deployment-7f5969cbc7-vk9ps" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vk9ps webserver-deployment-7f5969cbc7- deployment-2017  f8df5177-7571-4aaf-9159-02e39d0488d8 18621 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e53040 0xc003e53041}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v42qm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v42qm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:,StartTime:2023-09-04 17:53:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.507: INFO: Pod "webserver-deployment-7f5969cbc7-vq75n" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vq75n webserver-deployment-7f5969cbc7- deployment-2017  8921c9dc-f821-45d4-9bc5-4ed7cabf19b7 18571 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e53200 0xc003e53201}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sqhw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sqhw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.510: INFO: Pod "webserver-deployment-7f5969cbc7-xzcn5" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xzcn5 webserver-deployment-7f5969cbc7- deployment-2017  2415a95a-fae5-4aa5-b6a0-3618ce704c09 18432 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bdba8692ea098de716bb08fa50a0054de402920eea2ebeff0a9f204ff9d22353 cni.projectcalico.org/podIP:10.36.217.208/32 cni.projectcalico.org/podIPs:10.36.217.208/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e53380 0xc003e53381}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ch5wg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ch5wg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.208,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6204fb740d9ac54efd02b88734f024dc90cca74da1b341862e530e56c9dff168,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.208,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.510: INFO: Pod "webserver-deployment-d9f79cb5-2zxxc" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2zxxc webserver-deployment-d9f79cb5- deployment-2017  8b843c32-9602-4b4b-89c5-700ebc2a2797 18587 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003e5356f 0xc003e53580}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zmlj5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zmlj5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.510: INFO: Pod "webserver-deployment-d9f79cb5-52gt2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-52gt2 webserver-deployment-d9f79cb5- deployment-2017  55a57e1d-8dcc-4dca-b209-fbf5cac897c3 18691 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:89afd82209c0e6ce8da4e180f4460c4cd8347b47c43c56deac11d37744f728a0 cni.projectcalico.org/podIP:10.36.55.73/32 cni.projectcalico.org/podIPs:10.36.55.73/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003e536df 0xc003e53710}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r95m2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r95m2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.510: INFO: Pod "webserver-deployment-d9f79cb5-5bgtk" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5bgtk webserver-deployment-d9f79cb5- deployment-2017  ae113bfb-1724-4b7a-a654-6274f9210957 18669 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d28fcf8fa6f8d318732b4d2ab756012c64c4ba458dd07fadb53929cc59b94b86 cni.projectcalico.org/podIP:10.36.55.94/32 cni.projectcalico.org/podIPs:10.36.55.94/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003e5388f 0xc003e538c0}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2zq8h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2zq8h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.511: INFO: Pod "webserver-deployment-d9f79cb5-69w9j" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-69w9j webserver-deployment-d9f79cb5- deployment-2017  d6b7491a-9624-4146-8180-8d38e7a47bdb 18662 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7f638a96c789113296cba8fd5f9cfee1d86140f3005d6b721d0a228cab375518 cni.projectcalico.org/podIP:10.36.217.253/32 cni.projectcalico.org/podIPs:10.36.217.253/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003e53a3f 0xc003e53a70}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wnfzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wnfzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.514: INFO: Pod "webserver-deployment-d9f79cb5-7vzxl" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7vzxl webserver-deployment-d9f79cb5- deployment-2017  5eef31b3-3ada-469a-b337-f79281960cb4 18597 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003e53bef 0xc003e53c00}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47sm9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47sm9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:,StartTime:2023-09-04 17:53:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.514: INFO: Pod "webserver-deployment-d9f79cb5-bdpt8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bdpt8 webserver-deployment-d9f79cb5- deployment-2017  dd8030cd-01f1-469b-b536-b7f4c08c0565 18520 0 2023-09-04 17:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:501b2d5b581676d2a6563e181ed974a58d4f1769b474f797f9e4615e333b9046 cni.projectcalico.org/podIP:10.36.55.108/32 cni.projectcalico.org/podIPs:10.36.55.108/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003e53dcf 0xc003e53e00}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 17:53:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4s6p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4s6p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:,StartTime:2023-09-04 17:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.515: INFO: Pod "webserver-deployment-d9f79cb5-gj4r4" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gj4r4 webserver-deployment-d9f79cb5- deployment-2017  89100b53-ea0f-492b-bfd9-e486f614d560 18516 0 2023-09-04 17:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6f65ab53136a5c45670187900e063346aed7e784e2b8be617c069df336a1420d cni.projectcalico.org/podIP:10.36.55.127/32 cni.projectcalico.org/podIPs:10.36.55.127/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003e53fef 0xc003ada020}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 17:53:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2qk89,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2qk89,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:,StartTime:2023-09-04 17:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.515: INFO: Pod "webserver-deployment-d9f79cb5-gsbrl" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gsbrl webserver-deployment-d9f79cb5- deployment-2017  7a8754c3-94e5-48f1-9d59-826f5541b05e 18588 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003ada20f 0xc003ada220}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dxzkl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dxzkl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.518: INFO: Pod "webserver-deployment-d9f79cb5-lrjk7" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lrjk7 webserver-deployment-d9f79cb5- deployment-2017  a181e61c-0ef3-4cd7-84ce-af7c0eaf3843 18539 0 2023-09-04 17:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:91e18fe9b4188f6c597e71e3c06fcc0aabf06446397813bd822736909db62031 cni.projectcalico.org/podIP:10.36.217.230/32 cni.projectcalico.org/podIPs:10.36.217.230/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003ada37f 0xc003ada3b0}] [] [{calico Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-65lxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-65lxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.230,StartTime:2023-09-04 17:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.518: INFO: Pod "webserver-deployment-d9f79cb5-qqk89" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qqk89 webserver-deployment-d9f79cb5- deployment-2017  6b6e4b67-baaf-4e2c-92d6-a4d59c7ab61f 18673 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:12d7a4eba9cad51b8c7b4a54e2959c2c8bce0fb4512295ea27c70c9cfda8b94f cni.projectcalico.org/podIP:10.36.217.210/32 cni.projectcalico.org/podIPs:10.36.217.210/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003ada5df 0xc003ada610}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 17:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6lvzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6lvzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:,StartTime:2023-09-04 17:53:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.518: INFO: Pod "webserver-deployment-d9f79cb5-tv7st" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tv7st webserver-deployment-d9f79cb5- deployment-2017  d0635245-8350-4e76-95f1-9b99ccffb67b 18531 0 2023-09-04 17:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:4700ba3750d75388a7652df787ff79570272adf802513568c9c3e22407231d16 cni.projectcalico.org/podIP:10.36.55.84/32 cni.projectcalico.org/podIPs:10.36.55.84/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003ada81f 0xc003ada850}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 17:53:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gbh8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gbh8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:,StartTime:2023-09-04 17:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.519: INFO: Pod "webserver-deployment-d9f79cb5-vlkk8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vlkk8 webserver-deployment-d9f79cb5- deployment-2017  db3dfaff-4277-4a6b-a102-24e7e6a93260 18646 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003adaa4f 0xc003adaa60}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbqnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbqnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:,StartTime:2023-09-04 17:53:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 17:53:34.522: INFO: Pod "webserver-deployment-d9f79cb5-zpcts" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zpcts webserver-deployment-d9f79cb5- deployment-2017  605ed0cf-12a5-41b2-8513-f17194e77d2c 18504 0 2023-09-04 17:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:c83b7143ed051858fb24e345c743e765f2f4103068fa11e4006ec031c1acf97a cni.projectcalico.org/podIP:10.36.217.252/32 cni.projectcalico.org/podIPs:10.36.217.252/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003adac4f 0xc003adac80}] [] [{calico Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l466l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l466l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:,StartTime:2023-09-04 17:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 17:53:34.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2017" for this suite. 09/04/23 17:53:34.53
------------------------------
â€¢ [SLOW TEST] [8.897 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:53:25.687
    Sep  4 17:53:25.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename deployment 09/04/23 17:53:25.688
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:25.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:25.725
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Sep  4 17:53:25.731: INFO: Creating deployment "webserver-deployment"
    Sep  4 17:53:25.740: INFO: Waiting for observed generation 1
    Sep  4 17:53:27.769: INFO: Waiting for all required pods to come up
    Sep  4 17:53:27.789: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 09/04/23 17:53:27.789
    Sep  4 17:53:27.790: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-xzcn5" in namespace "deployment-2017" to be "running"
    Sep  4 17:53:27.790: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5m69k" in namespace "deployment-2017" to be "running"
    Sep  4 17:53:27.795: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-cm5mr" in namespace "deployment-2017" to be "running"
    Sep  4 17:53:27.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-g7hm6" in namespace "deployment-2017" to be "running"
    Sep  4 17:53:27.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-gwklk" in namespace "deployment-2017" to be "running"
    Sep  4 17:53:27.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jx4rc" in namespace "deployment-2017" to be "running"
    Sep  4 17:53:27.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-lzqvf" in namespace "deployment-2017" to be "running"
    Sep  4 17:53:27.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-m27rz" in namespace "deployment-2017" to be "running"
    Sep  4 17:53:27.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rkx9k" in namespace "deployment-2017" to be "running"
    Sep  4 17:53:27.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vcwp9" in namespace "deployment-2017" to be "running"
    Sep  4 17:53:27.803: INFO: Pod "webserver-deployment-7f5969cbc7-xzcn5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.997992ms
    Sep  4 17:53:27.803: INFO: Pod "webserver-deployment-7f5969cbc7-5m69k": Phase="Pending", Reason="", readiness=false. Elapsed: 8.195304ms
    Sep  4 17:53:27.821: INFO: Pod "webserver-deployment-7f5969cbc7-cm5mr": Phase="Pending", Reason="", readiness=false. Elapsed: 17.9682ms
    Sep  4 17:53:27.821: INFO: Pod "webserver-deployment-7f5969cbc7-g7hm6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.88319ms
    Sep  4 17:53:27.821: INFO: Pod "webserver-deployment-7f5969cbc7-gwklk": Phase="Pending", Reason="", readiness=false. Elapsed: 17.650861ms
    Sep  4 17:53:27.825: INFO: Pod "webserver-deployment-7f5969cbc7-lzqvf": Phase="Pending", Reason="", readiness=false. Elapsed: 21.006872ms
    Sep  4 17:53:27.825: INFO: Pod "webserver-deployment-7f5969cbc7-jx4rc": Phase="Pending", Reason="", readiness=false. Elapsed: 21.433025ms
    Sep  4 17:53:27.910: INFO: Pod "webserver-deployment-7f5969cbc7-m27rz": Phase="Pending", Reason="", readiness=false. Elapsed: 102.796689ms
    Sep  4 17:53:27.991: INFO: Pod "webserver-deployment-7f5969cbc7-vcwp9": Phase="Pending", Reason="", readiness=false. Elapsed: 172.46882ms
    Sep  4 17:53:27.991: INFO: Pod "webserver-deployment-7f5969cbc7-rkx9k": Phase="Pending", Reason="", readiness=false. Elapsed: 183.633387ms
    Sep  4 17:53:29.828: INFO: Pod "webserver-deployment-7f5969cbc7-xzcn5": Phase="Running", Reason="", readiness=true. Elapsed: 2.038380522s
    Sep  4 17:53:29.829: INFO: Pod "webserver-deployment-7f5969cbc7-xzcn5" satisfied condition "running"
    Sep  4 17:53:29.829: INFO: Pod "webserver-deployment-7f5969cbc7-cm5mr": Phase="Running", Reason="", readiness=true. Elapsed: 2.02587409s
    Sep  4 17:53:29.829: INFO: Pod "webserver-deployment-7f5969cbc7-cm5mr" satisfied condition "running"
    Sep  4 17:53:29.830: INFO: Pod "webserver-deployment-7f5969cbc7-g7hm6": Phase="Running", Reason="", readiness=true. Elapsed: 2.026364351s
    Sep  4 17:53:29.830: INFO: Pod "webserver-deployment-7f5969cbc7-g7hm6" satisfied condition "running"
    Sep  4 17:53:29.830: INFO: Pod "webserver-deployment-7f5969cbc7-5m69k": Phase="Running", Reason="", readiness=true. Elapsed: 2.035305145s
    Sep  4 17:53:29.830: INFO: Pod "webserver-deployment-7f5969cbc7-5m69k" satisfied condition "running"
    Sep  4 17:53:29.997: INFO: Pod "webserver-deployment-7f5969cbc7-jx4rc": Phase="Running", Reason="", readiness=true. Elapsed: 2.193380403s
    Sep  4 17:53:29.998: INFO: Pod "webserver-deployment-7f5969cbc7-jx4rc" satisfied condition "running"
    Sep  4 17:53:29.998: INFO: Pod "webserver-deployment-7f5969cbc7-lzqvf": Phase="Running", Reason="", readiness=true. Elapsed: 2.193696142s
    Sep  4 17:53:29.998: INFO: Pod "webserver-deployment-7f5969cbc7-lzqvf" satisfied condition "running"
    Sep  4 17:53:29.999: INFO: Pod "webserver-deployment-7f5969cbc7-m27rz": Phase="Running", Reason="", readiness=true. Elapsed: 2.191515175s
    Sep  4 17:53:29.999: INFO: Pod "webserver-deployment-7f5969cbc7-m27rz" satisfied condition "running"
    Sep  4 17:53:29.999: INFO: Pod "webserver-deployment-7f5969cbc7-rkx9k": Phase="Running", Reason="", readiness=true. Elapsed: 2.191685595s
    Sep  4 17:53:29.999: INFO: Pod "webserver-deployment-7f5969cbc7-rkx9k" satisfied condition "running"
    Sep  4 17:53:30.001: INFO: Pod "webserver-deployment-7f5969cbc7-gwklk": Phase="Running", Reason="", readiness=true. Elapsed: 2.197018849s
    Sep  4 17:53:30.001: INFO: Pod "webserver-deployment-7f5969cbc7-gwklk" satisfied condition "running"
    Sep  4 17:53:30.001: INFO: Pod "webserver-deployment-7f5969cbc7-vcwp9": Phase="Running", Reason="", readiness=true. Elapsed: 2.182525373s
    Sep  4 17:53:30.001: INFO: Pod "webserver-deployment-7f5969cbc7-vcwp9" satisfied condition "running"
    Sep  4 17:53:30.001: INFO: Waiting for deployment "webserver-deployment" to complete
    Sep  4 17:53:30.012: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Sep  4 17:53:30.026: INFO: Updating deployment webserver-deployment
    Sep  4 17:53:30.026: INFO: Waiting for observed generation 2
    Sep  4 17:53:32.047: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Sep  4 17:53:32.071: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Sep  4 17:53:32.083: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep  4 17:53:32.118: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Sep  4 17:53:32.118: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Sep  4 17:53:32.122: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep  4 17:53:32.132: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Sep  4 17:53:32.132: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Sep  4 17:53:32.147: INFO: Updating deployment webserver-deployment
    Sep  4 17:53:32.148: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Sep  4 17:53:32.166: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Sep  4 17:53:34.258: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 17:53:34.363: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-2017  68265505-45e8-4dfa-9b89-8e5a31185732 18611 3 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002fce358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-04 17:53:32 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-09-04 17:53:32 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Sep  4 17:53:34.421: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-2017  d68629d4-1bac-47cf-a9bb-d6b5f56cd449 18607 3 2023-09-04 17:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 68265505-45e8-4dfa-9b89-8e5a31185732 0xc002fcfdd7 0xc002fcfdd8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68265505-45e8-4dfa-9b89-8e5a31185732\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003010008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 17:53:34.421: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Sep  4 17:53:34.421: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-2017  907f1b0e-8d65-45da-acf7-b8228255f6d2 18603 3 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 68265505-45e8-4dfa-9b89-8e5a31185732 0xc002fcfab7 0xc002fcfab8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68265505-45e8-4dfa-9b89-8e5a31185732\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002fcfc78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 17:53:34.480: INFO: Pod "webserver-deployment-7f5969cbc7-5m69k" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5m69k webserver-deployment-7f5969cbc7- deployment-2017  4abf692e-6122-4dda-9bbf-63bc6c9e2464 18390 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:eff7e1819fb1328e0587f72069b653b09ba6984ad10d1b279bdab464e0e78a6c cni.projectcalico.org/podIP:10.36.55.112/32 cni.projectcalico.org/podIPs:10.36.55.112/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc0030112b7 0xc0030112b8}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rgfwn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rgfwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.112,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7bde6d9af2e098d4077d7d6126f742d3056c1f787a27fe5574b6bd2b7e32073b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.480: INFO: Pod "webserver-deployment-7f5969cbc7-82mmj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-82mmj webserver-deployment-7f5969cbc7- deployment-2017  d53c41ec-3cc3-4352-960a-3fe011f36945 18595 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003011cc0 0xc003011cc1}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmfnq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmfnq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.486: INFO: Pod "webserver-deployment-7f5969cbc7-9vbnn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9vbnn webserver-deployment-7f5969cbc7- deployment-2017  8fbcaff9-fc0c-42ef-b7c0-6dfc2b938967 18658 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e56e446ba6918aa225680037c8efea8d5634585034fd2a6ff3a6d0a9c30d2231 cni.projectcalico.org/podIP:10.36.55.124/32 cni.projectcalico.org/podIPs:10.36.55.124/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003026180 0xc003026181}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 17:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrdfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrdfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:,StartTime:2023-09-04 17:53:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.486: INFO: Pod "webserver-deployment-7f5969cbc7-cm5mr" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cm5mr webserver-deployment-7f5969cbc7- deployment-2017  1e375003-57e9-4ad4-a4bc-93ed78ed4036 18385 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6cc724dd2e061d8982cfc929e43dcc3b626281bbcdd39554baab27d096ec3e4d cni.projectcalico.org/podIP:10.36.55.109/32 cni.projectcalico.org/podIPs:10.36.55.109/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003026830 0xc003026831}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2l56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2l56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.109,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f1c6576b69ba0cfaab245f8a2a387d4fd45743f65ce709fdf99af8e232f4360b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.487: INFO: Pod "webserver-deployment-7f5969cbc7-hvw9m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hvw9m webserver-deployment-7f5969cbc7- deployment-2017  cb11f8d9-b46f-498c-8588-0ceab06f2c32 18560 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003026e50 0xc003026e51}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8fbz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8fbz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.490: INFO: Pod "webserver-deployment-7f5969cbc7-jx4rc" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jx4rc webserver-deployment-7f5969cbc7- deployment-2017  9f7dcc80-b94a-4957-9185-5113c6c47569 18392 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cf4f99aba7698f9d213fa09cfb9b9397e1648a9ff0eed38d9ca00dd1066b41b8 cni.projectcalico.org/podIP:10.36.217.219/32 cni.projectcalico.org/podIPs:10.36.217.219/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003027390 0xc003027391}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-74gnd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-74gnd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.219,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b58abcd9c2c0ded220099fcb596038168b1d795b80ec5f0d260e51280f7f6eb7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.493: INFO: Pod "webserver-deployment-7f5969cbc7-kxds4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kxds4 webserver-deployment-7f5969cbc7- deployment-2017  517ccca7-315f-4319-8fa2-4701430d1f2d 18678 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7270f86c2076e6c8ab4889deea26ea4b38a832188de80f12c7e2915b7e49b580 cni.projectcalico.org/podIP:10.36.217.228/32 cni.projectcalico.org/podIPs:10.36.217.228/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc0030277d0 0xc0030277d1}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l692d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l692d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.497: INFO: Pod "webserver-deployment-7f5969cbc7-lzqvf" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lzqvf webserver-deployment-7f5969cbc7- deployment-2017  d07049d9-559b-48f0-95fd-f6b132e889ad 18425 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d71db870f514f891884bb4fdeac43fac1c4b6897314e0e4020c7bdc15f2e68bf cni.projectcalico.org/podIP:10.36.217.236/32 cni.projectcalico.org/podIPs:10.36.217.236/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003027c40 0xc003027c41}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4vpqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4vpqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.236,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0107b45c0f80792a8e359d692bd6099bc52ad267c52656749c03de3c78747be3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.236,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.497: INFO: Pod "webserver-deployment-7f5969cbc7-m27rz" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m27rz webserver-deployment-7f5969cbc7- deployment-2017  cb9ef480-1475-4be4-9c17-97732d822253 18419 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f7dcef9e9e7fb8b261af2d09c3f105100a9c79c50a017d66568ac2f1e51da92c cni.projectcalico.org/podIP:10.36.55.117/32 cni.projectcalico.org/podIPs:10.36.55.117/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e520c0 0xc003e520c1}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bxdpx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bxdpx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.117,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://711848def79e06dbc91c01a40258f2f79f23f77d68b922bca5fe05034559a79c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.498: INFO: Pod "webserver-deployment-7f5969cbc7-q9sqr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q9sqr webserver-deployment-7f5969cbc7- deployment-2017  1906046a-6f30-42f4-a0da-95e2c75e4281 18609 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e522d0 0xc003e522d1}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zgvxk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zgvxk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:,StartTime:2023-09-04 17:53:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.498: INFO: Pod "webserver-deployment-7f5969cbc7-qqtf5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qqtf5 webserver-deployment-7f5969cbc7- deployment-2017  9de2e920-b9bf-4fed-89bd-a457399f1caa 18598 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e52490 0xc003e52491}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bh224,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bh224,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.500: INFO: Pod "webserver-deployment-7f5969cbc7-rcm6k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rcm6k webserver-deployment-7f5969cbc7- deployment-2017  84049411-ef6e-429d-bf05-a5410c4e07ee 18567 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e525f0 0xc003e525f1}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4f766,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4f766,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.502: INFO: Pod "webserver-deployment-7f5969cbc7-rkx9k" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rkx9k webserver-deployment-7f5969cbc7- deployment-2017  16fa7912-5088-41d0-a78a-fb2392f9f852 18396 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:058027ebe9182e0c2a9fc062ac48fccab4bb1e58e1189df814cb7e277aac9196 cni.projectcalico.org/podIP:10.36.217.237/32 cni.projectcalico.org/podIPs:10.36.217.237/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e52770 0xc003e52771}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d2clm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d2clm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.237,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a1cc085fc0e1803ba5be62dcfe0454211edfbe5eb8a7276db177b88a1524eb1a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.237,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.506: INFO: Pod "webserver-deployment-7f5969cbc7-rrffg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rrffg webserver-deployment-7f5969cbc7- deployment-2017  ac62dc42-d63e-4aad-bba4-1ce1672b1ad8 18687 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:143873bd93dab6db14b654ed88ddc1090ff62396c2f1239147d2b129f96a80f1 cni.projectcalico.org/podIP:10.36.55.114/32 cni.projectcalico.org/podIPs:10.36.55.114/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e52990 0xc003e52991}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-59ng8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59ng8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.506: INFO: Pod "webserver-deployment-7f5969cbc7-s6mf8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s6mf8 webserver-deployment-7f5969cbc7- deployment-2017  35dc2e4c-ddd4-48b9-a094-7ca671249a69 18594 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e52b10 0xc003e52b11}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrqx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrqx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.506: INFO: Pod "webserver-deployment-7f5969cbc7-tlr5f" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tlr5f webserver-deployment-7f5969cbc7- deployment-2017  b41f2734-2e7a-4642-9d50-5dc1a76dfe91 18697 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a513debc2c1ba650c562f873dc28d189beeaa866a75deaa1494cb4b92957a8bb cni.projectcalico.org/podIP:10.36.217.209/32 cni.projectcalico.org/podIPs:10.36.217.209/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e52c90 0xc003e52c91}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qjw6g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qjw6g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.506: INFO: Pod "webserver-deployment-7f5969cbc7-vcwp9" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vcwp9 webserver-deployment-7f5969cbc7- deployment-2017  bbc2d552-df07-43ae-9bd9-2970efae1a57 18427 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6cfae5216ffe24551aef6fb52de24d456d47342e8ab2b8141694e82d6a89c61e cni.projectcalico.org/podIP:10.36.217.234/32 cni.projectcalico.org/podIPs:10.36.217.234/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e52e30 0xc003e52e31}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8gvn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8gvn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.234,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2d2ccc7fac0547b7d93d2829d744df8eb69266ee7f6f38b87bc3dfad095d48c3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.234,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.507: INFO: Pod "webserver-deployment-7f5969cbc7-vk9ps" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vk9ps webserver-deployment-7f5969cbc7- deployment-2017  f8df5177-7571-4aaf-9159-02e39d0488d8 18621 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e53040 0xc003e53041}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v42qm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v42qm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:,StartTime:2023-09-04 17:53:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.507: INFO: Pod "webserver-deployment-7f5969cbc7-vq75n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vq75n webserver-deployment-7f5969cbc7- deployment-2017  8921c9dc-f821-45d4-9bc5-4ed7cabf19b7 18571 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e53200 0xc003e53201}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sqhw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sqhw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.510: INFO: Pod "webserver-deployment-7f5969cbc7-xzcn5" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xzcn5 webserver-deployment-7f5969cbc7- deployment-2017  2415a95a-fae5-4aa5-b6a0-3618ce704c09 18432 0 2023-09-04 17:53:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bdba8692ea098de716bb08fa50a0054de402920eea2ebeff0a9f204ff9d22353 cni.projectcalico.org/podIP:10.36.217.208/32 cni.projectcalico.org/podIPs:10.36.217.208/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 907f1b0e-8d65-45da-acf7-b8228255f6d2 0xc003e53380 0xc003e53381}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907f1b0e-8d65-45da-acf7-b8228255f6d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 17:53:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ch5wg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ch5wg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.208,StartTime:2023-09-04 17:53:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 17:53:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6204fb740d9ac54efd02b88734f024dc90cca74da1b341862e530e56c9dff168,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.208,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.510: INFO: Pod "webserver-deployment-d9f79cb5-2zxxc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2zxxc webserver-deployment-d9f79cb5- deployment-2017  8b843c32-9602-4b4b-89c5-700ebc2a2797 18587 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003e5356f 0xc003e53580}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zmlj5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zmlj5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.510: INFO: Pod "webserver-deployment-d9f79cb5-52gt2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-52gt2 webserver-deployment-d9f79cb5- deployment-2017  55a57e1d-8dcc-4dca-b209-fbf5cac897c3 18691 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:89afd82209c0e6ce8da4e180f4460c4cd8347b47c43c56deac11d37744f728a0 cni.projectcalico.org/podIP:10.36.55.73/32 cni.projectcalico.org/podIPs:10.36.55.73/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003e536df 0xc003e53710}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r95m2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r95m2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.510: INFO: Pod "webserver-deployment-d9f79cb5-5bgtk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5bgtk webserver-deployment-d9f79cb5- deployment-2017  ae113bfb-1724-4b7a-a654-6274f9210957 18669 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d28fcf8fa6f8d318732b4d2ab756012c64c4ba458dd07fadb53929cc59b94b86 cni.projectcalico.org/podIP:10.36.55.94/32 cni.projectcalico.org/podIPs:10.36.55.94/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003e5388f 0xc003e538c0}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2zq8h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2zq8h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.511: INFO: Pod "webserver-deployment-d9f79cb5-69w9j" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-69w9j webserver-deployment-d9f79cb5- deployment-2017  d6b7491a-9624-4146-8180-8d38e7a47bdb 18662 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7f638a96c789113296cba8fd5f9cfee1d86140f3005d6b721d0a228cab375518 cni.projectcalico.org/podIP:10.36.217.253/32 cni.projectcalico.org/podIPs:10.36.217.253/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003e53a3f 0xc003e53a70}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 17:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wnfzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wnfzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.514: INFO: Pod "webserver-deployment-d9f79cb5-7vzxl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7vzxl webserver-deployment-d9f79cb5- deployment-2017  5eef31b3-3ada-469a-b337-f79281960cb4 18597 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003e53bef 0xc003e53c00}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47sm9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47sm9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:,StartTime:2023-09-04 17:53:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.514: INFO: Pod "webserver-deployment-d9f79cb5-bdpt8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bdpt8 webserver-deployment-d9f79cb5- deployment-2017  dd8030cd-01f1-469b-b536-b7f4c08c0565 18520 0 2023-09-04 17:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:501b2d5b581676d2a6563e181ed974a58d4f1769b474f797f9e4615e333b9046 cni.projectcalico.org/podIP:10.36.55.108/32 cni.projectcalico.org/podIPs:10.36.55.108/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003e53dcf 0xc003e53e00}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 17:53:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4s6p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4s6p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:,StartTime:2023-09-04 17:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.515: INFO: Pod "webserver-deployment-d9f79cb5-gj4r4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gj4r4 webserver-deployment-d9f79cb5- deployment-2017  89100b53-ea0f-492b-bfd9-e486f614d560 18516 0 2023-09-04 17:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6f65ab53136a5c45670187900e063346aed7e784e2b8be617c069df336a1420d cni.projectcalico.org/podIP:10.36.55.127/32 cni.projectcalico.org/podIPs:10.36.55.127/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003e53fef 0xc003ada020}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 17:53:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2qk89,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2qk89,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:,StartTime:2023-09-04 17:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.515: INFO: Pod "webserver-deployment-d9f79cb5-gsbrl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gsbrl webserver-deployment-d9f79cb5- deployment-2017  7a8754c3-94e5-48f1-9d59-826f5541b05e 18588 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003ada20f 0xc003ada220}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dxzkl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dxzkl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.518: INFO: Pod "webserver-deployment-d9f79cb5-lrjk7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lrjk7 webserver-deployment-d9f79cb5- deployment-2017  a181e61c-0ef3-4cd7-84ce-af7c0eaf3843 18539 0 2023-09-04 17:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:91e18fe9b4188f6c597e71e3c06fcc0aabf06446397813bd822736909db62031 cni.projectcalico.org/podIP:10.36.217.230/32 cni.projectcalico.org/podIPs:10.36.217.230/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003ada37f 0xc003ada3b0}] [] [{calico Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-65lxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-65lxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.230,StartTime:2023-09-04 17:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.518: INFO: Pod "webserver-deployment-d9f79cb5-qqk89" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qqk89 webserver-deployment-d9f79cb5- deployment-2017  6b6e4b67-baaf-4e2c-92d6-a4d59c7ab61f 18673 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:12d7a4eba9cad51b8c7b4a54e2959c2c8bce0fb4512295ea27c70c9cfda8b94f cni.projectcalico.org/podIP:10.36.217.210/32 cni.projectcalico.org/podIPs:10.36.217.210/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003ada5df 0xc003ada610}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 17:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6lvzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6lvzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:,StartTime:2023-09-04 17:53:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.518: INFO: Pod "webserver-deployment-d9f79cb5-tv7st" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tv7st webserver-deployment-d9f79cb5- deployment-2017  d0635245-8350-4e76-95f1-9b99ccffb67b 18531 0 2023-09-04 17:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:4700ba3750d75388a7652df787ff79570272adf802513568c9c3e22407231d16 cni.projectcalico.org/podIP:10.36.55.84/32 cni.projectcalico.org/podIPs:10.36.55.84/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003ada81f 0xc003ada850}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 17:53:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gbh8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gbh8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:,StartTime:2023-09-04 17:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.519: INFO: Pod "webserver-deployment-d9f79cb5-vlkk8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vlkk8 webserver-deployment-d9f79cb5- deployment-2017  db3dfaff-4277-4a6b-a102-24e7e6a93260 18646 0 2023-09-04 17:53:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003adaa4f 0xc003adaa60}] [] [{kube-controller-manager Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbqnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbqnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:,StartTime:2023-09-04 17:53:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 17:53:34.522: INFO: Pod "webserver-deployment-d9f79cb5-zpcts" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zpcts webserver-deployment-d9f79cb5- deployment-2017  605ed0cf-12a5-41b2-8513-f17194e77d2c 18504 0 2023-09-04 17:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:c83b7143ed051858fb24e345c743e765f2f4103068fa11e4006ec031c1acf97a cni.projectcalico.org/podIP:10.36.217.252/32 cni.projectcalico.org/podIPs:10.36.217.252/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d68629d4-1bac-47cf-a9bb-d6b5f56cd449 0xc003adac4f 0xc003adac80}] [] [{calico Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d68629d4-1bac-47cf-a9bb-d6b5f56cd449\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 17:53:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l466l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l466l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 17:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:,StartTime:2023-09-04 17:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:53:34.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2017" for this suite. 09/04/23 17:53:34.53
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:53:34.679
Sep  4 17:53:34.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename sched-preemption 09/04/23 17:53:34.686
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:34.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:34.721
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  4 17:53:34.775: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  4 17:54:34.833: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:54:34.838
Sep  4 17:54:34.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename sched-preemption-path 09/04/23 17:54:34.84
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:54:34.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:54:34.872
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 09/04/23 17:54:34.877
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/04/23 17:54:34.877
Sep  4 17:54:34.891: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9735" to be "running"
Sep  4 17:54:34.906: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 15.449244ms
Sep  4 17:54:36.913: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.021931782s
Sep  4 17:54:36.913: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/04/23 17:54:36.92
Sep  4 17:54:36.949: INFO: found a healthy node: tenant-000001
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Sep  4 17:54:43.106: INFO: pods created so far: [1 1 1]
Sep  4 17:54:43.106: INFO: length of pods created so far: 3
Sep  4 17:54:45.122: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Sep  4 17:54:52.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:54:52.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-9735" for this suite. 09/04/23 17:54:52.237
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-804" for this suite. 09/04/23 17:54:52.248
------------------------------
â€¢ [SLOW TEST] [77.578 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:53:34.679
    Sep  4 17:53:34.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename sched-preemption 09/04/23 17:53:34.686
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:53:34.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:53:34.721
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  4 17:53:34.775: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  4 17:54:34.833: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:54:34.838
    Sep  4 17:54:34.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename sched-preemption-path 09/04/23 17:54:34.84
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:54:34.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:54:34.872
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 09/04/23 17:54:34.877
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/04/23 17:54:34.877
    Sep  4 17:54:34.891: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9735" to be "running"
    Sep  4 17:54:34.906: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 15.449244ms
    Sep  4 17:54:36.913: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.021931782s
    Sep  4 17:54:36.913: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/04/23 17:54:36.92
    Sep  4 17:54:36.949: INFO: found a healthy node: tenant-000001
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Sep  4 17:54:43.106: INFO: pods created so far: [1 1 1]
    Sep  4 17:54:43.106: INFO: length of pods created so far: 3
    Sep  4 17:54:45.122: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:54:52.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:54:52.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-9735" for this suite. 09/04/23 17:54:52.237
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-804" for this suite. 09/04/23 17:54:52.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:54:52.258
Sep  4 17:54:52.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename var-expansion 09/04/23 17:54:52.259
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:54:52.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:54:52.287
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 09/04/23 17:54:52.292
Sep  4 17:54:52.303: INFO: Waiting up to 5m0s for pod "var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8" in namespace "var-expansion-7076" to be "Succeeded or Failed"
Sep  4 17:54:52.322: INFO: Pod "var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.905441ms
Sep  4 17:54:54.328: INFO: Pod "var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025242085s
Sep  4 17:54:56.328: INFO: Pod "var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025018502s
STEP: Saw pod success 09/04/23 17:54:56.328
Sep  4 17:54:56.329: INFO: Pod "var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8" satisfied condition "Succeeded or Failed"
Sep  4 17:54:56.334: INFO: Trying to get logs from node tenant-000001 pod var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8 container dapi-container: <nil>
STEP: delete the pod 09/04/23 17:54:56.37
Sep  4 17:54:56.386: INFO: Waiting for pod var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8 to disappear
Sep  4 17:54:56.392: INFO: Pod var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 17:54:56.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7076" for this suite. 09/04/23 17:54:56.398
------------------------------
â€¢ [4.150 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:54:52.258
    Sep  4 17:54:52.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename var-expansion 09/04/23 17:54:52.259
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:54:52.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:54:52.287
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 09/04/23 17:54:52.292
    Sep  4 17:54:52.303: INFO: Waiting up to 5m0s for pod "var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8" in namespace "var-expansion-7076" to be "Succeeded or Failed"
    Sep  4 17:54:52.322: INFO: Pod "var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.905441ms
    Sep  4 17:54:54.328: INFO: Pod "var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025242085s
    Sep  4 17:54:56.328: INFO: Pod "var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025018502s
    STEP: Saw pod success 09/04/23 17:54:56.328
    Sep  4 17:54:56.329: INFO: Pod "var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8" satisfied condition "Succeeded or Failed"
    Sep  4 17:54:56.334: INFO: Trying to get logs from node tenant-000001 pod var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8 container dapi-container: <nil>
    STEP: delete the pod 09/04/23 17:54:56.37
    Sep  4 17:54:56.386: INFO: Waiting for pod var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8 to disappear
    Sep  4 17:54:56.392: INFO: Pod var-expansion-e2e8baa3-c268-4fd4-8c71-2fa8e1b0c6a8 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:54:56.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7076" for this suite. 09/04/23 17:54:56.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:54:56.412
Sep  4 17:54:56.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 17:54:56.414
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:54:56.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:54:56.442
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 09/04/23 17:54:56.448
Sep  4 17:54:56.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 create -f -'
Sep  4 17:54:57.269: INFO: stderr: ""
Sep  4 17:54:57.270: INFO: stdout: "pod/pause created\n"
Sep  4 17:54:57.270: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  4 17:54:57.270: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4736" to be "running and ready"
Sep  4 17:54:57.282: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.490829ms
Sep  4 17:54:57.283: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'tenant-000001' to be 'Running' but was 'Pending'
Sep  4 17:54:59.289: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.019488393s
Sep  4 17:54:59.290: INFO: Pod "pause" satisfied condition "running and ready"
Sep  4 17:54:59.290: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 09/04/23 17:54:59.29
Sep  4 17:54:59.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 label pods pause testing-label=testing-label-value'
Sep  4 17:54:59.403: INFO: stderr: ""
Sep  4 17:54:59.403: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 09/04/23 17:54:59.403
Sep  4 17:54:59.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 get pod pause -L testing-label'
Sep  4 17:54:59.494: INFO: stderr: ""
Sep  4 17:54:59.494: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 09/04/23 17:54:59.494
Sep  4 17:54:59.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 label pods pause testing-label-'
Sep  4 17:54:59.600: INFO: stderr: ""
Sep  4 17:54:59.600: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 09/04/23 17:54:59.6
Sep  4 17:54:59.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 get pod pause -L testing-label'
Sep  4 17:54:59.695: INFO: stderr: ""
Sep  4 17:54:59.695: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 09/04/23 17:54:59.695
Sep  4 17:54:59.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 delete --grace-period=0 --force -f -'
Sep  4 17:54:59.822: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 17:54:59.822: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  4 17:54:59.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 get rc,svc -l name=pause --no-headers'
Sep  4 17:54:59.918: INFO: stderr: "No resources found in kubectl-4736 namespace.\n"
Sep  4 17:54:59.918: INFO: stdout: ""
Sep  4 17:54:59.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  4 17:54:59.993: INFO: stderr: ""
Sep  4 17:54:59.993: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 17:54:59.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4736" for this suite. 09/04/23 17:54:59.999
------------------------------
â€¢ [3.600 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:54:56.412
    Sep  4 17:54:56.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 17:54:56.414
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:54:56.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:54:56.442
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 09/04/23 17:54:56.448
    Sep  4 17:54:56.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 create -f -'
    Sep  4 17:54:57.269: INFO: stderr: ""
    Sep  4 17:54:57.270: INFO: stdout: "pod/pause created\n"
    Sep  4 17:54:57.270: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Sep  4 17:54:57.270: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4736" to be "running and ready"
    Sep  4 17:54:57.282: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.490829ms
    Sep  4 17:54:57.283: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'tenant-000001' to be 'Running' but was 'Pending'
    Sep  4 17:54:59.289: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.019488393s
    Sep  4 17:54:59.290: INFO: Pod "pause" satisfied condition "running and ready"
    Sep  4 17:54:59.290: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 09/04/23 17:54:59.29
    Sep  4 17:54:59.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 label pods pause testing-label=testing-label-value'
    Sep  4 17:54:59.403: INFO: stderr: ""
    Sep  4 17:54:59.403: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 09/04/23 17:54:59.403
    Sep  4 17:54:59.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 get pod pause -L testing-label'
    Sep  4 17:54:59.494: INFO: stderr: ""
    Sep  4 17:54:59.494: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 09/04/23 17:54:59.494
    Sep  4 17:54:59.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 label pods pause testing-label-'
    Sep  4 17:54:59.600: INFO: stderr: ""
    Sep  4 17:54:59.600: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 09/04/23 17:54:59.6
    Sep  4 17:54:59.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 get pod pause -L testing-label'
    Sep  4 17:54:59.695: INFO: stderr: ""
    Sep  4 17:54:59.695: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 09/04/23 17:54:59.695
    Sep  4 17:54:59.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 delete --grace-period=0 --force -f -'
    Sep  4 17:54:59.822: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 17:54:59.822: INFO: stdout: "pod \"pause\" force deleted\n"
    Sep  4 17:54:59.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 get rc,svc -l name=pause --no-headers'
    Sep  4 17:54:59.918: INFO: stderr: "No resources found in kubectl-4736 namespace.\n"
    Sep  4 17:54:59.918: INFO: stdout: ""
    Sep  4 17:54:59.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-4736 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  4 17:54:59.993: INFO: stderr: ""
    Sep  4 17:54:59.993: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:54:59.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4736" for this suite. 09/04/23 17:54:59.999
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:55:00.012
Sep  4 17:55:00.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 17:55:00.013
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:55:00.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:55:00.05
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 09/04/23 17:55:00.056
Sep  4 17:55:00.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 create -f -'
Sep  4 17:55:00.874: INFO: stderr: ""
Sep  4 17:55:00.874: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 17:55:00.874
Sep  4 17:55:00.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  4 17:55:00.966: INFO: stderr: ""
Sep  4 17:55:00.966: INFO: stdout: "update-demo-nautilus-5xxfs update-demo-nautilus-g6ww9 "
Sep  4 17:55:00.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-5xxfs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 17:55:01.045: INFO: stderr: ""
Sep  4 17:55:01.045: INFO: stdout: ""
Sep  4 17:55:01.045: INFO: update-demo-nautilus-5xxfs is created but not running
Sep  4 17:55:06.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  4 17:55:06.322: INFO: stderr: ""
Sep  4 17:55:06.322: INFO: stdout: "update-demo-nautilus-5xxfs update-demo-nautilus-g6ww9 "
Sep  4 17:55:06.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-5xxfs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 17:55:06.456: INFO: stderr: ""
Sep  4 17:55:06.456: INFO: stdout: "true"
Sep  4 17:55:06.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-5xxfs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  4 17:55:06.567: INFO: stderr: ""
Sep  4 17:55:06.567: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  4 17:55:06.567: INFO: validating pod update-demo-nautilus-5xxfs
Sep  4 17:55:06.586: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  4 17:55:06.586: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  4 17:55:06.586: INFO: update-demo-nautilus-5xxfs is verified up and running
Sep  4 17:55:06.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-g6ww9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 17:55:06.781: INFO: stderr: ""
Sep  4 17:55:06.781: INFO: stdout: "true"
Sep  4 17:55:06.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-g6ww9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  4 17:55:06.864: INFO: stderr: ""
Sep  4 17:55:06.864: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  4 17:55:06.864: INFO: validating pod update-demo-nautilus-g6ww9
Sep  4 17:55:06.878: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  4 17:55:06.878: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  4 17:55:06.878: INFO: update-demo-nautilus-g6ww9 is verified up and running
STEP: scaling down the replication controller 09/04/23 17:55:06.878
Sep  4 17:55:06.879: INFO: scanned /root for discovery docs: <nil>
Sep  4 17:55:06.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Sep  4 17:55:07.977: INFO: stderr: ""
Sep  4 17:55:07.977: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 17:55:07.977
Sep  4 17:55:07.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  4 17:55:08.056: INFO: stderr: ""
Sep  4 17:55:08.056: INFO: stdout: "update-demo-nautilus-5xxfs update-demo-nautilus-g6ww9 "
STEP: Replicas for name=update-demo: expected=1 actual=2 09/04/23 17:55:08.056
Sep  4 17:55:13.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  4 17:55:13.140: INFO: stderr: ""
Sep  4 17:55:13.140: INFO: stdout: "update-demo-nautilus-5xxfs "
Sep  4 17:55:13.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-5xxfs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 17:55:13.222: INFO: stderr: ""
Sep  4 17:55:13.222: INFO: stdout: "true"
Sep  4 17:55:13.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-5xxfs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  4 17:55:13.309: INFO: stderr: ""
Sep  4 17:55:13.309: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  4 17:55:13.309: INFO: validating pod update-demo-nautilus-5xxfs
Sep  4 17:55:13.319: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  4 17:55:13.319: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  4 17:55:13.319: INFO: update-demo-nautilus-5xxfs is verified up and running
STEP: scaling up the replication controller 09/04/23 17:55:13.319
Sep  4 17:55:13.320: INFO: scanned /root for discovery docs: <nil>
Sep  4 17:55:13.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Sep  4 17:55:14.445: INFO: stderr: ""
Sep  4 17:55:14.445: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 17:55:14.445
Sep  4 17:55:14.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  4 17:55:14.529: INFO: stderr: ""
Sep  4 17:55:14.529: INFO: stdout: "update-demo-nautilus-5xxfs update-demo-nautilus-lktn6 "
Sep  4 17:55:14.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-5xxfs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 17:55:14.610: INFO: stderr: ""
Sep  4 17:55:14.610: INFO: stdout: "true"
Sep  4 17:55:14.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-5xxfs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  4 17:55:14.697: INFO: stderr: ""
Sep  4 17:55:14.697: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  4 17:55:14.697: INFO: validating pod update-demo-nautilus-5xxfs
Sep  4 17:55:14.707: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  4 17:55:14.707: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  4 17:55:14.707: INFO: update-demo-nautilus-5xxfs is verified up and running
Sep  4 17:55:14.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-lktn6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 17:55:14.815: INFO: stderr: ""
Sep  4 17:55:14.815: INFO: stdout: "true"
Sep  4 17:55:14.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-lktn6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  4 17:55:14.892: INFO: stderr: ""
Sep  4 17:55:14.892: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  4 17:55:14.892: INFO: validating pod update-demo-nautilus-lktn6
Sep  4 17:55:14.905: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  4 17:55:14.906: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  4 17:55:14.906: INFO: update-demo-nautilus-lktn6 is verified up and running
STEP: using delete to clean up resources 09/04/23 17:55:14.906
Sep  4 17:55:14.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 delete --grace-period=0 --force -f -'
Sep  4 17:55:14.986: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 17:55:14.986: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  4 17:55:14.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get rc,svc -l name=update-demo --no-headers'
Sep  4 17:55:15.105: INFO: stderr: "No resources found in kubectl-7154 namespace.\n"
Sep  4 17:55:15.105: INFO: stdout: ""
Sep  4 17:55:15.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  4 17:55:15.200: INFO: stderr: ""
Sep  4 17:55:15.200: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 17:55:15.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7154" for this suite. 09/04/23 17:55:15.206
------------------------------
â€¢ [SLOW TEST] [15.203 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:55:00.012
    Sep  4 17:55:00.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 17:55:00.013
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:55:00.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:55:00.05
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 09/04/23 17:55:00.056
    Sep  4 17:55:00.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 create -f -'
    Sep  4 17:55:00.874: INFO: stderr: ""
    Sep  4 17:55:00.874: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 17:55:00.874
    Sep  4 17:55:00.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  4 17:55:00.966: INFO: stderr: ""
    Sep  4 17:55:00.966: INFO: stdout: "update-demo-nautilus-5xxfs update-demo-nautilus-g6ww9 "
    Sep  4 17:55:00.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-5xxfs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 17:55:01.045: INFO: stderr: ""
    Sep  4 17:55:01.045: INFO: stdout: ""
    Sep  4 17:55:01.045: INFO: update-demo-nautilus-5xxfs is created but not running
    Sep  4 17:55:06.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  4 17:55:06.322: INFO: stderr: ""
    Sep  4 17:55:06.322: INFO: stdout: "update-demo-nautilus-5xxfs update-demo-nautilus-g6ww9 "
    Sep  4 17:55:06.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-5xxfs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 17:55:06.456: INFO: stderr: ""
    Sep  4 17:55:06.456: INFO: stdout: "true"
    Sep  4 17:55:06.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-5xxfs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  4 17:55:06.567: INFO: stderr: ""
    Sep  4 17:55:06.567: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  4 17:55:06.567: INFO: validating pod update-demo-nautilus-5xxfs
    Sep  4 17:55:06.586: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  4 17:55:06.586: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  4 17:55:06.586: INFO: update-demo-nautilus-5xxfs is verified up and running
    Sep  4 17:55:06.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-g6ww9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 17:55:06.781: INFO: stderr: ""
    Sep  4 17:55:06.781: INFO: stdout: "true"
    Sep  4 17:55:06.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-g6ww9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  4 17:55:06.864: INFO: stderr: ""
    Sep  4 17:55:06.864: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  4 17:55:06.864: INFO: validating pod update-demo-nautilus-g6ww9
    Sep  4 17:55:06.878: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  4 17:55:06.878: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  4 17:55:06.878: INFO: update-demo-nautilus-g6ww9 is verified up and running
    STEP: scaling down the replication controller 09/04/23 17:55:06.878
    Sep  4 17:55:06.879: INFO: scanned /root for discovery docs: <nil>
    Sep  4 17:55:06.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Sep  4 17:55:07.977: INFO: stderr: ""
    Sep  4 17:55:07.977: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 17:55:07.977
    Sep  4 17:55:07.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  4 17:55:08.056: INFO: stderr: ""
    Sep  4 17:55:08.056: INFO: stdout: "update-demo-nautilus-5xxfs update-demo-nautilus-g6ww9 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 09/04/23 17:55:08.056
    Sep  4 17:55:13.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  4 17:55:13.140: INFO: stderr: ""
    Sep  4 17:55:13.140: INFO: stdout: "update-demo-nautilus-5xxfs "
    Sep  4 17:55:13.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-5xxfs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 17:55:13.222: INFO: stderr: ""
    Sep  4 17:55:13.222: INFO: stdout: "true"
    Sep  4 17:55:13.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-5xxfs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  4 17:55:13.309: INFO: stderr: ""
    Sep  4 17:55:13.309: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  4 17:55:13.309: INFO: validating pod update-demo-nautilus-5xxfs
    Sep  4 17:55:13.319: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  4 17:55:13.319: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  4 17:55:13.319: INFO: update-demo-nautilus-5xxfs is verified up and running
    STEP: scaling up the replication controller 09/04/23 17:55:13.319
    Sep  4 17:55:13.320: INFO: scanned /root for discovery docs: <nil>
    Sep  4 17:55:13.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Sep  4 17:55:14.445: INFO: stderr: ""
    Sep  4 17:55:14.445: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 17:55:14.445
    Sep  4 17:55:14.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  4 17:55:14.529: INFO: stderr: ""
    Sep  4 17:55:14.529: INFO: stdout: "update-demo-nautilus-5xxfs update-demo-nautilus-lktn6 "
    Sep  4 17:55:14.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-5xxfs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 17:55:14.610: INFO: stderr: ""
    Sep  4 17:55:14.610: INFO: stdout: "true"
    Sep  4 17:55:14.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-5xxfs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  4 17:55:14.697: INFO: stderr: ""
    Sep  4 17:55:14.697: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  4 17:55:14.697: INFO: validating pod update-demo-nautilus-5xxfs
    Sep  4 17:55:14.707: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  4 17:55:14.707: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  4 17:55:14.707: INFO: update-demo-nautilus-5xxfs is verified up and running
    Sep  4 17:55:14.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-lktn6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 17:55:14.815: INFO: stderr: ""
    Sep  4 17:55:14.815: INFO: stdout: "true"
    Sep  4 17:55:14.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods update-demo-nautilus-lktn6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  4 17:55:14.892: INFO: stderr: ""
    Sep  4 17:55:14.892: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  4 17:55:14.892: INFO: validating pod update-demo-nautilus-lktn6
    Sep  4 17:55:14.905: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  4 17:55:14.906: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  4 17:55:14.906: INFO: update-demo-nautilus-lktn6 is verified up and running
    STEP: using delete to clean up resources 09/04/23 17:55:14.906
    Sep  4 17:55:14.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 delete --grace-period=0 --force -f -'
    Sep  4 17:55:14.986: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 17:55:14.986: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep  4 17:55:14.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get rc,svc -l name=update-demo --no-headers'
    Sep  4 17:55:15.105: INFO: stderr: "No resources found in kubectl-7154 namespace.\n"
    Sep  4 17:55:15.105: INFO: stdout: ""
    Sep  4 17:55:15.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-7154 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  4 17:55:15.200: INFO: stderr: ""
    Sep  4 17:55:15.200: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:55:15.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7154" for this suite. 09/04/23 17:55:15.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:55:15.216
Sep  4 17:55:15.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename endpointslice 09/04/23 17:55:15.217
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:55:15.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:55:15.245
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 09/04/23 17:55:20.389
STEP: referencing matching pods with named port 09/04/23 17:55:25.403
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/04/23 17:55:30.415
STEP: recreating EndpointSlices after they've been deleted 09/04/23 17:55:35.431
Sep  4 17:55:35.464: INFO: EndpointSlice for Service endpointslice-2358/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  4 17:55:45.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2358" for this suite. 09/04/23 17:55:45.485
------------------------------
â€¢ [SLOW TEST] [30.278 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:55:15.216
    Sep  4 17:55:15.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename endpointslice 09/04/23 17:55:15.217
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:55:15.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:55:15.245
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 09/04/23 17:55:20.389
    STEP: referencing matching pods with named port 09/04/23 17:55:25.403
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/04/23 17:55:30.415
    STEP: recreating EndpointSlices after they've been deleted 09/04/23 17:55:35.431
    Sep  4 17:55:35.464: INFO: EndpointSlice for Service endpointslice-2358/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:55:45.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2358" for this suite. 09/04/23 17:55:45.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:55:45.501
Sep  4 17:55:45.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 17:55:45.502
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:55:45.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:55:45.528
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2712 09/04/23 17:55:45.534
STEP: changing the ExternalName service to type=ClusterIP 09/04/23 17:55:45.543
STEP: creating replication controller externalname-service in namespace services-2712 09/04/23 17:55:45.573
I0904 17:55:45.584516      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2712, replica count: 2
I0904 17:55:48.635143      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 17:55:48.635: INFO: Creating new exec pod
Sep  4 17:55:48.647: INFO: Waiting up to 5m0s for pod "execpodqcjbr" in namespace "services-2712" to be "running"
Sep  4 17:55:48.655: INFO: Pod "execpodqcjbr": Phase="Pending", Reason="", readiness=false. Elapsed: 7.618058ms
Sep  4 17:55:50.662: INFO: Pod "execpodqcjbr": Phase="Running", Reason="", readiness=true. Elapsed: 2.014318579s
Sep  4 17:55:50.662: INFO: Pod "execpodqcjbr" satisfied condition "running"
Sep  4 17:55:51.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-2712 exec execpodqcjbr -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Sep  4 17:55:51.900: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  4 17:55:51.900: INFO: stdout: ""
Sep  4 17:55:51.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-2712 exec execpodqcjbr -- /bin/sh -x -c nc -v -z -w 2 10.96.106.23 80'
Sep  4 17:55:52.094: INFO: stderr: "+ nc -v -z -w 2 10.96.106.23 80\nConnection to 10.96.106.23 80 port [tcp/http] succeeded!\n"
Sep  4 17:55:52.094: INFO: stdout: ""
Sep  4 17:55:52.094: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 17:55:52.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2712" for this suite. 09/04/23 17:55:52.14
------------------------------
â€¢ [SLOW TEST] [6.651 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:55:45.501
    Sep  4 17:55:45.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 17:55:45.502
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:55:45.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:55:45.528
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2712 09/04/23 17:55:45.534
    STEP: changing the ExternalName service to type=ClusterIP 09/04/23 17:55:45.543
    STEP: creating replication controller externalname-service in namespace services-2712 09/04/23 17:55:45.573
    I0904 17:55:45.584516      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2712, replica count: 2
    I0904 17:55:48.635143      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 17:55:48.635: INFO: Creating new exec pod
    Sep  4 17:55:48.647: INFO: Waiting up to 5m0s for pod "execpodqcjbr" in namespace "services-2712" to be "running"
    Sep  4 17:55:48.655: INFO: Pod "execpodqcjbr": Phase="Pending", Reason="", readiness=false. Elapsed: 7.618058ms
    Sep  4 17:55:50.662: INFO: Pod "execpodqcjbr": Phase="Running", Reason="", readiness=true. Elapsed: 2.014318579s
    Sep  4 17:55:50.662: INFO: Pod "execpodqcjbr" satisfied condition "running"
    Sep  4 17:55:51.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-2712 exec execpodqcjbr -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Sep  4 17:55:51.900: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  4 17:55:51.900: INFO: stdout: ""
    Sep  4 17:55:51.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-2712 exec execpodqcjbr -- /bin/sh -x -c nc -v -z -w 2 10.96.106.23 80'
    Sep  4 17:55:52.094: INFO: stderr: "+ nc -v -z -w 2 10.96.106.23 80\nConnection to 10.96.106.23 80 port [tcp/http] succeeded!\n"
    Sep  4 17:55:52.094: INFO: stdout: ""
    Sep  4 17:55:52.094: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:55:52.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2712" for this suite. 09/04/23 17:55:52.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:55:52.156
Sep  4 17:55:52.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename limitrange 09/04/23 17:55:52.157
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:55:52.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:55:52.187
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 09/04/23 17:55:52.192
STEP: Setting up watch 09/04/23 17:55:52.192
STEP: Submitting a LimitRange 09/04/23 17:55:52.298
STEP: Verifying LimitRange creation was observed 09/04/23 17:55:52.316
STEP: Fetching the LimitRange to ensure it has proper values 09/04/23 17:55:52.316
Sep  4 17:55:52.322: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  4 17:55:52.323: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 09/04/23 17:55:52.323
STEP: Ensuring Pod has resource requirements applied from LimitRange 09/04/23 17:55:52.333
Sep  4 17:55:52.343: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  4 17:55:52.343: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 09/04/23 17:55:52.344
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/04/23 17:55:52.355
Sep  4 17:55:52.368: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep  4 17:55:52.368: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 09/04/23 17:55:52.368
STEP: Failing to create a Pod with more than max resources 09/04/23 17:55:52.371
STEP: Updating a LimitRange 09/04/23 17:55:52.374
STEP: Verifying LimitRange updating is effective 09/04/23 17:55:52.385
STEP: Creating a Pod with less than former min resources 09/04/23 17:55:54.391
STEP: Failing to create a Pod with more than max resources 09/04/23 17:55:54.402
STEP: Deleting a LimitRange 09/04/23 17:55:54.406
STEP: Verifying the LimitRange was deleted 09/04/23 17:55:54.418
Sep  4 17:55:59.427: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 09/04/23 17:55:59.427
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Sep  4 17:55:59.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-6282" for this suite. 09/04/23 17:55:59.451
------------------------------
â€¢ [SLOW TEST] [7.306 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:55:52.156
    Sep  4 17:55:52.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename limitrange 09/04/23 17:55:52.157
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:55:52.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:55:52.187
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 09/04/23 17:55:52.192
    STEP: Setting up watch 09/04/23 17:55:52.192
    STEP: Submitting a LimitRange 09/04/23 17:55:52.298
    STEP: Verifying LimitRange creation was observed 09/04/23 17:55:52.316
    STEP: Fetching the LimitRange to ensure it has proper values 09/04/23 17:55:52.316
    Sep  4 17:55:52.322: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep  4 17:55:52.323: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 09/04/23 17:55:52.323
    STEP: Ensuring Pod has resource requirements applied from LimitRange 09/04/23 17:55:52.333
    Sep  4 17:55:52.343: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep  4 17:55:52.343: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 09/04/23 17:55:52.344
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/04/23 17:55:52.355
    Sep  4 17:55:52.368: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Sep  4 17:55:52.368: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 09/04/23 17:55:52.368
    STEP: Failing to create a Pod with more than max resources 09/04/23 17:55:52.371
    STEP: Updating a LimitRange 09/04/23 17:55:52.374
    STEP: Verifying LimitRange updating is effective 09/04/23 17:55:52.385
    STEP: Creating a Pod with less than former min resources 09/04/23 17:55:54.391
    STEP: Failing to create a Pod with more than max resources 09/04/23 17:55:54.402
    STEP: Deleting a LimitRange 09/04/23 17:55:54.406
    STEP: Verifying the LimitRange was deleted 09/04/23 17:55:54.418
    Sep  4 17:55:59.427: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 09/04/23 17:55:59.427
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:55:59.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-6282" for this suite. 09/04/23 17:55:59.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:55:59.468
Sep  4 17:55:59.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 17:55:59.469
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:55:59.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:55:59.507
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 09/04/23 17:55:59.512
Sep  4 17:55:59.524: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a" in namespace "downward-api-1200" to be "Succeeded or Failed"
Sep  4 17:55:59.530: INFO: Pod "downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.439828ms
Sep  4 17:56:01.537: INFO: Pod "downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012442513s
Sep  4 17:56:03.537: INFO: Pod "downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012234763s
STEP: Saw pod success 09/04/23 17:56:03.537
Sep  4 17:56:03.537: INFO: Pod "downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a" satisfied condition "Succeeded or Failed"
Sep  4 17:56:03.542: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a container client-container: <nil>
STEP: delete the pod 09/04/23 17:56:03.554
Sep  4 17:56:03.574: INFO: Waiting for pod downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a to disappear
Sep  4 17:56:03.579: INFO: Pod downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 17:56:03.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1200" for this suite. 09/04/23 17:56:03.585
------------------------------
â€¢ [4.129 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:55:59.468
    Sep  4 17:55:59.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 17:55:59.469
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:55:59.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:55:59.507
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 09/04/23 17:55:59.512
    Sep  4 17:55:59.524: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a" in namespace "downward-api-1200" to be "Succeeded or Failed"
    Sep  4 17:55:59.530: INFO: Pod "downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.439828ms
    Sep  4 17:56:01.537: INFO: Pod "downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012442513s
    Sep  4 17:56:03.537: INFO: Pod "downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012234763s
    STEP: Saw pod success 09/04/23 17:56:03.537
    Sep  4 17:56:03.537: INFO: Pod "downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a" satisfied condition "Succeeded or Failed"
    Sep  4 17:56:03.542: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a container client-container: <nil>
    STEP: delete the pod 09/04/23 17:56:03.554
    Sep  4 17:56:03.574: INFO: Waiting for pod downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a to disappear
    Sep  4 17:56:03.579: INFO: Pod downwardapi-volume-a8aaabf4-2163-47c7-b18e-8c245f82459a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:56:03.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1200" for this suite. 09/04/23 17:56:03.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:56:03.604
Sep  4 17:56:03.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename resourcequota 09/04/23 17:56:03.605
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:56:03.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:56:03.633
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 09/04/23 17:56:03.638
STEP: Getting a ResourceQuota 09/04/23 17:56:03.645
STEP: Listing all ResourceQuotas with LabelSelector 09/04/23 17:56:03.651
STEP: Patching the ResourceQuota 09/04/23 17:56:03.656
STEP: Deleting a Collection of ResourceQuotas 09/04/23 17:56:03.665
STEP: Verifying the deleted ResourceQuota 09/04/23 17:56:03.68
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 17:56:03.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4969" for this suite. 09/04/23 17:56:03.693
------------------------------
â€¢ [0.103 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:56:03.604
    Sep  4 17:56:03.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename resourcequota 09/04/23 17:56:03.605
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:56:03.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:56:03.633
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 09/04/23 17:56:03.638
    STEP: Getting a ResourceQuota 09/04/23 17:56:03.645
    STEP: Listing all ResourceQuotas with LabelSelector 09/04/23 17:56:03.651
    STEP: Patching the ResourceQuota 09/04/23 17:56:03.656
    STEP: Deleting a Collection of ResourceQuotas 09/04/23 17:56:03.665
    STEP: Verifying the deleted ResourceQuota 09/04/23 17:56:03.68
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:56:03.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4969" for this suite. 09/04/23 17:56:03.693
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:56:03.71
Sep  4 17:56:03.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 17:56:03.714
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:56:03.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:56:03.746
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 17:56:03.752
Sep  4 17:56:03.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-3375 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Sep  4 17:56:03.838: INFO: stderr: ""
Sep  4 17:56:03.838: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 09/04/23 17:56:03.838
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Sep  4 17:56:03.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-3375 delete pods e2e-test-httpd-pod'
Sep  4 17:56:06.050: INFO: stderr: ""
Sep  4 17:56:06.050: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 17:56:06.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3375" for this suite. 09/04/23 17:56:06.056
------------------------------
â€¢ [2.357 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:56:03.71
    Sep  4 17:56:03.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 17:56:03.714
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:56:03.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:56:03.746
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 17:56:03.752
    Sep  4 17:56:03.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-3375 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Sep  4 17:56:03.838: INFO: stderr: ""
    Sep  4 17:56:03.838: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 09/04/23 17:56:03.838
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Sep  4 17:56:03.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-3375 delete pods e2e-test-httpd-pod'
    Sep  4 17:56:06.050: INFO: stderr: ""
    Sep  4 17:56:06.050: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:56:06.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3375" for this suite. 09/04/23 17:56:06.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:56:06.067
Sep  4 17:56:06.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename podtemplate 09/04/23 17:56:06.068
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:56:06.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:56:06.093
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  4 17:56:06.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-7114" for this suite. 09/04/23 17:56:06.161
------------------------------
â€¢ [0.105 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:56:06.067
    Sep  4 17:56:06.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename podtemplate 09/04/23 17:56:06.068
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:56:06.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:56:06.093
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:56:06.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-7114" for this suite. 09/04/23 17:56:06.161
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:56:06.175
Sep  4 17:56:06.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename dns 09/04/23 17:56:06.176
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:56:06.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:56:06.205
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/04/23 17:56:06.21
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/04/23 17:56:06.21
STEP: creating a pod to probe DNS 09/04/23 17:56:06.21
STEP: submitting the pod to kubernetes 09/04/23 17:56:06.21
Sep  4 17:56:06.224: INFO: Waiting up to 15m0s for pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f" in namespace "dns-9819" to be "running"
Sep  4 17:56:06.241: INFO: Pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.302164ms
Sep  4 17:56:08.248: INFO: Pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023280537s
Sep  4 17:56:10.248: INFO: Pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023542398s
Sep  4 17:56:12.248: INFO: Pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023930668s
Sep  4 17:56:14.249: INFO: Pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025027389s
Sep  4 17:56:16.248: INFO: Pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f": Phase="Running", Reason="", readiness=true. Elapsed: 10.023417833s
Sep  4 17:56:16.248: INFO: Pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f" satisfied condition "running"
STEP: retrieving the pod 09/04/23 17:56:16.248
STEP: looking for the results for each expected name from probers 09/04/23 17:56:16.255
Sep  4 17:56:16.296: INFO: DNS probes using dns-9819/dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f succeeded

STEP: deleting the pod 09/04/23 17:56:16.296
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 17:56:16.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9819" for this suite. 09/04/23 17:56:16.349
------------------------------
â€¢ [SLOW TEST] [10.193 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:56:06.175
    Sep  4 17:56:06.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename dns 09/04/23 17:56:06.176
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:56:06.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:56:06.205
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/04/23 17:56:06.21
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/04/23 17:56:06.21
    STEP: creating a pod to probe DNS 09/04/23 17:56:06.21
    STEP: submitting the pod to kubernetes 09/04/23 17:56:06.21
    Sep  4 17:56:06.224: INFO: Waiting up to 15m0s for pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f" in namespace "dns-9819" to be "running"
    Sep  4 17:56:06.241: INFO: Pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.302164ms
    Sep  4 17:56:08.248: INFO: Pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023280537s
    Sep  4 17:56:10.248: INFO: Pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023542398s
    Sep  4 17:56:12.248: INFO: Pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023930668s
    Sep  4 17:56:14.249: INFO: Pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025027389s
    Sep  4 17:56:16.248: INFO: Pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f": Phase="Running", Reason="", readiness=true. Elapsed: 10.023417833s
    Sep  4 17:56:16.248: INFO: Pod "dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 17:56:16.248
    STEP: looking for the results for each expected name from probers 09/04/23 17:56:16.255
    Sep  4 17:56:16.296: INFO: DNS probes using dns-9819/dns-test-d8a888f5-868e-44b2-85e6-8989aed0402f succeeded

    STEP: deleting the pod 09/04/23 17:56:16.296
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:56:16.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9819" for this suite. 09/04/23 17:56:16.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:56:16.392
Sep  4 17:56:16.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename statefulset 09/04/23 17:56:16.394
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:56:16.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:56:16.42
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4956 09/04/23 17:56:16.425
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 09/04/23 17:56:16.438
STEP: Creating stateful set ss in namespace statefulset-4956 09/04/23 17:56:16.45
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4956 09/04/23 17:56:16.459
Sep  4 17:56:16.469: INFO: Found 0 stateful pods, waiting for 1
Sep  4 17:56:26.476: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/04/23 17:56:26.476
Sep  4 17:56:26.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 17:56:26.666: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 17:56:26.666: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 17:56:26.666: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 17:56:26.671: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  4 17:56:36.678: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 17:56:36.678: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 17:56:36.703: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999996s
Sep  4 17:56:37.711: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992435241s
Sep  4 17:56:38.718: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.984374547s
Sep  4 17:56:39.726: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.977552709s
Sep  4 17:56:40.732: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.969784998s
Sep  4 17:56:41.739: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.963621903s
Sep  4 17:56:42.776: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.956856264s
Sep  4 17:56:43.782: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.919665142s
Sep  4 17:56:44.789: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.912884503s
Sep  4 17:56:45.796: INFO: Verifying statefulset ss doesn't scale past 1 for another 906.452888ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4956 09/04/23 17:56:46.796
Sep  4 17:56:46.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 17:56:46.988: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  4 17:56:46.988: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 17:56:46.988: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  4 17:56:46.995: INFO: Found 1 stateful pods, waiting for 3
Sep  4 17:56:57.002: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 17:56:57.002: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 17:56:57.002: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 09/04/23 17:56:57.002
STEP: Scale down will halt with unhealthy stateful pod 09/04/23 17:56:57.002
Sep  4 17:56:57.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 17:56:57.196: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 17:56:57.196: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 17:56:57.196: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 17:56:57.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 17:56:57.379: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 17:56:57.379: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 17:56:57.380: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 17:56:57.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 17:56:57.620: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 17:56:57.620: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 17:56:57.620: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 17:56:57.620: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 17:56:57.627: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  4 17:57:07.639: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 17:57:07.640: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 17:57:07.640: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 17:57:07.659: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999995s
Sep  4 17:57:08.666: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993650323s
Sep  4 17:57:09.673: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98656589s
Sep  4 17:57:10.680: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97898762s
Sep  4 17:57:11.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.971892807s
Sep  4 17:57:12.700: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.961753342s
Sep  4 17:57:13.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.951855697s
Sep  4 17:57:14.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.942447492s
Sep  4 17:57:15.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.93451791s
Sep  4 17:57:16.733: INFO: Verifying statefulset ss doesn't scale past 3 for another 927.521906ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4956 09/04/23 17:57:17.733
Sep  4 17:57:17.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 17:57:17.952: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  4 17:57:17.952: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 17:57:17.952: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  4 17:57:17.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 17:57:18.113: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  4 17:57:18.113: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 17:57:18.113: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  4 17:57:18.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 17:57:18.314: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  4 17:57:18.314: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 17:57:18.314: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  4 17:57:18.314: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 09/04/23 17:57:28.339
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 17:57:28.339: INFO: Deleting all statefulset in ns statefulset-4956
Sep  4 17:57:28.345: INFO: Scaling statefulset ss to 0
Sep  4 17:57:28.360: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 17:57:28.365: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 17:57:28.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4956" for this suite. 09/04/23 17:57:28.394
------------------------------
â€¢ [SLOW TEST] [72.012 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:56:16.392
    Sep  4 17:56:16.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename statefulset 09/04/23 17:56:16.394
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:56:16.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:56:16.42
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4956 09/04/23 17:56:16.425
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 09/04/23 17:56:16.438
    STEP: Creating stateful set ss in namespace statefulset-4956 09/04/23 17:56:16.45
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4956 09/04/23 17:56:16.459
    Sep  4 17:56:16.469: INFO: Found 0 stateful pods, waiting for 1
    Sep  4 17:56:26.476: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/04/23 17:56:26.476
    Sep  4 17:56:26.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 17:56:26.666: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 17:56:26.666: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 17:56:26.666: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 17:56:26.671: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep  4 17:56:36.678: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 17:56:36.678: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 17:56:36.703: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999996s
    Sep  4 17:56:37.711: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992435241s
    Sep  4 17:56:38.718: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.984374547s
    Sep  4 17:56:39.726: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.977552709s
    Sep  4 17:56:40.732: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.969784998s
    Sep  4 17:56:41.739: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.963621903s
    Sep  4 17:56:42.776: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.956856264s
    Sep  4 17:56:43.782: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.919665142s
    Sep  4 17:56:44.789: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.912884503s
    Sep  4 17:56:45.796: INFO: Verifying statefulset ss doesn't scale past 1 for another 906.452888ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4956 09/04/23 17:56:46.796
    Sep  4 17:56:46.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 17:56:46.988: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  4 17:56:46.988: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 17:56:46.988: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  4 17:56:46.995: INFO: Found 1 stateful pods, waiting for 3
    Sep  4 17:56:57.002: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 17:56:57.002: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 17:56:57.002: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 09/04/23 17:56:57.002
    STEP: Scale down will halt with unhealthy stateful pod 09/04/23 17:56:57.002
    Sep  4 17:56:57.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 17:56:57.196: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 17:56:57.196: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 17:56:57.196: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 17:56:57.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 17:56:57.379: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 17:56:57.379: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 17:56:57.380: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 17:56:57.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 17:56:57.620: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 17:56:57.620: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 17:56:57.620: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 17:56:57.620: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 17:56:57.627: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Sep  4 17:57:07.639: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 17:57:07.640: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 17:57:07.640: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 17:57:07.659: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999995s
    Sep  4 17:57:08.666: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993650323s
    Sep  4 17:57:09.673: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98656589s
    Sep  4 17:57:10.680: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97898762s
    Sep  4 17:57:11.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.971892807s
    Sep  4 17:57:12.700: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.961753342s
    Sep  4 17:57:13.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.951855697s
    Sep  4 17:57:14.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.942447492s
    Sep  4 17:57:15.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.93451791s
    Sep  4 17:57:16.733: INFO: Verifying statefulset ss doesn't scale past 3 for another 927.521906ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4956 09/04/23 17:57:17.733
    Sep  4 17:57:17.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 17:57:17.952: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  4 17:57:17.952: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 17:57:17.952: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  4 17:57:17.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 17:57:18.113: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  4 17:57:18.113: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 17:57:18.113: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  4 17:57:18.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-4956 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 17:57:18.314: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  4 17:57:18.314: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 17:57:18.314: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  4 17:57:18.314: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 09/04/23 17:57:28.339
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 17:57:28.339: INFO: Deleting all statefulset in ns statefulset-4956
    Sep  4 17:57:28.345: INFO: Scaling statefulset ss to 0
    Sep  4 17:57:28.360: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 17:57:28.365: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:57:28.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4956" for this suite. 09/04/23 17:57:28.394
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:57:28.408
Sep  4 17:57:28.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename resourcequota 09/04/23 17:57:28.411
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:57:28.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:57:28.436
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 09/04/23 17:57:28.441
STEP: Counting existing ResourceQuota 09/04/23 17:57:33.447
STEP: Creating a ResourceQuota 09/04/23 17:57:38.454
STEP: Ensuring resource quota status is calculated 09/04/23 17:57:38.462
STEP: Creating a Secret 09/04/23 17:57:40.468
STEP: Ensuring resource quota status captures secret creation 09/04/23 17:57:40.484
STEP: Deleting a secret 09/04/23 17:57:42.491
STEP: Ensuring resource quota status released usage 09/04/23 17:57:42.503
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 17:57:44.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3822" for this suite. 09/04/23 17:57:44.519
------------------------------
â€¢ [SLOW TEST] [16.121 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:57:28.408
    Sep  4 17:57:28.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename resourcequota 09/04/23 17:57:28.411
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:57:28.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:57:28.436
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 09/04/23 17:57:28.441
    STEP: Counting existing ResourceQuota 09/04/23 17:57:33.447
    STEP: Creating a ResourceQuota 09/04/23 17:57:38.454
    STEP: Ensuring resource quota status is calculated 09/04/23 17:57:38.462
    STEP: Creating a Secret 09/04/23 17:57:40.468
    STEP: Ensuring resource quota status captures secret creation 09/04/23 17:57:40.484
    STEP: Deleting a secret 09/04/23 17:57:42.491
    STEP: Ensuring resource quota status released usage 09/04/23 17:57:42.503
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:57:44.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3822" for this suite. 09/04/23 17:57:44.519
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:57:44.532
Sep  4 17:57:44.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename secrets 09/04/23 17:57:44.534
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:57:44.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:57:44.561
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-4ebdc23e-5431-495b-9945-ee2f83709c0e 09/04/23 17:57:44.566
STEP: Creating a pod to test consume secrets 09/04/23 17:57:44.574
Sep  4 17:57:44.587: INFO: Waiting up to 5m0s for pod "pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af" in namespace "secrets-3615" to be "Succeeded or Failed"
Sep  4 17:57:44.603: INFO: Pod "pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af": Phase="Pending", Reason="", readiness=false. Elapsed: 16.021834ms
Sep  4 17:57:46.610: INFO: Pod "pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02298619s
Sep  4 17:57:48.610: INFO: Pod "pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023227528s
STEP: Saw pod success 09/04/23 17:57:48.61
Sep  4 17:57:48.611: INFO: Pod "pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af" satisfied condition "Succeeded or Failed"
Sep  4 17:57:48.617: INFO: Trying to get logs from node tenant-000001 pod pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 17:57:48.658
Sep  4 17:57:48.683: INFO: Waiting for pod pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af to disappear
Sep  4 17:57:48.688: INFO: Pod pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 17:57:48.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3615" for this suite. 09/04/23 17:57:48.703
------------------------------
â€¢ [4.185 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:57:44.532
    Sep  4 17:57:44.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename secrets 09/04/23 17:57:44.534
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:57:44.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:57:44.561
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-4ebdc23e-5431-495b-9945-ee2f83709c0e 09/04/23 17:57:44.566
    STEP: Creating a pod to test consume secrets 09/04/23 17:57:44.574
    Sep  4 17:57:44.587: INFO: Waiting up to 5m0s for pod "pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af" in namespace "secrets-3615" to be "Succeeded or Failed"
    Sep  4 17:57:44.603: INFO: Pod "pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af": Phase="Pending", Reason="", readiness=false. Elapsed: 16.021834ms
    Sep  4 17:57:46.610: INFO: Pod "pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02298619s
    Sep  4 17:57:48.610: INFO: Pod "pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023227528s
    STEP: Saw pod success 09/04/23 17:57:48.61
    Sep  4 17:57:48.611: INFO: Pod "pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af" satisfied condition "Succeeded or Failed"
    Sep  4 17:57:48.617: INFO: Trying to get logs from node tenant-000001 pod pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 17:57:48.658
    Sep  4 17:57:48.683: INFO: Waiting for pod pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af to disappear
    Sep  4 17:57:48.688: INFO: Pod pod-secrets-2f23d25c-627c-454b-8d5c-6fcfd281c0af no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:57:48.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3615" for this suite. 09/04/23 17:57:48.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:57:48.734
Sep  4 17:57:48.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 17:57:48.736
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:57:48.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:57:48.762
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 09/04/23 17:57:48.768
Sep  4 17:57:48.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: mark a version not serverd 09/04/23 17:57:53.154
STEP: check the unserved version gets removed 09/04/23 17:57:53.183
STEP: check the other version is not changed 09/04/23 17:57:55.099
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:57:58.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5352" for this suite. 09/04/23 17:57:58.532
------------------------------
â€¢ [SLOW TEST] [9.808 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:57:48.734
    Sep  4 17:57:48.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 17:57:48.736
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:57:48.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:57:48.762
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 09/04/23 17:57:48.768
    Sep  4 17:57:48.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: mark a version not serverd 09/04/23 17:57:53.154
    STEP: check the unserved version gets removed 09/04/23 17:57:53.183
    STEP: check the other version is not changed 09/04/23 17:57:55.099
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:57:58.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5352" for this suite. 09/04/23 17:57:58.532
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:57:58.546
Sep  4 17:57:58.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 17:57:58.547
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:57:58.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:57:58.575
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/04/23 17:57:58.578
Sep  4 17:57:58.588: INFO: Waiting up to 5m0s for pod "pod-c0036348-0e5d-46c4-81f9-90199dc36c62" in namespace "emptydir-1811" to be "Succeeded or Failed"
Sep  4 17:57:58.593: INFO: Pod "pod-c0036348-0e5d-46c4-81f9-90199dc36c62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.996395ms
Sep  4 17:58:00.598: INFO: Pod "pod-c0036348-0e5d-46c4-81f9-90199dc36c62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010293204s
Sep  4 17:58:02.598: INFO: Pod "pod-c0036348-0e5d-46c4-81f9-90199dc36c62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010118433s
STEP: Saw pod success 09/04/23 17:58:02.599
Sep  4 17:58:02.599: INFO: Pod "pod-c0036348-0e5d-46c4-81f9-90199dc36c62" satisfied condition "Succeeded or Failed"
Sep  4 17:58:02.604: INFO: Trying to get logs from node tenant-000001 pod pod-c0036348-0e5d-46c4-81f9-90199dc36c62 container test-container: <nil>
STEP: delete the pod 09/04/23 17:58:02.635
Sep  4 17:58:02.649: INFO: Waiting for pod pod-c0036348-0e5d-46c4-81f9-90199dc36c62 to disappear
Sep  4 17:58:02.653: INFO: Pod pod-c0036348-0e5d-46c4-81f9-90199dc36c62 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 17:58:02.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1811" for this suite. 09/04/23 17:58:02.657
------------------------------
â€¢ [4.119 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:57:58.546
    Sep  4 17:57:58.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 17:57:58.547
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:57:58.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:57:58.575
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/04/23 17:57:58.578
    Sep  4 17:57:58.588: INFO: Waiting up to 5m0s for pod "pod-c0036348-0e5d-46c4-81f9-90199dc36c62" in namespace "emptydir-1811" to be "Succeeded or Failed"
    Sep  4 17:57:58.593: INFO: Pod "pod-c0036348-0e5d-46c4-81f9-90199dc36c62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.996395ms
    Sep  4 17:58:00.598: INFO: Pod "pod-c0036348-0e5d-46c4-81f9-90199dc36c62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010293204s
    Sep  4 17:58:02.598: INFO: Pod "pod-c0036348-0e5d-46c4-81f9-90199dc36c62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010118433s
    STEP: Saw pod success 09/04/23 17:58:02.599
    Sep  4 17:58:02.599: INFO: Pod "pod-c0036348-0e5d-46c4-81f9-90199dc36c62" satisfied condition "Succeeded or Failed"
    Sep  4 17:58:02.604: INFO: Trying to get logs from node tenant-000001 pod pod-c0036348-0e5d-46c4-81f9-90199dc36c62 container test-container: <nil>
    STEP: delete the pod 09/04/23 17:58:02.635
    Sep  4 17:58:02.649: INFO: Waiting for pod pod-c0036348-0e5d-46c4-81f9-90199dc36c62 to disappear
    Sep  4 17:58:02.653: INFO: Pod pod-c0036348-0e5d-46c4-81f9-90199dc36c62 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:58:02.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1811" for this suite. 09/04/23 17:58:02.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:58:02.675
Sep  4 17:58:02.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 17:58:02.676
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:58:02.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:58:02.701
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 09/04/23 17:58:02.704
Sep  4 17:58:02.720: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c" in namespace "projected-8193" to be "Succeeded or Failed"
Sep  4 17:58:02.726: INFO: Pod "downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.844764ms
Sep  4 17:58:04.734: INFO: Pod "downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013498613s
Sep  4 17:58:06.734: INFO: Pod "downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013239036s
STEP: Saw pod success 09/04/23 17:58:06.734
Sep  4 17:58:06.735: INFO: Pod "downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c" satisfied condition "Succeeded or Failed"
Sep  4 17:58:06.738: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c container client-container: <nil>
STEP: delete the pod 09/04/23 17:58:06.745
Sep  4 17:58:06.763: INFO: Waiting for pod downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c to disappear
Sep  4 17:58:06.774: INFO: Pod downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 17:58:06.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8193" for this suite. 09/04/23 17:58:06.784
------------------------------
â€¢ [4.118 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:58:02.675
    Sep  4 17:58:02.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 17:58:02.676
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:58:02.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:58:02.701
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 09/04/23 17:58:02.704
    Sep  4 17:58:02.720: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c" in namespace "projected-8193" to be "Succeeded or Failed"
    Sep  4 17:58:02.726: INFO: Pod "downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.844764ms
    Sep  4 17:58:04.734: INFO: Pod "downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013498613s
    Sep  4 17:58:06.734: INFO: Pod "downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013239036s
    STEP: Saw pod success 09/04/23 17:58:06.734
    Sep  4 17:58:06.735: INFO: Pod "downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c" satisfied condition "Succeeded or Failed"
    Sep  4 17:58:06.738: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c container client-container: <nil>
    STEP: delete the pod 09/04/23 17:58:06.745
    Sep  4 17:58:06.763: INFO: Waiting for pod downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c to disappear
    Sep  4 17:58:06.774: INFO: Pod downwardapi-volume-77894434-ad0c-4556-9003-f8a997ca9a4c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:58:06.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8193" for this suite. 09/04/23 17:58:06.784
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:58:06.797
Sep  4 17:58:06.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename job 09/04/23 17:58:06.799
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:58:06.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:58:06.826
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 09/04/23 17:58:06.834
STEP: Patching the Job 09/04/23 17:58:06.842
STEP: Watching for Job to be patched 09/04/23 17:58:06.864
Sep  4 17:58:06.866: INFO: Event ADDED observed for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking:]
Sep  4 17:58:06.866: INFO: Event MODIFIED observed for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking:]
Sep  4 17:58:06.867: INFO: Event MODIFIED found for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 09/04/23 17:58:06.867
STEP: Watching for Job to be updated 09/04/23 17:58:06.878
Sep  4 17:58:06.880: INFO: Event MODIFIED found for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  4 17:58:06.880: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 09/04/23 17:58:06.88
Sep  4 17:58:06.886: INFO: Job: e2e-4dbp7 as labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7]
STEP: Waiting for job to complete 09/04/23 17:58:06.886
STEP: Delete a job collection with a labelselector 09/04/23 17:58:14.892
STEP: Watching for Job to be deleted 09/04/23 17:58:14.906
Sep  4 17:58:14.908: INFO: Event MODIFIED observed for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  4 17:58:14.909: INFO: Event MODIFIED observed for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  4 17:58:14.909: INFO: Event MODIFIED observed for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  4 17:58:14.909: INFO: Event MODIFIED observed for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  4 17:58:14.909: INFO: Event MODIFIED observed for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  4 17:58:14.910: INFO: Event DELETED found for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 09/04/23 17:58:14.91
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  4 17:58:14.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-816" for this suite. 09/04/23 17:58:14.916
------------------------------
â€¢ [SLOW TEST] [8.129 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:58:06.797
    Sep  4 17:58:06.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename job 09/04/23 17:58:06.799
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:58:06.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:58:06.826
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 09/04/23 17:58:06.834
    STEP: Patching the Job 09/04/23 17:58:06.842
    STEP: Watching for Job to be patched 09/04/23 17:58:06.864
    Sep  4 17:58:06.866: INFO: Event ADDED observed for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking:]
    Sep  4 17:58:06.866: INFO: Event MODIFIED observed for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking:]
    Sep  4 17:58:06.867: INFO: Event MODIFIED found for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 09/04/23 17:58:06.867
    STEP: Watching for Job to be updated 09/04/23 17:58:06.878
    Sep  4 17:58:06.880: INFO: Event MODIFIED found for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  4 17:58:06.880: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 09/04/23 17:58:06.88
    Sep  4 17:58:06.886: INFO: Job: e2e-4dbp7 as labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7]
    STEP: Waiting for job to complete 09/04/23 17:58:06.886
    STEP: Delete a job collection with a labelselector 09/04/23 17:58:14.892
    STEP: Watching for Job to be deleted 09/04/23 17:58:14.906
    Sep  4 17:58:14.908: INFO: Event MODIFIED observed for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  4 17:58:14.909: INFO: Event MODIFIED observed for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  4 17:58:14.909: INFO: Event MODIFIED observed for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  4 17:58:14.909: INFO: Event MODIFIED observed for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  4 17:58:14.909: INFO: Event MODIFIED observed for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  4 17:58:14.910: INFO: Event DELETED found for Job e2e-4dbp7 in namespace job-816 with labels: map[e2e-4dbp7:patched e2e-job-label:e2e-4dbp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 09/04/23 17:58:14.91
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:58:14.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-816" for this suite. 09/04/23 17:58:14.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:58:14.927
Sep  4 17:58:14.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 17:58:14.928
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:58:14.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:58:14.984
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 09/04/23 17:58:14.988
Sep  4 17:58:15.001: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa" in namespace "downward-api-5115" to be "Succeeded or Failed"
Sep  4 17:58:15.015: INFO: Pod "downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa": Phase="Pending", Reason="", readiness=false. Elapsed: 13.723152ms
Sep  4 17:58:17.020: INFO: Pod "downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018953916s
Sep  4 17:58:19.019: INFO: Pod "downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017820316s
STEP: Saw pod success 09/04/23 17:58:19.02
Sep  4 17:58:19.020: INFO: Pod "downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa" satisfied condition "Succeeded or Failed"
Sep  4 17:58:19.023: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa container client-container: <nil>
STEP: delete the pod 09/04/23 17:58:19.034
Sep  4 17:58:19.052: INFO: Waiting for pod downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa to disappear
Sep  4 17:58:19.055: INFO: Pod downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 17:58:19.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5115" for this suite. 09/04/23 17:58:19.063
------------------------------
â€¢ [4.144 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:58:14.927
    Sep  4 17:58:14.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 17:58:14.928
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:58:14.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:58:14.984
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 09/04/23 17:58:14.988
    Sep  4 17:58:15.001: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa" in namespace "downward-api-5115" to be "Succeeded or Failed"
    Sep  4 17:58:15.015: INFO: Pod "downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa": Phase="Pending", Reason="", readiness=false. Elapsed: 13.723152ms
    Sep  4 17:58:17.020: INFO: Pod "downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018953916s
    Sep  4 17:58:19.019: INFO: Pod "downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017820316s
    STEP: Saw pod success 09/04/23 17:58:19.02
    Sep  4 17:58:19.020: INFO: Pod "downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa" satisfied condition "Succeeded or Failed"
    Sep  4 17:58:19.023: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa container client-container: <nil>
    STEP: delete the pod 09/04/23 17:58:19.034
    Sep  4 17:58:19.052: INFO: Waiting for pod downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa to disappear
    Sep  4 17:58:19.055: INFO: Pod downwardapi-volume-96a9b646-45f8-4db9-a38c-3c428ef6cffa no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:58:19.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5115" for this suite. 09/04/23 17:58:19.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:58:19.079
Sep  4 17:58:19.079: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename replication-controller 09/04/23 17:58:19.08
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:58:19.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:58:19.11
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191 09/04/23 17:58:19.113
Sep  4 17:58:19.127: INFO: Pod name my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191: Found 0 pods out of 1
Sep  4 17:58:24.135: INFO: Pod name my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191: Found 1 pods out of 1
Sep  4 17:58:24.135: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191" are running
Sep  4 17:58:24.135: INFO: Waiting up to 5m0s for pod "my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191-9996v" in namespace "replication-controller-8317" to be "running"
Sep  4 17:58:24.138: INFO: Pod "my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191-9996v": Phase="Running", Reason="", readiness=true. Elapsed: 2.603591ms
Sep  4 17:58:24.138: INFO: Pod "my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191-9996v" satisfied condition "running"
Sep  4 17:58:24.138: INFO: Pod "my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191-9996v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 17:58:19 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 17:58:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 17:58:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 17:58:19 +0000 UTC Reason: Message:}])
Sep  4 17:58:24.138: INFO: Trying to dial the pod
Sep  4 17:58:29.157: INFO: Controller my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191: Got expected result from replica 1 [my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191-9996v]: "my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191-9996v", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  4 17:58:29.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8317" for this suite. 09/04/23 17:58:29.161
------------------------------
â€¢ [SLOW TEST] [10.114 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:58:19.079
    Sep  4 17:58:19.079: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename replication-controller 09/04/23 17:58:19.08
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:58:19.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:58:19.11
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191 09/04/23 17:58:19.113
    Sep  4 17:58:19.127: INFO: Pod name my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191: Found 0 pods out of 1
    Sep  4 17:58:24.135: INFO: Pod name my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191: Found 1 pods out of 1
    Sep  4 17:58:24.135: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191" are running
    Sep  4 17:58:24.135: INFO: Waiting up to 5m0s for pod "my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191-9996v" in namespace "replication-controller-8317" to be "running"
    Sep  4 17:58:24.138: INFO: Pod "my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191-9996v": Phase="Running", Reason="", readiness=true. Elapsed: 2.603591ms
    Sep  4 17:58:24.138: INFO: Pod "my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191-9996v" satisfied condition "running"
    Sep  4 17:58:24.138: INFO: Pod "my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191-9996v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 17:58:19 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 17:58:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 17:58:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 17:58:19 +0000 UTC Reason: Message:}])
    Sep  4 17:58:24.138: INFO: Trying to dial the pod
    Sep  4 17:58:29.157: INFO: Controller my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191: Got expected result from replica 1 [my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191-9996v]: "my-hostname-basic-0ba59186-32a7-41b4-8573-ff7733e93191-9996v", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:58:29.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8317" for this suite. 09/04/23 17:58:29.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:58:29.203
Sep  4 17:58:29.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename statefulset 09/04/23 17:58:29.204
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:58:29.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:58:29.229
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-78 09/04/23 17:58:29.232
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Sep  4 17:58:29.256: INFO: Found 0 stateful pods, waiting for 1
Sep  4 17:58:39.264: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 09/04/23 17:58:39.271
W0904 17:58:39.283043      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  4 17:58:39.294: INFO: Found 1 stateful pods, waiting for 2
Sep  4 17:58:49.303: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 17:58:49.303: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 09/04/23 17:58:49.311
STEP: Delete all of the StatefulSets 09/04/23 17:58:49.321
STEP: Verify that StatefulSets have been deleted 09/04/23 17:58:49.332
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 17:58:49.337: INFO: Deleting all statefulset in ns statefulset-78
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 17:58:49.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-78" for this suite. 09/04/23 17:58:49.358
------------------------------
â€¢ [SLOW TEST] [20.194 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:58:29.203
    Sep  4 17:58:29.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename statefulset 09/04/23 17:58:29.204
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:58:29.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:58:29.229
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-78 09/04/23 17:58:29.232
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Sep  4 17:58:29.256: INFO: Found 0 stateful pods, waiting for 1
    Sep  4 17:58:39.264: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 09/04/23 17:58:39.271
    W0904 17:58:39.283043      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  4 17:58:39.294: INFO: Found 1 stateful pods, waiting for 2
    Sep  4 17:58:49.303: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 17:58:49.303: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 09/04/23 17:58:49.311
    STEP: Delete all of the StatefulSets 09/04/23 17:58:49.321
    STEP: Verify that StatefulSets have been deleted 09/04/23 17:58:49.332
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 17:58:49.337: INFO: Deleting all statefulset in ns statefulset-78
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:58:49.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-78" for this suite. 09/04/23 17:58:49.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:58:49.438
Sep  4 17:58:49.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename var-expansion 09/04/23 17:58:49.439
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:58:49.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:58:49.463
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 09/04/23 17:58:49.466
Sep  4 17:58:49.479: INFO: Waiting up to 5m0s for pod "var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309" in namespace "var-expansion-6183" to be "Succeeded or Failed"
Sep  4 17:58:49.491: INFO: Pod "var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309": Phase="Pending", Reason="", readiness=false. Elapsed: 12.194991ms
Sep  4 17:58:51.495: INFO: Pod "var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016459025s
Sep  4 17:58:53.496: INFO: Pod "var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017130796s
STEP: Saw pod success 09/04/23 17:58:53.496
Sep  4 17:58:53.497: INFO: Pod "var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309" satisfied condition "Succeeded or Failed"
Sep  4 17:58:53.502: INFO: Trying to get logs from node tenant-000001 pod var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309 container dapi-container: <nil>
STEP: delete the pod 09/04/23 17:58:53.515
Sep  4 17:58:53.531: INFO: Waiting for pod var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309 to disappear
Sep  4 17:58:53.534: INFO: Pod var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 17:58:53.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6183" for this suite. 09/04/23 17:58:53.538
------------------------------
â€¢ [4.109 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:58:49.438
    Sep  4 17:58:49.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename var-expansion 09/04/23 17:58:49.439
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:58:49.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:58:49.463
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 09/04/23 17:58:49.466
    Sep  4 17:58:49.479: INFO: Waiting up to 5m0s for pod "var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309" in namespace "var-expansion-6183" to be "Succeeded or Failed"
    Sep  4 17:58:49.491: INFO: Pod "var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309": Phase="Pending", Reason="", readiness=false. Elapsed: 12.194991ms
    Sep  4 17:58:51.495: INFO: Pod "var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016459025s
    Sep  4 17:58:53.496: INFO: Pod "var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017130796s
    STEP: Saw pod success 09/04/23 17:58:53.496
    Sep  4 17:58:53.497: INFO: Pod "var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309" satisfied condition "Succeeded or Failed"
    Sep  4 17:58:53.502: INFO: Trying to get logs from node tenant-000001 pod var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309 container dapi-container: <nil>
    STEP: delete the pod 09/04/23 17:58:53.515
    Sep  4 17:58:53.531: INFO: Waiting for pod var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309 to disappear
    Sep  4 17:58:53.534: INFO: Pod var-expansion-f3efa962-3804-47c0-bf17-4f2231ce6309 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:58:53.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6183" for this suite. 09/04/23 17:58:53.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:58:53.547
Sep  4 17:58:53.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename sched-preemption 09/04/23 17:58:53.548
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:58:53.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:58:53.569
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  4 17:58:53.589: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  4 17:59:53.610: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:59:53.616
Sep  4 17:59:53.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename sched-preemption-path 09/04/23 17:59:53.617
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:59:53.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:59:53.643
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Sep  4 17:59:53.664: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Sep  4 17:59:53.670: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Sep  4 17:59:53.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 17:59:53.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7017" for this suite. 09/04/23 17:59:53.763
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4102" for this suite. 09/04/23 17:59:53.774
------------------------------
â€¢ [SLOW TEST] [60.240 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:58:53.547
    Sep  4 17:58:53.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename sched-preemption 09/04/23 17:58:53.548
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:58:53.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:58:53.569
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  4 17:58:53.589: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  4 17:59:53.610: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:59:53.616
    Sep  4 17:59:53.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename sched-preemption-path 09/04/23 17:59:53.617
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:59:53.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:59:53.643
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Sep  4 17:59:53.664: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Sep  4 17:59:53.670: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:59:53.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:59:53.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7017" for this suite. 09/04/23 17:59:53.763
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4102" for this suite. 09/04/23 17:59:53.774
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:59:53.792
Sep  4 17:59:53.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-runtime 09/04/23 17:59:53.794
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:59:53.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:59:53.817
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 09/04/23 17:59:53.819
STEP: wait for the container to reach Succeeded 09/04/23 17:59:53.835
STEP: get the container status 09/04/23 17:59:57.884
STEP: the container should be terminated 09/04/23 17:59:57.897
STEP: the termination message should be set 09/04/23 17:59:57.897
Sep  4 17:59:57.897: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/04/23 17:59:57.897
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  4 17:59:57.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4041" for this suite. 09/04/23 17:59:57.926
------------------------------
â€¢ [4.141 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:59:53.792
    Sep  4 17:59:53.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-runtime 09/04/23 17:59:53.794
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:59:53.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:59:53.817
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 09/04/23 17:59:53.819
    STEP: wait for the container to reach Succeeded 09/04/23 17:59:53.835
    STEP: get the container status 09/04/23 17:59:57.884
    STEP: the container should be terminated 09/04/23 17:59:57.897
    STEP: the termination message should be set 09/04/23 17:59:57.897
    Sep  4 17:59:57.897: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/04/23 17:59:57.897
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:59:57.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4041" for this suite. 09/04/23 17:59:57.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:59:57.933
Sep  4 17:59:57.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename runtimeclass 09/04/23 17:59:57.935
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:59:57.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:59:57.955
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  4 17:59:57.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7009" for this suite. 09/04/23 17:59:57.979
------------------------------
â€¢ [0.053 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:59:57.933
    Sep  4 17:59:57.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename runtimeclass 09/04/23 17:59:57.935
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:59:57.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:59:57.955
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  4 17:59:57.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7009" for this suite. 09/04/23 17:59:57.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 17:59:57.994
Sep  4 17:59:57.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 17:59:57.995
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:59:58.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:59:58.021
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/04/23 17:59:58.023
Sep  4 17:59:58.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/04/23 18:00:05.015
Sep  4 18:00:05.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 18:00:06.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:00:14.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2521" for this suite. 09/04/23 18:00:14.582
------------------------------
â€¢ [SLOW TEST] [16.596 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 17:59:57.994
    Sep  4 17:59:57.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 17:59:57.995
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 17:59:58.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 17:59:58.021
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/04/23 17:59:58.023
    Sep  4 17:59:58.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/04/23 18:00:05.015
    Sep  4 18:00:05.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 18:00:06.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:00:14.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2521" for this suite. 09/04/23 18:00:14.582
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:00:14.594
Sep  4 18:00:14.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename subpath 09/04/23 18:00:14.596
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:00:14.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:00:14.62
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/04/23 18:00:14.624
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-smts 09/04/23 18:00:14.636
STEP: Creating a pod to test atomic-volume-subpath 09/04/23 18:00:14.636
Sep  4 18:00:14.654: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-smts" in namespace "subpath-8135" to be "Succeeded or Failed"
Sep  4 18:00:14.663: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Pending", Reason="", readiness=false. Elapsed: 9.063208ms
Sep  4 18:00:16.667: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 2.013460451s
Sep  4 18:00:18.669: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 4.015603521s
Sep  4 18:00:20.669: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 6.015511742s
Sep  4 18:00:22.667: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 8.013311524s
Sep  4 18:00:24.668: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 10.01392998s
Sep  4 18:00:26.669: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 12.015306435s
Sep  4 18:00:28.667: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 14.012862539s
Sep  4 18:00:30.668: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 16.013846969s
Sep  4 18:00:32.669: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 18.014958714s
Sep  4 18:00:34.670: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 20.015925877s
Sep  4 18:00:36.667: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=false. Elapsed: 22.012975003s
Sep  4 18:00:38.668: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014257532s
STEP: Saw pod success 09/04/23 18:00:38.668
Sep  4 18:00:38.669: INFO: Pod "pod-subpath-test-configmap-smts" satisfied condition "Succeeded or Failed"
Sep  4 18:00:38.674: INFO: Trying to get logs from node tenant-000001 pod pod-subpath-test-configmap-smts container test-container-subpath-configmap-smts: <nil>
STEP: delete the pod 09/04/23 18:00:38.712
Sep  4 18:00:38.730: INFO: Waiting for pod pod-subpath-test-configmap-smts to disappear
Sep  4 18:00:38.733: INFO: Pod pod-subpath-test-configmap-smts no longer exists
STEP: Deleting pod pod-subpath-test-configmap-smts 09/04/23 18:00:38.733
Sep  4 18:00:38.733: INFO: Deleting pod "pod-subpath-test-configmap-smts" in namespace "subpath-8135"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  4 18:00:38.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8135" for this suite. 09/04/23 18:00:38.742
------------------------------
â€¢ [SLOW TEST] [24.156 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:00:14.594
    Sep  4 18:00:14.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename subpath 09/04/23 18:00:14.596
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:00:14.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:00:14.62
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/04/23 18:00:14.624
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-smts 09/04/23 18:00:14.636
    STEP: Creating a pod to test atomic-volume-subpath 09/04/23 18:00:14.636
    Sep  4 18:00:14.654: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-smts" in namespace "subpath-8135" to be "Succeeded or Failed"
    Sep  4 18:00:14.663: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Pending", Reason="", readiness=false. Elapsed: 9.063208ms
    Sep  4 18:00:16.667: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 2.013460451s
    Sep  4 18:00:18.669: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 4.015603521s
    Sep  4 18:00:20.669: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 6.015511742s
    Sep  4 18:00:22.667: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 8.013311524s
    Sep  4 18:00:24.668: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 10.01392998s
    Sep  4 18:00:26.669: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 12.015306435s
    Sep  4 18:00:28.667: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 14.012862539s
    Sep  4 18:00:30.668: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 16.013846969s
    Sep  4 18:00:32.669: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 18.014958714s
    Sep  4 18:00:34.670: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=true. Elapsed: 20.015925877s
    Sep  4 18:00:36.667: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Running", Reason="", readiness=false. Elapsed: 22.012975003s
    Sep  4 18:00:38.668: INFO: Pod "pod-subpath-test-configmap-smts": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014257532s
    STEP: Saw pod success 09/04/23 18:00:38.668
    Sep  4 18:00:38.669: INFO: Pod "pod-subpath-test-configmap-smts" satisfied condition "Succeeded or Failed"
    Sep  4 18:00:38.674: INFO: Trying to get logs from node tenant-000001 pod pod-subpath-test-configmap-smts container test-container-subpath-configmap-smts: <nil>
    STEP: delete the pod 09/04/23 18:00:38.712
    Sep  4 18:00:38.730: INFO: Waiting for pod pod-subpath-test-configmap-smts to disappear
    Sep  4 18:00:38.733: INFO: Pod pod-subpath-test-configmap-smts no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-smts 09/04/23 18:00:38.733
    Sep  4 18:00:38.733: INFO: Deleting pod "pod-subpath-test-configmap-smts" in namespace "subpath-8135"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:00:38.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8135" for this suite. 09/04/23 18:00:38.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:00:38.756
Sep  4 18:00:38.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 18:00:38.757
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:00:38.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:00:38.781
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 09/04/23 18:00:38.783
Sep  4 18:00:38.790: INFO: Waiting up to 5m0s for pod "downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0" in namespace "downward-api-864" to be "Succeeded or Failed"
Sep  4 18:00:38.793: INFO: Pod "downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.768284ms
Sep  4 18:00:40.800: INFO: Pod "downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009116631s
Sep  4 18:00:42.811: INFO: Pod "downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020346804s
STEP: Saw pod success 09/04/23 18:00:42.811
Sep  4 18:00:42.811: INFO: Pod "downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0" satisfied condition "Succeeded or Failed"
Sep  4 18:00:42.826: INFO: Trying to get logs from node tenant-000001 pod downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0 container dapi-container: <nil>
STEP: delete the pod 09/04/23 18:00:42.87
Sep  4 18:00:42.887: INFO: Waiting for pod downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0 to disappear
Sep  4 18:00:42.890: INFO: Pod downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  4 18:00:42.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-864" for this suite. 09/04/23 18:00:42.893
------------------------------
â€¢ [4.148 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:00:38.756
    Sep  4 18:00:38.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 18:00:38.757
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:00:38.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:00:38.781
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 09/04/23 18:00:38.783
    Sep  4 18:00:38.790: INFO: Waiting up to 5m0s for pod "downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0" in namespace "downward-api-864" to be "Succeeded or Failed"
    Sep  4 18:00:38.793: INFO: Pod "downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.768284ms
    Sep  4 18:00:40.800: INFO: Pod "downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009116631s
    Sep  4 18:00:42.811: INFO: Pod "downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020346804s
    STEP: Saw pod success 09/04/23 18:00:42.811
    Sep  4 18:00:42.811: INFO: Pod "downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0" satisfied condition "Succeeded or Failed"
    Sep  4 18:00:42.826: INFO: Trying to get logs from node tenant-000001 pod downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0 container dapi-container: <nil>
    STEP: delete the pod 09/04/23 18:00:42.87
    Sep  4 18:00:42.887: INFO: Waiting for pod downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0 to disappear
    Sep  4 18:00:42.890: INFO: Pod downward-api-d4a6465e-8eb0-4760-a93c-cde4172c88c0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:00:42.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-864" for this suite. 09/04/23 18:00:42.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:00:42.907
Sep  4 18:00:42.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-probe 09/04/23 18:00:42.909
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:00:42.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:00:42.945
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-d70c7fa7-1298-492c-8922-cbee899cf3cb in namespace container-probe-8831 09/04/23 18:00:42.948
Sep  4 18:00:42.960: INFO: Waiting up to 5m0s for pod "test-webserver-d70c7fa7-1298-492c-8922-cbee899cf3cb" in namespace "container-probe-8831" to be "not pending"
Sep  4 18:00:42.977: INFO: Pod "test-webserver-d70c7fa7-1298-492c-8922-cbee899cf3cb": Phase="Pending", Reason="", readiness=false. Elapsed: 17.029679ms
Sep  4 18:00:44.982: INFO: Pod "test-webserver-d70c7fa7-1298-492c-8922-cbee899cf3cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.02175534s
Sep  4 18:00:44.982: INFO: Pod "test-webserver-d70c7fa7-1298-492c-8922-cbee899cf3cb" satisfied condition "not pending"
Sep  4 18:00:44.982: INFO: Started pod test-webserver-d70c7fa7-1298-492c-8922-cbee899cf3cb in namespace container-probe-8831
STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 18:00:44.982
Sep  4 18:00:44.986: INFO: Initial restart count of pod test-webserver-d70c7fa7-1298-492c-8922-cbee899cf3cb is 0
STEP: deleting the pod 09/04/23 18:04:45.625
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 18:04:45.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8831" for this suite. 09/04/23 18:04:45.694
------------------------------
â€¢ [SLOW TEST] [242.825 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:00:42.907
    Sep  4 18:00:42.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-probe 09/04/23 18:00:42.909
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:00:42.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:00:42.945
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-d70c7fa7-1298-492c-8922-cbee899cf3cb in namespace container-probe-8831 09/04/23 18:00:42.948
    Sep  4 18:00:42.960: INFO: Waiting up to 5m0s for pod "test-webserver-d70c7fa7-1298-492c-8922-cbee899cf3cb" in namespace "container-probe-8831" to be "not pending"
    Sep  4 18:00:42.977: INFO: Pod "test-webserver-d70c7fa7-1298-492c-8922-cbee899cf3cb": Phase="Pending", Reason="", readiness=false. Elapsed: 17.029679ms
    Sep  4 18:00:44.982: INFO: Pod "test-webserver-d70c7fa7-1298-492c-8922-cbee899cf3cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.02175534s
    Sep  4 18:00:44.982: INFO: Pod "test-webserver-d70c7fa7-1298-492c-8922-cbee899cf3cb" satisfied condition "not pending"
    Sep  4 18:00:44.982: INFO: Started pod test-webserver-d70c7fa7-1298-492c-8922-cbee899cf3cb in namespace container-probe-8831
    STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 18:00:44.982
    Sep  4 18:00:44.986: INFO: Initial restart count of pod test-webserver-d70c7fa7-1298-492c-8922-cbee899cf3cb is 0
    STEP: deleting the pod 09/04/23 18:04:45.625
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:04:45.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8831" for this suite. 09/04/23 18:04:45.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:04:45.733
Sep  4 18:04:45.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 18:04:45.734
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:04:45.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:04:45.775
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Sep  4 18:04:45.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:04:46.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1416" for this suite. 09/04/23 18:04:46.838
------------------------------
â€¢ [1.113 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:04:45.733
    Sep  4 18:04:45.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 18:04:45.734
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:04:45.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:04:45.775
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Sep  4 18:04:45.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:04:46.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1416" for this suite. 09/04/23 18:04:46.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:04:46.846
Sep  4 18:04:46.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 18:04:46.847
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:04:46.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:04:46.868
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 09/04/23 18:04:46.871
Sep  4 18:04:46.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-3432 create -f -'
Sep  4 18:04:47.587: INFO: stderr: ""
Sep  4 18:04:47.587: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/04/23 18:04:47.587
Sep  4 18:04:48.594: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 18:04:48.594: INFO: Found 1 / 1
Sep  4 18:04:48.594: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 09/04/23 18:04:48.594
Sep  4 18:04:48.597: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 18:04:48.597: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  4 18:04:48.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-3432 patch pod agnhost-primary-s2clh -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  4 18:04:48.711: INFO: stderr: ""
Sep  4 18:04:48.711: INFO: stdout: "pod/agnhost-primary-s2clh patched\n"
STEP: checking annotations 09/04/23 18:04:48.711
Sep  4 18:04:48.714: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 18:04:48.714: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 18:04:48.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3432" for this suite. 09/04/23 18:04:48.717
------------------------------
â€¢ [1.880 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:04:46.846
    Sep  4 18:04:46.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 18:04:46.847
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:04:46.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:04:46.868
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 09/04/23 18:04:46.871
    Sep  4 18:04:46.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-3432 create -f -'
    Sep  4 18:04:47.587: INFO: stderr: ""
    Sep  4 18:04:47.587: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/04/23 18:04:47.587
    Sep  4 18:04:48.594: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 18:04:48.594: INFO: Found 1 / 1
    Sep  4 18:04:48.594: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 09/04/23 18:04:48.594
    Sep  4 18:04:48.597: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 18:04:48.597: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  4 18:04:48.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-3432 patch pod agnhost-primary-s2clh -p {"metadata":{"annotations":{"x":"y"}}}'
    Sep  4 18:04:48.711: INFO: stderr: ""
    Sep  4 18:04:48.711: INFO: stdout: "pod/agnhost-primary-s2clh patched\n"
    STEP: checking annotations 09/04/23 18:04:48.711
    Sep  4 18:04:48.714: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 18:04:48.714: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:04:48.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3432" for this suite. 09/04/23 18:04:48.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:04:48.736
Sep  4 18:04:48.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename dns 09/04/23 18:04:48.737
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:04:48.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:04:48.765
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 09/04/23 18:04:48.768
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6859.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6859.svc.cluster.local; sleep 1; done
 09/04/23 18:04:48.776
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6859.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local; sleep 1; done
 09/04/23 18:04:48.776
STEP: creating a pod to probe DNS 09/04/23 18:04:48.776
STEP: submitting the pod to kubernetes 09/04/23 18:04:48.776
Sep  4 18:04:48.791: INFO: Waiting up to 15m0s for pod "dns-test-ce77d96c-78f0-4377-926e-1e5d26997f2e" in namespace "dns-6859" to be "running"
Sep  4 18:04:48.799: INFO: Pod "dns-test-ce77d96c-78f0-4377-926e-1e5d26997f2e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.3728ms
Sep  4 18:04:50.804: INFO: Pod "dns-test-ce77d96c-78f0-4377-926e-1e5d26997f2e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013145081s
Sep  4 18:04:50.804: INFO: Pod "dns-test-ce77d96c-78f0-4377-926e-1e5d26997f2e" satisfied condition "running"
STEP: retrieving the pod 09/04/23 18:04:50.804
STEP: looking for the results for each expected name from probers 09/04/23 18:04:50.808
Sep  4 18:04:50.821: INFO: DNS probes using dns-test-ce77d96c-78f0-4377-926e-1e5d26997f2e succeeded

STEP: deleting the pod 09/04/23 18:04:50.822
STEP: changing the externalName to bar.example.com 09/04/23 18:04:50.861
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6859.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6859.svc.cluster.local; sleep 1; done
 09/04/23 18:04:50.889
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6859.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local; sleep 1; done
 09/04/23 18:04:50.889
STEP: creating a second pod to probe DNS 09/04/23 18:04:50.89
STEP: submitting the pod to kubernetes 09/04/23 18:04:50.89
Sep  4 18:04:50.903: INFO: Waiting up to 15m0s for pod "dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c" in namespace "dns-6859" to be "running"
Sep  4 18:04:50.914: INFO: Pod "dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.995957ms
Sep  4 18:04:52.918: INFO: Pod "dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c": Phase="Running", Reason="", readiness=true. Elapsed: 2.014510263s
Sep  4 18:04:52.918: INFO: Pod "dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c" satisfied condition "running"
STEP: retrieving the pod 09/04/23 18:04:52.918
STEP: looking for the results for each expected name from probers 09/04/23 18:04:52.922
Sep  4 18:04:52.936: INFO: File jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local from pod  dns-6859/dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  4 18:04:52.936: INFO: Lookups using dns-6859/dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c failed for: [jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local]

Sep  4 18:04:57.951: INFO: File jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local from pod  dns-6859/dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  4 18:04:57.952: INFO: Lookups using dns-6859/dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c failed for: [jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local]

Sep  4 18:05:02.948: INFO: File jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local from pod  dns-6859/dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  4 18:05:02.948: INFO: Lookups using dns-6859/dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c failed for: [jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local]

Sep  4 18:05:07.951: INFO: DNS probes using dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c succeeded

STEP: deleting the pod 09/04/23 18:05:07.951
STEP: changing the service to type=ClusterIP 09/04/23 18:05:08
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6859.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6859.svc.cluster.local; sleep 1; done
 09/04/23 18:05:08.046
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6859.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local; sleep 1; done
 09/04/23 18:05:08.05
STEP: creating a third pod to probe DNS 09/04/23 18:05:08.05
STEP: submitting the pod to kubernetes 09/04/23 18:05:08.061
Sep  4 18:05:08.071: INFO: Waiting up to 15m0s for pod "dns-test-8918d468-5a53-48de-ae37-c64e216fd6cf" in namespace "dns-6859" to be "running"
Sep  4 18:05:08.088: INFO: Pod "dns-test-8918d468-5a53-48de-ae37-c64e216fd6cf": Phase="Pending", Reason="", readiness=false. Elapsed: 16.533309ms
Sep  4 18:05:10.092: INFO: Pod "dns-test-8918d468-5a53-48de-ae37-c64e216fd6cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.020904164s
Sep  4 18:05:10.092: INFO: Pod "dns-test-8918d468-5a53-48de-ae37-c64e216fd6cf" satisfied condition "running"
STEP: retrieving the pod 09/04/23 18:05:10.092
STEP: looking for the results for each expected name from probers 09/04/23 18:05:10.096
Sep  4 18:05:10.111: INFO: DNS probes using dns-test-8918d468-5a53-48de-ae37-c64e216fd6cf succeeded

STEP: deleting the pod 09/04/23 18:05:10.112
STEP: deleting the test externalName service 09/04/23 18:05:10.132
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 18:05:10.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6859" for this suite. 09/04/23 18:05:10.176
------------------------------
â€¢ [SLOW TEST] [21.454 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:04:48.736
    Sep  4 18:04:48.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename dns 09/04/23 18:04:48.737
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:04:48.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:04:48.765
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 09/04/23 18:04:48.768
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6859.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6859.svc.cluster.local; sleep 1; done
     09/04/23 18:04:48.776
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6859.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local; sleep 1; done
     09/04/23 18:04:48.776
    STEP: creating a pod to probe DNS 09/04/23 18:04:48.776
    STEP: submitting the pod to kubernetes 09/04/23 18:04:48.776
    Sep  4 18:04:48.791: INFO: Waiting up to 15m0s for pod "dns-test-ce77d96c-78f0-4377-926e-1e5d26997f2e" in namespace "dns-6859" to be "running"
    Sep  4 18:04:48.799: INFO: Pod "dns-test-ce77d96c-78f0-4377-926e-1e5d26997f2e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.3728ms
    Sep  4 18:04:50.804: INFO: Pod "dns-test-ce77d96c-78f0-4377-926e-1e5d26997f2e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013145081s
    Sep  4 18:04:50.804: INFO: Pod "dns-test-ce77d96c-78f0-4377-926e-1e5d26997f2e" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 18:04:50.804
    STEP: looking for the results for each expected name from probers 09/04/23 18:04:50.808
    Sep  4 18:04:50.821: INFO: DNS probes using dns-test-ce77d96c-78f0-4377-926e-1e5d26997f2e succeeded

    STEP: deleting the pod 09/04/23 18:04:50.822
    STEP: changing the externalName to bar.example.com 09/04/23 18:04:50.861
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6859.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6859.svc.cluster.local; sleep 1; done
     09/04/23 18:04:50.889
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6859.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local; sleep 1; done
     09/04/23 18:04:50.889
    STEP: creating a second pod to probe DNS 09/04/23 18:04:50.89
    STEP: submitting the pod to kubernetes 09/04/23 18:04:50.89
    Sep  4 18:04:50.903: INFO: Waiting up to 15m0s for pod "dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c" in namespace "dns-6859" to be "running"
    Sep  4 18:04:50.914: INFO: Pod "dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.995957ms
    Sep  4 18:04:52.918: INFO: Pod "dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c": Phase="Running", Reason="", readiness=true. Elapsed: 2.014510263s
    Sep  4 18:04:52.918: INFO: Pod "dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 18:04:52.918
    STEP: looking for the results for each expected name from probers 09/04/23 18:04:52.922
    Sep  4 18:04:52.936: INFO: File jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local from pod  dns-6859/dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  4 18:04:52.936: INFO: Lookups using dns-6859/dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c failed for: [jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local]

    Sep  4 18:04:57.951: INFO: File jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local from pod  dns-6859/dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  4 18:04:57.952: INFO: Lookups using dns-6859/dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c failed for: [jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local]

    Sep  4 18:05:02.948: INFO: File jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local from pod  dns-6859/dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  4 18:05:02.948: INFO: Lookups using dns-6859/dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c failed for: [jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local]

    Sep  4 18:05:07.951: INFO: DNS probes using dns-test-41c78359-dbd3-437c-a23f-2ad19dc5559c succeeded

    STEP: deleting the pod 09/04/23 18:05:07.951
    STEP: changing the service to type=ClusterIP 09/04/23 18:05:08
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6859.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6859.svc.cluster.local; sleep 1; done
     09/04/23 18:05:08.046
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6859.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6859.svc.cluster.local; sleep 1; done
     09/04/23 18:05:08.05
    STEP: creating a third pod to probe DNS 09/04/23 18:05:08.05
    STEP: submitting the pod to kubernetes 09/04/23 18:05:08.061
    Sep  4 18:05:08.071: INFO: Waiting up to 15m0s for pod "dns-test-8918d468-5a53-48de-ae37-c64e216fd6cf" in namespace "dns-6859" to be "running"
    Sep  4 18:05:08.088: INFO: Pod "dns-test-8918d468-5a53-48de-ae37-c64e216fd6cf": Phase="Pending", Reason="", readiness=false. Elapsed: 16.533309ms
    Sep  4 18:05:10.092: INFO: Pod "dns-test-8918d468-5a53-48de-ae37-c64e216fd6cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.020904164s
    Sep  4 18:05:10.092: INFO: Pod "dns-test-8918d468-5a53-48de-ae37-c64e216fd6cf" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 18:05:10.092
    STEP: looking for the results for each expected name from probers 09/04/23 18:05:10.096
    Sep  4 18:05:10.111: INFO: DNS probes using dns-test-8918d468-5a53-48de-ae37-c64e216fd6cf succeeded

    STEP: deleting the pod 09/04/23 18:05:10.112
    STEP: deleting the test externalName service 09/04/23 18:05:10.132
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:05:10.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6859" for this suite. 09/04/23 18:05:10.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:05:10.199
Sep  4 18:05:10.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 18:05:10.201
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:10.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:10.244
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-2323 09/04/23 18:05:10.252
STEP: creating replication controller nodeport-test in namespace services-2323 09/04/23 18:05:10.275
I0904 18:05:10.291668      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2323, replica count: 2
I0904 18:05:13.342965      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 18:05:13.343: INFO: Creating new exec pod
Sep  4 18:05:13.354: INFO: Waiting up to 5m0s for pod "execpod26tc9" in namespace "services-2323" to be "running"
Sep  4 18:05:13.358: INFO: Pod "execpod26tc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.343855ms
Sep  4 18:05:15.363: INFO: Pod "execpod26tc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009070877s
Sep  4 18:05:15.363: INFO: Pod "execpod26tc9" satisfied condition "running"
Sep  4 18:05:16.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-2323 exec execpod26tc9 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Sep  4 18:05:16.568: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep  4 18:05:16.568: INFO: stdout: ""
Sep  4 18:05:16.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-2323 exec execpod26tc9 -- /bin/sh -x -c nc -v -z -w 2 10.96.90.95 80'
Sep  4 18:05:16.748: INFO: stderr: "+ nc -v -z -w 2 10.96.90.95 80\nConnection to 10.96.90.95 80 port [tcp/http] succeeded!\n"
Sep  4 18:05:16.748: INFO: stdout: ""
Sep  4 18:05:16.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-2323 exec execpod26tc9 -- /bin/sh -x -c nc -v -z -w 2 10.225.0.5 31327'
Sep  4 18:05:16.923: INFO: stderr: "+ nc -v -z -w 2 10.225.0.5 31327\nConnection to 10.225.0.5 31327 port [tcp/*] succeeded!\n"
Sep  4 18:05:16.923: INFO: stdout: ""
Sep  4 18:05:16.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-2323 exec execpod26tc9 -- /bin/sh -x -c nc -v -z -w 2 10.225.0.7 31327'
Sep  4 18:05:17.109: INFO: stderr: "+ nc -v -z -w 2 10.225.0.7 31327\nConnection to 10.225.0.7 31327 port [tcp/*] succeeded!\n"
Sep  4 18:05:17.109: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 18:05:17.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2323" for this suite. 09/04/23 18:05:17.112
------------------------------
â€¢ [SLOW TEST] [6.921 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:05:10.199
    Sep  4 18:05:10.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 18:05:10.201
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:10.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:10.244
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-2323 09/04/23 18:05:10.252
    STEP: creating replication controller nodeport-test in namespace services-2323 09/04/23 18:05:10.275
    I0904 18:05:10.291668      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2323, replica count: 2
    I0904 18:05:13.342965      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 18:05:13.343: INFO: Creating new exec pod
    Sep  4 18:05:13.354: INFO: Waiting up to 5m0s for pod "execpod26tc9" in namespace "services-2323" to be "running"
    Sep  4 18:05:13.358: INFO: Pod "execpod26tc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.343855ms
    Sep  4 18:05:15.363: INFO: Pod "execpod26tc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009070877s
    Sep  4 18:05:15.363: INFO: Pod "execpod26tc9" satisfied condition "running"
    Sep  4 18:05:16.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-2323 exec execpod26tc9 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Sep  4 18:05:16.568: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Sep  4 18:05:16.568: INFO: stdout: ""
    Sep  4 18:05:16.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-2323 exec execpod26tc9 -- /bin/sh -x -c nc -v -z -w 2 10.96.90.95 80'
    Sep  4 18:05:16.748: INFO: stderr: "+ nc -v -z -w 2 10.96.90.95 80\nConnection to 10.96.90.95 80 port [tcp/http] succeeded!\n"
    Sep  4 18:05:16.748: INFO: stdout: ""
    Sep  4 18:05:16.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-2323 exec execpod26tc9 -- /bin/sh -x -c nc -v -z -w 2 10.225.0.5 31327'
    Sep  4 18:05:16.923: INFO: stderr: "+ nc -v -z -w 2 10.225.0.5 31327\nConnection to 10.225.0.5 31327 port [tcp/*] succeeded!\n"
    Sep  4 18:05:16.923: INFO: stdout: ""
    Sep  4 18:05:16.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-2323 exec execpod26tc9 -- /bin/sh -x -c nc -v -z -w 2 10.225.0.7 31327'
    Sep  4 18:05:17.109: INFO: stderr: "+ nc -v -z -w 2 10.225.0.7 31327\nConnection to 10.225.0.7 31327 port [tcp/*] succeeded!\n"
    Sep  4 18:05:17.109: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:05:17.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2323" for this suite. 09/04/23 18:05:17.112
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:05:17.12
Sep  4 18:05:17.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename proxy 09/04/23 18:05:17.121
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:17.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:17.147
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Sep  4 18:05:17.150: INFO: Creating pod...
Sep  4 18:05:17.159: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6417" to be "running"
Sep  4 18:05:17.176: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 16.450564ms
Sep  4 18:05:19.182: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.022701276s
Sep  4 18:05:19.182: INFO: Pod "agnhost" satisfied condition "running"
Sep  4 18:05:19.182: INFO: Creating service...
Sep  4 18:05:19.198: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/pods/agnhost/proxy?method=DELETE
Sep  4 18:05:19.217: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  4 18:05:19.218: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/pods/agnhost/proxy?method=OPTIONS
Sep  4 18:05:19.229: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  4 18:05:19.229: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/pods/agnhost/proxy?method=PATCH
Sep  4 18:05:19.243: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  4 18:05:19.243: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/pods/agnhost/proxy?method=POST
Sep  4 18:05:19.256: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  4 18:05:19.257: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/pods/agnhost/proxy?method=PUT
Sep  4 18:05:19.268: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  4 18:05:19.268: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/services/e2e-proxy-test-service/proxy?method=DELETE
Sep  4 18:05:19.280: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  4 18:05:19.280: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/services/e2e-proxy-test-service/proxy?method=OPTIONS
Sep  4 18:05:19.293: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  4 18:05:19.293: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/services/e2e-proxy-test-service/proxy?method=PATCH
Sep  4 18:05:19.306: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  4 18:05:19.306: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/services/e2e-proxy-test-service/proxy?method=POST
Sep  4 18:05:19.331: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  4 18:05:19.331: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/services/e2e-proxy-test-service/proxy?method=PUT
Sep  4 18:05:19.339: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  4 18:05:19.339: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/pods/agnhost/proxy?method=GET
Sep  4 18:05:19.342: INFO: http.Client request:GET StatusCode:301
Sep  4 18:05:19.342: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/services/e2e-proxy-test-service/proxy?method=GET
Sep  4 18:05:19.348: INFO: http.Client request:GET StatusCode:301
Sep  4 18:05:19.348: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/pods/agnhost/proxy?method=HEAD
Sep  4 18:05:19.350: INFO: http.Client request:HEAD StatusCode:301
Sep  4 18:05:19.350: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/services/e2e-proxy-test-service/proxy?method=HEAD
Sep  4 18:05:19.354: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  4 18:05:19.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6417" for this suite. 09/04/23 18:05:19.358
------------------------------
â€¢ [2.246 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:05:17.12
    Sep  4 18:05:17.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename proxy 09/04/23 18:05:17.121
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:17.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:17.147
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Sep  4 18:05:17.150: INFO: Creating pod...
    Sep  4 18:05:17.159: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6417" to be "running"
    Sep  4 18:05:17.176: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 16.450564ms
    Sep  4 18:05:19.182: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.022701276s
    Sep  4 18:05:19.182: INFO: Pod "agnhost" satisfied condition "running"
    Sep  4 18:05:19.182: INFO: Creating service...
    Sep  4 18:05:19.198: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/pods/agnhost/proxy?method=DELETE
    Sep  4 18:05:19.217: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  4 18:05:19.218: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/pods/agnhost/proxy?method=OPTIONS
    Sep  4 18:05:19.229: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  4 18:05:19.229: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/pods/agnhost/proxy?method=PATCH
    Sep  4 18:05:19.243: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  4 18:05:19.243: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/pods/agnhost/proxy?method=POST
    Sep  4 18:05:19.256: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  4 18:05:19.257: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/pods/agnhost/proxy?method=PUT
    Sep  4 18:05:19.268: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  4 18:05:19.268: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/services/e2e-proxy-test-service/proxy?method=DELETE
    Sep  4 18:05:19.280: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  4 18:05:19.280: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Sep  4 18:05:19.293: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  4 18:05:19.293: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/services/e2e-proxy-test-service/proxy?method=PATCH
    Sep  4 18:05:19.306: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  4 18:05:19.306: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/services/e2e-proxy-test-service/proxy?method=POST
    Sep  4 18:05:19.331: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  4 18:05:19.331: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/services/e2e-proxy-test-service/proxy?method=PUT
    Sep  4 18:05:19.339: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  4 18:05:19.339: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/pods/agnhost/proxy?method=GET
    Sep  4 18:05:19.342: INFO: http.Client request:GET StatusCode:301
    Sep  4 18:05:19.342: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/services/e2e-proxy-test-service/proxy?method=GET
    Sep  4 18:05:19.348: INFO: http.Client request:GET StatusCode:301
    Sep  4 18:05:19.348: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/pods/agnhost/proxy?method=HEAD
    Sep  4 18:05:19.350: INFO: http.Client request:HEAD StatusCode:301
    Sep  4 18:05:19.350: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6417/services/e2e-proxy-test-service/proxy?method=HEAD
    Sep  4 18:05:19.354: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:05:19.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6417" for this suite. 09/04/23 18:05:19.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:05:19.372
Sep  4 18:05:19.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 18:05:19.373
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:19.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:19.403
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 09/04/23 18:05:19.406
Sep  4 18:05:19.417: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836" in namespace "downward-api-2342" to be "Succeeded or Failed"
Sep  4 18:05:19.424: INFO: Pod "downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836": Phase="Pending", Reason="", readiness=false. Elapsed: 6.405976ms
Sep  4 18:05:21.428: INFO: Pod "downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010784918s
Sep  4 18:05:23.429: INFO: Pod "downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011770246s
STEP: Saw pod success 09/04/23 18:05:23.429
Sep  4 18:05:23.429: INFO: Pod "downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836" satisfied condition "Succeeded or Failed"
Sep  4 18:05:23.432: INFO: Trying to get logs from node tenant-000003 pod downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836 container client-container: <nil>
STEP: delete the pod 09/04/23 18:05:23.483
Sep  4 18:05:23.506: INFO: Waiting for pod downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836 to disappear
Sep  4 18:05:23.509: INFO: Pod downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 18:05:23.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2342" for this suite. 09/04/23 18:05:23.514
------------------------------
â€¢ [4.157 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:05:19.372
    Sep  4 18:05:19.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 18:05:19.373
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:19.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:19.403
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 09/04/23 18:05:19.406
    Sep  4 18:05:19.417: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836" in namespace "downward-api-2342" to be "Succeeded or Failed"
    Sep  4 18:05:19.424: INFO: Pod "downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836": Phase="Pending", Reason="", readiness=false. Elapsed: 6.405976ms
    Sep  4 18:05:21.428: INFO: Pod "downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010784918s
    Sep  4 18:05:23.429: INFO: Pod "downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011770246s
    STEP: Saw pod success 09/04/23 18:05:23.429
    Sep  4 18:05:23.429: INFO: Pod "downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836" satisfied condition "Succeeded or Failed"
    Sep  4 18:05:23.432: INFO: Trying to get logs from node tenant-000003 pod downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836 container client-container: <nil>
    STEP: delete the pod 09/04/23 18:05:23.483
    Sep  4 18:05:23.506: INFO: Waiting for pod downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836 to disappear
    Sep  4 18:05:23.509: INFO: Pod downwardapi-volume-7c9105a7-a948-48ee-8285-1e28606f4836 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:05:23.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2342" for this suite. 09/04/23 18:05:23.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:05:23.536
Sep  4 18:05:23.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename replicaset 09/04/23 18:05:23.537
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:23.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:23.566
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/04/23 18:05:23.569
Sep  4 18:05:23.578: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-426" to be "running and ready"
Sep  4 18:05:23.590: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 12.076817ms
Sep  4 18:05:23.590: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:05:25.598: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.020271774s
Sep  4 18:05:25.598: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Sep  4 18:05:25.599: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 09/04/23 18:05:25.601
STEP: Then the orphan pod is adopted 09/04/23 18:05:25.607
STEP: When the matched label of one of its pods change 09/04/23 18:05:26.62
Sep  4 18:05:26.624: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 09/04/23 18:05:26.638
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  4 18:05:27.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-426" for this suite. 09/04/23 18:05:27.662
------------------------------
â€¢ [4.138 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:05:23.536
    Sep  4 18:05:23.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename replicaset 09/04/23 18:05:23.537
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:23.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:23.566
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/04/23 18:05:23.569
    Sep  4 18:05:23.578: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-426" to be "running and ready"
    Sep  4 18:05:23.590: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 12.076817ms
    Sep  4 18:05:23.590: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:05:25.598: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.020271774s
    Sep  4 18:05:25.598: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Sep  4 18:05:25.599: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 09/04/23 18:05:25.601
    STEP: Then the orphan pod is adopted 09/04/23 18:05:25.607
    STEP: When the matched label of one of its pods change 09/04/23 18:05:26.62
    Sep  4 18:05:26.624: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/04/23 18:05:26.638
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:05:27.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-426" for this suite. 09/04/23 18:05:27.662
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:05:27.678
Sep  4 18:05:27.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 18:05:27.679
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:27.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:27.714
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Sep  4 18:05:27.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/04/23 18:05:29.626
Sep  4 18:05:29.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-9755 --namespace=crd-publish-openapi-9755 create -f -'
Sep  4 18:05:30.283: INFO: stderr: ""
Sep  4 18:05:30.283: INFO: stdout: "e2e-test-crd-publish-openapi-8914-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  4 18:05:30.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-9755 --namespace=crd-publish-openapi-9755 delete e2e-test-crd-publish-openapi-8914-crds test-cr'
Sep  4 18:05:30.393: INFO: stderr: ""
Sep  4 18:05:30.393: INFO: stdout: "e2e-test-crd-publish-openapi-8914-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep  4 18:05:30.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-9755 --namespace=crd-publish-openapi-9755 apply -f -'
Sep  4 18:05:30.636: INFO: stderr: ""
Sep  4 18:05:30.636: INFO: stdout: "e2e-test-crd-publish-openapi-8914-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  4 18:05:30.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-9755 --namespace=crd-publish-openapi-9755 delete e2e-test-crd-publish-openapi-8914-crds test-cr'
Sep  4 18:05:30.803: INFO: stderr: ""
Sep  4 18:05:30.803: INFO: stdout: "e2e-test-crd-publish-openapi-8914-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/04/23 18:05:30.803
Sep  4 18:05:30.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-9755 explain e2e-test-crd-publish-openapi-8914-crds'
Sep  4 18:05:31.321: INFO: stderr: ""
Sep  4 18:05:31.321: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8914-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:05:34.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9755" for this suite. 09/04/23 18:05:34.302
------------------------------
â€¢ [SLOW TEST] [6.638 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:05:27.678
    Sep  4 18:05:27.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 18:05:27.679
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:27.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:27.714
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Sep  4 18:05:27.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/04/23 18:05:29.626
    Sep  4 18:05:29.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-9755 --namespace=crd-publish-openapi-9755 create -f -'
    Sep  4 18:05:30.283: INFO: stderr: ""
    Sep  4 18:05:30.283: INFO: stdout: "e2e-test-crd-publish-openapi-8914-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep  4 18:05:30.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-9755 --namespace=crd-publish-openapi-9755 delete e2e-test-crd-publish-openapi-8914-crds test-cr'
    Sep  4 18:05:30.393: INFO: stderr: ""
    Sep  4 18:05:30.393: INFO: stdout: "e2e-test-crd-publish-openapi-8914-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Sep  4 18:05:30.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-9755 --namespace=crd-publish-openapi-9755 apply -f -'
    Sep  4 18:05:30.636: INFO: stderr: ""
    Sep  4 18:05:30.636: INFO: stdout: "e2e-test-crd-publish-openapi-8914-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep  4 18:05:30.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-9755 --namespace=crd-publish-openapi-9755 delete e2e-test-crd-publish-openapi-8914-crds test-cr'
    Sep  4 18:05:30.803: INFO: stderr: ""
    Sep  4 18:05:30.803: INFO: stdout: "e2e-test-crd-publish-openapi-8914-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/04/23 18:05:30.803
    Sep  4 18:05:30.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-9755 explain e2e-test-crd-publish-openapi-8914-crds'
    Sep  4 18:05:31.321: INFO: stderr: ""
    Sep  4 18:05:31.321: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8914-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:05:34.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9755" for this suite. 09/04/23 18:05:34.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:05:34.328
Sep  4 18:05:34.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename podtemplate 09/04/23 18:05:34.329
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:34.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:34.361
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 09/04/23 18:05:34.374
STEP: Replace a pod template 09/04/23 18:05:34.384
Sep  4 18:05:34.399: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  4 18:05:34.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-2954" for this suite. 09/04/23 18:05:34.408
------------------------------
â€¢ [0.089 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:05:34.328
    Sep  4 18:05:34.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename podtemplate 09/04/23 18:05:34.329
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:34.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:34.361
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 09/04/23 18:05:34.374
    STEP: Replace a pod template 09/04/23 18:05:34.384
    Sep  4 18:05:34.399: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:05:34.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-2954" for this suite. 09/04/23 18:05:34.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:05:34.427
Sep  4 18:05:34.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 18:05:34.429
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:34.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:34.46
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/04/23 18:05:34.467
Sep  4 18:05:34.486: INFO: Waiting up to 5m0s for pod "pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2" in namespace "emptydir-3879" to be "Succeeded or Failed"
Sep  4 18:05:34.494: INFO: Pod "pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.145587ms
Sep  4 18:05:36.500: INFO: Pod "pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013493766s
Sep  4 18:05:38.500: INFO: Pod "pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014401532s
STEP: Saw pod success 09/04/23 18:05:38.501
Sep  4 18:05:38.501: INFO: Pod "pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2" satisfied condition "Succeeded or Failed"
Sep  4 18:05:38.509: INFO: Trying to get logs from node tenant-000001 pod pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2 container test-container: <nil>
STEP: delete the pod 09/04/23 18:05:38.547
Sep  4 18:05:38.565: INFO: Waiting for pod pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2 to disappear
Sep  4 18:05:38.572: INFO: Pod pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 18:05:38.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3879" for this suite. 09/04/23 18:05:38.583
------------------------------
â€¢ [4.166 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:05:34.427
    Sep  4 18:05:34.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 18:05:34.429
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:34.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:34.46
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/04/23 18:05:34.467
    Sep  4 18:05:34.486: INFO: Waiting up to 5m0s for pod "pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2" in namespace "emptydir-3879" to be "Succeeded or Failed"
    Sep  4 18:05:34.494: INFO: Pod "pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.145587ms
    Sep  4 18:05:36.500: INFO: Pod "pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013493766s
    Sep  4 18:05:38.500: INFO: Pod "pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014401532s
    STEP: Saw pod success 09/04/23 18:05:38.501
    Sep  4 18:05:38.501: INFO: Pod "pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2" satisfied condition "Succeeded or Failed"
    Sep  4 18:05:38.509: INFO: Trying to get logs from node tenant-000001 pod pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2 container test-container: <nil>
    STEP: delete the pod 09/04/23 18:05:38.547
    Sep  4 18:05:38.565: INFO: Waiting for pod pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2 to disappear
    Sep  4 18:05:38.572: INFO: Pod pod-ffbdc6c9-8d85-4045-9c17-5f8dc98709e2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:05:38.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3879" for this suite. 09/04/23 18:05:38.583
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:05:38.596
Sep  4 18:05:38.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename subpath 09/04/23 18:05:38.599
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:38.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:38.635
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/04/23 18:05:38.642
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-xn5f 09/04/23 18:05:38.656
STEP: Creating a pod to test atomic-volume-subpath 09/04/23 18:05:38.656
Sep  4 18:05:38.669: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-xn5f" in namespace "subpath-1420" to be "Succeeded or Failed"
Sep  4 18:05:38.685: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.424009ms
Sep  4 18:05:40.692: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.022183521s
Sep  4 18:05:42.697: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 4.027258333s
Sep  4 18:05:44.692: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 6.022433204s
Sep  4 18:05:46.692: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 8.021915178s
Sep  4 18:05:48.698: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 10.027923138s
Sep  4 18:05:50.691: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 12.021436158s
Sep  4 18:05:52.692: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 14.022387372s
Sep  4 18:05:54.697: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 16.027604588s
Sep  4 18:05:56.692: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 18.021916284s
Sep  4 18:05:58.692: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 20.021924104s
Sep  4 18:06:00.695: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=false. Elapsed: 22.025787284s
Sep  4 18:06:02.692: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.021864523s
STEP: Saw pod success 09/04/23 18:06:02.692
Sep  4 18:06:02.692: INFO: Pod "pod-subpath-test-projected-xn5f" satisfied condition "Succeeded or Failed"
Sep  4 18:06:02.698: INFO: Trying to get logs from node tenant-000001 pod pod-subpath-test-projected-xn5f container test-container-subpath-projected-xn5f: <nil>
STEP: delete the pod 09/04/23 18:06:02.711
Sep  4 18:06:02.728: INFO: Waiting for pod pod-subpath-test-projected-xn5f to disappear
Sep  4 18:06:02.736: INFO: Pod pod-subpath-test-projected-xn5f no longer exists
STEP: Deleting pod pod-subpath-test-projected-xn5f 09/04/23 18:06:02.736
Sep  4 18:06:02.736: INFO: Deleting pod "pod-subpath-test-projected-xn5f" in namespace "subpath-1420"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  4 18:06:02.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1420" for this suite. 09/04/23 18:06:02.753
------------------------------
â€¢ [SLOW TEST] [24.170 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:05:38.596
    Sep  4 18:05:38.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename subpath 09/04/23 18:05:38.599
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:05:38.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:05:38.635
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/04/23 18:05:38.642
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-xn5f 09/04/23 18:05:38.656
    STEP: Creating a pod to test atomic-volume-subpath 09/04/23 18:05:38.656
    Sep  4 18:05:38.669: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-xn5f" in namespace "subpath-1420" to be "Succeeded or Failed"
    Sep  4 18:05:38.685: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.424009ms
    Sep  4 18:05:40.692: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.022183521s
    Sep  4 18:05:42.697: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 4.027258333s
    Sep  4 18:05:44.692: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 6.022433204s
    Sep  4 18:05:46.692: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 8.021915178s
    Sep  4 18:05:48.698: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 10.027923138s
    Sep  4 18:05:50.691: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 12.021436158s
    Sep  4 18:05:52.692: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 14.022387372s
    Sep  4 18:05:54.697: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 16.027604588s
    Sep  4 18:05:56.692: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 18.021916284s
    Sep  4 18:05:58.692: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=true. Elapsed: 20.021924104s
    Sep  4 18:06:00.695: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Running", Reason="", readiness=false. Elapsed: 22.025787284s
    Sep  4 18:06:02.692: INFO: Pod "pod-subpath-test-projected-xn5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.021864523s
    STEP: Saw pod success 09/04/23 18:06:02.692
    Sep  4 18:06:02.692: INFO: Pod "pod-subpath-test-projected-xn5f" satisfied condition "Succeeded or Failed"
    Sep  4 18:06:02.698: INFO: Trying to get logs from node tenant-000001 pod pod-subpath-test-projected-xn5f container test-container-subpath-projected-xn5f: <nil>
    STEP: delete the pod 09/04/23 18:06:02.711
    Sep  4 18:06:02.728: INFO: Waiting for pod pod-subpath-test-projected-xn5f to disappear
    Sep  4 18:06:02.736: INFO: Pod pod-subpath-test-projected-xn5f no longer exists
    STEP: Deleting pod pod-subpath-test-projected-xn5f 09/04/23 18:06:02.736
    Sep  4 18:06:02.736: INFO: Deleting pod "pod-subpath-test-projected-xn5f" in namespace "subpath-1420"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:06:02.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1420" for this suite. 09/04/23 18:06:02.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:06:02.81
Sep  4 18:06:02.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename deployment 09/04/23 18:06:02.843
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:02.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:02.933
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 09/04/23 18:06:02.946
STEP: waiting for Deployment to be created 09/04/23 18:06:02.961
STEP: waiting for all Replicas to be Ready 09/04/23 18:06:02.965
Sep  4 18:06:02.969: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 18:06:02.969: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 18:06:02.983: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 18:06:02.983: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 18:06:03.009: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 18:06:03.009: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 18:06:03.076: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 18:06:03.076: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 18:06:04.573: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  4 18:06:04.574: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  4 18:06:04.873: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 09/04/23 18:06:04.873
W0904 18:06:04.891158      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  4 18:06:04.895: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 09/04/23 18:06:04.895
Sep  4 18:06:04.899: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
Sep  4 18:06:04.899: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
Sep  4 18:06:04.899: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
Sep  4 18:06:04.899: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
Sep  4 18:06:04.899: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
Sep  4 18:06:04.900: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
Sep  4 18:06:04.900: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
Sep  4 18:06:04.900: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
Sep  4 18:06:04.900: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
Sep  4 18:06:04.900: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
Sep  4 18:06:04.900: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
Sep  4 18:06:04.900: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
Sep  4 18:06:04.901: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
Sep  4 18:06:04.901: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
Sep  4 18:06:04.922: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
Sep  4 18:06:04.922: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
Sep  4 18:06:04.938: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
Sep  4 18:06:04.938: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
Sep  4 18:06:04.950: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
Sep  4 18:06:04.950: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
Sep  4 18:06:04.978: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
Sep  4 18:06:04.978: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
Sep  4 18:06:05.899: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
Sep  4 18:06:05.899: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
Sep  4 18:06:05.946: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
STEP: listing Deployments 09/04/23 18:06:05.946
Sep  4 18:06:05.952: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 09/04/23 18:06:05.953
Sep  4 18:06:05.976: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 09/04/23 18:06:05.976
Sep  4 18:06:05.993: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 18:06:05.993: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 18:06:06.017: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 18:06:06.051: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 18:06:06.068: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 18:06:06.078: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 18:06:07.652: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 18:06:07.968: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 18:06:08.022: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 18:06:08.037: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 18:06:09.635: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 09/04/23 18:06:09.674
STEP: fetching the DeploymentStatus 09/04/23 18:06:09.685
Sep  4 18:06:09.694: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
Sep  4 18:06:09.695: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
Sep  4 18:06:09.695: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
Sep  4 18:06:09.695: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
Sep  4 18:06:09.695: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
Sep  4 18:06:09.695: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
Sep  4 18:06:09.696: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
Sep  4 18:06:09.696: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 3
Sep  4 18:06:09.696: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
Sep  4 18:06:09.696: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
Sep  4 18:06:09.696: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 3
STEP: deleting the Deployment 09/04/23 18:06:09.696
Sep  4 18:06:09.721: INFO: observed event type MODIFIED
Sep  4 18:06:09.721: INFO: observed event type MODIFIED
Sep  4 18:06:09.721: INFO: observed event type MODIFIED
Sep  4 18:06:09.721: INFO: observed event type MODIFIED
Sep  4 18:06:09.721: INFO: observed event type MODIFIED
Sep  4 18:06:09.721: INFO: observed event type MODIFIED
Sep  4 18:06:09.721: INFO: observed event type MODIFIED
Sep  4 18:06:09.721: INFO: observed event type MODIFIED
Sep  4 18:06:09.721: INFO: observed event type MODIFIED
Sep  4 18:06:09.721: INFO: observed event type MODIFIED
Sep  4 18:06:09.721: INFO: observed event type MODIFIED
Sep  4 18:06:09.721: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 18:06:09.728: INFO: Log out all the ReplicaSets if there is no deployment created
Sep  4 18:06:09.739: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-802  1f4919c1-bff3-4e2b-bf51-e8cfbd251d39 22797 2 2023-09-04 18:06:05 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 4d7f869f-8c6f-430f-a6f8-1d950b5960ee 0xc00430c757 0xc00430c758}] [] [{kube-controller-manager Update apps/v1 2023-09-04 18:06:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d7f869f-8c6f-430f-a6f8-1d950b5960ee\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:06:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00430c7e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Sep  4 18:06:09.752: INFO: pod: "test-deployment-7b7876f9d6-5lmkd":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-5lmkd test-deployment-7b7876f9d6- deployment-802  d9677072-41fc-4b00-9a2a-5864ed6548c4 22796 0 2023-09-04 18:06:07 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:bddad01fa29fb05ae1428c89745076a1573308298306ad3d64592b1ebf3cfe61 cni.projectcalico.org/podIP:10.36.217.242/32 cni.projectcalico.org/podIPs:10.36.217.242/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 1f4919c1-bff3-4e2b-bf51-e8cfbd251d39 0xc00430ce47 0xc00430ce48}] [] [{kube-controller-manager Update v1 2023-09-04 18:06:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f4919c1-bff3-4e2b-bf51-e8cfbd251d39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 18:06:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 18:06:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-24z6c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-24z6c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.242,StartTime:2023-09-04 18:06:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 18:06:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bd0b11fc9946d0caa2543ac7ad1d8445b0324714e98e8715a2f9f381b7a06b3b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  4 18:06:09.752: INFO: pod: "test-deployment-7b7876f9d6-l8krp":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-l8krp test-deployment-7b7876f9d6- deployment-802  ae0d37af-7bf1-4604-b847-1a01fc9079dc 22756 0 2023-09-04 18:06:06 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:56466ec33b6d77bdcdaa59a9a2680697e21c9c5cd47c6b064933fd96f2e583c7 cni.projectcalico.org/podIP:10.36.55.121/32 cni.projectcalico.org/podIPs:10.36.55.121/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 1f4919c1-bff3-4e2b-bf51-e8cfbd251d39 0xc00430d157 0xc00430d158}] [] [{calico Update v1 2023-09-04 18:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-04 18:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f4919c1-bff3-4e2b-bf51-e8cfbd251d39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 18:06:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79bbv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79bbv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.121,StartTime:2023-09-04 18:06:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 18:06:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a0dc15846ad9b1995a77a55cd96c39fab7d62a7640fad8f70306c8b214e9a408,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  4 18:06:09.752: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-802  28fc4d08-e57e-45cf-9f51-c4fbdcd34904 22806 4 2023-09-04 18:06:04 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 4d7f869f-8c6f-430f-a6f8-1d950b5960ee 0xc00430c847 0xc00430c848}] [] [{kube-controller-manager Update apps/v1 2023-09-04 18:06:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d7f869f-8c6f-430f-a6f8-1d950b5960ee\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:06:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00430c8d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Sep  4 18:06:09.762: INFO: pod: "test-deployment-7df74c55ff-hvwk4":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-hvwk4 test-deployment-7df74c55ff- deployment-802  3f94962f-c0cb-4643-9a8c-a2a22c20ceb2 22792 0 2023-09-04 18:06:04 +0000 UTC 2023-09-04 18:06:08 +0000 UTC 0xc003e52588 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:98e893965e377ba4a4ada42c19fc983c6a1f98006c7d691369a785f821150f26 cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 28fc4d08-e57e-45cf-9f51-c4fbdcd34904 0xc003e525b7 0xc003e525b8}] [] [{kube-controller-manager Update v1 2023-09-04 18:06:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28fc4d08-e57e-45cf-9f51-c4fbdcd34904\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 18:06:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2023-09-04 18:06:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gcc76,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gcc76,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.108,StartTime:2023-09-04 18:06:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 18:06:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://6b173f00de0e7e94f9094eea772c6523b8799bd824f48dd8a77533d82f235b3b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  4 18:06:09.763: INFO: pod: "test-deployment-7df74c55ff-n49kn":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-n49kn test-deployment-7df74c55ff- deployment-802  4c0048be-339f-44e4-b936-ae9a59a26c89 22802 0 2023-09-04 18:06:05 +0000 UTC 2023-09-04 18:06:10 +0000 UTC 0xc003e527a0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:e21cd46c633b865be0a8b3069116d23ddb888084dd7aa4150f7a6797ec9dbdae cni.projectcalico.org/podIP:10.36.217.226/32 cni.projectcalico.org/podIPs:10.36.217.226/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 28fc4d08-e57e-45cf-9f51-c4fbdcd34904 0xc003e527f7 0xc003e527f8}] [] [{kube-controller-manager Update v1 2023-09-04 18:06:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28fc4d08-e57e-45cf-9f51-c4fbdcd34904\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 18:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 18:06:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6lwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6lwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.226,StartTime:2023-09-04 18:06:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 18:06:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://2d6709aea9520a22d95589fbdcdd77bb5c692da7d2e8eb1c0f2292d94212b64a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 18:06:09.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-802" for this suite. 09/04/23 18:06:09.78
------------------------------
â€¢ [SLOW TEST] [6.987 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:06:02.81
    Sep  4 18:06:02.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename deployment 09/04/23 18:06:02.843
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:02.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:02.933
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 09/04/23 18:06:02.946
    STEP: waiting for Deployment to be created 09/04/23 18:06:02.961
    STEP: waiting for all Replicas to be Ready 09/04/23 18:06:02.965
    Sep  4 18:06:02.969: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 18:06:02.969: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 18:06:02.983: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 18:06:02.983: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 18:06:03.009: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 18:06:03.009: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 18:06:03.076: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 18:06:03.076: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 18:06:04.573: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep  4 18:06:04.574: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep  4 18:06:04.873: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 09/04/23 18:06:04.873
    W0904 18:06:04.891158      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  4 18:06:04.895: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 09/04/23 18:06:04.895
    Sep  4 18:06:04.899: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
    Sep  4 18:06:04.899: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
    Sep  4 18:06:04.899: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
    Sep  4 18:06:04.899: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
    Sep  4 18:06:04.899: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
    Sep  4 18:06:04.900: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
    Sep  4 18:06:04.900: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
    Sep  4 18:06:04.900: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 0
    Sep  4 18:06:04.900: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
    Sep  4 18:06:04.900: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
    Sep  4 18:06:04.900: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
    Sep  4 18:06:04.900: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
    Sep  4 18:06:04.901: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
    Sep  4 18:06:04.901: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
    Sep  4 18:06:04.922: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
    Sep  4 18:06:04.922: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
    Sep  4 18:06:04.938: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
    Sep  4 18:06:04.938: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
    Sep  4 18:06:04.950: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
    Sep  4 18:06:04.950: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
    Sep  4 18:06:04.978: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
    Sep  4 18:06:04.978: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
    Sep  4 18:06:05.899: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
    Sep  4 18:06:05.899: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
    Sep  4 18:06:05.946: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
    STEP: listing Deployments 09/04/23 18:06:05.946
    Sep  4 18:06:05.952: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 09/04/23 18:06:05.953
    Sep  4 18:06:05.976: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 09/04/23 18:06:05.976
    Sep  4 18:06:05.993: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 18:06:05.993: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 18:06:06.017: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 18:06:06.051: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 18:06:06.068: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 18:06:06.078: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 18:06:07.652: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 18:06:07.968: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 18:06:08.022: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 18:06:08.037: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 18:06:09.635: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 09/04/23 18:06:09.674
    STEP: fetching the DeploymentStatus 09/04/23 18:06:09.685
    Sep  4 18:06:09.694: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
    Sep  4 18:06:09.695: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
    Sep  4 18:06:09.695: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
    Sep  4 18:06:09.695: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
    Sep  4 18:06:09.695: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
    Sep  4 18:06:09.695: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 1
    Sep  4 18:06:09.696: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
    Sep  4 18:06:09.696: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 3
    Sep  4 18:06:09.696: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
    Sep  4 18:06:09.696: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 2
    Sep  4 18:06:09.696: INFO: observed Deployment test-deployment in namespace deployment-802 with ReadyReplicas 3
    STEP: deleting the Deployment 09/04/23 18:06:09.696
    Sep  4 18:06:09.721: INFO: observed event type MODIFIED
    Sep  4 18:06:09.721: INFO: observed event type MODIFIED
    Sep  4 18:06:09.721: INFO: observed event type MODIFIED
    Sep  4 18:06:09.721: INFO: observed event type MODIFIED
    Sep  4 18:06:09.721: INFO: observed event type MODIFIED
    Sep  4 18:06:09.721: INFO: observed event type MODIFIED
    Sep  4 18:06:09.721: INFO: observed event type MODIFIED
    Sep  4 18:06:09.721: INFO: observed event type MODIFIED
    Sep  4 18:06:09.721: INFO: observed event type MODIFIED
    Sep  4 18:06:09.721: INFO: observed event type MODIFIED
    Sep  4 18:06:09.721: INFO: observed event type MODIFIED
    Sep  4 18:06:09.721: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 18:06:09.728: INFO: Log out all the ReplicaSets if there is no deployment created
    Sep  4 18:06:09.739: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-802  1f4919c1-bff3-4e2b-bf51-e8cfbd251d39 22797 2 2023-09-04 18:06:05 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 4d7f869f-8c6f-430f-a6f8-1d950b5960ee 0xc00430c757 0xc00430c758}] [] [{kube-controller-manager Update apps/v1 2023-09-04 18:06:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d7f869f-8c6f-430f-a6f8-1d950b5960ee\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:06:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00430c7e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Sep  4 18:06:09.752: INFO: pod: "test-deployment-7b7876f9d6-5lmkd":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-5lmkd test-deployment-7b7876f9d6- deployment-802  d9677072-41fc-4b00-9a2a-5864ed6548c4 22796 0 2023-09-04 18:06:07 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:bddad01fa29fb05ae1428c89745076a1573308298306ad3d64592b1ebf3cfe61 cni.projectcalico.org/podIP:10.36.217.242/32 cni.projectcalico.org/podIPs:10.36.217.242/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 1f4919c1-bff3-4e2b-bf51-e8cfbd251d39 0xc00430ce47 0xc00430ce48}] [] [{kube-controller-manager Update v1 2023-09-04 18:06:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f4919c1-bff3-4e2b-bf51-e8cfbd251d39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 18:06:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 18:06:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-24z6c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-24z6c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.242,StartTime:2023-09-04 18:06:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 18:06:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bd0b11fc9946d0caa2543ac7ad1d8445b0324714e98e8715a2f9f381b7a06b3b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  4 18:06:09.752: INFO: pod: "test-deployment-7b7876f9d6-l8krp":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-l8krp test-deployment-7b7876f9d6- deployment-802  ae0d37af-7bf1-4604-b847-1a01fc9079dc 22756 0 2023-09-04 18:06:06 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:56466ec33b6d77bdcdaa59a9a2680697e21c9c5cd47c6b064933fd96f2e583c7 cni.projectcalico.org/podIP:10.36.55.121/32 cni.projectcalico.org/podIPs:10.36.55.121/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 1f4919c1-bff3-4e2b-bf51-e8cfbd251d39 0xc00430d157 0xc00430d158}] [] [{calico Update v1 2023-09-04 18:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-04 18:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f4919c1-bff3-4e2b-bf51-e8cfbd251d39\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 18:06:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79bbv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79bbv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.121,StartTime:2023-09-04 18:06:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 18:06:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a0dc15846ad9b1995a77a55cd96c39fab7d62a7640fad8f70306c8b214e9a408,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  4 18:06:09.752: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-802  28fc4d08-e57e-45cf-9f51-c4fbdcd34904 22806 4 2023-09-04 18:06:04 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 4d7f869f-8c6f-430f-a6f8-1d950b5960ee 0xc00430c847 0xc00430c848}] [] [{kube-controller-manager Update apps/v1 2023-09-04 18:06:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d7f869f-8c6f-430f-a6f8-1d950b5960ee\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:06:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00430c8d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Sep  4 18:06:09.762: INFO: pod: "test-deployment-7df74c55ff-hvwk4":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-hvwk4 test-deployment-7df74c55ff- deployment-802  3f94962f-c0cb-4643-9a8c-a2a22c20ceb2 22792 0 2023-09-04 18:06:04 +0000 UTC 2023-09-04 18:06:08 +0000 UTC 0xc003e52588 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:98e893965e377ba4a4ada42c19fc983c6a1f98006c7d691369a785f821150f26 cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 28fc4d08-e57e-45cf-9f51-c4fbdcd34904 0xc003e525b7 0xc003e525b8}] [] [{kube-controller-manager Update v1 2023-09-04 18:06:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28fc4d08-e57e-45cf-9f51-c4fbdcd34904\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 18:06:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2023-09-04 18:06:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gcc76,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gcc76,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.108,StartTime:2023-09-04 18:06:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 18:06:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://6b173f00de0e7e94f9094eea772c6523b8799bd824f48dd8a77533d82f235b3b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  4 18:06:09.763: INFO: pod: "test-deployment-7df74c55ff-n49kn":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-n49kn test-deployment-7df74c55ff- deployment-802  4c0048be-339f-44e4-b936-ae9a59a26c89 22802 0 2023-09-04 18:06:05 +0000 UTC 2023-09-04 18:06:10 +0000 UTC 0xc003e527a0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:e21cd46c633b865be0a8b3069116d23ddb888084dd7aa4150f7a6797ec9dbdae cni.projectcalico.org/podIP:10.36.217.226/32 cni.projectcalico.org/podIPs:10.36.217.226/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 28fc4d08-e57e-45cf-9f51-c4fbdcd34904 0xc003e527f7 0xc003e527f8}] [] [{kube-controller-manager Update v1 2023-09-04 18:06:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28fc4d08-e57e-45cf-9f51-c4fbdcd34904\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 18:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 18:06:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.217.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6lwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6lwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:06:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:10.36.217.226,StartTime:2023-09-04 18:06:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 18:06:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://2d6709aea9520a22d95589fbdcdd77bb5c692da7d2e8eb1c0f2292d94212b64a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.217.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:06:09.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-802" for this suite. 09/04/23 18:06:09.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:06:09.804
Sep  4 18:06:09.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 18:06:09.805
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:09.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:09.832
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 09/04/23 18:06:09.839
Sep  4 18:06:09.856: INFO: Waiting up to 5m0s for pod "pod-313ed877-8eea-4c33-b913-d385668e93c8" in namespace "emptydir-1340" to be "Succeeded or Failed"
Sep  4 18:06:09.862: INFO: Pod "pod-313ed877-8eea-4c33-b913-d385668e93c8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.666154ms
Sep  4 18:06:11.871: INFO: Pod "pod-313ed877-8eea-4c33-b913-d385668e93c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.014497915s
Sep  4 18:06:13.867: INFO: Pod "pod-313ed877-8eea-4c33-b913-d385668e93c8": Phase="Running", Reason="", readiness=false. Elapsed: 4.01120332s
Sep  4 18:06:15.868: INFO: Pod "pod-313ed877-8eea-4c33-b913-d385668e93c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011742057s
STEP: Saw pod success 09/04/23 18:06:15.868
Sep  4 18:06:15.869: INFO: Pod "pod-313ed877-8eea-4c33-b913-d385668e93c8" satisfied condition "Succeeded or Failed"
Sep  4 18:06:15.878: INFO: Trying to get logs from node tenant-000001 pod pod-313ed877-8eea-4c33-b913-d385668e93c8 container test-container: <nil>
STEP: delete the pod 09/04/23 18:06:15.889
Sep  4 18:06:15.911: INFO: Waiting for pod pod-313ed877-8eea-4c33-b913-d385668e93c8 to disappear
Sep  4 18:06:15.916: INFO: Pod pod-313ed877-8eea-4c33-b913-d385668e93c8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 18:06:15.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1340" for this suite. 09/04/23 18:06:15.925
------------------------------
â€¢ [SLOW TEST] [6.138 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:06:09.804
    Sep  4 18:06:09.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 18:06:09.805
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:09.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:09.832
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/04/23 18:06:09.839
    Sep  4 18:06:09.856: INFO: Waiting up to 5m0s for pod "pod-313ed877-8eea-4c33-b913-d385668e93c8" in namespace "emptydir-1340" to be "Succeeded or Failed"
    Sep  4 18:06:09.862: INFO: Pod "pod-313ed877-8eea-4c33-b913-d385668e93c8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.666154ms
    Sep  4 18:06:11.871: INFO: Pod "pod-313ed877-8eea-4c33-b913-d385668e93c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.014497915s
    Sep  4 18:06:13.867: INFO: Pod "pod-313ed877-8eea-4c33-b913-d385668e93c8": Phase="Running", Reason="", readiness=false. Elapsed: 4.01120332s
    Sep  4 18:06:15.868: INFO: Pod "pod-313ed877-8eea-4c33-b913-d385668e93c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011742057s
    STEP: Saw pod success 09/04/23 18:06:15.868
    Sep  4 18:06:15.869: INFO: Pod "pod-313ed877-8eea-4c33-b913-d385668e93c8" satisfied condition "Succeeded or Failed"
    Sep  4 18:06:15.878: INFO: Trying to get logs from node tenant-000001 pod pod-313ed877-8eea-4c33-b913-d385668e93c8 container test-container: <nil>
    STEP: delete the pod 09/04/23 18:06:15.889
    Sep  4 18:06:15.911: INFO: Waiting for pod pod-313ed877-8eea-4c33-b913-d385668e93c8 to disappear
    Sep  4 18:06:15.916: INFO: Pod pod-313ed877-8eea-4c33-b913-d385668e93c8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:06:15.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1340" for this suite. 09/04/23 18:06:15.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:06:15.949
Sep  4 18:06:15.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename events 09/04/23 18:06:15.95
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:15.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:15.979
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 09/04/23 18:06:15.986
Sep  4 18:06:16.000: INFO: created test-event-1
Sep  4 18:06:16.011: INFO: created test-event-2
Sep  4 18:06:16.025: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 09/04/23 18:06:16.026
STEP: delete collection of events 09/04/23 18:06:16.033
Sep  4 18:06:16.034: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/04/23 18:06:16.069
Sep  4 18:06:16.069: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Sep  4 18:06:16.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4686" for this suite. 09/04/23 18:06:16.083
------------------------------
â€¢ [0.144 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:06:15.949
    Sep  4 18:06:15.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename events 09/04/23 18:06:15.95
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:15.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:15.979
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 09/04/23 18:06:15.986
    Sep  4 18:06:16.000: INFO: created test-event-1
    Sep  4 18:06:16.011: INFO: created test-event-2
    Sep  4 18:06:16.025: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 09/04/23 18:06:16.026
    STEP: delete collection of events 09/04/23 18:06:16.033
    Sep  4 18:06:16.034: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/04/23 18:06:16.069
    Sep  4 18:06:16.069: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:06:16.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4686" for this suite. 09/04/23 18:06:16.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:06:16.098
Sep  4 18:06:16.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pods 09/04/23 18:06:16.1
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:16.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:16.142
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 09/04/23 18:06:16.149
STEP: submitting the pod to kubernetes 09/04/23 18:06:16.15
Sep  4 18:06:16.166: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0" in namespace "pods-2845" to be "running and ready"
Sep  4 18:06:16.181: INFO: Pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.546487ms
Sep  4 18:06:16.181: INFO: The phase of Pod pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:06:18.190: INFO: Pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.02401782s
Sep  4 18:06:18.190: INFO: The phase of Pod pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0 is Running (Ready = true)
Sep  4 18:06:18.190: INFO: Pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/04/23 18:06:18.195
STEP: updating the pod 09/04/23 18:06:18.201
Sep  4 18:06:18.737: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0"
Sep  4 18:06:18.737: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0" in namespace "pods-2845" to be "terminated with reason DeadlineExceeded"
Sep  4 18:06:18.743: INFO: Pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0": Phase="Running", Reason="", readiness=true. Elapsed: 5.352006ms
Sep  4 18:06:20.751: INFO: Pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.013976147s
Sep  4 18:06:22.776: INFO: Pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.038335187s
Sep  4 18:06:22.776: INFO: Pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 18:06:22.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2845" for this suite. 09/04/23 18:06:22.799
------------------------------
â€¢ [SLOW TEST] [6.712 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:06:16.098
    Sep  4 18:06:16.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pods 09/04/23 18:06:16.1
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:16.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:16.142
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 09/04/23 18:06:16.149
    STEP: submitting the pod to kubernetes 09/04/23 18:06:16.15
    Sep  4 18:06:16.166: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0" in namespace "pods-2845" to be "running and ready"
    Sep  4 18:06:16.181: INFO: Pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.546487ms
    Sep  4 18:06:16.181: INFO: The phase of Pod pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:06:18.190: INFO: Pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.02401782s
    Sep  4 18:06:18.190: INFO: The phase of Pod pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0 is Running (Ready = true)
    Sep  4 18:06:18.190: INFO: Pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/04/23 18:06:18.195
    STEP: updating the pod 09/04/23 18:06:18.201
    Sep  4 18:06:18.737: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0"
    Sep  4 18:06:18.737: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0" in namespace "pods-2845" to be "terminated with reason DeadlineExceeded"
    Sep  4 18:06:18.743: INFO: Pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0": Phase="Running", Reason="", readiness=true. Elapsed: 5.352006ms
    Sep  4 18:06:20.751: INFO: Pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.013976147s
    Sep  4 18:06:22.776: INFO: Pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.038335187s
    Sep  4 18:06:22.776: INFO: Pod "pod-update-activedeadlineseconds-3d5449d7-464d-4980-98cc-f948abff36a0" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:06:22.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2845" for this suite. 09/04/23 18:06:22.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:06:22.811
Sep  4 18:06:22.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename replicaset 09/04/23 18:06:22.831
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:22.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:22.913
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/04/23 18:06:22.936
Sep  4 18:06:22.957: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  4 18:06:27.966: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/04/23 18:06:27.966
STEP: getting scale subresource 09/04/23 18:06:27.967
STEP: updating a scale subresource 09/04/23 18:06:27.974
STEP: verifying the replicaset Spec.Replicas was modified 09/04/23 18:06:27.988
STEP: Patch a scale subresource 09/04/23 18:06:28.082
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  4 18:06:28.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1834" for this suite. 09/04/23 18:06:28.118
------------------------------
â€¢ [SLOW TEST] [5.332 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:06:22.811
    Sep  4 18:06:22.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename replicaset 09/04/23 18:06:22.831
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:22.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:22.913
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/04/23 18:06:22.936
    Sep  4 18:06:22.957: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  4 18:06:27.966: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/04/23 18:06:27.966
    STEP: getting scale subresource 09/04/23 18:06:27.967
    STEP: updating a scale subresource 09/04/23 18:06:27.974
    STEP: verifying the replicaset Spec.Replicas was modified 09/04/23 18:06:27.988
    STEP: Patch a scale subresource 09/04/23 18:06:28.082
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:06:28.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1834" for this suite. 09/04/23 18:06:28.118
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:06:28.145
Sep  4 18:06:28.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename endpointslicemirroring 09/04/23 18:06:28.146
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:28.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:28.183
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 09/04/23 18:06:28.21
Sep  4 18:06:28.227: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 09/04/23 18:06:30.234
STEP: mirroring deletion of a custom Endpoint 09/04/23 18:06:30.255
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Sep  4 18:06:30.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-360" for this suite. 09/04/23 18:06:30.286
------------------------------
â€¢ [2.155 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:06:28.145
    Sep  4 18:06:28.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename endpointslicemirroring 09/04/23 18:06:28.146
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:28.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:28.183
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 09/04/23 18:06:28.21
    Sep  4 18:06:28.227: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 09/04/23 18:06:30.234
    STEP: mirroring deletion of a custom Endpoint 09/04/23 18:06:30.255
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:06:30.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-360" for this suite. 09/04/23 18:06:30.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:06:30.3
Sep  4 18:06:30.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 18:06:30.301
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:30.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:30.335
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 09/04/23 18:06:30.345
Sep  4 18:06:30.365: INFO: Waiting up to 5m0s for pod "annotationupdateac53a99e-6a54-43d9-9cd5-0ee85035ffa0" in namespace "projected-6654" to be "running and ready"
Sep  4 18:06:30.371: INFO: Pod "annotationupdateac53a99e-6a54-43d9-9cd5-0ee85035ffa0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.236549ms
Sep  4 18:06:30.371: INFO: The phase of Pod annotationupdateac53a99e-6a54-43d9-9cd5-0ee85035ffa0 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:06:32.382: INFO: Pod "annotationupdateac53a99e-6a54-43d9-9cd5-0ee85035ffa0": Phase="Running", Reason="", readiness=true. Elapsed: 2.016798807s
Sep  4 18:06:32.382: INFO: The phase of Pod annotationupdateac53a99e-6a54-43d9-9cd5-0ee85035ffa0 is Running (Ready = true)
Sep  4 18:06:32.382: INFO: Pod "annotationupdateac53a99e-6a54-43d9-9cd5-0ee85035ffa0" satisfied condition "running and ready"
Sep  4 18:06:32.931: INFO: Successfully updated pod "annotationupdateac53a99e-6a54-43d9-9cd5-0ee85035ffa0"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 18:06:34.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6654" for this suite. 09/04/23 18:06:34.97
------------------------------
â€¢ [4.683 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:06:30.3
    Sep  4 18:06:30.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 18:06:30.301
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:30.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:30.335
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 09/04/23 18:06:30.345
    Sep  4 18:06:30.365: INFO: Waiting up to 5m0s for pod "annotationupdateac53a99e-6a54-43d9-9cd5-0ee85035ffa0" in namespace "projected-6654" to be "running and ready"
    Sep  4 18:06:30.371: INFO: Pod "annotationupdateac53a99e-6a54-43d9-9cd5-0ee85035ffa0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.236549ms
    Sep  4 18:06:30.371: INFO: The phase of Pod annotationupdateac53a99e-6a54-43d9-9cd5-0ee85035ffa0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:06:32.382: INFO: Pod "annotationupdateac53a99e-6a54-43d9-9cd5-0ee85035ffa0": Phase="Running", Reason="", readiness=true. Elapsed: 2.016798807s
    Sep  4 18:06:32.382: INFO: The phase of Pod annotationupdateac53a99e-6a54-43d9-9cd5-0ee85035ffa0 is Running (Ready = true)
    Sep  4 18:06:32.382: INFO: Pod "annotationupdateac53a99e-6a54-43d9-9cd5-0ee85035ffa0" satisfied condition "running and ready"
    Sep  4 18:06:32.931: INFO: Successfully updated pod "annotationupdateac53a99e-6a54-43d9-9cd5-0ee85035ffa0"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:06:34.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6654" for this suite. 09/04/23 18:06:34.97
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:06:34.987
Sep  4 18:06:34.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 18:06:34.989
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:35.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:35.02
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 09/04/23 18:06:35.027
Sep  4 18:06:35.040: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f" in namespace "projected-9452" to be "Succeeded or Failed"
Sep  4 18:06:35.045: INFO: Pod "downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.825576ms
Sep  4 18:06:37.056: INFO: Pod "downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f": Phase="Running", Reason="", readiness=false. Elapsed: 2.016226274s
Sep  4 18:06:39.114: INFO: Pod "downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074381642s
STEP: Saw pod success 09/04/23 18:06:39.118
Sep  4 18:06:39.118: INFO: Pod "downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f" satisfied condition "Succeeded or Failed"
Sep  4 18:06:39.126: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f container client-container: <nil>
STEP: delete the pod 09/04/23 18:06:39.136
Sep  4 18:06:39.159: INFO: Waiting for pod downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f to disappear
Sep  4 18:06:39.166: INFO: Pod downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 18:06:39.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9452" for this suite. 09/04/23 18:06:39.176
------------------------------
â€¢ [4.199 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:06:34.987
    Sep  4 18:06:34.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 18:06:34.989
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:35.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:35.02
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 09/04/23 18:06:35.027
    Sep  4 18:06:35.040: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f" in namespace "projected-9452" to be "Succeeded or Failed"
    Sep  4 18:06:35.045: INFO: Pod "downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.825576ms
    Sep  4 18:06:37.056: INFO: Pod "downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f": Phase="Running", Reason="", readiness=false. Elapsed: 2.016226274s
    Sep  4 18:06:39.114: INFO: Pod "downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074381642s
    STEP: Saw pod success 09/04/23 18:06:39.118
    Sep  4 18:06:39.118: INFO: Pod "downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f" satisfied condition "Succeeded or Failed"
    Sep  4 18:06:39.126: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f container client-container: <nil>
    STEP: delete the pod 09/04/23 18:06:39.136
    Sep  4 18:06:39.159: INFO: Waiting for pod downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f to disappear
    Sep  4 18:06:39.166: INFO: Pod downwardapi-volume-0e89bf4d-f695-42e2-81b7-35225ccad33f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:06:39.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9452" for this suite. 09/04/23 18:06:39.176
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:06:39.187
Sep  4 18:06:39.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename svcaccounts 09/04/23 18:06:39.189
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:39.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:39.218
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Sep  4 18:06:39.229: INFO: Got root ca configmap in namespace "svcaccounts-9922"
Sep  4 18:06:39.240: INFO: Deleted root ca configmap in namespace "svcaccounts-9922"
STEP: waiting for a new root ca configmap created 09/04/23 18:06:39.74
Sep  4 18:06:39.747: INFO: Recreated root ca configmap in namespace "svcaccounts-9922"
Sep  4 18:06:39.759: INFO: Updated root ca configmap in namespace "svcaccounts-9922"
STEP: waiting for the root ca configmap reconciled 09/04/23 18:06:40.266
Sep  4 18:06:40.278: INFO: Reconciled root ca configmap in namespace "svcaccounts-9922"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  4 18:06:40.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9922" for this suite. 09/04/23 18:06:40.286
------------------------------
â€¢ [1.113 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:06:39.187
    Sep  4 18:06:39.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename svcaccounts 09/04/23 18:06:39.189
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:39.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:39.218
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Sep  4 18:06:39.229: INFO: Got root ca configmap in namespace "svcaccounts-9922"
    Sep  4 18:06:39.240: INFO: Deleted root ca configmap in namespace "svcaccounts-9922"
    STEP: waiting for a new root ca configmap created 09/04/23 18:06:39.74
    Sep  4 18:06:39.747: INFO: Recreated root ca configmap in namespace "svcaccounts-9922"
    Sep  4 18:06:39.759: INFO: Updated root ca configmap in namespace "svcaccounts-9922"
    STEP: waiting for the root ca configmap reconciled 09/04/23 18:06:40.266
    Sep  4 18:06:40.278: INFO: Reconciled root ca configmap in namespace "svcaccounts-9922"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:06:40.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9922" for this suite. 09/04/23 18:06:40.286
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:06:40.303
Sep  4 18:06:40.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename secrets 09/04/23 18:06:40.304
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:40.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:40.329
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-c424a70e-8d7e-4b66-a167-0f1bc0af4e09 09/04/23 18:06:40.335
STEP: Creating a pod to test consume secrets 09/04/23 18:06:40.343
Sep  4 18:06:40.353: INFO: Waiting up to 5m0s for pod "pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de" in namespace "secrets-1030" to be "Succeeded or Failed"
Sep  4 18:06:40.371: INFO: Pod "pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de": Phase="Pending", Reason="", readiness=false. Elapsed: 18.389588ms
Sep  4 18:06:42.380: INFO: Pod "pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026500051s
Sep  4 18:06:44.377: INFO: Pod "pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024181953s
STEP: Saw pod success 09/04/23 18:06:44.378
Sep  4 18:06:44.378: INFO: Pod "pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de" satisfied condition "Succeeded or Failed"
Sep  4 18:06:44.388: INFO: Trying to get logs from node tenant-000001 pod pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 18:06:44.403
Sep  4 18:06:44.420: INFO: Waiting for pod pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de to disappear
Sep  4 18:06:44.425: INFO: Pod pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 18:06:44.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1030" for this suite. 09/04/23 18:06:44.433
------------------------------
â€¢ [4.144 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:06:40.303
    Sep  4 18:06:40.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename secrets 09/04/23 18:06:40.304
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:40.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:40.329
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-c424a70e-8d7e-4b66-a167-0f1bc0af4e09 09/04/23 18:06:40.335
    STEP: Creating a pod to test consume secrets 09/04/23 18:06:40.343
    Sep  4 18:06:40.353: INFO: Waiting up to 5m0s for pod "pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de" in namespace "secrets-1030" to be "Succeeded or Failed"
    Sep  4 18:06:40.371: INFO: Pod "pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de": Phase="Pending", Reason="", readiness=false. Elapsed: 18.389588ms
    Sep  4 18:06:42.380: INFO: Pod "pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026500051s
    Sep  4 18:06:44.377: INFO: Pod "pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024181953s
    STEP: Saw pod success 09/04/23 18:06:44.378
    Sep  4 18:06:44.378: INFO: Pod "pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de" satisfied condition "Succeeded or Failed"
    Sep  4 18:06:44.388: INFO: Trying to get logs from node tenant-000001 pod pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 18:06:44.403
    Sep  4 18:06:44.420: INFO: Waiting for pod pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de to disappear
    Sep  4 18:06:44.425: INFO: Pod pod-secrets-c2fe2485-04d7-4d04-a95e-80eb2b9159de no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:06:44.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1030" for this suite. 09/04/23 18:06:44.433
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:06:44.45
Sep  4 18:06:44.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename sysctl 09/04/23 18:06:44.451
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:44.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:44.485
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 09/04/23 18:06:44.493
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:06:44.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-2612" for this suite. 09/04/23 18:06:44.507
------------------------------
â€¢ [0.066 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:06:44.45
    Sep  4 18:06:44.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename sysctl 09/04/23 18:06:44.451
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:44.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:44.485
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 09/04/23 18:06:44.493
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:06:44.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-2612" for this suite. 09/04/23 18:06:44.507
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:06:44.526
Sep  4 18:06:44.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-probe 09/04/23 18:06:44.527
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:44.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:44.561
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Sep  4 18:06:44.583: INFO: Waiting up to 5m0s for pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d" in namespace "container-probe-7669" to be "running and ready"
Sep  4 18:06:44.598: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.363871ms
Sep  4 18:06:44.598: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:06:46.604: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 2.020873848s
Sep  4 18:06:46.605: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
Sep  4 18:06:48.605: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 4.021530028s
Sep  4 18:06:48.605: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
Sep  4 18:06:50.608: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 6.024974078s
Sep  4 18:06:50.609: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
Sep  4 18:06:52.605: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 8.021856552s
Sep  4 18:06:52.605: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
Sep  4 18:06:54.605: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 10.021501084s
Sep  4 18:06:54.605: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
Sep  4 18:06:56.606: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 12.022639168s
Sep  4 18:06:56.606: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
Sep  4 18:06:58.604: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 14.020596095s
Sep  4 18:06:58.604: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
Sep  4 18:07:00.604: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 16.021153952s
Sep  4 18:07:00.604: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
Sep  4 18:07:02.606: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 18.023124226s
Sep  4 18:07:02.607: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
Sep  4 18:07:04.604: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 20.020904351s
Sep  4 18:07:04.605: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
Sep  4 18:07:06.606: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=true. Elapsed: 22.022269162s
Sep  4 18:07:06.606: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = true)
Sep  4 18:07:06.606: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d" satisfied condition "running and ready"
Sep  4 18:07:06.613: INFO: Container started at 2023-09-04 18:06:45 +0000 UTC, pod became ready at 2023-09-04 18:07:04 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 18:07:06.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7669" for this suite. 09/04/23 18:07:06.625
------------------------------
â€¢ [SLOW TEST] [22.119 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:06:44.526
    Sep  4 18:06:44.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-probe 09/04/23 18:06:44.527
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:06:44.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:06:44.561
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Sep  4 18:06:44.583: INFO: Waiting up to 5m0s for pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d" in namespace "container-probe-7669" to be "running and ready"
    Sep  4 18:06:44.598: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.363871ms
    Sep  4 18:06:44.598: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:06:46.604: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 2.020873848s
    Sep  4 18:06:46.605: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
    Sep  4 18:06:48.605: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 4.021530028s
    Sep  4 18:06:48.605: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
    Sep  4 18:06:50.608: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 6.024974078s
    Sep  4 18:06:50.609: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
    Sep  4 18:06:52.605: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 8.021856552s
    Sep  4 18:06:52.605: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
    Sep  4 18:06:54.605: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 10.021501084s
    Sep  4 18:06:54.605: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
    Sep  4 18:06:56.606: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 12.022639168s
    Sep  4 18:06:56.606: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
    Sep  4 18:06:58.604: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 14.020596095s
    Sep  4 18:06:58.604: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
    Sep  4 18:07:00.604: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 16.021153952s
    Sep  4 18:07:00.604: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
    Sep  4 18:07:02.606: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 18.023124226s
    Sep  4 18:07:02.607: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
    Sep  4 18:07:04.604: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=false. Elapsed: 20.020904351s
    Sep  4 18:07:04.605: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = false)
    Sep  4 18:07:06.606: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d": Phase="Running", Reason="", readiness=true. Elapsed: 22.022269162s
    Sep  4 18:07:06.606: INFO: The phase of Pod test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d is Running (Ready = true)
    Sep  4 18:07:06.606: INFO: Pod "test-webserver-56cb5be0-3c32-4a8b-95e2-0b5efa1fe93d" satisfied condition "running and ready"
    Sep  4 18:07:06.613: INFO: Container started at 2023-09-04 18:06:45 +0000 UTC, pod became ready at 2023-09-04 18:07:04 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:07:06.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7669" for this suite. 09/04/23 18:07:06.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:07:06.651
Sep  4 18:07:06.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename runtimeclass 09/04/23 18:07:06.652
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:06.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:06.686
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Sep  4 18:07:06.714: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5597 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  4 18:07:06.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5597" for this suite. 09/04/23 18:07:06.749
------------------------------
â€¢ [0.109 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:07:06.651
    Sep  4 18:07:06.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename runtimeclass 09/04/23 18:07:06.652
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:06.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:06.686
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Sep  4 18:07:06.714: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5597 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:07:06.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5597" for this suite. 09/04/23 18:07:06.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:07:06.765
Sep  4 18:07:06.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 18:07:06.766
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:06.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:06.799
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 18:07:06.823
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:07:07.199
STEP: Deploying the webhook pod 09/04/23 18:07:07.209
STEP: Wait for the deployment to be ready 09/04/23 18:07:07.232
Sep  4 18:07:07.259: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 18:07:09.275
STEP: Verifying the service has paired with the endpoint 09/04/23 18:07:09.298
Sep  4 18:07:10.299: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/04/23 18:07:10.306
STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 18:07:10.307
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/04/23 18:07:10.34
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/04/23 18:07:11.36
STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 18:07:11.36
STEP: Having no error when timeout is longer than webhook latency 09/04/23 18:07:12.422
STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 18:07:12.423
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/04/23 18:07:17.486
STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 18:07:17.486
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:07:22.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1896" for this suite. 09/04/23 18:07:22.633
STEP: Destroying namespace "webhook-1896-markers" for this suite. 09/04/23 18:07:22.644
------------------------------
â€¢ [SLOW TEST] [15.891 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:07:06.765
    Sep  4 18:07:06.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 18:07:06.766
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:06.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:06.799
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 18:07:06.823
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:07:07.199
    STEP: Deploying the webhook pod 09/04/23 18:07:07.209
    STEP: Wait for the deployment to be ready 09/04/23 18:07:07.232
    Sep  4 18:07:07.259: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 18:07:09.275
    STEP: Verifying the service has paired with the endpoint 09/04/23 18:07:09.298
    Sep  4 18:07:10.299: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/04/23 18:07:10.306
    STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 18:07:10.307
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/04/23 18:07:10.34
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/04/23 18:07:11.36
    STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 18:07:11.36
    STEP: Having no error when timeout is longer than webhook latency 09/04/23 18:07:12.422
    STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 18:07:12.423
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/04/23 18:07:17.486
    STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 18:07:17.486
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:07:22.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1896" for this suite. 09/04/23 18:07:22.633
    STEP: Destroying namespace "webhook-1896-markers" for this suite. 09/04/23 18:07:22.644
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:07:22.656
Sep  4 18:07:22.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename runtimeclass 09/04/23 18:07:22.656
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:22.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:22.69
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Sep  4 18:07:22.721: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9460 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  4 18:07:22.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9460" for this suite. 09/04/23 18:07:22.755
------------------------------
â€¢ [0.120 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:07:22.656
    Sep  4 18:07:22.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename runtimeclass 09/04/23 18:07:22.656
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:22.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:22.69
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Sep  4 18:07:22.721: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9460 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:07:22.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9460" for this suite. 09/04/23 18:07:22.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:07:22.91
Sep  4 18:07:22.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename var-expansion 09/04/23 18:07:22.916
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:22.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:22.961
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 09/04/23 18:07:22.969
Sep  4 18:07:22.984: INFO: Waiting up to 5m0s for pod "var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812" in namespace "var-expansion-9157" to be "Succeeded or Failed"
Sep  4 18:07:23.000: INFO: Pod "var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812": Phase="Pending", Reason="", readiness=false. Elapsed: 15.803859ms
Sep  4 18:07:25.007: INFO: Pod "var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022546661s
Sep  4 18:07:27.007: INFO: Pod "var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023189231s
STEP: Saw pod success 09/04/23 18:07:27.008
Sep  4 18:07:27.008: INFO: Pod "var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812" satisfied condition "Succeeded or Failed"
Sep  4 18:07:27.016: INFO: Trying to get logs from node tenant-000001 pod var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812 container dapi-container: <nil>
STEP: delete the pod 09/04/23 18:07:27.028
Sep  4 18:07:27.054: INFO: Waiting for pod var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812 to disappear
Sep  4 18:07:27.060: INFO: Pod var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 18:07:27.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9157" for this suite. 09/04/23 18:07:27.073
------------------------------
â€¢ [4.176 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:07:22.91
    Sep  4 18:07:22.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename var-expansion 09/04/23 18:07:22.916
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:22.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:22.961
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 09/04/23 18:07:22.969
    Sep  4 18:07:22.984: INFO: Waiting up to 5m0s for pod "var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812" in namespace "var-expansion-9157" to be "Succeeded or Failed"
    Sep  4 18:07:23.000: INFO: Pod "var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812": Phase="Pending", Reason="", readiness=false. Elapsed: 15.803859ms
    Sep  4 18:07:25.007: INFO: Pod "var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022546661s
    Sep  4 18:07:27.007: INFO: Pod "var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023189231s
    STEP: Saw pod success 09/04/23 18:07:27.008
    Sep  4 18:07:27.008: INFO: Pod "var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812" satisfied condition "Succeeded or Failed"
    Sep  4 18:07:27.016: INFO: Trying to get logs from node tenant-000001 pod var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812 container dapi-container: <nil>
    STEP: delete the pod 09/04/23 18:07:27.028
    Sep  4 18:07:27.054: INFO: Waiting for pod var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812 to disappear
    Sep  4 18:07:27.060: INFO: Pod var-expansion-a7fd9f00-86a1-454c-95d0-77058f567812 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:07:27.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9157" for this suite. 09/04/23 18:07:27.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:07:27.094
Sep  4 18:07:27.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 18:07:27.095
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:27.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:27.123
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 18:07:27.131
Sep  4 18:07:27.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8846 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep  4 18:07:27.230: INFO: stderr: ""
Sep  4 18:07:27.230: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 09/04/23 18:07:27.23
Sep  4 18:07:27.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8846 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Sep  4 18:07:28.114: INFO: stderr: ""
Sep  4 18:07:28.114: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 18:07:28.114
Sep  4 18:07:28.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8846 delete pods e2e-test-httpd-pod'
Sep  4 18:07:31.174: INFO: stderr: ""
Sep  4 18:07:31.174: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 18:07:31.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8846" for this suite. 09/04/23 18:07:31.182
------------------------------
â€¢ [4.099 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:07:27.094
    Sep  4 18:07:27.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 18:07:27.095
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:27.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:27.123
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 18:07:27.131
    Sep  4 18:07:27.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8846 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep  4 18:07:27.230: INFO: stderr: ""
    Sep  4 18:07:27.230: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 09/04/23 18:07:27.23
    Sep  4 18:07:27.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8846 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Sep  4 18:07:28.114: INFO: stderr: ""
    Sep  4 18:07:28.114: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 18:07:28.114
    Sep  4 18:07:28.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8846 delete pods e2e-test-httpd-pod'
    Sep  4 18:07:31.174: INFO: stderr: ""
    Sep  4 18:07:31.174: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:07:31.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8846" for this suite. 09/04/23 18:07:31.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:07:31.193
Sep  4 18:07:31.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 18:07:31.195
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:31.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:31.231
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 09/04/23 18:07:31.241
Sep  4 18:07:31.258: INFO: Waiting up to 5m0s for pod "pod-91bcfe85-314f-44fa-aeb4-56cc507994d3" in namespace "emptydir-9568" to be "Succeeded or Failed"
Sep  4 18:07:31.266: INFO: Pod "pod-91bcfe85-314f-44fa-aeb4-56cc507994d3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.660238ms
Sep  4 18:07:33.271: INFO: Pod "pod-91bcfe85-314f-44fa-aeb4-56cc507994d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012993694s
Sep  4 18:07:35.272: INFO: Pod "pod-91bcfe85-314f-44fa-aeb4-56cc507994d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013894596s
STEP: Saw pod success 09/04/23 18:07:35.273
Sep  4 18:07:35.273: INFO: Pod "pod-91bcfe85-314f-44fa-aeb4-56cc507994d3" satisfied condition "Succeeded or Failed"
Sep  4 18:07:35.281: INFO: Trying to get logs from node tenant-000001 pod pod-91bcfe85-314f-44fa-aeb4-56cc507994d3 container test-container: <nil>
STEP: delete the pod 09/04/23 18:07:35.294
Sep  4 18:07:35.312: INFO: Waiting for pod pod-91bcfe85-314f-44fa-aeb4-56cc507994d3 to disappear
Sep  4 18:07:35.321: INFO: Pod pod-91bcfe85-314f-44fa-aeb4-56cc507994d3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 18:07:35.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9568" for this suite. 09/04/23 18:07:35.329
------------------------------
â€¢ [4.146 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:07:31.193
    Sep  4 18:07:31.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 18:07:31.195
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:31.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:31.231
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/04/23 18:07:31.241
    Sep  4 18:07:31.258: INFO: Waiting up to 5m0s for pod "pod-91bcfe85-314f-44fa-aeb4-56cc507994d3" in namespace "emptydir-9568" to be "Succeeded or Failed"
    Sep  4 18:07:31.266: INFO: Pod "pod-91bcfe85-314f-44fa-aeb4-56cc507994d3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.660238ms
    Sep  4 18:07:33.271: INFO: Pod "pod-91bcfe85-314f-44fa-aeb4-56cc507994d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012993694s
    Sep  4 18:07:35.272: INFO: Pod "pod-91bcfe85-314f-44fa-aeb4-56cc507994d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013894596s
    STEP: Saw pod success 09/04/23 18:07:35.273
    Sep  4 18:07:35.273: INFO: Pod "pod-91bcfe85-314f-44fa-aeb4-56cc507994d3" satisfied condition "Succeeded or Failed"
    Sep  4 18:07:35.281: INFO: Trying to get logs from node tenant-000001 pod pod-91bcfe85-314f-44fa-aeb4-56cc507994d3 container test-container: <nil>
    STEP: delete the pod 09/04/23 18:07:35.294
    Sep  4 18:07:35.312: INFO: Waiting for pod pod-91bcfe85-314f-44fa-aeb4-56cc507994d3 to disappear
    Sep  4 18:07:35.321: INFO: Pod pod-91bcfe85-314f-44fa-aeb4-56cc507994d3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:07:35.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9568" for this suite. 09/04/23 18:07:35.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:07:35.347
Sep  4 18:07:35.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename secrets 09/04/23 18:07:35.348
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:35.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:35.38
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-238f5dda-07e5-4fda-9831-b7b55ea668ee 09/04/23 18:07:35.387
STEP: Creating a pod to test consume secrets 09/04/23 18:07:35.398
Sep  4 18:07:35.413: INFO: Waiting up to 5m0s for pod "pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589" in namespace "secrets-7867" to be "Succeeded or Failed"
Sep  4 18:07:35.425: INFO: Pod "pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589": Phase="Pending", Reason="", readiness=false. Elapsed: 12.688026ms
Sep  4 18:07:37.432: INFO: Pod "pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019679628s
Sep  4 18:07:39.433: INFO: Pod "pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020353587s
STEP: Saw pod success 09/04/23 18:07:39.433
Sep  4 18:07:39.433: INFO: Pod "pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589" satisfied condition "Succeeded or Failed"
Sep  4 18:07:39.441: INFO: Trying to get logs from node tenant-000001 pod pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589 container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 18:07:39.452
Sep  4 18:07:39.468: INFO: Waiting for pod pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589 to disappear
Sep  4 18:07:39.473: INFO: Pod pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 18:07:39.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7867" for this suite. 09/04/23 18:07:39.484
------------------------------
â€¢ [4.150 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:07:35.347
    Sep  4 18:07:35.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename secrets 09/04/23 18:07:35.348
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:35.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:35.38
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-238f5dda-07e5-4fda-9831-b7b55ea668ee 09/04/23 18:07:35.387
    STEP: Creating a pod to test consume secrets 09/04/23 18:07:35.398
    Sep  4 18:07:35.413: INFO: Waiting up to 5m0s for pod "pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589" in namespace "secrets-7867" to be "Succeeded or Failed"
    Sep  4 18:07:35.425: INFO: Pod "pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589": Phase="Pending", Reason="", readiness=false. Elapsed: 12.688026ms
    Sep  4 18:07:37.432: INFO: Pod "pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019679628s
    Sep  4 18:07:39.433: INFO: Pod "pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020353587s
    STEP: Saw pod success 09/04/23 18:07:39.433
    Sep  4 18:07:39.433: INFO: Pod "pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589" satisfied condition "Succeeded or Failed"
    Sep  4 18:07:39.441: INFO: Trying to get logs from node tenant-000001 pod pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589 container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 18:07:39.452
    Sep  4 18:07:39.468: INFO: Waiting for pod pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589 to disappear
    Sep  4 18:07:39.473: INFO: Pod pod-secrets-db153274-6089-4ce6-b82b-a7d31cf90589 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:07:39.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7867" for this suite. 09/04/23 18:07:39.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:07:39.499
Sep  4 18:07:39.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename gc 09/04/23 18:07:39.5
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:39.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:39.538
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 09/04/23 18:07:39.55
STEP: delete the rc 09/04/23 18:07:44.577
STEP: wait for the rc to be deleted 09/04/23 18:07:44.605
Sep  4 18:07:45.651: INFO: 80 pods remaining
Sep  4 18:07:45.655: INFO: 80 pods has nil DeletionTimestamp
Sep  4 18:07:45.655: INFO: 
Sep  4 18:07:46.637: INFO: 70 pods remaining
Sep  4 18:07:46.637: INFO: 70 pods has nil DeletionTimestamp
Sep  4 18:07:46.637: INFO: 
Sep  4 18:07:47.650: INFO: 60 pods remaining
Sep  4 18:07:47.650: INFO: 60 pods has nil DeletionTimestamp
Sep  4 18:07:47.650: INFO: 
Sep  4 18:07:48.720: INFO: 40 pods remaining
Sep  4 18:07:48.720: INFO: 40 pods has nil DeletionTimestamp
Sep  4 18:07:48.720: INFO: 
Sep  4 18:07:49.858: INFO: 26 pods remaining
Sep  4 18:07:49.858: INFO: 26 pods has nil DeletionTimestamp
Sep  4 18:07:49.903: INFO: 
Sep  4 18:07:50.618: INFO: 20 pods remaining
Sep  4 18:07:50.618: INFO: 20 pods has nil DeletionTimestamp
Sep  4 18:07:50.618: INFO: 
STEP: Gathering metrics 09/04/23 18:07:51.629
W0904 18:07:51.663210      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep  4 18:07:51.663: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  4 18:07:51.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5721" for this suite. 09/04/23 18:07:51.692
------------------------------
â€¢ [SLOW TEST] [12.215 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:07:39.499
    Sep  4 18:07:39.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename gc 09/04/23 18:07:39.5
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:39.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:39.538
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 09/04/23 18:07:39.55
    STEP: delete the rc 09/04/23 18:07:44.577
    STEP: wait for the rc to be deleted 09/04/23 18:07:44.605
    Sep  4 18:07:45.651: INFO: 80 pods remaining
    Sep  4 18:07:45.655: INFO: 80 pods has nil DeletionTimestamp
    Sep  4 18:07:45.655: INFO: 
    Sep  4 18:07:46.637: INFO: 70 pods remaining
    Sep  4 18:07:46.637: INFO: 70 pods has nil DeletionTimestamp
    Sep  4 18:07:46.637: INFO: 
    Sep  4 18:07:47.650: INFO: 60 pods remaining
    Sep  4 18:07:47.650: INFO: 60 pods has nil DeletionTimestamp
    Sep  4 18:07:47.650: INFO: 
    Sep  4 18:07:48.720: INFO: 40 pods remaining
    Sep  4 18:07:48.720: INFO: 40 pods has nil DeletionTimestamp
    Sep  4 18:07:48.720: INFO: 
    Sep  4 18:07:49.858: INFO: 26 pods remaining
    Sep  4 18:07:49.858: INFO: 26 pods has nil DeletionTimestamp
    Sep  4 18:07:49.903: INFO: 
    Sep  4 18:07:50.618: INFO: 20 pods remaining
    Sep  4 18:07:50.618: INFO: 20 pods has nil DeletionTimestamp
    Sep  4 18:07:50.618: INFO: 
    STEP: Gathering metrics 09/04/23 18:07:51.629
    W0904 18:07:51.663210      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep  4 18:07:51.663: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:07:51.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5721" for this suite. 09/04/23 18:07:51.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:07:51.741
Sep  4 18:07:51.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pods 09/04/23 18:07:51.757
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:51.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:51.841
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Sep  4 18:07:51.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: creating the pod 09/04/23 18:07:51.849
STEP: submitting the pod to kubernetes 09/04/23 18:07:51.849
Sep  4 18:07:51.861: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8" in namespace "pods-2745" to be "running and ready"
Sep  4 18:07:51.873: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033277ms
Sep  4 18:07:51.874: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:07:53.880: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018508274s
Sep  4 18:07:53.880: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:07:55.880: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018334825s
Sep  4 18:07:55.880: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:07:57.881: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019143631s
Sep  4 18:07:57.881: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:07:59.881: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019605217s
Sep  4 18:07:59.881: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:08:01.880: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018826873s
Sep  4 18:08:01.880: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:08:03.879: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017614364s
Sep  4 18:08:03.879: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:08:05.881: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01920041s
Sep  4 18:08:05.881: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:08:07.882: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.020269527s
Sep  4 18:08:07.882: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:08:09.879: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Running", Reason="", readiness=true. Elapsed: 18.017948398s
Sep  4 18:08:09.880: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Running (Ready = true)
Sep  4 18:08:09.880: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 18:08:09.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2745" for this suite. 09/04/23 18:08:09.942
------------------------------
â€¢ [SLOW TEST] [18.218 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:07:51.741
    Sep  4 18:07:51.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pods 09/04/23 18:07:51.757
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:07:51.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:07:51.841
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Sep  4 18:07:51.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: creating the pod 09/04/23 18:07:51.849
    STEP: submitting the pod to kubernetes 09/04/23 18:07:51.849
    Sep  4 18:07:51.861: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8" in namespace "pods-2745" to be "running and ready"
    Sep  4 18:07:51.873: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033277ms
    Sep  4 18:07:51.874: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:07:53.880: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018508274s
    Sep  4 18:07:53.880: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:07:55.880: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018334825s
    Sep  4 18:07:55.880: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:07:57.881: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019143631s
    Sep  4 18:07:57.881: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:07:59.881: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019605217s
    Sep  4 18:07:59.881: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:08:01.880: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018826873s
    Sep  4 18:08:01.880: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:08:03.879: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017614364s
    Sep  4 18:08:03.879: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:08:05.881: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01920041s
    Sep  4 18:08:05.881: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:08:07.882: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.020269527s
    Sep  4 18:08:07.882: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:08:09.879: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8": Phase="Running", Reason="", readiness=true. Elapsed: 18.017948398s
    Sep  4 18:08:09.880: INFO: The phase of Pod pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8 is Running (Ready = true)
    Sep  4 18:08:09.880: INFO: Pod "pod-logs-websocket-0492aa7a-e187-405d-b1d6-b200369b4fe8" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:08:09.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2745" for this suite. 09/04/23 18:08:09.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:08:09.972
Sep  4 18:08:09.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename watch 09/04/23 18:08:09.973
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:08:09.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:08:10.003
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 09/04/23 18:08:10.01
STEP: creating a watch on configmaps with label B 09/04/23 18:08:10.014
STEP: creating a watch on configmaps with label A or B 09/04/23 18:08:10.018
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/04/23 18:08:10.022
Sep  4 18:08:10.031: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24859 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 18:08:10.031: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24859 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/04/23 18:08:10.032
Sep  4 18:08:10.044: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24860 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 18:08:10.044: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24860 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/04/23 18:08:10.044
Sep  4 18:08:10.059: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24861 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 18:08:10.060: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24861 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/04/23 18:08:10.06
Sep  4 18:08:10.071: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24862 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 18:08:10.072: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24862 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/04/23 18:08:10.072
Sep  4 18:08:10.081: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7136  6d0f61bd-f046-40b1-af57-33b21d23d42d 24863 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 18:08:10.082: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7136  6d0f61bd-f046-40b1-af57-33b21d23d42d 24863 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/04/23 18:08:20.082
Sep  4 18:08:20.094: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7136  6d0f61bd-f046-40b1-af57-33b21d23d42d 24891 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 18:08:20.095: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7136  6d0f61bd-f046-40b1-af57-33b21d23d42d 24891 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  4 18:08:30.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7136" for this suite. 09/04/23 18:08:30.108
------------------------------
â€¢ [SLOW TEST] [20.156 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:08:09.972
    Sep  4 18:08:09.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename watch 09/04/23 18:08:09.973
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:08:09.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:08:10.003
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 09/04/23 18:08:10.01
    STEP: creating a watch on configmaps with label B 09/04/23 18:08:10.014
    STEP: creating a watch on configmaps with label A or B 09/04/23 18:08:10.018
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/04/23 18:08:10.022
    Sep  4 18:08:10.031: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24859 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 18:08:10.031: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24859 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/04/23 18:08:10.032
    Sep  4 18:08:10.044: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24860 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 18:08:10.044: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24860 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/04/23 18:08:10.044
    Sep  4 18:08:10.059: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24861 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 18:08:10.060: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24861 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/04/23 18:08:10.06
    Sep  4 18:08:10.071: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24862 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 18:08:10.072: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7136  89e1623f-1210-4d16-b0b5-987ebda9cb78 24862 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/04/23 18:08:10.072
    Sep  4 18:08:10.081: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7136  6d0f61bd-f046-40b1-af57-33b21d23d42d 24863 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 18:08:10.082: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7136  6d0f61bd-f046-40b1-af57-33b21d23d42d 24863 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/04/23 18:08:20.082
    Sep  4 18:08:20.094: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7136  6d0f61bd-f046-40b1-af57-33b21d23d42d 24891 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 18:08:20.095: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7136  6d0f61bd-f046-40b1-af57-33b21d23d42d 24891 0 2023-09-04 18:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 18:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:08:30.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7136" for this suite. 09/04/23 18:08:30.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:08:30.136
Sep  4 18:08:30.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename taint-single-pod 09/04/23 18:08:30.138
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:08:30.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:08:30.173
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Sep  4 18:08:30.181: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  4 18:09:30.219: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Sep  4 18:09:30.227: INFO: Starting informer...
STEP: Starting pod... 09/04/23 18:09:30.227
Sep  4 18:09:30.258: INFO: Pod is running on tenant-000001. Tainting Node
STEP: Trying to apply a taint on the Node 09/04/23 18:09:30.258
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 18:09:30.291
STEP: Waiting short time to make sure Pod is queued for deletion 09/04/23 18:09:30.315
Sep  4 18:09:30.316: INFO: Pod wasn't evicted. Proceeding
Sep  4 18:09:30.316: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 18:09:30.377
STEP: Waiting some time to make sure that toleration time passed. 09/04/23 18:09:30.383
Sep  4 18:10:45.383: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:10:45.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-9160" for this suite. 09/04/23 18:10:45.394
------------------------------
â€¢ [SLOW TEST] [135.270 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:08:30.136
    Sep  4 18:08:30.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename taint-single-pod 09/04/23 18:08:30.138
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:08:30.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:08:30.173
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Sep  4 18:08:30.181: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  4 18:09:30.219: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Sep  4 18:09:30.227: INFO: Starting informer...
    STEP: Starting pod... 09/04/23 18:09:30.227
    Sep  4 18:09:30.258: INFO: Pod is running on tenant-000001. Tainting Node
    STEP: Trying to apply a taint on the Node 09/04/23 18:09:30.258
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 18:09:30.291
    STEP: Waiting short time to make sure Pod is queued for deletion 09/04/23 18:09:30.315
    Sep  4 18:09:30.316: INFO: Pod wasn't evicted. Proceeding
    Sep  4 18:09:30.316: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 18:09:30.377
    STEP: Waiting some time to make sure that toleration time passed. 09/04/23 18:09:30.383
    Sep  4 18:10:45.383: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:10:45.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-9160" for this suite. 09/04/23 18:10:45.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:10:45.415
Sep  4 18:10:45.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename resourcequota 09/04/23 18:10:45.416
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:10:45.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:10:45.447
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 09/04/23 18:10:45.46
STEP: Ensuring ResourceQuota status is calculated 09/04/23 18:10:45.467
STEP: Creating a ResourceQuota with not best effort scope 09/04/23 18:10:47.473
STEP: Ensuring ResourceQuota status is calculated 09/04/23 18:10:47.485
STEP: Creating a best-effort pod 09/04/23 18:10:49.491
STEP: Ensuring resource quota with best effort scope captures the pod usage 09/04/23 18:10:49.509
STEP: Ensuring resource quota with not best effort ignored the pod usage 09/04/23 18:10:51.519
STEP: Deleting the pod 09/04/23 18:10:53.525
STEP: Ensuring resource quota status released the pod usage 09/04/23 18:10:53.551
STEP: Creating a not best-effort pod 09/04/23 18:10:55.559
STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/04/23 18:10:55.578
STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/04/23 18:10:57.584
STEP: Deleting the pod 09/04/23 18:10:59.59
STEP: Ensuring resource quota status released the pod usage 09/04/23 18:10:59.607
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 18:11:01.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9382" for this suite. 09/04/23 18:11:01.626
------------------------------
â€¢ [SLOW TEST] [16.221 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:10:45.415
    Sep  4 18:10:45.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename resourcequota 09/04/23 18:10:45.416
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:10:45.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:10:45.447
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 09/04/23 18:10:45.46
    STEP: Ensuring ResourceQuota status is calculated 09/04/23 18:10:45.467
    STEP: Creating a ResourceQuota with not best effort scope 09/04/23 18:10:47.473
    STEP: Ensuring ResourceQuota status is calculated 09/04/23 18:10:47.485
    STEP: Creating a best-effort pod 09/04/23 18:10:49.491
    STEP: Ensuring resource quota with best effort scope captures the pod usage 09/04/23 18:10:49.509
    STEP: Ensuring resource quota with not best effort ignored the pod usage 09/04/23 18:10:51.519
    STEP: Deleting the pod 09/04/23 18:10:53.525
    STEP: Ensuring resource quota status released the pod usage 09/04/23 18:10:53.551
    STEP: Creating a not best-effort pod 09/04/23 18:10:55.559
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/04/23 18:10:55.578
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/04/23 18:10:57.584
    STEP: Deleting the pod 09/04/23 18:10:59.59
    STEP: Ensuring resource quota status released the pod usage 09/04/23 18:10:59.607
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:11:01.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9382" for this suite. 09/04/23 18:11:01.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:11:01.642
Sep  4 18:11:01.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename secrets 09/04/23 18:11:01.643
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:01.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:01.674
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-e5c36968-b927-4f21-a7c0-52626fcf15eb 09/04/23 18:11:01.681
STEP: Creating a pod to test consume secrets 09/04/23 18:11:01.691
Sep  4 18:11:01.708: INFO: Waiting up to 5m0s for pod "pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00" in namespace "secrets-3550" to be "Succeeded or Failed"
Sep  4 18:11:01.718: INFO: Pod "pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00": Phase="Pending", Reason="", readiness=false. Elapsed: 9.829254ms
Sep  4 18:11:03.730: INFO: Pod "pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021238201s
Sep  4 18:11:05.725: INFO: Pod "pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016682194s
STEP: Saw pod success 09/04/23 18:11:05.725
Sep  4 18:11:05.726: INFO: Pod "pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00" satisfied condition "Succeeded or Failed"
Sep  4 18:11:05.731: INFO: Trying to get logs from node tenant-000001 pod pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00 container secret-env-test: <nil>
STEP: delete the pod 09/04/23 18:11:05.77
Sep  4 18:11:05.792: INFO: Waiting for pod pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00 to disappear
Sep  4 18:11:05.797: INFO: Pod pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 18:11:05.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3550" for this suite. 09/04/23 18:11:05.806
------------------------------
â€¢ [4.173 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:11:01.642
    Sep  4 18:11:01.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename secrets 09/04/23 18:11:01.643
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:01.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:01.674
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-e5c36968-b927-4f21-a7c0-52626fcf15eb 09/04/23 18:11:01.681
    STEP: Creating a pod to test consume secrets 09/04/23 18:11:01.691
    Sep  4 18:11:01.708: INFO: Waiting up to 5m0s for pod "pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00" in namespace "secrets-3550" to be "Succeeded or Failed"
    Sep  4 18:11:01.718: INFO: Pod "pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00": Phase="Pending", Reason="", readiness=false. Elapsed: 9.829254ms
    Sep  4 18:11:03.730: INFO: Pod "pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021238201s
    Sep  4 18:11:05.725: INFO: Pod "pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016682194s
    STEP: Saw pod success 09/04/23 18:11:05.725
    Sep  4 18:11:05.726: INFO: Pod "pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00" satisfied condition "Succeeded or Failed"
    Sep  4 18:11:05.731: INFO: Trying to get logs from node tenant-000001 pod pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00 container secret-env-test: <nil>
    STEP: delete the pod 09/04/23 18:11:05.77
    Sep  4 18:11:05.792: INFO: Waiting for pod pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00 to disappear
    Sep  4 18:11:05.797: INFO: Pod pod-secrets-315d71fd-e6e6-4be1-b005-c9e2f75d8b00 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:11:05.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3550" for this suite. 09/04/23 18:11:05.806
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:11:05.818
Sep  4 18:11:05.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 18:11:05.82
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:05.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:05.849
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 09/04/23 18:11:05.857
Sep  4 18:11:05.868: INFO: Waiting up to 5m0s for pod "downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb" in namespace "downward-api-711" to be "Succeeded or Failed"
Sep  4 18:11:05.883: INFO: Pod "downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb": Phase="Pending", Reason="", readiness=false. Elapsed: 15.040085ms
Sep  4 18:11:07.889: INFO: Pod "downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021542081s
Sep  4 18:11:09.890: INFO: Pod "downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022379944s
STEP: Saw pod success 09/04/23 18:11:09.891
Sep  4 18:11:09.891: INFO: Pod "downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb" satisfied condition "Succeeded or Failed"
Sep  4 18:11:09.899: INFO: Trying to get logs from node tenant-000001 pod downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb container dapi-container: <nil>
STEP: delete the pod 09/04/23 18:11:09.911
Sep  4 18:11:09.929: INFO: Waiting for pod downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb to disappear
Sep  4 18:11:09.934: INFO: Pod downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  4 18:11:09.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-711" for this suite. 09/04/23 18:11:09.942
------------------------------
â€¢ [4.135 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:11:05.818
    Sep  4 18:11:05.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 18:11:05.82
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:05.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:05.849
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 09/04/23 18:11:05.857
    Sep  4 18:11:05.868: INFO: Waiting up to 5m0s for pod "downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb" in namespace "downward-api-711" to be "Succeeded or Failed"
    Sep  4 18:11:05.883: INFO: Pod "downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb": Phase="Pending", Reason="", readiness=false. Elapsed: 15.040085ms
    Sep  4 18:11:07.889: INFO: Pod "downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021542081s
    Sep  4 18:11:09.890: INFO: Pod "downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022379944s
    STEP: Saw pod success 09/04/23 18:11:09.891
    Sep  4 18:11:09.891: INFO: Pod "downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb" satisfied condition "Succeeded or Failed"
    Sep  4 18:11:09.899: INFO: Trying to get logs from node tenant-000001 pod downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb container dapi-container: <nil>
    STEP: delete the pod 09/04/23 18:11:09.911
    Sep  4 18:11:09.929: INFO: Waiting for pod downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb to disappear
    Sep  4 18:11:09.934: INFO: Pod downward-api-5a0ff326-b2a9-41e6-8dfa-ffbc020d24bb no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:11:09.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-711" for this suite. 09/04/23 18:11:09.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:11:09.96
Sep  4 18:11:09.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename lease-test 09/04/23 18:11:09.962
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:09.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:09.998
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Sep  4 18:11:10.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-2318" for this suite. 09/04/23 18:11:10.121
------------------------------
â€¢ [0.170 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:11:09.96
    Sep  4 18:11:09.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename lease-test 09/04/23 18:11:09.962
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:09.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:09.998
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:11:10.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-2318" for this suite. 09/04/23 18:11:10.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:11:10.138
Sep  4 18:11:10.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename gc 09/04/23 18:11:10.139
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:10.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:10.173
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Sep  4 18:11:10.259: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9c2f70de-3091-489f-b84f-a5a4dca7780e", Controller:(*bool)(0xc003adbb96), BlockOwnerDeletion:(*bool)(0xc003adbb97)}}
Sep  4 18:11:10.267: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f56adedf-d469-444d-aa4b-035096c33946", Controller:(*bool)(0xc003adbe2e), BlockOwnerDeletion:(*bool)(0xc003adbe2f)}}
Sep  4 18:11:10.279: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"9baf4421-0fbb-4ab3-91c0-50ea120d4115", Controller:(*bool)(0xc003d360de), BlockOwnerDeletion:(*bool)(0xc003d360df)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  4 18:11:15.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6880" for this suite. 09/04/23 18:11:15.312
------------------------------
â€¢ [SLOW TEST] [5.188 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:11:10.138
    Sep  4 18:11:10.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename gc 09/04/23 18:11:10.139
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:10.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:10.173
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Sep  4 18:11:10.259: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9c2f70de-3091-489f-b84f-a5a4dca7780e", Controller:(*bool)(0xc003adbb96), BlockOwnerDeletion:(*bool)(0xc003adbb97)}}
    Sep  4 18:11:10.267: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f56adedf-d469-444d-aa4b-035096c33946", Controller:(*bool)(0xc003adbe2e), BlockOwnerDeletion:(*bool)(0xc003adbe2f)}}
    Sep  4 18:11:10.279: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"9baf4421-0fbb-4ab3-91c0-50ea120d4115", Controller:(*bool)(0xc003d360de), BlockOwnerDeletion:(*bool)(0xc003d360df)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:11:15.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6880" for this suite. 09/04/23 18:11:15.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:11:15.327
Sep  4 18:11:15.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename job 09/04/23 18:11:15.328
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:15.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:15.356
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 09/04/23 18:11:15.363
STEP: Ensuring job reaches completions 09/04/23 18:11:15.373
STEP: Ensuring pods with index for job exist 09/04/23 18:11:25.382
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  4 18:11:25.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3971" for this suite. 09/04/23 18:11:25.401
------------------------------
â€¢ [SLOW TEST] [10.085 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:11:15.327
    Sep  4 18:11:15.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename job 09/04/23 18:11:15.328
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:15.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:15.356
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 09/04/23 18:11:15.363
    STEP: Ensuring job reaches completions 09/04/23 18:11:15.373
    STEP: Ensuring pods with index for job exist 09/04/23 18:11:25.382
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:11:25.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3971" for this suite. 09/04/23 18:11:25.401
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:11:25.417
Sep  4 18:11:25.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pods 09/04/23 18:11:25.419
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:25.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:25.452
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 09/04/23 18:11:25.459
Sep  4 18:11:25.474: INFO: Waiting up to 5m0s for pod "pod-jdx7r" in namespace "pods-1665" to be "running"
Sep  4 18:11:25.485: INFO: Pod "pod-jdx7r": Phase="Pending", Reason="", readiness=false. Elapsed: 10.791879ms
Sep  4 18:11:27.490: INFO: Pod "pod-jdx7r": Phase="Running", Reason="", readiness=true. Elapsed: 2.01652345s
Sep  4 18:11:27.491: INFO: Pod "pod-jdx7r" satisfied condition "running"
STEP: patching /status 09/04/23 18:11:27.491
Sep  4 18:11:27.518: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 18:11:27.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1665" for this suite. 09/04/23 18:11:27.527
------------------------------
â€¢ [2.121 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:11:25.417
    Sep  4 18:11:25.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pods 09/04/23 18:11:25.419
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:25.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:25.452
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 09/04/23 18:11:25.459
    Sep  4 18:11:25.474: INFO: Waiting up to 5m0s for pod "pod-jdx7r" in namespace "pods-1665" to be "running"
    Sep  4 18:11:25.485: INFO: Pod "pod-jdx7r": Phase="Pending", Reason="", readiness=false. Elapsed: 10.791879ms
    Sep  4 18:11:27.490: INFO: Pod "pod-jdx7r": Phase="Running", Reason="", readiness=true. Elapsed: 2.01652345s
    Sep  4 18:11:27.491: INFO: Pod "pod-jdx7r" satisfied condition "running"
    STEP: patching /status 09/04/23 18:11:27.491
    Sep  4 18:11:27.518: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:11:27.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1665" for this suite. 09/04/23 18:11:27.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:11:27.546
Sep  4 18:11:27.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename security-context-test 09/04/23 18:11:27.548
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:27.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:27.579
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Sep  4 18:11:27.599: INFO: Waiting up to 5m0s for pod "busybox-user-65534-f3359912-d89c-40fd-9c3c-ea50d1d10016" in namespace "security-context-test-4862" to be "Succeeded or Failed"
Sep  4 18:11:27.612: INFO: Pod "busybox-user-65534-f3359912-d89c-40fd-9c3c-ea50d1d10016": Phase="Pending", Reason="", readiness=false. Elapsed: 13.603882ms
Sep  4 18:11:29.619: INFO: Pod "busybox-user-65534-f3359912-d89c-40fd-9c3c-ea50d1d10016": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020131038s
Sep  4 18:11:31.620: INFO: Pod "busybox-user-65534-f3359912-d89c-40fd-9c3c-ea50d1d10016": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02133515s
Sep  4 18:11:31.620: INFO: Pod "busybox-user-65534-f3359912-d89c-40fd-9c3c-ea50d1d10016" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  4 18:11:31.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4862" for this suite. 09/04/23 18:11:31.631
------------------------------
â€¢ [4.094 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:11:27.546
    Sep  4 18:11:27.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename security-context-test 09/04/23 18:11:27.548
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:27.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:27.579
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Sep  4 18:11:27.599: INFO: Waiting up to 5m0s for pod "busybox-user-65534-f3359912-d89c-40fd-9c3c-ea50d1d10016" in namespace "security-context-test-4862" to be "Succeeded or Failed"
    Sep  4 18:11:27.612: INFO: Pod "busybox-user-65534-f3359912-d89c-40fd-9c3c-ea50d1d10016": Phase="Pending", Reason="", readiness=false. Elapsed: 13.603882ms
    Sep  4 18:11:29.619: INFO: Pod "busybox-user-65534-f3359912-d89c-40fd-9c3c-ea50d1d10016": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020131038s
    Sep  4 18:11:31.620: INFO: Pod "busybox-user-65534-f3359912-d89c-40fd-9c3c-ea50d1d10016": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02133515s
    Sep  4 18:11:31.620: INFO: Pod "busybox-user-65534-f3359912-d89c-40fd-9c3c-ea50d1d10016" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:11:31.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4862" for this suite. 09/04/23 18:11:31.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:11:31.647
Sep  4 18:11:31.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-probe 09/04/23 18:11:31.648
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:31.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:31.681
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe in namespace container-probe-4821 09/04/23 18:11:31.689
Sep  4 18:11:31.706: INFO: Waiting up to 5m0s for pod "liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe" in namespace "container-probe-4821" to be "not pending"
Sep  4 18:11:31.718: INFO: Pod "liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.770587ms
Sep  4 18:11:33.724: INFO: Pod "liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.018003945s
Sep  4 18:11:33.724: INFO: Pod "liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe" satisfied condition "not pending"
Sep  4 18:11:33.724: INFO: Started pod liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe in namespace container-probe-4821
STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 18:11:33.725
Sep  4 18:11:33.732: INFO: Initial restart count of pod liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe is 0
Sep  4 18:11:53.815: INFO: Restart count of pod container-probe-4821/liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe is now 1 (20.083280267s elapsed)
STEP: deleting the pod 09/04/23 18:11:53.816
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 18:11:53.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4821" for this suite. 09/04/23 18:11:53.874
------------------------------
â€¢ [SLOW TEST] [22.241 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:11:31.647
    Sep  4 18:11:31.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-probe 09/04/23 18:11:31.648
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:31.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:31.681
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe in namespace container-probe-4821 09/04/23 18:11:31.689
    Sep  4 18:11:31.706: INFO: Waiting up to 5m0s for pod "liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe" in namespace "container-probe-4821" to be "not pending"
    Sep  4 18:11:31.718: INFO: Pod "liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.770587ms
    Sep  4 18:11:33.724: INFO: Pod "liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.018003945s
    Sep  4 18:11:33.724: INFO: Pod "liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe" satisfied condition "not pending"
    Sep  4 18:11:33.724: INFO: Started pod liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe in namespace container-probe-4821
    STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 18:11:33.725
    Sep  4 18:11:33.732: INFO: Initial restart count of pod liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe is 0
    Sep  4 18:11:53.815: INFO: Restart count of pod container-probe-4821/liveness-a9fad825-4c9b-4e89-b31c-ef22c96e0dbe is now 1 (20.083280267s elapsed)
    STEP: deleting the pod 09/04/23 18:11:53.816
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:11:53.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4821" for this suite. 09/04/23 18:11:53.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:11:53.888
Sep  4 18:11:53.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 18:11:53.889
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:53.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:53.916
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 18:11:53.945
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:11:54.529
STEP: Deploying the webhook pod 09/04/23 18:11:54.55
STEP: Wait for the deployment to be ready 09/04/23 18:11:54.57
Sep  4 18:11:54.589: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 18:11:56.61
STEP: Verifying the service has paired with the endpoint 09/04/23 18:11:56.632
Sep  4 18:11:57.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Sep  4 18:11:57.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/04/23 18:11:58.156
STEP: Creating a custom resource that should be denied by the webhook 09/04/23 18:11:58.185
STEP: Creating a custom resource whose deletion would be denied by the webhook 09/04/23 18:12:00.229
STEP: Updating the custom resource with disallowed data should be denied 09/04/23 18:12:00.244
STEP: Deleting the custom resource should be denied 09/04/23 18:12:00.26
STEP: Remove the offending key and value from the custom resource data 09/04/23 18:12:00.272
STEP: Deleting the updated custom resource should be successful 09/04/23 18:12:00.291
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:12:00.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-460" for this suite. 09/04/23 18:12:00.951
STEP: Destroying namespace "webhook-460-markers" for this suite. 09/04/23 18:12:00.966
------------------------------
â€¢ [SLOW TEST] [7.123 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:11:53.888
    Sep  4 18:11:53.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 18:11:53.889
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:11:53.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:11:53.916
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 18:11:53.945
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:11:54.529
    STEP: Deploying the webhook pod 09/04/23 18:11:54.55
    STEP: Wait for the deployment to be ready 09/04/23 18:11:54.57
    Sep  4 18:11:54.589: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 18:11:56.61
    STEP: Verifying the service has paired with the endpoint 09/04/23 18:11:56.632
    Sep  4 18:11:57.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Sep  4 18:11:57.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/04/23 18:11:58.156
    STEP: Creating a custom resource that should be denied by the webhook 09/04/23 18:11:58.185
    STEP: Creating a custom resource whose deletion would be denied by the webhook 09/04/23 18:12:00.229
    STEP: Updating the custom resource with disallowed data should be denied 09/04/23 18:12:00.244
    STEP: Deleting the custom resource should be denied 09/04/23 18:12:00.26
    STEP: Remove the offending key and value from the custom resource data 09/04/23 18:12:00.272
    STEP: Deleting the updated custom resource should be successful 09/04/23 18:12:00.291
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:12:00.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-460" for this suite. 09/04/23 18:12:00.951
    STEP: Destroying namespace "webhook-460-markers" for this suite. 09/04/23 18:12:00.966
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:12:01.014
Sep  4 18:12:01.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename disruption 09/04/23 18:12:01.015
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:12:01.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:12:01.079
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 09/04/23 18:12:01.111
STEP: Updating PodDisruptionBudget status 09/04/23 18:12:03.126
STEP: Waiting for all pods to be running 09/04/23 18:12:03.141
Sep  4 18:12:03.156: INFO: running pods: 0 < 1
STEP: locating a running pod 09/04/23 18:12:05.166
STEP: Waiting for the pdb to be processed 09/04/23 18:12:05.189
STEP: Patching PodDisruptionBudget status 09/04/23 18:12:05.199
STEP: Waiting for the pdb to be processed 09/04/23 18:12:05.213
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  4 18:12:05.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4671" for this suite. 09/04/23 18:12:05.23
------------------------------
â€¢ [4.228 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:12:01.014
    Sep  4 18:12:01.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename disruption 09/04/23 18:12:01.015
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:12:01.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:12:01.079
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 09/04/23 18:12:01.111
    STEP: Updating PodDisruptionBudget status 09/04/23 18:12:03.126
    STEP: Waiting for all pods to be running 09/04/23 18:12:03.141
    Sep  4 18:12:03.156: INFO: running pods: 0 < 1
    STEP: locating a running pod 09/04/23 18:12:05.166
    STEP: Waiting for the pdb to be processed 09/04/23 18:12:05.189
    STEP: Patching PodDisruptionBudget status 09/04/23 18:12:05.199
    STEP: Waiting for the pdb to be processed 09/04/23 18:12:05.213
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:12:05.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4671" for this suite. 09/04/23 18:12:05.23
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:12:05.245
Sep  4 18:12:05.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 18:12:05.247
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:12:05.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:12:05.275
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 09/04/23 18:12:05.282
Sep  4 18:12:05.292: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5" in namespace "projected-4439" to be "Succeeded or Failed"
Sep  4 18:12:05.310: INFO: Pod "downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.839263ms
Sep  4 18:12:07.318: INFO: Pod "downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025229534s
Sep  4 18:12:09.320: INFO: Pod "downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027196933s
STEP: Saw pod success 09/04/23 18:12:09.32
Sep  4 18:12:09.320: INFO: Pod "downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5" satisfied condition "Succeeded or Failed"
Sep  4 18:12:09.327: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5 container client-container: <nil>
STEP: delete the pod 09/04/23 18:12:09.338
Sep  4 18:12:09.356: INFO: Waiting for pod downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5 to disappear
Sep  4 18:12:09.365: INFO: Pod downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 18:12:09.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4439" for this suite. 09/04/23 18:12:09.375
------------------------------
â€¢ [4.141 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:12:05.245
    Sep  4 18:12:05.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 18:12:05.247
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:12:05.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:12:05.275
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 09/04/23 18:12:05.282
    Sep  4 18:12:05.292: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5" in namespace "projected-4439" to be "Succeeded or Failed"
    Sep  4 18:12:05.310: INFO: Pod "downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.839263ms
    Sep  4 18:12:07.318: INFO: Pod "downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025229534s
    Sep  4 18:12:09.320: INFO: Pod "downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027196933s
    STEP: Saw pod success 09/04/23 18:12:09.32
    Sep  4 18:12:09.320: INFO: Pod "downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5" satisfied condition "Succeeded or Failed"
    Sep  4 18:12:09.327: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5 container client-container: <nil>
    STEP: delete the pod 09/04/23 18:12:09.338
    Sep  4 18:12:09.356: INFO: Waiting for pod downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5 to disappear
    Sep  4 18:12:09.365: INFO: Pod downwardapi-volume-c1bc92c7-f0ad-49ed-bc58-e9825ea1fde5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:12:09.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4439" for this suite. 09/04/23 18:12:09.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:12:09.4
Sep  4 18:12:09.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename var-expansion 09/04/23 18:12:09.402
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:12:09.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:12:09.466
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Sep  4 18:12:09.493: INFO: Waiting up to 2m0s for pod "var-expansion-9c7a4dd0-482c-43a1-b61b-3a8ab7e30834" in namespace "var-expansion-2308" to be "container 0 failed with reason CreateContainerConfigError"
Sep  4 18:12:09.502: INFO: Pod "var-expansion-9c7a4dd0-482c-43a1-b61b-3a8ab7e30834": Phase="Pending", Reason="", readiness=false. Elapsed: 9.056208ms
Sep  4 18:12:11.511: INFO: Pod "var-expansion-9c7a4dd0-482c-43a1-b61b-3a8ab7e30834": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017273085s
Sep  4 18:12:11.511: INFO: Pod "var-expansion-9c7a4dd0-482c-43a1-b61b-3a8ab7e30834" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep  4 18:12:11.511: INFO: Deleting pod "var-expansion-9c7a4dd0-482c-43a1-b61b-3a8ab7e30834" in namespace "var-expansion-2308"
Sep  4 18:12:11.546: INFO: Wait up to 5m0s for pod "var-expansion-9c7a4dd0-482c-43a1-b61b-3a8ab7e30834" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 18:12:13.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2308" for this suite. 09/04/23 18:12:13.573
------------------------------
â€¢ [4.185 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:12:09.4
    Sep  4 18:12:09.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename var-expansion 09/04/23 18:12:09.402
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:12:09.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:12:09.466
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Sep  4 18:12:09.493: INFO: Waiting up to 2m0s for pod "var-expansion-9c7a4dd0-482c-43a1-b61b-3a8ab7e30834" in namespace "var-expansion-2308" to be "container 0 failed with reason CreateContainerConfigError"
    Sep  4 18:12:09.502: INFO: Pod "var-expansion-9c7a4dd0-482c-43a1-b61b-3a8ab7e30834": Phase="Pending", Reason="", readiness=false. Elapsed: 9.056208ms
    Sep  4 18:12:11.511: INFO: Pod "var-expansion-9c7a4dd0-482c-43a1-b61b-3a8ab7e30834": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017273085s
    Sep  4 18:12:11.511: INFO: Pod "var-expansion-9c7a4dd0-482c-43a1-b61b-3a8ab7e30834" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep  4 18:12:11.511: INFO: Deleting pod "var-expansion-9c7a4dd0-482c-43a1-b61b-3a8ab7e30834" in namespace "var-expansion-2308"
    Sep  4 18:12:11.546: INFO: Wait up to 5m0s for pod "var-expansion-9c7a4dd0-482c-43a1-b61b-3a8ab7e30834" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:12:13.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2308" for this suite. 09/04/23 18:12:13.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:12:13.59
Sep  4 18:12:13.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 18:12:13.591
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:12:13.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:12:13.62
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-8bb76ef1-2fef-4d57-b3c2-21953f8a293c 09/04/23 18:12:13.627
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 18:12:13.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4464" for this suite. 09/04/23 18:12:13.638
------------------------------
â€¢ [0.067 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:12:13.59
    Sep  4 18:12:13.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 18:12:13.591
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:12:13.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:12:13.62
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-8bb76ef1-2fef-4d57-b3c2-21953f8a293c 09/04/23 18:12:13.627
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:12:13.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4464" for this suite. 09/04/23 18:12:13.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:12:13.664
Sep  4 18:12:13.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 18:12:13.665
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:12:13.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:12:13.694
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-7172 09/04/23 18:12:13.704
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7172 to expose endpoints map[] 09/04/23 18:12:13.723
Sep  4 18:12:13.746: INFO: successfully validated that service endpoint-test2 in namespace services-7172 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7172 09/04/23 18:12:13.746
Sep  4 18:12:13.763: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7172" to be "running and ready"
Sep  4 18:12:13.783: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.725342ms
Sep  4 18:12:13.783: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:12:15.790: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.026390198s
Sep  4 18:12:15.790: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  4 18:12:15.790: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7172 to expose endpoints map[pod1:[80]] 09/04/23 18:12:15.797
Sep  4 18:12:15.817: INFO: successfully validated that service endpoint-test2 in namespace services-7172 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 09/04/23 18:12:15.818
Sep  4 18:12:15.818: INFO: Creating new exec pod
Sep  4 18:12:15.830: INFO: Waiting up to 5m0s for pod "execpodfr8qm" in namespace "services-7172" to be "running"
Sep  4 18:12:15.849: INFO: Pod "execpodfr8qm": Phase="Pending", Reason="", readiness=false. Elapsed: 18.264328ms
Sep  4 18:12:17.854: INFO: Pod "execpodfr8qm": Phase="Running", Reason="", readiness=true. Elapsed: 2.023984471s
Sep  4 18:12:17.854: INFO: Pod "execpodfr8qm" satisfied condition "running"
Sep  4 18:12:18.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7172 exec execpodfr8qm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  4 18:12:19.039: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  4 18:12:19.039: INFO: stdout: ""
Sep  4 18:12:19.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7172 exec execpodfr8qm -- /bin/sh -x -c nc -v -z -w 2 10.96.244.132 80'
Sep  4 18:12:19.225: INFO: stderr: "+ nc -v -z -w 2 10.96.244.132 80\nConnection to 10.96.244.132 80 port [tcp/http] succeeded!\n"
Sep  4 18:12:19.225: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-7172 09/04/23 18:12:19.225
Sep  4 18:12:19.238: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7172" to be "running and ready"
Sep  4 18:12:19.245: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.498384ms
Sep  4 18:12:19.245: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:12:21.252: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013317057s
Sep  4 18:12:21.252: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  4 18:12:21.252: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7172 to expose endpoints map[pod1:[80] pod2:[80]] 09/04/23 18:12:21.261
Sep  4 18:12:21.286: INFO: successfully validated that service endpoint-test2 in namespace services-7172 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 09/04/23 18:12:21.286
Sep  4 18:12:22.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7172 exec execpodfr8qm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  4 18:12:22.491: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  4 18:12:22.491: INFO: stdout: ""
Sep  4 18:12:22.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7172 exec execpodfr8qm -- /bin/sh -x -c nc -v -z -w 2 10.96.244.132 80'
Sep  4 18:12:22.699: INFO: stderr: "+ nc -v -z -w 2 10.96.244.132 80\nConnection to 10.96.244.132 80 port [tcp/http] succeeded!\n"
Sep  4 18:12:22.699: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-7172 09/04/23 18:12:22.699
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7172 to expose endpoints map[pod2:[80]] 09/04/23 18:12:22.722
Sep  4 18:12:23.928: INFO: successfully validated that service endpoint-test2 in namespace services-7172 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 09/04/23 18:12:23.928
Sep  4 18:12:24.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7172 exec execpodfr8qm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  4 18:12:25.104: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  4 18:12:25.104: INFO: stdout: ""
Sep  4 18:12:25.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7172 exec execpodfr8qm -- /bin/sh -x -c nc -v -z -w 2 10.96.244.132 80'
Sep  4 18:12:25.294: INFO: stderr: "+ nc -v -z -w 2 10.96.244.132 80\nConnection to 10.96.244.132 80 port [tcp/http] succeeded!\n"
Sep  4 18:12:25.294: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-7172 09/04/23 18:12:25.294
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7172 to expose endpoints map[] 09/04/23 18:12:25.312
Sep  4 18:12:26.360: INFO: successfully validated that service endpoint-test2 in namespace services-7172 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 18:12:26.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7172" for this suite. 09/04/23 18:12:26.424
------------------------------
â€¢ [SLOW TEST] [12.774 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:12:13.664
    Sep  4 18:12:13.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 18:12:13.665
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:12:13.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:12:13.694
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-7172 09/04/23 18:12:13.704
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7172 to expose endpoints map[] 09/04/23 18:12:13.723
    Sep  4 18:12:13.746: INFO: successfully validated that service endpoint-test2 in namespace services-7172 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7172 09/04/23 18:12:13.746
    Sep  4 18:12:13.763: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7172" to be "running and ready"
    Sep  4 18:12:13.783: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.725342ms
    Sep  4 18:12:13.783: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:12:15.790: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.026390198s
    Sep  4 18:12:15.790: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  4 18:12:15.790: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7172 to expose endpoints map[pod1:[80]] 09/04/23 18:12:15.797
    Sep  4 18:12:15.817: INFO: successfully validated that service endpoint-test2 in namespace services-7172 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 09/04/23 18:12:15.818
    Sep  4 18:12:15.818: INFO: Creating new exec pod
    Sep  4 18:12:15.830: INFO: Waiting up to 5m0s for pod "execpodfr8qm" in namespace "services-7172" to be "running"
    Sep  4 18:12:15.849: INFO: Pod "execpodfr8qm": Phase="Pending", Reason="", readiness=false. Elapsed: 18.264328ms
    Sep  4 18:12:17.854: INFO: Pod "execpodfr8qm": Phase="Running", Reason="", readiness=true. Elapsed: 2.023984471s
    Sep  4 18:12:17.854: INFO: Pod "execpodfr8qm" satisfied condition "running"
    Sep  4 18:12:18.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7172 exec execpodfr8qm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  4 18:12:19.039: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  4 18:12:19.039: INFO: stdout: ""
    Sep  4 18:12:19.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7172 exec execpodfr8qm -- /bin/sh -x -c nc -v -z -w 2 10.96.244.132 80'
    Sep  4 18:12:19.225: INFO: stderr: "+ nc -v -z -w 2 10.96.244.132 80\nConnection to 10.96.244.132 80 port [tcp/http] succeeded!\n"
    Sep  4 18:12:19.225: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-7172 09/04/23 18:12:19.225
    Sep  4 18:12:19.238: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7172" to be "running and ready"
    Sep  4 18:12:19.245: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.498384ms
    Sep  4 18:12:19.245: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:12:21.252: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013317057s
    Sep  4 18:12:21.252: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  4 18:12:21.252: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7172 to expose endpoints map[pod1:[80] pod2:[80]] 09/04/23 18:12:21.261
    Sep  4 18:12:21.286: INFO: successfully validated that service endpoint-test2 in namespace services-7172 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 09/04/23 18:12:21.286
    Sep  4 18:12:22.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7172 exec execpodfr8qm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  4 18:12:22.491: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  4 18:12:22.491: INFO: stdout: ""
    Sep  4 18:12:22.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7172 exec execpodfr8qm -- /bin/sh -x -c nc -v -z -w 2 10.96.244.132 80'
    Sep  4 18:12:22.699: INFO: stderr: "+ nc -v -z -w 2 10.96.244.132 80\nConnection to 10.96.244.132 80 port [tcp/http] succeeded!\n"
    Sep  4 18:12:22.699: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-7172 09/04/23 18:12:22.699
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7172 to expose endpoints map[pod2:[80]] 09/04/23 18:12:22.722
    Sep  4 18:12:23.928: INFO: successfully validated that service endpoint-test2 in namespace services-7172 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 09/04/23 18:12:23.928
    Sep  4 18:12:24.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7172 exec execpodfr8qm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  4 18:12:25.104: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  4 18:12:25.104: INFO: stdout: ""
    Sep  4 18:12:25.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7172 exec execpodfr8qm -- /bin/sh -x -c nc -v -z -w 2 10.96.244.132 80'
    Sep  4 18:12:25.294: INFO: stderr: "+ nc -v -z -w 2 10.96.244.132 80\nConnection to 10.96.244.132 80 port [tcp/http] succeeded!\n"
    Sep  4 18:12:25.294: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-7172 09/04/23 18:12:25.294
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7172 to expose endpoints map[] 09/04/23 18:12:25.312
    Sep  4 18:12:26.360: INFO: successfully validated that service endpoint-test2 in namespace services-7172 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:12:26.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7172" for this suite. 09/04/23 18:12:26.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:12:26.451
Sep  4 18:12:26.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename sched-pred 09/04/23 18:12:26.453
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:12:26.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:12:26.48
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  4 18:12:26.487: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  4 18:12:26.499: INFO: Waiting for terminating namespaces to be deleted...
Sep  4 18:12:26.507: INFO: 
Logging pods the apiserver thinks is on node tenant-000001 before test
Sep  4 18:12:26.517: INFO: calico-node-dj2mb from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
Sep  4 18:12:26.517: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 18:12:26.517: INFO: konnectivity-agent-k8gs4 from kube-system started at 2023-09-04 18:09:31 +0000 UTC (1 container statuses recorded)
Sep  4 18:12:26.517: INFO: 	Container konnectivity-agent ready: true, restart count 0
Sep  4 18:12:26.517: INFO: kube-proxy-959c5 from kube-system started at 2023-09-04 17:14:03 +0000 UTC (1 container statuses recorded)
Sep  4 18:12:26.517: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 18:12:26.517: INFO: execpodfr8qm from services-7172 started at 2023-09-04 18:12:15 +0000 UTC (1 container statuses recorded)
Sep  4 18:12:26.518: INFO: 	Container agnhost-container ready: true, restart count 0
Sep  4 18:12:26.518: INFO: sonobuoy from sonobuoy started at 2023-09-04 17:16:08 +0000 UTC (1 container statuses recorded)
Sep  4 18:12:26.518: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  4 18:12:26.518: INFO: sonobuoy-e2e-job-8da28a692bc241e2 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
Sep  4 18:12:26.518: INFO: 	Container e2e ready: true, restart count 0
Sep  4 18:12:26.518: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  4 18:12:26.518: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-64p86 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
Sep  4 18:12:26.518: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  4 18:12:26.518: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  4 18:12:26.518: INFO: 
Logging pods the apiserver thinks is on node tenant-000003 before test
Sep  4 18:12:26.526: INFO: calico-kube-controllers-5f94594857-4wpdt from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 18:12:26.526: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  4 18:12:26.526: INFO: calico-node-wlgx4 from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
Sep  4 18:12:26.526: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 18:12:26.526: INFO: coredns-6bfc4dc876-rnzz9 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 18:12:26.526: INFO: 	Container coredns ready: true, restart count 0
Sep  4 18:12:26.526: INFO: coredns-6bfc4dc876-xp5ff from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 18:12:26.526: INFO: 	Container coredns ready: true, restart count 0
Sep  4 18:12:26.526: INFO: konnectivity-agent-j62r4 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 18:12:26.526: INFO: 	Container konnectivity-agent ready: true, restart count 0
Sep  4 18:12:26.526: INFO: kube-proxy-z2kxt from kube-system started at 2023-09-04 17:15:09 +0000 UTC (1 container statuses recorded)
Sep  4 18:12:26.526: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 18:12:26.526: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-7wr4k from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
Sep  4 18:12:26.526: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  4 18:12:26.526: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node tenant-000001 09/04/23 18:12:26.562
STEP: verifying the node has the label node tenant-000003 09/04/23 18:12:26.585
Sep  4 18:12:26.600: INFO: Pod calico-kube-controllers-5f94594857-4wpdt requesting resource cpu=0m on Node tenant-000003
Sep  4 18:12:26.601: INFO: Pod calico-node-dj2mb requesting resource cpu=250m on Node tenant-000001
Sep  4 18:12:26.601: INFO: Pod calico-node-wlgx4 requesting resource cpu=250m on Node tenant-000003
Sep  4 18:12:26.601: INFO: Pod coredns-6bfc4dc876-rnzz9 requesting resource cpu=100m on Node tenant-000003
Sep  4 18:12:26.601: INFO: Pod coredns-6bfc4dc876-xp5ff requesting resource cpu=100m on Node tenant-000003
Sep  4 18:12:26.601: INFO: Pod konnectivity-agent-j62r4 requesting resource cpu=0m on Node tenant-000003
Sep  4 18:12:26.603: INFO: Pod konnectivity-agent-k8gs4 requesting resource cpu=0m on Node tenant-000001
Sep  4 18:12:26.604: INFO: Pod kube-proxy-959c5 requesting resource cpu=0m on Node tenant-000001
Sep  4 18:12:26.604: INFO: Pod kube-proxy-z2kxt requesting resource cpu=0m on Node tenant-000003
Sep  4 18:12:26.604: INFO: Pod execpodfr8qm requesting resource cpu=0m on Node tenant-000001
Sep  4 18:12:26.604: INFO: Pod sonobuoy requesting resource cpu=0m on Node tenant-000001
Sep  4 18:12:26.604: INFO: Pod sonobuoy-e2e-job-8da28a692bc241e2 requesting resource cpu=0m on Node tenant-000001
Sep  4 18:12:26.604: INFO: Pod sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-64p86 requesting resource cpu=0m on Node tenant-000001
Sep  4 18:12:26.604: INFO: Pod sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-7wr4k requesting resource cpu=0m on Node tenant-000003
STEP: Starting Pods to consume most of the cluster CPU. 09/04/23 18:12:26.604
Sep  4 18:12:26.605: INFO: Creating a pod which consumes cpu=385m on Node tenant-000003
Sep  4 18:12:26.615: INFO: Creating a pod which consumes cpu=525m on Node tenant-000001
Sep  4 18:12:26.630: INFO: Waiting up to 5m0s for pod "filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9" in namespace "sched-pred-9723" to be "running"
Sep  4 18:12:26.636: INFO: Pod "filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.708697ms
Sep  4 18:12:28.642: INFO: Pod "filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9": Phase="Running", Reason="", readiness=true. Elapsed: 2.012027679s
Sep  4 18:12:28.642: INFO: Pod "filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9" satisfied condition "running"
Sep  4 18:12:28.643: INFO: Waiting up to 5m0s for pod "filler-pod-056f5392-a881-4eb0-a82d-558084c8629f" in namespace "sched-pred-9723" to be "running"
Sep  4 18:12:28.650: INFO: Pod "filler-pod-056f5392-a881-4eb0-a82d-558084c8629f": Phase="Running", Reason="", readiness=true. Elapsed: 7.585527ms
Sep  4 18:12:28.650: INFO: Pod "filler-pod-056f5392-a881-4eb0-a82d-558084c8629f" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 09/04/23 18:12:28.651
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-056f5392-a881-4eb0-a82d-558084c8629f.1781c4a4ba3d5f7a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9723/filler-pod-056f5392-a881-4eb0-a82d-558084c8629f to tenant-000001] 09/04/23 18:12:28.656
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-056f5392-a881-4eb0-a82d-558084c8629f.1781c4a4e5e79007], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/04/23 18:12:28.656
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-056f5392-a881-4eb0-a82d-558084c8629f.1781c4a4ea1c778d], Reason = [Created], Message = [Created container filler-pod-056f5392-a881-4eb0-a82d-558084c8629f] 09/04/23 18:12:28.657
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-056f5392-a881-4eb0-a82d-558084c8629f.1781c4a4efa27818], Reason = [Started], Message = [Started container filler-pod-056f5392-a881-4eb0-a82d-558084c8629f] 09/04/23 18:12:28.657
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9.1781c4a4b9090423], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9723/filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9 to tenant-000003] 09/04/23 18:12:28.657
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9.1781c4a4ea7228e7], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/04/23 18:12:28.657
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9.1781c4a504a6abf8], Reason = [Created], Message = [Created container filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9] 09/04/23 18:12:28.657
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9.1781c4a509d59b35], Reason = [Started], Message = [Started container filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9] 09/04/23 18:12:28.658
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1781c4a532fddda9], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 09/04/23 18:12:28.674
STEP: removing the label node off the node tenant-000001 09/04/23 18:12:29.677
STEP: verifying the node doesn't have the label node 09/04/23 18:12:29.699
STEP: removing the label node off the node tenant-000003 09/04/23 18:12:29.709
STEP: verifying the node doesn't have the label node 09/04/23 18:12:29.738
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:12:29.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9723" for this suite. 09/04/23 18:12:29.751
------------------------------
â€¢ [3.312 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:12:26.451
    Sep  4 18:12:26.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename sched-pred 09/04/23 18:12:26.453
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:12:26.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:12:26.48
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  4 18:12:26.487: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  4 18:12:26.499: INFO: Waiting for terminating namespaces to be deleted...
    Sep  4 18:12:26.507: INFO: 
    Logging pods the apiserver thinks is on node tenant-000001 before test
    Sep  4 18:12:26.517: INFO: calico-node-dj2mb from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
    Sep  4 18:12:26.517: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 18:12:26.517: INFO: konnectivity-agent-k8gs4 from kube-system started at 2023-09-04 18:09:31 +0000 UTC (1 container statuses recorded)
    Sep  4 18:12:26.517: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Sep  4 18:12:26.517: INFO: kube-proxy-959c5 from kube-system started at 2023-09-04 17:14:03 +0000 UTC (1 container statuses recorded)
    Sep  4 18:12:26.517: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 18:12:26.517: INFO: execpodfr8qm from services-7172 started at 2023-09-04 18:12:15 +0000 UTC (1 container statuses recorded)
    Sep  4 18:12:26.518: INFO: 	Container agnhost-container ready: true, restart count 0
    Sep  4 18:12:26.518: INFO: sonobuoy from sonobuoy started at 2023-09-04 17:16:08 +0000 UTC (1 container statuses recorded)
    Sep  4 18:12:26.518: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  4 18:12:26.518: INFO: sonobuoy-e2e-job-8da28a692bc241e2 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
    Sep  4 18:12:26.518: INFO: 	Container e2e ready: true, restart count 0
    Sep  4 18:12:26.518: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  4 18:12:26.518: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-64p86 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
    Sep  4 18:12:26.518: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  4 18:12:26.518: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  4 18:12:26.518: INFO: 
    Logging pods the apiserver thinks is on node tenant-000003 before test
    Sep  4 18:12:26.526: INFO: calico-kube-controllers-5f94594857-4wpdt from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 18:12:26.526: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  4 18:12:26.526: INFO: calico-node-wlgx4 from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
    Sep  4 18:12:26.526: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 18:12:26.526: INFO: coredns-6bfc4dc876-rnzz9 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 18:12:26.526: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 18:12:26.526: INFO: coredns-6bfc4dc876-xp5ff from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 18:12:26.526: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 18:12:26.526: INFO: konnectivity-agent-j62r4 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 18:12:26.526: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Sep  4 18:12:26.526: INFO: kube-proxy-z2kxt from kube-system started at 2023-09-04 17:15:09 +0000 UTC (1 container statuses recorded)
    Sep  4 18:12:26.526: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 18:12:26.526: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-7wr4k from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
    Sep  4 18:12:26.526: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  4 18:12:26.526: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node tenant-000001 09/04/23 18:12:26.562
    STEP: verifying the node has the label node tenant-000003 09/04/23 18:12:26.585
    Sep  4 18:12:26.600: INFO: Pod calico-kube-controllers-5f94594857-4wpdt requesting resource cpu=0m on Node tenant-000003
    Sep  4 18:12:26.601: INFO: Pod calico-node-dj2mb requesting resource cpu=250m on Node tenant-000001
    Sep  4 18:12:26.601: INFO: Pod calico-node-wlgx4 requesting resource cpu=250m on Node tenant-000003
    Sep  4 18:12:26.601: INFO: Pod coredns-6bfc4dc876-rnzz9 requesting resource cpu=100m on Node tenant-000003
    Sep  4 18:12:26.601: INFO: Pod coredns-6bfc4dc876-xp5ff requesting resource cpu=100m on Node tenant-000003
    Sep  4 18:12:26.601: INFO: Pod konnectivity-agent-j62r4 requesting resource cpu=0m on Node tenant-000003
    Sep  4 18:12:26.603: INFO: Pod konnectivity-agent-k8gs4 requesting resource cpu=0m on Node tenant-000001
    Sep  4 18:12:26.604: INFO: Pod kube-proxy-959c5 requesting resource cpu=0m on Node tenant-000001
    Sep  4 18:12:26.604: INFO: Pod kube-proxy-z2kxt requesting resource cpu=0m on Node tenant-000003
    Sep  4 18:12:26.604: INFO: Pod execpodfr8qm requesting resource cpu=0m on Node tenant-000001
    Sep  4 18:12:26.604: INFO: Pod sonobuoy requesting resource cpu=0m on Node tenant-000001
    Sep  4 18:12:26.604: INFO: Pod sonobuoy-e2e-job-8da28a692bc241e2 requesting resource cpu=0m on Node tenant-000001
    Sep  4 18:12:26.604: INFO: Pod sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-64p86 requesting resource cpu=0m on Node tenant-000001
    Sep  4 18:12:26.604: INFO: Pod sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-7wr4k requesting resource cpu=0m on Node tenant-000003
    STEP: Starting Pods to consume most of the cluster CPU. 09/04/23 18:12:26.604
    Sep  4 18:12:26.605: INFO: Creating a pod which consumes cpu=385m on Node tenant-000003
    Sep  4 18:12:26.615: INFO: Creating a pod which consumes cpu=525m on Node tenant-000001
    Sep  4 18:12:26.630: INFO: Waiting up to 5m0s for pod "filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9" in namespace "sched-pred-9723" to be "running"
    Sep  4 18:12:26.636: INFO: Pod "filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.708697ms
    Sep  4 18:12:28.642: INFO: Pod "filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9": Phase="Running", Reason="", readiness=true. Elapsed: 2.012027679s
    Sep  4 18:12:28.642: INFO: Pod "filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9" satisfied condition "running"
    Sep  4 18:12:28.643: INFO: Waiting up to 5m0s for pod "filler-pod-056f5392-a881-4eb0-a82d-558084c8629f" in namespace "sched-pred-9723" to be "running"
    Sep  4 18:12:28.650: INFO: Pod "filler-pod-056f5392-a881-4eb0-a82d-558084c8629f": Phase="Running", Reason="", readiness=true. Elapsed: 7.585527ms
    Sep  4 18:12:28.650: INFO: Pod "filler-pod-056f5392-a881-4eb0-a82d-558084c8629f" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 09/04/23 18:12:28.651
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-056f5392-a881-4eb0-a82d-558084c8629f.1781c4a4ba3d5f7a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9723/filler-pod-056f5392-a881-4eb0-a82d-558084c8629f to tenant-000001] 09/04/23 18:12:28.656
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-056f5392-a881-4eb0-a82d-558084c8629f.1781c4a4e5e79007], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/04/23 18:12:28.656
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-056f5392-a881-4eb0-a82d-558084c8629f.1781c4a4ea1c778d], Reason = [Created], Message = [Created container filler-pod-056f5392-a881-4eb0-a82d-558084c8629f] 09/04/23 18:12:28.657
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-056f5392-a881-4eb0-a82d-558084c8629f.1781c4a4efa27818], Reason = [Started], Message = [Started container filler-pod-056f5392-a881-4eb0-a82d-558084c8629f] 09/04/23 18:12:28.657
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9.1781c4a4b9090423], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9723/filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9 to tenant-000003] 09/04/23 18:12:28.657
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9.1781c4a4ea7228e7], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/04/23 18:12:28.657
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9.1781c4a504a6abf8], Reason = [Created], Message = [Created container filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9] 09/04/23 18:12:28.657
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9.1781c4a509d59b35], Reason = [Started], Message = [Started container filler-pod-fb6020a9-2093-412e-8e33-560c6e8acef9] 09/04/23 18:12:28.658
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1781c4a532fddda9], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 09/04/23 18:12:28.674
    STEP: removing the label node off the node tenant-000001 09/04/23 18:12:29.677
    STEP: verifying the node doesn't have the label node 09/04/23 18:12:29.699
    STEP: removing the label node off the node tenant-000003 09/04/23 18:12:29.709
    STEP: verifying the node doesn't have the label node 09/04/23 18:12:29.738
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:12:29.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9723" for this suite. 09/04/23 18:12:29.751
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:12:29.768
Sep  4 18:12:29.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename resourcequota 09/04/23 18:12:29.77
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:12:29.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:12:29.804
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-t4m2m" 09/04/23 18:12:29.821
Sep  4 18:12:29.838: INFO: Resource quota "e2e-rq-status-t4m2m" reports spec: hard cpu limit of 500m
Sep  4 18:12:29.838: INFO: Resource quota "e2e-rq-status-t4m2m" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-t4m2m" /status 09/04/23 18:12:29.838
STEP: Confirm /status for "e2e-rq-status-t4m2m" resourceQuota via watch 09/04/23 18:12:29.854
Sep  4 18:12:29.857: INFO: observed resourceQuota "e2e-rq-status-t4m2m" in namespace "resourcequota-857" with hard status: v1.ResourceList(nil)
Sep  4 18:12:29.857: INFO: Found resourceQuota "e2e-rq-status-t4m2m" in namespace "resourcequota-857" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Sep  4 18:12:29.857: INFO: ResourceQuota "e2e-rq-status-t4m2m" /status was updated
STEP: Patching hard spec values for cpu & memory 09/04/23 18:12:29.862
Sep  4 18:12:29.874: INFO: Resource quota "e2e-rq-status-t4m2m" reports spec: hard cpu limit of 1
Sep  4 18:12:29.874: INFO: Resource quota "e2e-rq-status-t4m2m" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-t4m2m" /status 09/04/23 18:12:29.874
STEP: Confirm /status for "e2e-rq-status-t4m2m" resourceQuota via watch 09/04/23 18:12:29.886
Sep  4 18:12:29.891: INFO: observed resourceQuota "e2e-rq-status-t4m2m" in namespace "resourcequota-857" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Sep  4 18:12:29.891: INFO: Found resourceQuota "e2e-rq-status-t4m2m" in namespace "resourcequota-857" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Sep  4 18:12:29.891: INFO: ResourceQuota "e2e-rq-status-t4m2m" /status was patched
STEP: Get "e2e-rq-status-t4m2m" /status 09/04/23 18:12:29.891
Sep  4 18:12:29.898: INFO: Resourcequota "e2e-rq-status-t4m2m" reports status: hard cpu of 1
Sep  4 18:12:29.898: INFO: Resourcequota "e2e-rq-status-t4m2m" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-t4m2m" /status before checking Spec is unchanged 09/04/23 18:12:29.903
Sep  4 18:12:29.914: INFO: Resourcequota "e2e-rq-status-t4m2m" reports status: hard cpu of 2
Sep  4 18:12:29.914: INFO: Resourcequota "e2e-rq-status-t4m2m" reports status: hard memory of 2Gi
Sep  4 18:12:29.918: INFO: Found resourceQuota "e2e-rq-status-t4m2m" in namespace "resourcequota-857" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Sep  4 18:15:19.933: INFO: ResourceQuota "e2e-rq-status-t4m2m" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 18:15:19.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-857" for this suite. 09/04/23 18:15:19.942
------------------------------
â€¢ [SLOW TEST] [170.187 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:12:29.768
    Sep  4 18:12:29.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename resourcequota 09/04/23 18:12:29.77
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:12:29.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:12:29.804
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-t4m2m" 09/04/23 18:12:29.821
    Sep  4 18:12:29.838: INFO: Resource quota "e2e-rq-status-t4m2m" reports spec: hard cpu limit of 500m
    Sep  4 18:12:29.838: INFO: Resource quota "e2e-rq-status-t4m2m" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-t4m2m" /status 09/04/23 18:12:29.838
    STEP: Confirm /status for "e2e-rq-status-t4m2m" resourceQuota via watch 09/04/23 18:12:29.854
    Sep  4 18:12:29.857: INFO: observed resourceQuota "e2e-rq-status-t4m2m" in namespace "resourcequota-857" with hard status: v1.ResourceList(nil)
    Sep  4 18:12:29.857: INFO: Found resourceQuota "e2e-rq-status-t4m2m" in namespace "resourcequota-857" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Sep  4 18:12:29.857: INFO: ResourceQuota "e2e-rq-status-t4m2m" /status was updated
    STEP: Patching hard spec values for cpu & memory 09/04/23 18:12:29.862
    Sep  4 18:12:29.874: INFO: Resource quota "e2e-rq-status-t4m2m" reports spec: hard cpu limit of 1
    Sep  4 18:12:29.874: INFO: Resource quota "e2e-rq-status-t4m2m" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-t4m2m" /status 09/04/23 18:12:29.874
    STEP: Confirm /status for "e2e-rq-status-t4m2m" resourceQuota via watch 09/04/23 18:12:29.886
    Sep  4 18:12:29.891: INFO: observed resourceQuota "e2e-rq-status-t4m2m" in namespace "resourcequota-857" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Sep  4 18:12:29.891: INFO: Found resourceQuota "e2e-rq-status-t4m2m" in namespace "resourcequota-857" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Sep  4 18:12:29.891: INFO: ResourceQuota "e2e-rq-status-t4m2m" /status was patched
    STEP: Get "e2e-rq-status-t4m2m" /status 09/04/23 18:12:29.891
    Sep  4 18:12:29.898: INFO: Resourcequota "e2e-rq-status-t4m2m" reports status: hard cpu of 1
    Sep  4 18:12:29.898: INFO: Resourcequota "e2e-rq-status-t4m2m" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-t4m2m" /status before checking Spec is unchanged 09/04/23 18:12:29.903
    Sep  4 18:12:29.914: INFO: Resourcequota "e2e-rq-status-t4m2m" reports status: hard cpu of 2
    Sep  4 18:12:29.914: INFO: Resourcequota "e2e-rq-status-t4m2m" reports status: hard memory of 2Gi
    Sep  4 18:12:29.918: INFO: Found resourceQuota "e2e-rq-status-t4m2m" in namespace "resourcequota-857" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Sep  4 18:15:19.933: INFO: ResourceQuota "e2e-rq-status-t4m2m" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:15:19.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-857" for this suite. 09/04/23 18:15:19.942
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:15:19.956
Sep  4 18:15:19.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename var-expansion 09/04/23 18:15:19.956
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:19.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:19.986
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 09/04/23 18:15:19.994
Sep  4 18:15:20.005: INFO: Waiting up to 5m0s for pod "var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c" in namespace "var-expansion-9933" to be "Succeeded or Failed"
Sep  4 18:15:20.020: INFO: Pod "var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.22628ms
Sep  4 18:15:22.029: INFO: Pod "var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023240825s
Sep  4 18:15:24.026: INFO: Pod "var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020503053s
STEP: Saw pod success 09/04/23 18:15:24.027
Sep  4 18:15:24.027: INFO: Pod "var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c" satisfied condition "Succeeded or Failed"
Sep  4 18:15:24.032: INFO: Trying to get logs from node tenant-000001 pod var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c container dapi-container: <nil>
STEP: delete the pod 09/04/23 18:15:24.068
Sep  4 18:15:24.087: INFO: Waiting for pod var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c to disappear
Sep  4 18:15:24.092: INFO: Pod var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 18:15:24.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9933" for this suite. 09/04/23 18:15:24.104
------------------------------
â€¢ [4.161 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:15:19.956
    Sep  4 18:15:19.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename var-expansion 09/04/23 18:15:19.956
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:19.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:19.986
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 09/04/23 18:15:19.994
    Sep  4 18:15:20.005: INFO: Waiting up to 5m0s for pod "var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c" in namespace "var-expansion-9933" to be "Succeeded or Failed"
    Sep  4 18:15:20.020: INFO: Pod "var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.22628ms
    Sep  4 18:15:22.029: INFO: Pod "var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023240825s
    Sep  4 18:15:24.026: INFO: Pod "var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020503053s
    STEP: Saw pod success 09/04/23 18:15:24.027
    Sep  4 18:15:24.027: INFO: Pod "var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c" satisfied condition "Succeeded or Failed"
    Sep  4 18:15:24.032: INFO: Trying to get logs from node tenant-000001 pod var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c container dapi-container: <nil>
    STEP: delete the pod 09/04/23 18:15:24.068
    Sep  4 18:15:24.087: INFO: Waiting for pod var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c to disappear
    Sep  4 18:15:24.092: INFO: Pod var-expansion-35b12ae5-8da2-4754-a9fc-8e7cdc9b746c no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:15:24.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9933" for this suite. 09/04/23 18:15:24.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:15:24.121
Sep  4 18:15:24.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename csiinlinevolumes 09/04/23 18:15:24.123
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:24.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:24.152
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 09/04/23 18:15:24.16
STEP: getting 09/04/23 18:15:24.187
STEP: listing 09/04/23 18:15:24.201
STEP: deleting 09/04/23 18:15:24.207
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Sep  4 18:15:24.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-6901" for this suite. 09/04/23 18:15:24.244
------------------------------
â€¢ [0.133 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:15:24.121
    Sep  4 18:15:24.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename csiinlinevolumes 09/04/23 18:15:24.123
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:24.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:24.152
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 09/04/23 18:15:24.16
    STEP: getting 09/04/23 18:15:24.187
    STEP: listing 09/04/23 18:15:24.201
    STEP: deleting 09/04/23 18:15:24.207
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:15:24.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-6901" for this suite. 09/04/23 18:15:24.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:15:24.257
Sep  4 18:15:24.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename endpointslice 09/04/23 18:15:24.258
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:24.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:24.29
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  4 18:15:24.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4351" for this suite. 09/04/23 18:15:24.412
------------------------------
â€¢ [0.171 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:15:24.257
    Sep  4 18:15:24.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename endpointslice 09/04/23 18:15:24.258
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:24.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:24.29
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:15:24.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4351" for this suite. 09/04/23 18:15:24.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:15:24.434
Sep  4 18:15:24.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename namespaces 09/04/23 18:15:24.437
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:24.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:24.475
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-5711" 09/04/23 18:15:24.485
Sep  4 18:15:24.499: INFO: Namespace "namespaces-5711" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"8135def3-9470-4ff0-8b37-b89f5d81782b", "kubernetes.io/metadata.name":"namespaces-5711", "namespaces-5711":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:15:24.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5711" for this suite. 09/04/23 18:15:24.509
------------------------------
â€¢ [0.088 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:15:24.434
    Sep  4 18:15:24.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename namespaces 09/04/23 18:15:24.437
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:24.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:24.475
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-5711" 09/04/23 18:15:24.485
    Sep  4 18:15:24.499: INFO: Namespace "namespaces-5711" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"8135def3-9470-4ff0-8b37-b89f5d81782b", "kubernetes.io/metadata.name":"namespaces-5711", "namespaces-5711":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:15:24.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5711" for this suite. 09/04/23 18:15:24.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:15:24.531
Sep  4 18:15:24.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 18:15:24.532
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:24.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:24.561
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-4c5669c6-17e3-4827-89c9-aa5e793dc9ab 09/04/23 18:15:24.568
STEP: Creating a pod to test consume configMaps 09/04/23 18:15:24.577
Sep  4 18:15:24.588: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd" in namespace "projected-8231" to be "Succeeded or Failed"
Sep  4 18:15:24.603: INFO: Pod "pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.479544ms
Sep  4 18:15:26.609: INFO: Pod "pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020891537s
Sep  4 18:15:28.609: INFO: Pod "pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020804612s
STEP: Saw pod success 09/04/23 18:15:28.609
Sep  4 18:15:28.610: INFO: Pod "pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd" satisfied condition "Succeeded or Failed"
Sep  4 18:15:28.617: INFO: Trying to get logs from node tenant-000001 pod pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd container agnhost-container: <nil>
STEP: delete the pod 09/04/23 18:15:28.63
Sep  4 18:15:28.647: INFO: Waiting for pod pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd to disappear
Sep  4 18:15:28.651: INFO: Pod pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 18:15:28.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8231" for this suite. 09/04/23 18:15:28.66
------------------------------
â€¢ [4.142 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:15:24.531
    Sep  4 18:15:24.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 18:15:24.532
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:24.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:24.561
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-4c5669c6-17e3-4827-89c9-aa5e793dc9ab 09/04/23 18:15:24.568
    STEP: Creating a pod to test consume configMaps 09/04/23 18:15:24.577
    Sep  4 18:15:24.588: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd" in namespace "projected-8231" to be "Succeeded or Failed"
    Sep  4 18:15:24.603: INFO: Pod "pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.479544ms
    Sep  4 18:15:26.609: INFO: Pod "pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020891537s
    Sep  4 18:15:28.609: INFO: Pod "pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020804612s
    STEP: Saw pod success 09/04/23 18:15:28.609
    Sep  4 18:15:28.610: INFO: Pod "pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd" satisfied condition "Succeeded or Failed"
    Sep  4 18:15:28.617: INFO: Trying to get logs from node tenant-000001 pod pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 18:15:28.63
    Sep  4 18:15:28.647: INFO: Waiting for pod pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd to disappear
    Sep  4 18:15:28.651: INFO: Pod pod-projected-configmaps-4e93f7dc-d6ff-4d85-af6d-27806671dacd no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:15:28.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8231" for this suite. 09/04/23 18:15:28.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:15:28.679
Sep  4 18:15:28.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename security-context 09/04/23 18:15:28.68
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:28.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:28.708
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/04/23 18:15:28.715
Sep  4 18:15:28.735: INFO: Waiting up to 5m0s for pod "security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206" in namespace "security-context-9548" to be "Succeeded or Failed"
Sep  4 18:15:28.740: INFO: Pod "security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206": Phase="Pending", Reason="", readiness=false. Elapsed: 5.733542ms
Sep  4 18:15:30.746: INFO: Pod "security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011241001s
Sep  4 18:15:32.783: INFO: Pod "security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048250431s
STEP: Saw pod success 09/04/23 18:15:32.783
Sep  4 18:15:32.784: INFO: Pod "security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206" satisfied condition "Succeeded or Failed"
Sep  4 18:15:32.801: INFO: Trying to get logs from node tenant-000001 pod security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206 container test-container: <nil>
STEP: delete the pod 09/04/23 18:15:32.86
Sep  4 18:15:32.893: INFO: Waiting for pod security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206 to disappear
Sep  4 18:15:32.901: INFO: Pod security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  4 18:15:32.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-9548" for this suite. 09/04/23 18:15:32.913
------------------------------
â€¢ [4.248 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:15:28.679
    Sep  4 18:15:28.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename security-context 09/04/23 18:15:28.68
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:28.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:28.708
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/04/23 18:15:28.715
    Sep  4 18:15:28.735: INFO: Waiting up to 5m0s for pod "security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206" in namespace "security-context-9548" to be "Succeeded or Failed"
    Sep  4 18:15:28.740: INFO: Pod "security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206": Phase="Pending", Reason="", readiness=false. Elapsed: 5.733542ms
    Sep  4 18:15:30.746: INFO: Pod "security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011241001s
    Sep  4 18:15:32.783: INFO: Pod "security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048250431s
    STEP: Saw pod success 09/04/23 18:15:32.783
    Sep  4 18:15:32.784: INFO: Pod "security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206" satisfied condition "Succeeded or Failed"
    Sep  4 18:15:32.801: INFO: Trying to get logs from node tenant-000001 pod security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206 container test-container: <nil>
    STEP: delete the pod 09/04/23 18:15:32.86
    Sep  4 18:15:32.893: INFO: Waiting for pod security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206 to disappear
    Sep  4 18:15:32.901: INFO: Pod security-context-d3d4d643-c300-47f1-a4d0-b245d2c62206 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:15:32.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-9548" for this suite. 09/04/23 18:15:32.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:15:32.932
Sep  4 18:15:32.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pod-network-test 09/04/23 18:15:32.933
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:32.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:32.966
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-1101 09/04/23 18:15:32.973
STEP: creating a selector 09/04/23 18:15:32.973
STEP: Creating the service pods in kubernetes 09/04/23 18:15:32.973
Sep  4 18:15:32.974: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  4 18:15:33.013: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1101" to be "running and ready"
Sep  4 18:15:33.020: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.418102ms
Sep  4 18:15:33.020: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:15:35.026: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01362129s
Sep  4 18:15:35.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:15:37.028: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015644004s
Sep  4 18:15:37.029: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:15:39.029: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016678639s
Sep  4 18:15:39.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:15:41.026: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01348646s
Sep  4 18:15:41.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:15:43.028: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014920925s
Sep  4 18:15:43.028: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:15:45.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.020321846s
Sep  4 18:15:45.033: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  4 18:15:45.033: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  4 18:15:45.038: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1101" to be "running and ready"
Sep  4 18:15:45.044: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.921521ms
Sep  4 18:15:45.044: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  4 18:15:45.044: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/04/23 18:15:45.052
Sep  4 18:15:45.081: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1101" to be "running"
Sep  4 18:15:45.093: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.930477ms
Sep  4 18:15:47.099: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017857298s
Sep  4 18:15:47.100: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  4 18:15:47.105: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1101" to be "running"
Sep  4 18:15:47.112: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.045159ms
Sep  4 18:15:47.113: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep  4 18:15:47.120: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  4 18:15:47.120: INFO: Going to poll 10.36.55.100 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep  4 18:15:47.126: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.36.55.100 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1101 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 18:15:47.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 18:15:47.126: INFO: ExecWithOptions: Clientset creation
Sep  4 18:15:47.126: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1101/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.36.55.100+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  4 18:15:48.240: INFO: Found all 1 expected endpoints: [netserver-0]
Sep  4 18:15:48.240: INFO: Going to poll 10.36.217.233 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep  4 18:15:48.246: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.36.217.233 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1101 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 18:15:48.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 18:15:48.247: INFO: ExecWithOptions: Clientset creation
Sep  4 18:15:48.247: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1101/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.36.217.233+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  4 18:15:49.373: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  4 18:15:49.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1101" for this suite. 09/04/23 18:15:49.382
------------------------------
â€¢ [SLOW TEST] [16.462 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:15:32.932
    Sep  4 18:15:32.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pod-network-test 09/04/23 18:15:32.933
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:32.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:32.966
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-1101 09/04/23 18:15:32.973
    STEP: creating a selector 09/04/23 18:15:32.973
    STEP: Creating the service pods in kubernetes 09/04/23 18:15:32.973
    Sep  4 18:15:32.974: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  4 18:15:33.013: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1101" to be "running and ready"
    Sep  4 18:15:33.020: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.418102ms
    Sep  4 18:15:33.020: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:15:35.026: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01362129s
    Sep  4 18:15:35.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:15:37.028: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015644004s
    Sep  4 18:15:37.029: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:15:39.029: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016678639s
    Sep  4 18:15:39.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:15:41.026: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01348646s
    Sep  4 18:15:41.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:15:43.028: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014920925s
    Sep  4 18:15:43.028: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:15:45.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.020321846s
    Sep  4 18:15:45.033: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  4 18:15:45.033: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  4 18:15:45.038: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1101" to be "running and ready"
    Sep  4 18:15:45.044: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.921521ms
    Sep  4 18:15:45.044: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  4 18:15:45.044: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/04/23 18:15:45.052
    Sep  4 18:15:45.081: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1101" to be "running"
    Sep  4 18:15:45.093: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.930477ms
    Sep  4 18:15:47.099: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017857298s
    Sep  4 18:15:47.100: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  4 18:15:47.105: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1101" to be "running"
    Sep  4 18:15:47.112: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.045159ms
    Sep  4 18:15:47.113: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep  4 18:15:47.120: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  4 18:15:47.120: INFO: Going to poll 10.36.55.100 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Sep  4 18:15:47.126: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.36.55.100 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1101 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 18:15:47.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 18:15:47.126: INFO: ExecWithOptions: Clientset creation
    Sep  4 18:15:47.126: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1101/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.36.55.100+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  4 18:15:48.240: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep  4 18:15:48.240: INFO: Going to poll 10.36.217.233 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Sep  4 18:15:48.246: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.36.217.233 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1101 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 18:15:48.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 18:15:48.247: INFO: ExecWithOptions: Clientset creation
    Sep  4 18:15:48.247: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1101/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.36.217.233+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  4 18:15:49.373: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:15:49.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1101" for this suite. 09/04/23 18:15:49.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:15:49.402
Sep  4 18:15:49.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename replication-controller 09/04/23 18:15:49.403
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:49.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:49.437
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-fv6xg" 09/04/23 18:15:49.446
Sep  4 18:15:49.456: INFO: Get Replication Controller "e2e-rc-fv6xg" to confirm replicas
Sep  4 18:15:50.504: INFO: Get Replication Controller "e2e-rc-fv6xg" to confirm replicas
Sep  4 18:15:50.512: INFO: Found 1 replicas for "e2e-rc-fv6xg" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-fv6xg" 09/04/23 18:15:50.512
STEP: Updating a scale subresource 09/04/23 18:15:50.519
STEP: Verifying replicas where modified for replication controller "e2e-rc-fv6xg" 09/04/23 18:15:50.529
Sep  4 18:15:50.529: INFO: Get Replication Controller "e2e-rc-fv6xg" to confirm replicas
Sep  4 18:15:51.553: INFO: Get Replication Controller "e2e-rc-fv6xg" to confirm replicas
Sep  4 18:15:51.562: INFO: Found 2 replicas for "e2e-rc-fv6xg" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  4 18:15:51.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1125" for this suite. 09/04/23 18:15:51.57
------------------------------
â€¢ [2.180 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:15:49.402
    Sep  4 18:15:49.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename replication-controller 09/04/23 18:15:49.403
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:49.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:49.437
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-fv6xg" 09/04/23 18:15:49.446
    Sep  4 18:15:49.456: INFO: Get Replication Controller "e2e-rc-fv6xg" to confirm replicas
    Sep  4 18:15:50.504: INFO: Get Replication Controller "e2e-rc-fv6xg" to confirm replicas
    Sep  4 18:15:50.512: INFO: Found 1 replicas for "e2e-rc-fv6xg" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-fv6xg" 09/04/23 18:15:50.512
    STEP: Updating a scale subresource 09/04/23 18:15:50.519
    STEP: Verifying replicas where modified for replication controller "e2e-rc-fv6xg" 09/04/23 18:15:50.529
    Sep  4 18:15:50.529: INFO: Get Replication Controller "e2e-rc-fv6xg" to confirm replicas
    Sep  4 18:15:51.553: INFO: Get Replication Controller "e2e-rc-fv6xg" to confirm replicas
    Sep  4 18:15:51.562: INFO: Found 2 replicas for "e2e-rc-fv6xg" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:15:51.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1125" for this suite. 09/04/23 18:15:51.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:15:51.591
Sep  4 18:15:51.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename dns 09/04/23 18:15:51.592
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:51.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:51.625
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 09/04/23 18:15:51.632
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3954.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3954.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 09/04/23 18:15:51.64
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3954.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3954.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 09/04/23 18:15:51.64
STEP: creating a pod to probe DNS 09/04/23 18:15:51.64
STEP: submitting the pod to kubernetes 09/04/23 18:15:51.641
Sep  4 18:15:51.659: INFO: Waiting up to 15m0s for pod "dns-test-9d83004c-4c1a-42e0-bb81-22d9516c21c7" in namespace "dns-3954" to be "running"
Sep  4 18:15:51.675: INFO: Pod "dns-test-9d83004c-4c1a-42e0-bb81-22d9516c21c7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.489455ms
Sep  4 18:15:53.681: INFO: Pod "dns-test-9d83004c-4c1a-42e0-bb81-22d9516c21c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021905791s
Sep  4 18:15:55.680: INFO: Pod "dns-test-9d83004c-4c1a-42e0-bb81-22d9516c21c7": Phase="Running", Reason="", readiness=true. Elapsed: 4.021300242s
Sep  4 18:15:55.680: INFO: Pod "dns-test-9d83004c-4c1a-42e0-bb81-22d9516c21c7" satisfied condition "running"
STEP: retrieving the pod 09/04/23 18:15:55.681
STEP: looking for the results for each expected name from probers 09/04/23 18:15:55.698
Sep  4 18:15:55.758: INFO: DNS probes using dns-3954/dns-test-9d83004c-4c1a-42e0-bb81-22d9516c21c7 succeeded

STEP: deleting the pod 09/04/23 18:15:55.759
STEP: deleting the test headless service 09/04/23 18:15:55.794
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 18:15:55.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3954" for this suite. 09/04/23 18:15:55.84
------------------------------
â€¢ [4.262 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:15:51.591
    Sep  4 18:15:51.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename dns 09/04/23 18:15:51.592
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:51.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:51.625
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 09/04/23 18:15:51.632
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3954.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3954.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     09/04/23 18:15:51.64
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3954.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3954.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     09/04/23 18:15:51.64
    STEP: creating a pod to probe DNS 09/04/23 18:15:51.64
    STEP: submitting the pod to kubernetes 09/04/23 18:15:51.641
    Sep  4 18:15:51.659: INFO: Waiting up to 15m0s for pod "dns-test-9d83004c-4c1a-42e0-bb81-22d9516c21c7" in namespace "dns-3954" to be "running"
    Sep  4 18:15:51.675: INFO: Pod "dns-test-9d83004c-4c1a-42e0-bb81-22d9516c21c7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.489455ms
    Sep  4 18:15:53.681: INFO: Pod "dns-test-9d83004c-4c1a-42e0-bb81-22d9516c21c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021905791s
    Sep  4 18:15:55.680: INFO: Pod "dns-test-9d83004c-4c1a-42e0-bb81-22d9516c21c7": Phase="Running", Reason="", readiness=true. Elapsed: 4.021300242s
    Sep  4 18:15:55.680: INFO: Pod "dns-test-9d83004c-4c1a-42e0-bb81-22d9516c21c7" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 18:15:55.681
    STEP: looking for the results for each expected name from probers 09/04/23 18:15:55.698
    Sep  4 18:15:55.758: INFO: DNS probes using dns-3954/dns-test-9d83004c-4c1a-42e0-bb81-22d9516c21c7 succeeded

    STEP: deleting the pod 09/04/23 18:15:55.759
    STEP: deleting the test headless service 09/04/23 18:15:55.794
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:15:55.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3954" for this suite. 09/04/23 18:15:55.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:15:55.853
Sep  4 18:15:55.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 18:15:55.855
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:55.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:55.888
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-4ccf59da-1368-4cc4-97f8-f42925de2e37 09/04/23 18:15:55.898
STEP: Creating a pod to test consume configMaps 09/04/23 18:15:55.906
Sep  4 18:15:55.922: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1" in namespace "configmap-5638" to be "Succeeded or Failed"
Sep  4 18:15:55.927: INFO: Pod "pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.637073ms
Sep  4 18:15:57.934: INFO: Pod "pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012547223s
Sep  4 18:15:59.936: INFO: Pod "pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013735721s
Sep  4 18:16:01.941: INFO: Pod "pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019279974s
STEP: Saw pod success 09/04/23 18:16:01.941
Sep  4 18:16:01.941: INFO: Pod "pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1" satisfied condition "Succeeded or Failed"
Sep  4 18:16:01.947: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 18:16:01.962
Sep  4 18:16:01.984: INFO: Waiting for pod pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1 to disappear
Sep  4 18:16:01.989: INFO: Pod pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 18:16:01.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5638" for this suite. 09/04/23 18:16:01.997
------------------------------
â€¢ [SLOW TEST] [6.155 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:15:55.853
    Sep  4 18:15:55.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 18:15:55.855
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:15:55.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:15:55.888
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-4ccf59da-1368-4cc4-97f8-f42925de2e37 09/04/23 18:15:55.898
    STEP: Creating a pod to test consume configMaps 09/04/23 18:15:55.906
    Sep  4 18:15:55.922: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1" in namespace "configmap-5638" to be "Succeeded or Failed"
    Sep  4 18:15:55.927: INFO: Pod "pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.637073ms
    Sep  4 18:15:57.934: INFO: Pod "pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012547223s
    Sep  4 18:15:59.936: INFO: Pod "pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013735721s
    Sep  4 18:16:01.941: INFO: Pod "pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019279974s
    STEP: Saw pod success 09/04/23 18:16:01.941
    Sep  4 18:16:01.941: INFO: Pod "pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1" satisfied condition "Succeeded or Failed"
    Sep  4 18:16:01.947: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 18:16:01.962
    Sep  4 18:16:01.984: INFO: Waiting for pod pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1 to disappear
    Sep  4 18:16:01.989: INFO: Pod pod-configmaps-cf1c1481-23dc-4e12-b4ed-1275331121f1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:16:01.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5638" for this suite. 09/04/23 18:16:01.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:16:02.028
Sep  4 18:16:02.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 18:16:02.029
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:16:02.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:16:02.084
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/04/23 18:16:02.1
Sep  4 18:16:02.119: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-395" to be "running and ready"
Sep  4 18:16:02.130: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.634883ms
Sep  4 18:16:02.130: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:16:04.137: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01737004s
Sep  4 18:16:04.137: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  4 18:16:04.137: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 09/04/23 18:16:04.142
Sep  4 18:16:04.155: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-395" to be "running and ready"
Sep  4 18:16:04.169: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 14.223713ms
Sep  4 18:16:04.169: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:16:06.176: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.020564945s
Sep  4 18:16:06.176: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Sep  4 18:16:06.176: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/04/23 18:16:06.184
Sep  4 18:16:06.197: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  4 18:16:06.212: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  4 18:16:08.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  4 18:16:08.220: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  4 18:16:10.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  4 18:16:10.220: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 09/04/23 18:16:10.22
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  4 18:16:10.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-395" for this suite. 09/04/23 18:16:10.266
------------------------------
â€¢ [SLOW TEST] [8.251 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:16:02.028
    Sep  4 18:16:02.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 18:16:02.029
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:16:02.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:16:02.084
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/04/23 18:16:02.1
    Sep  4 18:16:02.119: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-395" to be "running and ready"
    Sep  4 18:16:02.130: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.634883ms
    Sep  4 18:16:02.130: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:16:04.137: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01737004s
    Sep  4 18:16:04.137: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  4 18:16:04.137: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 09/04/23 18:16:04.142
    Sep  4 18:16:04.155: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-395" to be "running and ready"
    Sep  4 18:16:04.169: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 14.223713ms
    Sep  4 18:16:04.169: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:16:06.176: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.020564945s
    Sep  4 18:16:06.176: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Sep  4 18:16:06.176: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/04/23 18:16:06.184
    Sep  4 18:16:06.197: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  4 18:16:06.212: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep  4 18:16:08.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  4 18:16:08.220: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep  4 18:16:10.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  4 18:16:10.220: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 09/04/23 18:16:10.22
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:16:10.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-395" for this suite. 09/04/23 18:16:10.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:16:10.284
Sep  4 18:16:10.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename dns 09/04/23 18:16:10.285
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:16:10.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:16:10.31
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 09/04/23 18:16:10.317
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3283.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3283.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3283.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3283.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 141.8.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.8.141_udp@PTR;check="$$(dig +tcp +noall +answer +search 141.8.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.8.141_tcp@PTR;sleep 1; done
 09/04/23 18:16:10.357
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3283.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3283.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3283.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3283.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 141.8.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.8.141_udp@PTR;check="$$(dig +tcp +noall +answer +search 141.8.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.8.141_tcp@PTR;sleep 1; done
 09/04/23 18:16:10.36
STEP: creating a pod to probe DNS 09/04/23 18:16:10.36
STEP: submitting the pod to kubernetes 09/04/23 18:16:10.361
Sep  4 18:16:10.377: INFO: Waiting up to 15m0s for pod "dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de" in namespace "dns-3283" to be "running"
Sep  4 18:16:10.398: INFO: Pod "dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de": Phase="Pending", Reason="", readiness=false. Elapsed: 20.925631ms
Sep  4 18:16:12.407: INFO: Pod "dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de": Phase="Running", Reason="", readiness=true. Elapsed: 2.029492623s
Sep  4 18:16:12.407: INFO: Pod "dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de" satisfied condition "running"
STEP: retrieving the pod 09/04/23 18:16:12.407
STEP: looking for the results for each expected name from probers 09/04/23 18:16:12.412
Sep  4 18:16:12.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:12.435: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:12.441: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:12.449: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:12.497: INFO: Unable to read jessie_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:12.504: INFO: Unable to read jessie_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:12.514: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:12.521: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:12.555: INFO: Lookups using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de failed for: [wheezy_udp@dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_udp@dns-test-service.dns-3283.svc.cluster.local jessie_tcp@dns-test-service.dns-3283.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local]

Sep  4 18:16:17.565: INFO: Unable to read wheezy_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:17.576: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:17.586: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:17.593: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:17.664: INFO: Unable to read jessie_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:17.674: INFO: Unable to read jessie_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:17.687: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:17.693: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:17.738: INFO: Lookups using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de failed for: [wheezy_udp@dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_udp@dns-test-service.dns-3283.svc.cluster.local jessie_tcp@dns-test-service.dns-3283.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local]

Sep  4 18:16:22.565: INFO: Unable to read wheezy_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:22.572: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:22.579: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:22.589: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:22.628: INFO: Unable to read jessie_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:22.637: INFO: Unable to read jessie_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:22.643: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:22.652: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:22.690: INFO: Lookups using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de failed for: [wheezy_udp@dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_udp@dns-test-service.dns-3283.svc.cluster.local jessie_tcp@dns-test-service.dns-3283.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local]

Sep  4 18:16:27.567: INFO: Unable to read wheezy_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:27.574: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:27.584: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:27.593: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:27.644: INFO: Unable to read jessie_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:27.651: INFO: Unable to read jessie_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:27.662: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:27.671: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:27.715: INFO: Lookups using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de failed for: [wheezy_udp@dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_udp@dns-test-service.dns-3283.svc.cluster.local jessie_tcp@dns-test-service.dns-3283.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local]

Sep  4 18:16:32.565: INFO: Unable to read wheezy_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:32.576: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:32.583: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:32.591: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:32.630: INFO: Unable to read jessie_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:32.640: INFO: Unable to read jessie_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:32.649: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:32.656: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:32.689: INFO: Lookups using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de failed for: [wheezy_udp@dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_udp@dns-test-service.dns-3283.svc.cluster.local jessie_tcp@dns-test-service.dns-3283.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local]

Sep  4 18:16:37.567: INFO: Unable to read wheezy_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:37.575: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:37.584: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:37.597: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:37.655: INFO: Unable to read jessie_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:37.669: INFO: Unable to read jessie_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:37.676: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:37.683: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:37.723: INFO: Lookups using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de failed for: [wheezy_udp@dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_udp@dns-test-service.dns-3283.svc.cluster.local jessie_tcp@dns-test-service.dns-3283.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local]

Sep  4 18:16:42.565: INFO: Unable to read wheezy_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:42.583: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:42.590: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
Sep  4 18:16:42.716: INFO: Lookups using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de failed for: [wheezy_udp@dns-test-service.dns-3283.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local]

Sep  4 18:16:47.711: INFO: DNS probes using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de succeeded

STEP: deleting the pod 09/04/23 18:16:47.712
STEP: deleting the test service 09/04/23 18:16:47.793
STEP: deleting the test headless service 09/04/23 18:16:47.849
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 18:16:47.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3283" for this suite. 09/04/23 18:16:47.892
------------------------------
â€¢ [SLOW TEST] [37.619 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:16:10.284
    Sep  4 18:16:10.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename dns 09/04/23 18:16:10.285
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:16:10.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:16:10.31
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 09/04/23 18:16:10.317
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3283.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3283.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3283.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3283.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 141.8.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.8.141_udp@PTR;check="$$(dig +tcp +noall +answer +search 141.8.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.8.141_tcp@PTR;sleep 1; done
     09/04/23 18:16:10.357
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3283.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3283.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3283.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3283.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3283.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 141.8.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.8.141_udp@PTR;check="$$(dig +tcp +noall +answer +search 141.8.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.8.141_tcp@PTR;sleep 1; done
     09/04/23 18:16:10.36
    STEP: creating a pod to probe DNS 09/04/23 18:16:10.36
    STEP: submitting the pod to kubernetes 09/04/23 18:16:10.361
    Sep  4 18:16:10.377: INFO: Waiting up to 15m0s for pod "dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de" in namespace "dns-3283" to be "running"
    Sep  4 18:16:10.398: INFO: Pod "dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de": Phase="Pending", Reason="", readiness=false. Elapsed: 20.925631ms
    Sep  4 18:16:12.407: INFO: Pod "dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de": Phase="Running", Reason="", readiness=true. Elapsed: 2.029492623s
    Sep  4 18:16:12.407: INFO: Pod "dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 18:16:12.407
    STEP: looking for the results for each expected name from probers 09/04/23 18:16:12.412
    Sep  4 18:16:12.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:12.435: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:12.441: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:12.449: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:12.497: INFO: Unable to read jessie_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:12.504: INFO: Unable to read jessie_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:12.514: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:12.521: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:12.555: INFO: Lookups using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de failed for: [wheezy_udp@dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_udp@dns-test-service.dns-3283.svc.cluster.local jessie_tcp@dns-test-service.dns-3283.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local]

    Sep  4 18:16:17.565: INFO: Unable to read wheezy_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:17.576: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:17.586: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:17.593: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:17.664: INFO: Unable to read jessie_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:17.674: INFO: Unable to read jessie_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:17.687: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:17.693: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:17.738: INFO: Lookups using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de failed for: [wheezy_udp@dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_udp@dns-test-service.dns-3283.svc.cluster.local jessie_tcp@dns-test-service.dns-3283.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local]

    Sep  4 18:16:22.565: INFO: Unable to read wheezy_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:22.572: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:22.579: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:22.589: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:22.628: INFO: Unable to read jessie_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:22.637: INFO: Unable to read jessie_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:22.643: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:22.652: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:22.690: INFO: Lookups using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de failed for: [wheezy_udp@dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_udp@dns-test-service.dns-3283.svc.cluster.local jessie_tcp@dns-test-service.dns-3283.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local]

    Sep  4 18:16:27.567: INFO: Unable to read wheezy_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:27.574: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:27.584: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:27.593: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:27.644: INFO: Unable to read jessie_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:27.651: INFO: Unable to read jessie_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:27.662: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:27.671: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:27.715: INFO: Lookups using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de failed for: [wheezy_udp@dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_udp@dns-test-service.dns-3283.svc.cluster.local jessie_tcp@dns-test-service.dns-3283.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local]

    Sep  4 18:16:32.565: INFO: Unable to read wheezy_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:32.576: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:32.583: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:32.591: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:32.630: INFO: Unable to read jessie_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:32.640: INFO: Unable to read jessie_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:32.649: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:32.656: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:32.689: INFO: Lookups using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de failed for: [wheezy_udp@dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_udp@dns-test-service.dns-3283.svc.cluster.local jessie_tcp@dns-test-service.dns-3283.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local]

    Sep  4 18:16:37.567: INFO: Unable to read wheezy_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:37.575: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:37.584: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:37.597: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:37.655: INFO: Unable to read jessie_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:37.669: INFO: Unable to read jessie_tcp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:37.676: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:37.683: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:37.723: INFO: Lookups using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de failed for: [wheezy_udp@dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@dns-test-service.dns-3283.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_udp@dns-test-service.dns-3283.svc.cluster.local jessie_tcp@dns-test-service.dns-3283.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local]

    Sep  4 18:16:42.565: INFO: Unable to read wheezy_udp@dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:42.583: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:42.590: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local from pod dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de: the server could not find the requested resource (get pods dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de)
    Sep  4 18:16:42.716: INFO: Lookups using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de failed for: [wheezy_udp@dns-test-service.dns-3283.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3283.svc.cluster.local]

    Sep  4 18:16:47.711: INFO: DNS probes using dns-3283/dns-test-eb9ea0bd-1762-491e-b9cc-8e144ae004de succeeded

    STEP: deleting the pod 09/04/23 18:16:47.712
    STEP: deleting the test service 09/04/23 18:16:47.793
    STEP: deleting the test headless service 09/04/23 18:16:47.849
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:16:47.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3283" for this suite. 09/04/23 18:16:47.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:16:47.911
Sep  4 18:16:47.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 18:16:47.914
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:16:47.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:16:47.954
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Sep  4 18:16:47.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:16:48.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9536" for this suite. 09/04/23 18:16:48.619
------------------------------
â€¢ [0.722 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:16:47.911
    Sep  4 18:16:47.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 18:16:47.914
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:16:47.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:16:47.954
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Sep  4 18:16:47.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:16:48.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9536" for this suite. 09/04/23 18:16:48.619
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:16:48.636
Sep  4 18:16:48.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 18:16:48.638
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:16:48.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:16:48.664
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 18:16:48.688
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:16:49.769
STEP: Deploying the webhook pod 09/04/23 18:16:49.783
STEP: Wait for the deployment to be ready 09/04/23 18:16:49.804
Sep  4 18:16:49.825: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 18:16:51.844
STEP: Verifying the service has paired with the endpoint 09/04/23 18:16:51.861
Sep  4 18:16:52.868: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/04/23 18:16:52.882
STEP: create a pod that should be updated by the webhook 09/04/23 18:16:52.909
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:16:52.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1322" for this suite. 09/04/23 18:16:53.043
STEP: Destroying namespace "webhook-1322-markers" for this suite. 09/04/23 18:16:53.076
------------------------------
â€¢ [4.464 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:16:48.636
    Sep  4 18:16:48.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 18:16:48.638
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:16:48.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:16:48.664
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 18:16:48.688
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:16:49.769
    STEP: Deploying the webhook pod 09/04/23 18:16:49.783
    STEP: Wait for the deployment to be ready 09/04/23 18:16:49.804
    Sep  4 18:16:49.825: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 18:16:51.844
    STEP: Verifying the service has paired with the endpoint 09/04/23 18:16:51.861
    Sep  4 18:16:52.868: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/04/23 18:16:52.882
    STEP: create a pod that should be updated by the webhook 09/04/23 18:16:52.909
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:16:52.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1322" for this suite. 09/04/23 18:16:53.043
    STEP: Destroying namespace "webhook-1322-markers" for this suite. 09/04/23 18:16:53.076
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:16:53.118
Sep  4 18:16:53.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 18:16:53.121
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:16:53.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:16:53.15
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-b508b3c0-cbcd-4022-bf24-cd02bcb0d7fd 09/04/23 18:16:53.157
STEP: Creating a pod to test consume configMaps 09/04/23 18:16:53.17
Sep  4 18:16:53.187: INFO: Waiting up to 5m0s for pod "pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634" in namespace "configmap-3516" to be "Succeeded or Failed"
Sep  4 18:16:53.204: INFO: Pod "pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634": Phase="Pending", Reason="", readiness=false. Elapsed: 16.884052ms
Sep  4 18:16:55.210: INFO: Pod "pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634": Phase="Running", Reason="", readiness=false. Elapsed: 2.022842714s
Sep  4 18:16:57.211: INFO: Pod "pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023143905s
STEP: Saw pod success 09/04/23 18:16:57.211
Sep  4 18:16:57.211: INFO: Pod "pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634" satisfied condition "Succeeded or Failed"
Sep  4 18:16:57.224: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 18:16:57.235
Sep  4 18:16:57.257: INFO: Waiting for pod pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634 to disappear
Sep  4 18:16:57.262: INFO: Pod pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 18:16:57.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3516" for this suite. 09/04/23 18:16:57.273
------------------------------
â€¢ [4.169 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:16:53.118
    Sep  4 18:16:53.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 18:16:53.121
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:16:53.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:16:53.15
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-b508b3c0-cbcd-4022-bf24-cd02bcb0d7fd 09/04/23 18:16:53.157
    STEP: Creating a pod to test consume configMaps 09/04/23 18:16:53.17
    Sep  4 18:16:53.187: INFO: Waiting up to 5m0s for pod "pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634" in namespace "configmap-3516" to be "Succeeded or Failed"
    Sep  4 18:16:53.204: INFO: Pod "pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634": Phase="Pending", Reason="", readiness=false. Elapsed: 16.884052ms
    Sep  4 18:16:55.210: INFO: Pod "pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634": Phase="Running", Reason="", readiness=false. Elapsed: 2.022842714s
    Sep  4 18:16:57.211: INFO: Pod "pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023143905s
    STEP: Saw pod success 09/04/23 18:16:57.211
    Sep  4 18:16:57.211: INFO: Pod "pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634" satisfied condition "Succeeded or Failed"
    Sep  4 18:16:57.224: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 18:16:57.235
    Sep  4 18:16:57.257: INFO: Waiting for pod pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634 to disappear
    Sep  4 18:16:57.262: INFO: Pod pod-configmaps-0dc61a58-dd0b-4538-a818-7e47fb5b5634 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:16:57.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3516" for this suite. 09/04/23 18:16:57.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:16:57.288
Sep  4 18:16:57.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename secrets 09/04/23 18:16:57.289
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:16:57.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:16:57.32
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-0764e158-2891-4e78-9b47-02a05abd3864 09/04/23 18:16:57.327
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 18:16:57.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5890" for this suite. 09/04/23 18:16:57.338
------------------------------
â€¢ [0.064 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:16:57.288
    Sep  4 18:16:57.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename secrets 09/04/23 18:16:57.289
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:16:57.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:16:57.32
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-0764e158-2891-4e78-9b47-02a05abd3864 09/04/23 18:16:57.327
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:16:57.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5890" for this suite. 09/04/23 18:16:57.338
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:16:57.362
Sep  4 18:16:57.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 18:16:57.364
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:16:57.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:16:57.418
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 09/04/23 18:16:57.426
Sep  4 18:16:57.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2" in namespace "downward-api-6760" to be "Succeeded or Failed"
Sep  4 18:16:57.454: INFO: Pod "downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.26417ms
Sep  4 18:16:59.462: INFO: Pod "downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022574435s
Sep  4 18:17:01.460: INFO: Pod "downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020639303s
STEP: Saw pod success 09/04/23 18:17:01.461
Sep  4 18:17:01.462: INFO: Pod "downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2" satisfied condition "Succeeded or Failed"
Sep  4 18:17:01.469: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2 container client-container: <nil>
STEP: delete the pod 09/04/23 18:17:01.483
Sep  4 18:17:01.500: INFO: Waiting for pod downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2 to disappear
Sep  4 18:17:01.505: INFO: Pod downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 18:17:01.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6760" for this suite. 09/04/23 18:17:01.517
------------------------------
â€¢ [4.165 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:16:57.362
    Sep  4 18:16:57.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 18:16:57.364
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:16:57.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:16:57.418
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 09/04/23 18:16:57.426
    Sep  4 18:16:57.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2" in namespace "downward-api-6760" to be "Succeeded or Failed"
    Sep  4 18:16:57.454: INFO: Pod "downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.26417ms
    Sep  4 18:16:59.462: INFO: Pod "downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022574435s
    Sep  4 18:17:01.460: INFO: Pod "downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020639303s
    STEP: Saw pod success 09/04/23 18:17:01.461
    Sep  4 18:17:01.462: INFO: Pod "downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2" satisfied condition "Succeeded or Failed"
    Sep  4 18:17:01.469: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2 container client-container: <nil>
    STEP: delete the pod 09/04/23 18:17:01.483
    Sep  4 18:17:01.500: INFO: Waiting for pod downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2 to disappear
    Sep  4 18:17:01.505: INFO: Pod downwardapi-volume-02d74679-4823-4568-a37d-79a03dc75dd2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:17:01.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6760" for this suite. 09/04/23 18:17:01.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:17:01.533
Sep  4 18:17:01.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename resourcequota 09/04/23 18:17:01.534
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:17:01.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:17:01.565
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 09/04/23 18:17:01.573
STEP: Creating a ResourceQuota 09/04/23 18:17:06.581
STEP: Ensuring resource quota status is calculated 09/04/23 18:17:06.59
STEP: Creating a Service 09/04/23 18:17:08.597
STEP: Creating a NodePort Service 09/04/23 18:17:08.628
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/04/23 18:17:08.659
STEP: Ensuring resource quota status captures service creation 09/04/23 18:17:08.705
STEP: Deleting Services 09/04/23 18:17:10.713
STEP: Ensuring resource quota status released usage 09/04/23 18:17:10.773
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 18:17:12.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4481" for this suite. 09/04/23 18:17:12.826
------------------------------
â€¢ [SLOW TEST] [11.332 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:17:01.533
    Sep  4 18:17:01.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename resourcequota 09/04/23 18:17:01.534
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:17:01.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:17:01.565
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 09/04/23 18:17:01.573
    STEP: Creating a ResourceQuota 09/04/23 18:17:06.581
    STEP: Ensuring resource quota status is calculated 09/04/23 18:17:06.59
    STEP: Creating a Service 09/04/23 18:17:08.597
    STEP: Creating a NodePort Service 09/04/23 18:17:08.628
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/04/23 18:17:08.659
    STEP: Ensuring resource quota status captures service creation 09/04/23 18:17:08.705
    STEP: Deleting Services 09/04/23 18:17:10.713
    STEP: Ensuring resource quota status released usage 09/04/23 18:17:10.773
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:17:12.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4481" for this suite. 09/04/23 18:17:12.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:17:12.865
Sep  4 18:17:12.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename daemonsets 09/04/23 18:17:12.877
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:17:12.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:17:12.936
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 09/04/23 18:17:12.974
STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 18:17:12.984
Sep  4 18:17:12.999: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:17:12.999: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:17:14.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:17:14.031: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:17:15.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 18:17:15.014: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/04/23 18:17:15.021
Sep  4 18:17:15.087: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 18:17:15.087: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:17:16.105: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 18:17:16.105: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:17:17.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 18:17:17.104: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 09/04/23 18:17:17.104
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/04/23 18:17:17.113
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-235, will wait for the garbage collector to delete the pods 09/04/23 18:17:17.113
Sep  4 18:17:17.179: INFO: Deleting DaemonSet.extensions daemon-set took: 11.656642ms
Sep  4 18:17:17.280: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.224272ms
Sep  4 18:17:19.387: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:17:19.387: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  4 18:17:19.392: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27480"},"items":null}

Sep  4 18:17:19.399: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27480"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:17:19.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-235" for this suite. 09/04/23 18:17:19.428
------------------------------
â€¢ [SLOW TEST] [6.574 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:17:12.865
    Sep  4 18:17:12.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename daemonsets 09/04/23 18:17:12.877
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:17:12.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:17:12.936
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 09/04/23 18:17:12.974
    STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 18:17:12.984
    Sep  4 18:17:12.999: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:17:12.999: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:17:14.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:17:14.031: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:17:15.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 18:17:15.014: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/04/23 18:17:15.021
    Sep  4 18:17:15.087: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 18:17:15.087: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:17:16.105: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 18:17:16.105: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:17:17.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 18:17:17.104: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 09/04/23 18:17:17.104
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/04/23 18:17:17.113
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-235, will wait for the garbage collector to delete the pods 09/04/23 18:17:17.113
    Sep  4 18:17:17.179: INFO: Deleting DaemonSet.extensions daemon-set took: 11.656642ms
    Sep  4 18:17:17.280: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.224272ms
    Sep  4 18:17:19.387: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:17:19.387: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  4 18:17:19.392: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27480"},"items":null}

    Sep  4 18:17:19.399: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27480"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:17:19.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-235" for this suite. 09/04/23 18:17:19.428
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:17:19.444
Sep  4 18:17:19.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename watch 09/04/23 18:17:19.445
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:17:19.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:17:19.485
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 09/04/23 18:17:19.492
STEP: creating a new configmap 09/04/23 18:17:19.496
STEP: modifying the configmap once 09/04/23 18:17:19.502
STEP: changing the label value of the configmap 09/04/23 18:17:19.516
STEP: Expecting to observe a delete notification for the watched object 09/04/23 18:17:19.529
Sep  4 18:17:19.529: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9957  e158505e-9404-4f15-973d-7a921557a738 27485 0 2023-09-04 18:17:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 18:17:19.530: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9957  e158505e-9404-4f15-973d-7a921557a738 27486 0 2023-09-04 18:17:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 18:17:19.530: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9957  e158505e-9404-4f15-973d-7a921557a738 27487 0 2023-09-04 18:17:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 09/04/23 18:17:19.53
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/04/23 18:17:19.543
STEP: changing the label value of the configmap back 09/04/23 18:17:29.544
STEP: modifying the configmap a third time 09/04/23 18:17:29.561
STEP: deleting the configmap 09/04/23 18:17:29.576
STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/04/23 18:17:29.587
Sep  4 18:17:29.587: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9957  e158505e-9404-4f15-973d-7a921557a738 27532 0 2023-09-04 18:17:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 18:17:29.587: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9957  e158505e-9404-4f15-973d-7a921557a738 27533 0 2023-09-04 18:17:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 18:17:29.587: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9957  e158505e-9404-4f15-973d-7a921557a738 27534 0 2023-09-04 18:17:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  4 18:17:29.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9957" for this suite. 09/04/23 18:17:29.601
------------------------------
â€¢ [SLOW TEST] [10.172 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:17:19.444
    Sep  4 18:17:19.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename watch 09/04/23 18:17:19.445
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:17:19.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:17:19.485
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 09/04/23 18:17:19.492
    STEP: creating a new configmap 09/04/23 18:17:19.496
    STEP: modifying the configmap once 09/04/23 18:17:19.502
    STEP: changing the label value of the configmap 09/04/23 18:17:19.516
    STEP: Expecting to observe a delete notification for the watched object 09/04/23 18:17:19.529
    Sep  4 18:17:19.529: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9957  e158505e-9404-4f15-973d-7a921557a738 27485 0 2023-09-04 18:17:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 18:17:19.530: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9957  e158505e-9404-4f15-973d-7a921557a738 27486 0 2023-09-04 18:17:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 18:17:19.530: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9957  e158505e-9404-4f15-973d-7a921557a738 27487 0 2023-09-04 18:17:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 09/04/23 18:17:19.53
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/04/23 18:17:19.543
    STEP: changing the label value of the configmap back 09/04/23 18:17:29.544
    STEP: modifying the configmap a third time 09/04/23 18:17:29.561
    STEP: deleting the configmap 09/04/23 18:17:29.576
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/04/23 18:17:29.587
    Sep  4 18:17:29.587: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9957  e158505e-9404-4f15-973d-7a921557a738 27532 0 2023-09-04 18:17:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 18:17:29.587: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9957  e158505e-9404-4f15-973d-7a921557a738 27533 0 2023-09-04 18:17:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 18:17:29.587: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9957  e158505e-9404-4f15-973d-7a921557a738 27534 0 2023-09-04 18:17:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:17:29.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9957" for this suite. 09/04/23 18:17:29.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:17:29.622
Sep  4 18:17:29.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename statefulset 09/04/23 18:17:29.623
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:17:29.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:17:29.658
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2709 09/04/23 18:17:29.666
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-2709 09/04/23 18:17:29.681
Sep  4 18:17:29.696: INFO: Found 0 stateful pods, waiting for 1
Sep  4 18:17:39.707: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 09/04/23 18:17:39.722
STEP: Getting /status 09/04/23 18:17:39.732
Sep  4 18:17:39.738: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 09/04/23 18:17:39.738
Sep  4 18:17:39.758: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 09/04/23 18:17:39.758
Sep  4 18:17:39.762: INFO: Observed &StatefulSet event: ADDED
Sep  4 18:17:39.762: INFO: Found Statefulset ss in namespace statefulset-2709 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  4 18:17:39.762: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 09/04/23 18:17:39.762
Sep  4 18:17:39.763: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  4 18:17:39.783: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 09/04/23 18:17:39.783
Sep  4 18:17:39.788: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 18:17:39.788: INFO: Deleting all statefulset in ns statefulset-2709
Sep  4 18:17:39.797: INFO: Scaling statefulset ss to 0
Sep  4 18:17:49.828: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 18:17:49.834: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 18:17:49.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2709" for this suite. 09/04/23 18:17:49.869
------------------------------
â€¢ [SLOW TEST] [20.268 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:17:29.622
    Sep  4 18:17:29.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename statefulset 09/04/23 18:17:29.623
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:17:29.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:17:29.658
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2709 09/04/23 18:17:29.666
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-2709 09/04/23 18:17:29.681
    Sep  4 18:17:29.696: INFO: Found 0 stateful pods, waiting for 1
    Sep  4 18:17:39.707: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 09/04/23 18:17:39.722
    STEP: Getting /status 09/04/23 18:17:39.732
    Sep  4 18:17:39.738: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 09/04/23 18:17:39.738
    Sep  4 18:17:39.758: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 09/04/23 18:17:39.758
    Sep  4 18:17:39.762: INFO: Observed &StatefulSet event: ADDED
    Sep  4 18:17:39.762: INFO: Found Statefulset ss in namespace statefulset-2709 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  4 18:17:39.762: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 09/04/23 18:17:39.762
    Sep  4 18:17:39.763: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  4 18:17:39.783: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 09/04/23 18:17:39.783
    Sep  4 18:17:39.788: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 18:17:39.788: INFO: Deleting all statefulset in ns statefulset-2709
    Sep  4 18:17:39.797: INFO: Scaling statefulset ss to 0
    Sep  4 18:17:49.828: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 18:17:49.834: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:17:49.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2709" for this suite. 09/04/23 18:17:49.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:17:49.897
Sep  4 18:17:49.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename daemonsets 09/04/23 18:17:49.898
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:17:49.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:17:49.924
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Sep  4 18:17:49.965: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 09/04/23 18:17:49.973
Sep  4 18:17:49.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:17:49.981: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 09/04/23 18:17:49.981
Sep  4 18:17:50.014: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:17:50.014: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:17:51.021: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:17:51.021: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:17:52.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 18:17:52.022: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 09/04/23 18:17:52.028
Sep  4 18:17:52.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:17:52.094: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/04/23 18:17:52.094
Sep  4 18:17:52.130: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:17:52.130: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:17:53.139: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:17:53.140: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:17:54.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:17:54.141: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:17:55.139: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:17:55.139: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:17:56.137: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:17:56.138: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:17:57.139: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 18:17:57.139: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/04/23 18:17:57.151
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-708, will wait for the garbage collector to delete the pods 09/04/23 18:17:57.152
Sep  4 18:17:57.228: INFO: Deleting DaemonSet.extensions daemon-set took: 20.500032ms
Sep  4 18:17:57.329: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.118469ms
Sep  4 18:17:59.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:17:59.437: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  4 18:17:59.441: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27727"},"items":null}

Sep  4 18:17:59.447: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27727"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:17:59.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-708" for this suite. 09/04/23 18:17:59.493
------------------------------
â€¢ [SLOW TEST] [9.614 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:17:49.897
    Sep  4 18:17:49.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename daemonsets 09/04/23 18:17:49.898
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:17:49.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:17:49.924
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Sep  4 18:17:49.965: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 09/04/23 18:17:49.973
    Sep  4 18:17:49.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:17:49.981: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 09/04/23 18:17:49.981
    Sep  4 18:17:50.014: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:17:50.014: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:17:51.021: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:17:51.021: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:17:52.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 18:17:52.022: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 09/04/23 18:17:52.028
    Sep  4 18:17:52.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:17:52.094: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/04/23 18:17:52.094
    Sep  4 18:17:52.130: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:17:52.130: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:17:53.139: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:17:53.140: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:17:54.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:17:54.141: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:17:55.139: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:17:55.139: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:17:56.137: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:17:56.138: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:17:57.139: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 18:17:57.139: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/04/23 18:17:57.151
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-708, will wait for the garbage collector to delete the pods 09/04/23 18:17:57.152
    Sep  4 18:17:57.228: INFO: Deleting DaemonSet.extensions daemon-set took: 20.500032ms
    Sep  4 18:17:57.329: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.118469ms
    Sep  4 18:17:59.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:17:59.437: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  4 18:17:59.441: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27727"},"items":null}

    Sep  4 18:17:59.447: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27727"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:17:59.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-708" for this suite. 09/04/23 18:17:59.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:17:59.522
Sep  4 18:17:59.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename watch 09/04/23 18:17:59.525
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:17:59.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:17:59.565
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 09/04/23 18:17:59.575
STEP: modifying the configmap once 09/04/23 18:17:59.586
STEP: modifying the configmap a second time 09/04/23 18:17:59.603
STEP: deleting the configmap 09/04/23 18:17:59.619
STEP: creating a watch on configmaps from the resource version returned by the first update 09/04/23 18:17:59.63
STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/04/23 18:17:59.633
Sep  4 18:17:59.633: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2461  941482e1-fe90-4505-b5a0-7377179761c7 27735 0 2023-09-04 18:17:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 18:17:59.634: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2461  941482e1-fe90-4505-b5a0-7377179761c7 27736 0 2023-09-04 18:17:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  4 18:17:59.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2461" for this suite. 09/04/23 18:17:59.64
------------------------------
â€¢ [0.129 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:17:59.522
    Sep  4 18:17:59.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename watch 09/04/23 18:17:59.525
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:17:59.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:17:59.565
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 09/04/23 18:17:59.575
    STEP: modifying the configmap once 09/04/23 18:17:59.586
    STEP: modifying the configmap a second time 09/04/23 18:17:59.603
    STEP: deleting the configmap 09/04/23 18:17:59.619
    STEP: creating a watch on configmaps from the resource version returned by the first update 09/04/23 18:17:59.63
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/04/23 18:17:59.633
    Sep  4 18:17:59.633: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2461  941482e1-fe90-4505-b5a0-7377179761c7 27735 0 2023-09-04 18:17:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 18:17:59.634: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2461  941482e1-fe90-4505-b5a0-7377179761c7 27736 0 2023-09-04 18:17:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-04 18:17:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:17:59.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2461" for this suite. 09/04/23 18:17:59.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:17:59.657
Sep  4 18:17:59.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-probe 09/04/23 18:17:59.659
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:17:59.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:17:59.689
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411 in namespace container-probe-635 09/04/23 18:17:59.697
Sep  4 18:17:59.712: INFO: Waiting up to 5m0s for pod "busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411" in namespace "container-probe-635" to be "not pending"
Sep  4 18:17:59.717: INFO: Pod "busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411": Phase="Pending", Reason="", readiness=false. Elapsed: 4.6237ms
Sep  4 18:18:01.725: INFO: Pod "busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411": Phase="Running", Reason="", readiness=true. Elapsed: 2.012633171s
Sep  4 18:18:01.725: INFO: Pod "busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411" satisfied condition "not pending"
Sep  4 18:18:01.725: INFO: Started pod busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411 in namespace container-probe-635
STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 18:18:01.725
Sep  4 18:18:01.733: INFO: Initial restart count of pod busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411 is 0
Sep  4 18:18:51.926: INFO: Restart count of pod container-probe-635/busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411 is now 1 (50.193568092s elapsed)
STEP: deleting the pod 09/04/23 18:18:51.927
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 18:18:51.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-635" for this suite. 09/04/23 18:18:51.986
------------------------------
â€¢ [SLOW TEST] [52.343 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:17:59.657
    Sep  4 18:17:59.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-probe 09/04/23 18:17:59.659
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:17:59.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:17:59.689
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411 in namespace container-probe-635 09/04/23 18:17:59.697
    Sep  4 18:17:59.712: INFO: Waiting up to 5m0s for pod "busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411" in namespace "container-probe-635" to be "not pending"
    Sep  4 18:17:59.717: INFO: Pod "busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411": Phase="Pending", Reason="", readiness=false. Elapsed: 4.6237ms
    Sep  4 18:18:01.725: INFO: Pod "busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411": Phase="Running", Reason="", readiness=true. Elapsed: 2.012633171s
    Sep  4 18:18:01.725: INFO: Pod "busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411" satisfied condition "not pending"
    Sep  4 18:18:01.725: INFO: Started pod busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411 in namespace container-probe-635
    STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 18:18:01.725
    Sep  4 18:18:01.733: INFO: Initial restart count of pod busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411 is 0
    Sep  4 18:18:51.926: INFO: Restart count of pod container-probe-635/busybox-0c93a3cf-c226-4888-a0a7-eddbf98d1411 is now 1 (50.193568092s elapsed)
    STEP: deleting the pod 09/04/23 18:18:51.927
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:18:51.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-635" for this suite. 09/04/23 18:18:51.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:18:52.005
Sep  4 18:18:52.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename server-version 09/04/23 18:18:52.007
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:18:52.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:18:52.048
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 09/04/23 18:18:52.055
STEP: Confirm major version 09/04/23 18:18:52.06
Sep  4 18:18:52.060: INFO: Major version: 1
STEP: Confirm minor version 09/04/23 18:18:52.061
Sep  4 18:18:52.061: INFO: cleanMinorVersion: 26
Sep  4 18:18:52.061: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Sep  4 18:18:52.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-6469" for this suite. 09/04/23 18:18:52.074
------------------------------
â€¢ [0.101 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:18:52.005
    Sep  4 18:18:52.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename server-version 09/04/23 18:18:52.007
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:18:52.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:18:52.048
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 09/04/23 18:18:52.055
    STEP: Confirm major version 09/04/23 18:18:52.06
    Sep  4 18:18:52.060: INFO: Major version: 1
    STEP: Confirm minor version 09/04/23 18:18:52.061
    Sep  4 18:18:52.061: INFO: cleanMinorVersion: 26
    Sep  4 18:18:52.061: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:18:52.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-6469" for this suite. 09/04/23 18:18:52.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:18:52.116
Sep  4 18:18:52.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 18:18:52.117
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:18:52.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:18:52.151
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/04/23 18:18:52.158
Sep  4 18:18:52.170: INFO: Waiting up to 5m0s for pod "pod-2599d141-8761-4281-8a1d-5cc710022262" in namespace "emptydir-3879" to be "Succeeded or Failed"
Sep  4 18:18:52.190: INFO: Pod "pod-2599d141-8761-4281-8a1d-5cc710022262": Phase="Pending", Reason="", readiness=false. Elapsed: 19.103937ms
Sep  4 18:18:54.199: INFO: Pod "pod-2599d141-8761-4281-8a1d-5cc710022262": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028783573s
Sep  4 18:18:56.196: INFO: Pod "pod-2599d141-8761-4281-8a1d-5cc710022262": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025768213s
STEP: Saw pod success 09/04/23 18:18:56.197
Sep  4 18:18:56.197: INFO: Pod "pod-2599d141-8761-4281-8a1d-5cc710022262" satisfied condition "Succeeded or Failed"
Sep  4 18:18:56.203: INFO: Trying to get logs from node tenant-000001 pod pod-2599d141-8761-4281-8a1d-5cc710022262 container test-container: <nil>
STEP: delete the pod 09/04/23 18:18:56.24
Sep  4 18:18:56.259: INFO: Waiting for pod pod-2599d141-8761-4281-8a1d-5cc710022262 to disappear
Sep  4 18:18:56.264: INFO: Pod pod-2599d141-8761-4281-8a1d-5cc710022262 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 18:18:56.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3879" for this suite. 09/04/23 18:18:56.273
------------------------------
â€¢ [4.174 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:18:52.116
    Sep  4 18:18:52.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 18:18:52.117
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:18:52.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:18:52.151
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/04/23 18:18:52.158
    Sep  4 18:18:52.170: INFO: Waiting up to 5m0s for pod "pod-2599d141-8761-4281-8a1d-5cc710022262" in namespace "emptydir-3879" to be "Succeeded or Failed"
    Sep  4 18:18:52.190: INFO: Pod "pod-2599d141-8761-4281-8a1d-5cc710022262": Phase="Pending", Reason="", readiness=false. Elapsed: 19.103937ms
    Sep  4 18:18:54.199: INFO: Pod "pod-2599d141-8761-4281-8a1d-5cc710022262": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028783573s
    Sep  4 18:18:56.196: INFO: Pod "pod-2599d141-8761-4281-8a1d-5cc710022262": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025768213s
    STEP: Saw pod success 09/04/23 18:18:56.197
    Sep  4 18:18:56.197: INFO: Pod "pod-2599d141-8761-4281-8a1d-5cc710022262" satisfied condition "Succeeded or Failed"
    Sep  4 18:18:56.203: INFO: Trying to get logs from node tenant-000001 pod pod-2599d141-8761-4281-8a1d-5cc710022262 container test-container: <nil>
    STEP: delete the pod 09/04/23 18:18:56.24
    Sep  4 18:18:56.259: INFO: Waiting for pod pod-2599d141-8761-4281-8a1d-5cc710022262 to disappear
    Sep  4 18:18:56.264: INFO: Pod pod-2599d141-8761-4281-8a1d-5cc710022262 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:18:56.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3879" for this suite. 09/04/23 18:18:56.273
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:18:56.293
Sep  4 18:18:56.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename tables 09/04/23 18:18:56.295
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:18:56.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:18:56.321
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Sep  4 18:18:56.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-7530" for this suite. 09/04/23 18:18:56.343
------------------------------
â€¢ [0.070 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:18:56.293
    Sep  4 18:18:56.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename tables 09/04/23 18:18:56.295
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:18:56.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:18:56.321
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:18:56.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-7530" for this suite. 09/04/23 18:18:56.343
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:18:56.366
Sep  4 18:18:56.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pods 09/04/23 18:18:56.368
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:18:56.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:18:56.394
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Sep  4 18:18:56.415: INFO: Waiting up to 5m0s for pod "server-envvars-fead9c48-b6b9-4579-bebe-515d5c4607cc" in namespace "pods-3667" to be "running and ready"
Sep  4 18:18:56.428: INFO: Pod "server-envvars-fead9c48-b6b9-4579-bebe-515d5c4607cc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.909107ms
Sep  4 18:18:56.428: INFO: The phase of Pod server-envvars-fead9c48-b6b9-4579-bebe-515d5c4607cc is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:18:58.435: INFO: Pod "server-envvars-fead9c48-b6b9-4579-bebe-515d5c4607cc": Phase="Running", Reason="", readiness=true. Elapsed: 2.019491148s
Sep  4 18:18:58.435: INFO: The phase of Pod server-envvars-fead9c48-b6b9-4579-bebe-515d5c4607cc is Running (Ready = true)
Sep  4 18:18:58.435: INFO: Pod "server-envvars-fead9c48-b6b9-4579-bebe-515d5c4607cc" satisfied condition "running and ready"
Sep  4 18:18:58.483: INFO: Waiting up to 5m0s for pod "client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b" in namespace "pods-3667" to be "Succeeded or Failed"
Sep  4 18:18:58.488: INFO: Pod "client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.319273ms
Sep  4 18:19:00.495: INFO: Pod "client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011788118s
Sep  4 18:19:02.498: INFO: Pod "client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015007135s
STEP: Saw pod success 09/04/23 18:19:02.498
Sep  4 18:19:02.499: INFO: Pod "client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b" satisfied condition "Succeeded or Failed"
Sep  4 18:19:02.504: INFO: Trying to get logs from node tenant-000001 pod client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b container env3cont: <nil>
STEP: delete the pod 09/04/23 18:19:02.52
Sep  4 18:19:02.543: INFO: Waiting for pod client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b to disappear
Sep  4 18:19:02.550: INFO: Pod client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 18:19:02.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3667" for this suite. 09/04/23 18:19:02.573
------------------------------
â€¢ [SLOW TEST] [6.219 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:18:56.366
    Sep  4 18:18:56.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pods 09/04/23 18:18:56.368
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:18:56.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:18:56.394
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Sep  4 18:18:56.415: INFO: Waiting up to 5m0s for pod "server-envvars-fead9c48-b6b9-4579-bebe-515d5c4607cc" in namespace "pods-3667" to be "running and ready"
    Sep  4 18:18:56.428: INFO: Pod "server-envvars-fead9c48-b6b9-4579-bebe-515d5c4607cc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.909107ms
    Sep  4 18:18:56.428: INFO: The phase of Pod server-envvars-fead9c48-b6b9-4579-bebe-515d5c4607cc is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:18:58.435: INFO: Pod "server-envvars-fead9c48-b6b9-4579-bebe-515d5c4607cc": Phase="Running", Reason="", readiness=true. Elapsed: 2.019491148s
    Sep  4 18:18:58.435: INFO: The phase of Pod server-envvars-fead9c48-b6b9-4579-bebe-515d5c4607cc is Running (Ready = true)
    Sep  4 18:18:58.435: INFO: Pod "server-envvars-fead9c48-b6b9-4579-bebe-515d5c4607cc" satisfied condition "running and ready"
    Sep  4 18:18:58.483: INFO: Waiting up to 5m0s for pod "client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b" in namespace "pods-3667" to be "Succeeded or Failed"
    Sep  4 18:18:58.488: INFO: Pod "client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.319273ms
    Sep  4 18:19:00.495: INFO: Pod "client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011788118s
    Sep  4 18:19:02.498: INFO: Pod "client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015007135s
    STEP: Saw pod success 09/04/23 18:19:02.498
    Sep  4 18:19:02.499: INFO: Pod "client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b" satisfied condition "Succeeded or Failed"
    Sep  4 18:19:02.504: INFO: Trying to get logs from node tenant-000001 pod client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b container env3cont: <nil>
    STEP: delete the pod 09/04/23 18:19:02.52
    Sep  4 18:19:02.543: INFO: Waiting for pod client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b to disappear
    Sep  4 18:19:02.550: INFO: Pod client-envvars-7a131cc4-2a14-4891-a6a3-5024e805569b no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:19:02.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3667" for this suite. 09/04/23 18:19:02.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:19:02.589
Sep  4 18:19:02.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 18:19:02.591
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:02.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:02.622
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 09/04/23 18:19:02.635
STEP: waiting for available Endpoint 09/04/23 18:19:02.645
STEP: listing all Endpoints 09/04/23 18:19:02.648
STEP: updating the Endpoint 09/04/23 18:19:02.653
STEP: fetching the Endpoint 09/04/23 18:19:02.665
STEP: patching the Endpoint 09/04/23 18:19:02.671
STEP: fetching the Endpoint 09/04/23 18:19:02.683
STEP: deleting the Endpoint by Collection 09/04/23 18:19:02.688
STEP: waiting for Endpoint deletion 09/04/23 18:19:02.705
STEP: fetching the Endpoint 09/04/23 18:19:02.709
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 18:19:02.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8221" for this suite. 09/04/23 18:19:02.723
------------------------------
â€¢ [0.144 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:19:02.589
    Sep  4 18:19:02.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 18:19:02.591
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:02.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:02.622
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 09/04/23 18:19:02.635
    STEP: waiting for available Endpoint 09/04/23 18:19:02.645
    STEP: listing all Endpoints 09/04/23 18:19:02.648
    STEP: updating the Endpoint 09/04/23 18:19:02.653
    STEP: fetching the Endpoint 09/04/23 18:19:02.665
    STEP: patching the Endpoint 09/04/23 18:19:02.671
    STEP: fetching the Endpoint 09/04/23 18:19:02.683
    STEP: deleting the Endpoint by Collection 09/04/23 18:19:02.688
    STEP: waiting for Endpoint deletion 09/04/23 18:19:02.705
    STEP: fetching the Endpoint 09/04/23 18:19:02.709
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:19:02.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8221" for this suite. 09/04/23 18:19:02.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:19:02.736
Sep  4 18:19:02.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename deployment 09/04/23 18:19:02.74
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:02.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:02.773
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Sep  4 18:19:02.823: INFO: Creating simple deployment test-new-deployment
Sep  4 18:19:02.858: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 09/04/23 18:19:04.893
STEP: updating a scale subresource 09/04/23 18:19:04.897
STEP: verifying the deployment Spec.Replicas was modified 09/04/23 18:19:04.91
STEP: Patch a scale subresource 09/04/23 18:19:04.919
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 18:19:04.964: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-2295  975cab90-a6bd-4d1e-bf94-1a7843a1b436 28042 3 2023-09-04 18:19:02 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-09-04 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00430c138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-09-04 18:19:04 +0000 UTC,LastTransitionTime:2023-09-04 18:19:02 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-04 18:19:04 +0000 UTC,LastTransitionTime:2023-09-04 18:19:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  4 18:19:04.984: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-2295  f7e170fd-d775-47f0-87ef-8d710897d5bd 28046 3 2023-09-04 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 975cab90-a6bd-4d1e-bf94-1a7843a1b436 0xc00430c557 0xc00430c558}] [] [{kube-controller-manager Update apps/v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"975cab90-a6bd-4d1e-bf94-1a7843a1b436\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00430c5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  4 18:19:04.995: INFO: Pod "test-new-deployment-7f5969cbc7-jjfk5" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-jjfk5 test-new-deployment-7f5969cbc7- deployment-2295  5114e855-c3a9-439d-a1f5-d946cb6e0cec 28049 0 2023-09-04 18:19:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f7e170fd-d775-47f0-87ef-8d710897d5bd 0xc00430c9f7 0xc00430c9f8}] [] [{kube-controller-manager Update v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7e170fd-d775-47f0-87ef-8d710897d5bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lsdc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lsdc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 18:19:04.996: INFO: Pod "test-new-deployment-7f5969cbc7-k82dl" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-k82dl test-new-deployment-7f5969cbc7- deployment-2295  28d05b4f-ece1-4a9a-be84-615336b076a4 28052 0 2023-09-04 18:19:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f7e170fd-d775-47f0-87ef-8d710897d5bd 0xc00430ccf7 0xc00430ccf8}] [] [{kube-controller-manager Update v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7e170fd-d775-47f0-87ef-8d710897d5bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pptfq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pptfq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 18:19:04.996: INFO: Pod "test-new-deployment-7f5969cbc7-mkhk2" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-mkhk2 test-new-deployment-7f5969cbc7- deployment-2295  57c3435b-4f20-426d-ac79-e1a19272e701 28031 0 2023-09-04 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a23c24648bc644d6ec58247280a12f9a9d5db63588a70e6d72d7e9779a477b13 cni.projectcalico.org/podIP:10.36.55.88/32 cni.projectcalico.org/podIPs:10.36.55.88/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f7e170fd-d775-47f0-87ef-8d710897d5bd 0xc00430d017 0xc00430d018}] [] [{kube-controller-manager Update v1 2023-09-04 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7e170fd-d775-47f0-87ef-8d710897d5bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 18:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zc642,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zc642,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.88,StartTime:2023-09-04 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 18:19:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://59591a95718a0409070939bc418978fc06e60b6c4da850ae3ae926c7838ddf44,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 18:19:04.997: INFO: Pod "test-new-deployment-7f5969cbc7-xjvv9" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-xjvv9 test-new-deployment-7f5969cbc7- deployment-2295  1bec9386-8dac-40a3-949d-c4ee53b3fb73 28045 0 2023-09-04 18:19:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f7e170fd-d775-47f0-87ef-8d710897d5bd 0xc00430d220 0xc00430d221}] [] [{kube-controller-manager Update v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7e170fd-d775-47f0-87ef-8d710897d5bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-568g6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-568g6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:,StartTime:2023-09-04 18:19:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 18:19:04.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2295" for this suite. 09/04/23 18:19:05.009
------------------------------
â€¢ [2.292 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:19:02.736
    Sep  4 18:19:02.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename deployment 09/04/23 18:19:02.74
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:02.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:02.773
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Sep  4 18:19:02.823: INFO: Creating simple deployment test-new-deployment
    Sep  4 18:19:02.858: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 09/04/23 18:19:04.893
    STEP: updating a scale subresource 09/04/23 18:19:04.897
    STEP: verifying the deployment Spec.Replicas was modified 09/04/23 18:19:04.91
    STEP: Patch a scale subresource 09/04/23 18:19:04.919
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 18:19:04.964: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-2295  975cab90-a6bd-4d1e-bf94-1a7843a1b436 28042 3 2023-09-04 18:19:02 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-09-04 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00430c138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-09-04 18:19:04 +0000 UTC,LastTransitionTime:2023-09-04 18:19:02 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-04 18:19:04 +0000 UTC,LastTransitionTime:2023-09-04 18:19:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  4 18:19:04.984: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-2295  f7e170fd-d775-47f0-87ef-8d710897d5bd 28046 3 2023-09-04 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 975cab90-a6bd-4d1e-bf94-1a7843a1b436 0xc00430c557 0xc00430c558}] [] [{kube-controller-manager Update apps/v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"975cab90-a6bd-4d1e-bf94-1a7843a1b436\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00430c5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 18:19:04.995: INFO: Pod "test-new-deployment-7f5969cbc7-jjfk5" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-jjfk5 test-new-deployment-7f5969cbc7- deployment-2295  5114e855-c3a9-439d-a1f5-d946cb6e0cec 28049 0 2023-09-04 18:19:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f7e170fd-d775-47f0-87ef-8d710897d5bd 0xc00430c9f7 0xc00430c9f8}] [] [{kube-controller-manager Update v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7e170fd-d775-47f0-87ef-8d710897d5bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lsdc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lsdc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 18:19:04.996: INFO: Pod "test-new-deployment-7f5969cbc7-k82dl" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-k82dl test-new-deployment-7f5969cbc7- deployment-2295  28d05b4f-ece1-4a9a-be84-615336b076a4 28052 0 2023-09-04 18:19:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f7e170fd-d775-47f0-87ef-8d710897d5bd 0xc00430ccf7 0xc00430ccf8}] [] [{kube-controller-manager Update v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7e170fd-d775-47f0-87ef-8d710897d5bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pptfq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pptfq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 18:19:04.996: INFO: Pod "test-new-deployment-7f5969cbc7-mkhk2" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-mkhk2 test-new-deployment-7f5969cbc7- deployment-2295  57c3435b-4f20-426d-ac79-e1a19272e701 28031 0 2023-09-04 18:19:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a23c24648bc644d6ec58247280a12f9a9d5db63588a70e6d72d7e9779a477b13 cni.projectcalico.org/podIP:10.36.55.88/32 cni.projectcalico.org/podIPs:10.36.55.88/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f7e170fd-d775-47f0-87ef-8d710897d5bd 0xc00430d017 0xc00430d018}] [] [{kube-controller-manager Update v1 2023-09-04 18:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7e170fd-d775-47f0-87ef-8d710897d5bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 18:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zc642,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zc642,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.88,StartTime:2023-09-04 18:19:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 18:19:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://59591a95718a0409070939bc418978fc06e60b6c4da850ae3ae926c7838ddf44,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 18:19:04.997: INFO: Pod "test-new-deployment-7f5969cbc7-xjvv9" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-xjvv9 test-new-deployment-7f5969cbc7- deployment-2295  1bec9386-8dac-40a3-949d-c4ee53b3fb73 28045 0 2023-09-04 18:19:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f7e170fd-d775-47f0-87ef-8d710897d5bd 0xc00430d220 0xc00430d221}] [] [{kube-controller-manager Update v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7e170fd-d775-47f0-87ef-8d710897d5bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 18:19:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-568g6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-568g6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:19:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.7,PodIP:,StartTime:2023-09-04 18:19:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:19:04.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2295" for this suite. 09/04/23 18:19:05.009
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:19:05.034
Sep  4 18:19:05.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 18:19:05.036
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:05.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:05.09
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 09/04/23 18:19:05.097
Sep  4 18:19:05.113: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313" in namespace "downward-api-9715" to be "Succeeded or Failed"
Sep  4 18:19:05.127: INFO: Pod "downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313": Phase="Pending", Reason="", readiness=false. Elapsed: 13.807316ms
Sep  4 18:19:07.134: INFO: Pod "downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020487265s
Sep  4 18:19:09.135: INFO: Pod "downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021085811s
STEP: Saw pod success 09/04/23 18:19:09.135
Sep  4 18:19:09.135: INFO: Pod "downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313" satisfied condition "Succeeded or Failed"
Sep  4 18:19:09.143: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313 container client-container: <nil>
STEP: delete the pod 09/04/23 18:19:09.155
Sep  4 18:19:09.172: INFO: Waiting for pod downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313 to disappear
Sep  4 18:19:09.178: INFO: Pod downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 18:19:09.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9715" for this suite. 09/04/23 18:19:09.188
------------------------------
â€¢ [4.165 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:19:05.034
    Sep  4 18:19:05.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 18:19:05.036
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:05.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:05.09
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 09/04/23 18:19:05.097
    Sep  4 18:19:05.113: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313" in namespace "downward-api-9715" to be "Succeeded or Failed"
    Sep  4 18:19:05.127: INFO: Pod "downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313": Phase="Pending", Reason="", readiness=false. Elapsed: 13.807316ms
    Sep  4 18:19:07.134: INFO: Pod "downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020487265s
    Sep  4 18:19:09.135: INFO: Pod "downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021085811s
    STEP: Saw pod success 09/04/23 18:19:09.135
    Sep  4 18:19:09.135: INFO: Pod "downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313" satisfied condition "Succeeded or Failed"
    Sep  4 18:19:09.143: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313 container client-container: <nil>
    STEP: delete the pod 09/04/23 18:19:09.155
    Sep  4 18:19:09.172: INFO: Waiting for pod downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313 to disappear
    Sep  4 18:19:09.178: INFO: Pod downwardapi-volume-6c60bb61-c32b-4448-b65f-8483e9a4e313 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:19:09.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9715" for this suite. 09/04/23 18:19:09.188
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:19:09.201
Sep  4 18:19:09.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename secrets 09/04/23 18:19:09.203
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:09.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:09.248
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 09/04/23 18:19:09.256
STEP: listing secrets in all namespaces to ensure that there are more than zero 09/04/23 18:19:09.264
STEP: patching the secret 09/04/23 18:19:09.271
STEP: deleting the secret using a LabelSelector 09/04/23 18:19:09.282
STEP: listing secrets in all namespaces, searching for label name and value in patch 09/04/23 18:19:09.294
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 18:19:09.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-284" for this suite. 09/04/23 18:19:09.31
------------------------------
â€¢ [0.123 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:19:09.201
    Sep  4 18:19:09.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename secrets 09/04/23 18:19:09.203
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:09.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:09.248
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 09/04/23 18:19:09.256
    STEP: listing secrets in all namespaces to ensure that there are more than zero 09/04/23 18:19:09.264
    STEP: patching the secret 09/04/23 18:19:09.271
    STEP: deleting the secret using a LabelSelector 09/04/23 18:19:09.282
    STEP: listing secrets in all namespaces, searching for label name and value in patch 09/04/23 18:19:09.294
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:19:09.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-284" for this suite. 09/04/23 18:19:09.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:19:09.331
Sep  4 18:19:09.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 18:19:09.332
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:09.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:09.358
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 09/04/23 18:19:09.366
Sep  4 18:19:09.381: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6" in namespace "downward-api-6805" to be "Succeeded or Failed"
Sep  4 18:19:09.390: INFO: Pod "downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.351754ms
Sep  4 18:19:11.397: INFO: Pod "downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015493934s
Sep  4 18:19:13.403: INFO: Pod "downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021765554s
STEP: Saw pod success 09/04/23 18:19:13.403
Sep  4 18:19:13.404: INFO: Pod "downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6" satisfied condition "Succeeded or Failed"
Sep  4 18:19:13.412: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6 container client-container: <nil>
STEP: delete the pod 09/04/23 18:19:13.426
Sep  4 18:19:13.446: INFO: Waiting for pod downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6 to disappear
Sep  4 18:19:13.455: INFO: Pod downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 18:19:13.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6805" for this suite. 09/04/23 18:19:13.465
------------------------------
â€¢ [4.143 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:19:09.331
    Sep  4 18:19:09.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 18:19:09.332
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:09.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:09.358
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 09/04/23 18:19:09.366
    Sep  4 18:19:09.381: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6" in namespace "downward-api-6805" to be "Succeeded or Failed"
    Sep  4 18:19:09.390: INFO: Pod "downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.351754ms
    Sep  4 18:19:11.397: INFO: Pod "downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015493934s
    Sep  4 18:19:13.403: INFO: Pod "downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021765554s
    STEP: Saw pod success 09/04/23 18:19:13.403
    Sep  4 18:19:13.404: INFO: Pod "downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6" satisfied condition "Succeeded or Failed"
    Sep  4 18:19:13.412: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6 container client-container: <nil>
    STEP: delete the pod 09/04/23 18:19:13.426
    Sep  4 18:19:13.446: INFO: Waiting for pod downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6 to disappear
    Sep  4 18:19:13.455: INFO: Pod downwardapi-volume-9a193268-f583-46e9-a24e-e1090b5a94a6 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:19:13.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6805" for this suite. 09/04/23 18:19:13.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:19:13.482
Sep  4 18:19:13.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 18:19:13.483
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:13.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:13.514
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 09/04/23 18:19:13.521
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 18:19:13.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8917" for this suite. 09/04/23 18:19:13.537
------------------------------
â€¢ [0.063 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:19:13.482
    Sep  4 18:19:13.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 18:19:13.483
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:13.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:13.514
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 09/04/23 18:19:13.521
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:19:13.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8917" for this suite. 09/04/23 18:19:13.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:19:13.546
Sep  4 18:19:13.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename init-container 09/04/23 18:19:13.547
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:13.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:13.575
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 09/04/23 18:19:13.583
Sep  4 18:19:13.583: INFO: PodSpec: initContainers in spec.initContainers
Sep  4 18:19:54.770: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9a954fed-8c2c-4ab1-83f5-12d1bad008b8", GenerateName:"", Namespace:"init-container-1255", SelfLink:"", UID:"0454ac73-e698-4c1e-89e6-c4f43157cdec", ResourceVersion:"28338", Generation:0, CreationTimestamp:time.Date(2023, time.September, 4, 18, 19, 13, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"583403867"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"a49e7a0480c61300d529e3015e9cb8504c96a7b73e62147a6dcd76be52fac97c", "cni.projectcalico.org/podIP":"10.36.55.95/32", "cni.projectcalico.org/podIPs":"10.36.55.95/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 4, 18, 19, 13, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004ee17a0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 4, 18, 19, 14, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004ee17d0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 4, 18, 19, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004ee1818), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-f8rvq", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004316e20), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f8rvq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f8rvq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f8rvq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004f06728), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"tenant-000001", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0005e25b0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f067b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f067d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004f067d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004f067dc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000f36c60), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 18, 19, 13, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 18, 19, 13, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 18, 19, 13, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 18, 19, 13, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.225.0.5", PodIP:"10.36.55.95", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.36.55.95"}}, StartTime:time.Date(2023, time.September, 4, 18, 19, 13, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005e2690)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005e2700)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://56b7e7d8ee748972e5b3389ca432a7066ff135278b70f3a14282916da50d849b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004316ea0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004316e80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004f0685f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:19:54.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1255" for this suite. 09/04/23 18:19:54.796
------------------------------
â€¢ [SLOW TEST] [41.262 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:19:13.546
    Sep  4 18:19:13.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename init-container 09/04/23 18:19:13.547
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:13.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:13.575
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 09/04/23 18:19:13.583
    Sep  4 18:19:13.583: INFO: PodSpec: initContainers in spec.initContainers
    Sep  4 18:19:54.770: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9a954fed-8c2c-4ab1-83f5-12d1bad008b8", GenerateName:"", Namespace:"init-container-1255", SelfLink:"", UID:"0454ac73-e698-4c1e-89e6-c4f43157cdec", ResourceVersion:"28338", Generation:0, CreationTimestamp:time.Date(2023, time.September, 4, 18, 19, 13, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"583403867"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"a49e7a0480c61300d529e3015e9cb8504c96a7b73e62147a6dcd76be52fac97c", "cni.projectcalico.org/podIP":"10.36.55.95/32", "cni.projectcalico.org/podIPs":"10.36.55.95/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 4, 18, 19, 13, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004ee17a0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 4, 18, 19, 14, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004ee17d0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 4, 18, 19, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004ee1818), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-f8rvq", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004316e20), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f8rvq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f8rvq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f8rvq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004f06728), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"tenant-000001", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0005e25b0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f067b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f067d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004f067d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004f067dc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000f36c60), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 18, 19, 13, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 18, 19, 13, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 18, 19, 13, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 18, 19, 13, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.225.0.5", PodIP:"10.36.55.95", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.36.55.95"}}, StartTime:time.Date(2023, time.September, 4, 18, 19, 13, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005e2690)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005e2700)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://56b7e7d8ee748972e5b3389ca432a7066ff135278b70f3a14282916da50d849b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004316ea0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004316e80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004f0685f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:19:54.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1255" for this suite. 09/04/23 18:19:54.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:19:54.808
Sep  4 18:19:54.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 18:19:54.811
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:54.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:54.85
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 09/04/23 18:19:54.858
Sep  4 18:19:54.870: INFO: Waiting up to 5m0s for pod "pod-a712379c-7a39-4e97-a550-09787d6e14ba" in namespace "emptydir-7402" to be "Succeeded or Failed"
Sep  4 18:19:54.889: INFO: Pod "pod-a712379c-7a39-4e97-a550-09787d6e14ba": Phase="Pending", Reason="", readiness=false. Elapsed: 18.730819ms
Sep  4 18:19:56.899: INFO: Pod "pod-a712379c-7a39-4e97-a550-09787d6e14ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028688611s
Sep  4 18:19:58.899: INFO: Pod "pod-a712379c-7a39-4e97-a550-09787d6e14ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02875546s
STEP: Saw pod success 09/04/23 18:19:58.899
Sep  4 18:19:58.899: INFO: Pod "pod-a712379c-7a39-4e97-a550-09787d6e14ba" satisfied condition "Succeeded or Failed"
Sep  4 18:19:58.904: INFO: Trying to get logs from node tenant-000003 pod pod-a712379c-7a39-4e97-a550-09787d6e14ba container test-container: <nil>
STEP: delete the pod 09/04/23 18:19:58.957
Sep  4 18:19:58.974: INFO: Waiting for pod pod-a712379c-7a39-4e97-a550-09787d6e14ba to disappear
Sep  4 18:19:58.981: INFO: Pod pod-a712379c-7a39-4e97-a550-09787d6e14ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 18:19:58.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7402" for this suite. 09/04/23 18:19:58.99
------------------------------
â€¢ [4.193 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:19:54.808
    Sep  4 18:19:54.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 18:19:54.811
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:54.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:54.85
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 09/04/23 18:19:54.858
    Sep  4 18:19:54.870: INFO: Waiting up to 5m0s for pod "pod-a712379c-7a39-4e97-a550-09787d6e14ba" in namespace "emptydir-7402" to be "Succeeded or Failed"
    Sep  4 18:19:54.889: INFO: Pod "pod-a712379c-7a39-4e97-a550-09787d6e14ba": Phase="Pending", Reason="", readiness=false. Elapsed: 18.730819ms
    Sep  4 18:19:56.899: INFO: Pod "pod-a712379c-7a39-4e97-a550-09787d6e14ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028688611s
    Sep  4 18:19:58.899: INFO: Pod "pod-a712379c-7a39-4e97-a550-09787d6e14ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02875546s
    STEP: Saw pod success 09/04/23 18:19:58.899
    Sep  4 18:19:58.899: INFO: Pod "pod-a712379c-7a39-4e97-a550-09787d6e14ba" satisfied condition "Succeeded or Failed"
    Sep  4 18:19:58.904: INFO: Trying to get logs from node tenant-000003 pod pod-a712379c-7a39-4e97-a550-09787d6e14ba container test-container: <nil>
    STEP: delete the pod 09/04/23 18:19:58.957
    Sep  4 18:19:58.974: INFO: Waiting for pod pod-a712379c-7a39-4e97-a550-09787d6e14ba to disappear
    Sep  4 18:19:58.981: INFO: Pod pod-a712379c-7a39-4e97-a550-09787d6e14ba no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:19:58.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7402" for this suite. 09/04/23 18:19:58.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:19:59.009
Sep  4 18:19:59.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename containers 09/04/23 18:19:59.01
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:59.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:59.038
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 09/04/23 18:19:59.045
Sep  4 18:19:59.057: INFO: Waiting up to 5m0s for pod "client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3" in namespace "containers-9064" to be "Succeeded or Failed"
Sep  4 18:19:59.064: INFO: Pod "client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.498423ms
Sep  4 18:20:01.073: INFO: Pod "client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015513354s
Sep  4 18:20:03.070: INFO: Pod "client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0128693s
STEP: Saw pod success 09/04/23 18:20:03.071
Sep  4 18:20:03.071: INFO: Pod "client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3" satisfied condition "Succeeded or Failed"
Sep  4 18:20:03.077: INFO: Trying to get logs from node tenant-000003 pod client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 18:20:03.091
Sep  4 18:20:03.113: INFO: Waiting for pod client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3 to disappear
Sep  4 18:20:03.118: INFO: Pod client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  4 18:20:03.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9064" for this suite. 09/04/23 18:20:03.128
------------------------------
â€¢ [4.132 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:19:59.009
    Sep  4 18:19:59.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename containers 09/04/23 18:19:59.01
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:19:59.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:19:59.038
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 09/04/23 18:19:59.045
    Sep  4 18:19:59.057: INFO: Waiting up to 5m0s for pod "client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3" in namespace "containers-9064" to be "Succeeded or Failed"
    Sep  4 18:19:59.064: INFO: Pod "client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.498423ms
    Sep  4 18:20:01.073: INFO: Pod "client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015513354s
    Sep  4 18:20:03.070: INFO: Pod "client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0128693s
    STEP: Saw pod success 09/04/23 18:20:03.071
    Sep  4 18:20:03.071: INFO: Pod "client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3" satisfied condition "Succeeded or Failed"
    Sep  4 18:20:03.077: INFO: Trying to get logs from node tenant-000003 pod client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 18:20:03.091
    Sep  4 18:20:03.113: INFO: Waiting for pod client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3 to disappear
    Sep  4 18:20:03.118: INFO: Pod client-containers-dcc11be0-373a-45c3-8506-a4eaf750bdf3 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:20:03.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9064" for this suite. 09/04/23 18:20:03.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:20:03.149
Sep  4 18:20:03.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename resourcequota 09/04/23 18:20:03.15
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:20:03.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:20:03.186
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 09/04/23 18:20:03.193
STEP: Creating a ResourceQuota 09/04/23 18:20:08.201
STEP: Ensuring resource quota status is calculated 09/04/23 18:20:08.21
STEP: Creating a Pod that fits quota 09/04/23 18:20:10.217
STEP: Ensuring ResourceQuota status captures the pod usage 09/04/23 18:20:10.241
STEP: Not allowing a pod to be created that exceeds remaining quota 09/04/23 18:20:12.25
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/04/23 18:20:12.256
STEP: Ensuring a pod cannot update its resource requirements 09/04/23 18:20:12.262
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/04/23 18:20:12.275
STEP: Deleting the pod 09/04/23 18:20:14.281
STEP: Ensuring resource quota status released the pod usage 09/04/23 18:20:14.303
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 18:20:16.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3637" for this suite. 09/04/23 18:20:16.329
------------------------------
â€¢ [SLOW TEST] [13.192 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:20:03.149
    Sep  4 18:20:03.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename resourcequota 09/04/23 18:20:03.15
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:20:03.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:20:03.186
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 09/04/23 18:20:03.193
    STEP: Creating a ResourceQuota 09/04/23 18:20:08.201
    STEP: Ensuring resource quota status is calculated 09/04/23 18:20:08.21
    STEP: Creating a Pod that fits quota 09/04/23 18:20:10.217
    STEP: Ensuring ResourceQuota status captures the pod usage 09/04/23 18:20:10.241
    STEP: Not allowing a pod to be created that exceeds remaining quota 09/04/23 18:20:12.25
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/04/23 18:20:12.256
    STEP: Ensuring a pod cannot update its resource requirements 09/04/23 18:20:12.262
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/04/23 18:20:12.275
    STEP: Deleting the pod 09/04/23 18:20:14.281
    STEP: Ensuring resource quota status released the pod usage 09/04/23 18:20:14.303
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:20:16.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3637" for this suite. 09/04/23 18:20:16.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:20:16.35
Sep  4 18:20:16.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename init-container 09/04/23 18:20:16.351
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:20:16.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:20:16.384
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 09/04/23 18:20:16.391
Sep  4 18:20:16.392: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:20:20.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3290" for this suite. 09/04/23 18:20:20.888
------------------------------
â€¢ [4.589 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:20:16.35
    Sep  4 18:20:16.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename init-container 09/04/23 18:20:16.351
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:20:16.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:20:16.384
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 09/04/23 18:20:16.391
    Sep  4 18:20:16.392: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:20:20.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3290" for this suite. 09/04/23 18:20:20.888
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:20:20.939
Sep  4 18:20:20.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 18:20:20.939
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:20:20.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:20:20.967
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 09/04/23 18:20:20.978
Sep  4 18:20:20.996: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34" in namespace "projected-7050" to be "Succeeded or Failed"
Sep  4 18:20:21.003: INFO: Pod "downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34": Phase="Pending", Reason="", readiness=false. Elapsed: 6.955723ms
Sep  4 18:20:23.010: INFO: Pod "downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013384261s
Sep  4 18:20:25.010: INFO: Pod "downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013528005s
STEP: Saw pod success 09/04/23 18:20:25.01
Sep  4 18:20:25.011: INFO: Pod "downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34" satisfied condition "Succeeded or Failed"
Sep  4 18:20:25.019: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34 container client-container: <nil>
STEP: delete the pod 09/04/23 18:20:25.028
Sep  4 18:20:25.047: INFO: Waiting for pod downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34 to disappear
Sep  4 18:20:25.052: INFO: Pod downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 18:20:25.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7050" for this suite. 09/04/23 18:20:25.064
------------------------------
â€¢ [4.140 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:20:20.939
    Sep  4 18:20:20.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 18:20:20.939
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:20:20.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:20:20.967
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 09/04/23 18:20:20.978
    Sep  4 18:20:20.996: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34" in namespace "projected-7050" to be "Succeeded or Failed"
    Sep  4 18:20:21.003: INFO: Pod "downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34": Phase="Pending", Reason="", readiness=false. Elapsed: 6.955723ms
    Sep  4 18:20:23.010: INFO: Pod "downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013384261s
    Sep  4 18:20:25.010: INFO: Pod "downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013528005s
    STEP: Saw pod success 09/04/23 18:20:25.01
    Sep  4 18:20:25.011: INFO: Pod "downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34" satisfied condition "Succeeded or Failed"
    Sep  4 18:20:25.019: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34 container client-container: <nil>
    STEP: delete the pod 09/04/23 18:20:25.028
    Sep  4 18:20:25.047: INFO: Waiting for pod downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34 to disappear
    Sep  4 18:20:25.052: INFO: Pod downwardapi-volume-6b8d4f9c-7bed-4cd7-8d15-c42b6c1e3d34 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:20:25.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7050" for this suite. 09/04/23 18:20:25.064
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:20:25.084
Sep  4 18:20:25.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename cronjob 09/04/23 18:20:25.085
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:20:25.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:20:25.11
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 09/04/23 18:20:25.117
STEP: Ensuring no jobs are scheduled 09/04/23 18:20:25.127
STEP: Ensuring no job exists by listing jobs explicitly 09/04/23 18:25:25.141
STEP: Removing cronjob 09/04/23 18:25:25.147
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  4 18:25:25.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1057" for this suite. 09/04/23 18:25:25.165
------------------------------
â€¢ [SLOW TEST] [300.095 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:20:25.084
    Sep  4 18:20:25.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename cronjob 09/04/23 18:20:25.085
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:20:25.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:20:25.11
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 09/04/23 18:20:25.117
    STEP: Ensuring no jobs are scheduled 09/04/23 18:20:25.127
    STEP: Ensuring no job exists by listing jobs explicitly 09/04/23 18:25:25.141
    STEP: Removing cronjob 09/04/23 18:25:25.147
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:25:25.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1057" for this suite. 09/04/23 18:25:25.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:25:25.189
Sep  4 18:25:25.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 18:25:25.19
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:25:25.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:25:25.217
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/04/23 18:25:25.23
Sep  4 18:25:25.242: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7024" to be "running and ready"
Sep  4 18:25:25.254: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.119478ms
Sep  4 18:25:25.254: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:25:27.263: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021434959s
Sep  4 18:25:27.264: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:25:29.261: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.018620663s
Sep  4 18:25:29.261: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  4 18:25:29.261: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 09/04/23 18:25:29.269
Sep  4 18:25:29.279: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7024" to be "running and ready"
Sep  4 18:25:29.285: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.493007ms
Sep  4 18:25:29.285: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:25:31.294: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014809387s
Sep  4 18:25:31.295: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Sep  4 18:25:31.295: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/04/23 18:25:31.3
STEP: delete the pod with lifecycle hook 09/04/23 18:25:31.337
Sep  4 18:25:31.351: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  4 18:25:31.357: INFO: Pod pod-with-poststart-http-hook still exists
Sep  4 18:25:33.358: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  4 18:25:33.368: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  4 18:25:33.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7024" for this suite. 09/04/23 18:25:33.377
------------------------------
â€¢ [SLOW TEST] [8.202 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:25:25.189
    Sep  4 18:25:25.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 18:25:25.19
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:25:25.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:25:25.217
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/04/23 18:25:25.23
    Sep  4 18:25:25.242: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7024" to be "running and ready"
    Sep  4 18:25:25.254: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.119478ms
    Sep  4 18:25:25.254: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:25:27.263: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021434959s
    Sep  4 18:25:27.264: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:25:29.261: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.018620663s
    Sep  4 18:25:29.261: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  4 18:25:29.261: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 09/04/23 18:25:29.269
    Sep  4 18:25:29.279: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7024" to be "running and ready"
    Sep  4 18:25:29.285: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.493007ms
    Sep  4 18:25:29.285: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:25:31.294: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014809387s
    Sep  4 18:25:31.295: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Sep  4 18:25:31.295: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/04/23 18:25:31.3
    STEP: delete the pod with lifecycle hook 09/04/23 18:25:31.337
    Sep  4 18:25:31.351: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  4 18:25:31.357: INFO: Pod pod-with-poststart-http-hook still exists
    Sep  4 18:25:33.358: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  4 18:25:33.368: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:25:33.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7024" for this suite. 09/04/23 18:25:33.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:25:33.395
Sep  4 18:25:33.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename controllerrevisions 09/04/23 18:25:33.397
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:25:33.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:25:33.423
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-ht94j-daemon-set" 09/04/23 18:25:33.461
STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 18:25:33.473
Sep  4 18:25:33.488: INFO: Number of nodes with available pods controlled by daemonset e2e-ht94j-daemon-set: 0
Sep  4 18:25:33.488: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:25:34.518: INFO: Number of nodes with available pods controlled by daemonset e2e-ht94j-daemon-set: 1
Sep  4 18:25:34.518: INFO: Node tenant-000003 is running 0 daemon pod, expected 1
Sep  4 18:25:35.506: INFO: Number of nodes with available pods controlled by daemonset e2e-ht94j-daemon-set: 2
Sep  4 18:25:35.506: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-ht94j-daemon-set
STEP: Confirm DaemonSet "e2e-ht94j-daemon-set" successfully created with "daemonset-name=e2e-ht94j-daemon-set" label 09/04/23 18:25:35.511
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-ht94j-daemon-set" 09/04/23 18:25:35.525
Sep  4 18:25:35.534: INFO: Located ControllerRevision: "e2e-ht94j-daemon-set-5cf9b46"
STEP: Patching ControllerRevision "e2e-ht94j-daemon-set-5cf9b46" 09/04/23 18:25:35.539
Sep  4 18:25:35.550: INFO: e2e-ht94j-daemon-set-5cf9b46 has been patched
STEP: Create a new ControllerRevision 09/04/23 18:25:35.551
Sep  4 18:25:35.563: INFO: Created ControllerRevision: e2e-ht94j-daemon-set-59b68dfd84
STEP: Confirm that there are two ControllerRevisions 09/04/23 18:25:35.563
Sep  4 18:25:35.563: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  4 18:25:35.568: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-ht94j-daemon-set-5cf9b46" 09/04/23 18:25:35.568
STEP: Confirm that there is only one ControllerRevision 09/04/23 18:25:35.582
Sep  4 18:25:35.582: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  4 18:25:35.587: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-ht94j-daemon-set-59b68dfd84" 09/04/23 18:25:35.592
Sep  4 18:25:35.609: INFO: e2e-ht94j-daemon-set-59b68dfd84 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 09/04/23 18:25:35.609
W0904 18:25:35.618044      19 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 09/04/23 18:25:35.618
Sep  4 18:25:35.618: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  4 18:25:36.625: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  4 18:25:36.634: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-ht94j-daemon-set-59b68dfd84=updated" 09/04/23 18:25:36.635
STEP: Confirm that there is only one ControllerRevision 09/04/23 18:25:36.647
Sep  4 18:25:36.647: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  4 18:25:36.652: INFO: Found 1 ControllerRevisions
Sep  4 18:25:36.658: INFO: ControllerRevision "e2e-ht94j-daemon-set-548c947565" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-ht94j-daemon-set" 09/04/23 18:25:36.665
STEP: deleting DaemonSet.extensions e2e-ht94j-daemon-set in namespace controllerrevisions-2173, will wait for the garbage collector to delete the pods 09/04/23 18:25:36.665
Sep  4 18:25:36.732: INFO: Deleting DaemonSet.extensions e2e-ht94j-daemon-set took: 12.222375ms
Sep  4 18:25:36.833: INFO: Terminating DaemonSet.extensions e2e-ht94j-daemon-set pods took: 100.71476ms
Sep  4 18:25:38.440: INFO: Number of nodes with available pods controlled by daemonset e2e-ht94j-daemon-set: 0
Sep  4 18:25:38.440: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-ht94j-daemon-set
Sep  4 18:25:38.447: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29241"},"items":null}

Sep  4 18:25:38.453: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29242"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:25:38.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-2173" for this suite. 09/04/23 18:25:38.48
------------------------------
â€¢ [SLOW TEST] [5.104 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:25:33.395
    Sep  4 18:25:33.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename controllerrevisions 09/04/23 18:25:33.397
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:25:33.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:25:33.423
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-ht94j-daemon-set" 09/04/23 18:25:33.461
    STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 18:25:33.473
    Sep  4 18:25:33.488: INFO: Number of nodes with available pods controlled by daemonset e2e-ht94j-daemon-set: 0
    Sep  4 18:25:33.488: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:25:34.518: INFO: Number of nodes with available pods controlled by daemonset e2e-ht94j-daemon-set: 1
    Sep  4 18:25:34.518: INFO: Node tenant-000003 is running 0 daemon pod, expected 1
    Sep  4 18:25:35.506: INFO: Number of nodes with available pods controlled by daemonset e2e-ht94j-daemon-set: 2
    Sep  4 18:25:35.506: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-ht94j-daemon-set
    STEP: Confirm DaemonSet "e2e-ht94j-daemon-set" successfully created with "daemonset-name=e2e-ht94j-daemon-set" label 09/04/23 18:25:35.511
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-ht94j-daemon-set" 09/04/23 18:25:35.525
    Sep  4 18:25:35.534: INFO: Located ControllerRevision: "e2e-ht94j-daemon-set-5cf9b46"
    STEP: Patching ControllerRevision "e2e-ht94j-daemon-set-5cf9b46" 09/04/23 18:25:35.539
    Sep  4 18:25:35.550: INFO: e2e-ht94j-daemon-set-5cf9b46 has been patched
    STEP: Create a new ControllerRevision 09/04/23 18:25:35.551
    Sep  4 18:25:35.563: INFO: Created ControllerRevision: e2e-ht94j-daemon-set-59b68dfd84
    STEP: Confirm that there are two ControllerRevisions 09/04/23 18:25:35.563
    Sep  4 18:25:35.563: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  4 18:25:35.568: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-ht94j-daemon-set-5cf9b46" 09/04/23 18:25:35.568
    STEP: Confirm that there is only one ControllerRevision 09/04/23 18:25:35.582
    Sep  4 18:25:35.582: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  4 18:25:35.587: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-ht94j-daemon-set-59b68dfd84" 09/04/23 18:25:35.592
    Sep  4 18:25:35.609: INFO: e2e-ht94j-daemon-set-59b68dfd84 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 09/04/23 18:25:35.609
    W0904 18:25:35.618044      19 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 09/04/23 18:25:35.618
    Sep  4 18:25:35.618: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  4 18:25:36.625: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  4 18:25:36.634: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-ht94j-daemon-set-59b68dfd84=updated" 09/04/23 18:25:36.635
    STEP: Confirm that there is only one ControllerRevision 09/04/23 18:25:36.647
    Sep  4 18:25:36.647: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  4 18:25:36.652: INFO: Found 1 ControllerRevisions
    Sep  4 18:25:36.658: INFO: ControllerRevision "e2e-ht94j-daemon-set-548c947565" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-ht94j-daemon-set" 09/04/23 18:25:36.665
    STEP: deleting DaemonSet.extensions e2e-ht94j-daemon-set in namespace controllerrevisions-2173, will wait for the garbage collector to delete the pods 09/04/23 18:25:36.665
    Sep  4 18:25:36.732: INFO: Deleting DaemonSet.extensions e2e-ht94j-daemon-set took: 12.222375ms
    Sep  4 18:25:36.833: INFO: Terminating DaemonSet.extensions e2e-ht94j-daemon-set pods took: 100.71476ms
    Sep  4 18:25:38.440: INFO: Number of nodes with available pods controlled by daemonset e2e-ht94j-daemon-set: 0
    Sep  4 18:25:38.440: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-ht94j-daemon-set
    Sep  4 18:25:38.447: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29241"},"items":null}

    Sep  4 18:25:38.453: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29242"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:25:38.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-2173" for this suite. 09/04/23 18:25:38.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:25:38.505
Sep  4 18:25:38.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename dns 09/04/23 18:25:38.508
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:25:38.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:25:38.539
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5049.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5049.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 09/04/23 18:25:38.545
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5049.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5049.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 09/04/23 18:25:38.545
STEP: creating a pod to probe /etc/hosts 09/04/23 18:25:38.545
STEP: submitting the pod to kubernetes 09/04/23 18:25:38.546
Sep  4 18:25:38.563: INFO: Waiting up to 15m0s for pod "dns-test-1b495cd4-b2c5-4671-9b58-083a78bb548b" in namespace "dns-5049" to be "running"
Sep  4 18:25:38.580: INFO: Pod "dns-test-1b495cd4-b2c5-4671-9b58-083a78bb548b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.095732ms
Sep  4 18:25:40.587: INFO: Pod "dns-test-1b495cd4-b2c5-4671-9b58-083a78bb548b": Phase="Running", Reason="", readiness=true. Elapsed: 2.02348858s
Sep  4 18:25:40.587: INFO: Pod "dns-test-1b495cd4-b2c5-4671-9b58-083a78bb548b" satisfied condition "running"
STEP: retrieving the pod 09/04/23 18:25:40.587
STEP: looking for the results for each expected name from probers 09/04/23 18:25:40.595
Sep  4 18:25:40.628: INFO: DNS probes using dns-5049/dns-test-1b495cd4-b2c5-4671-9b58-083a78bb548b succeeded

STEP: deleting the pod 09/04/23 18:25:40.629
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 18:25:40.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5049" for this suite. 09/04/23 18:25:40.676
------------------------------
â€¢ [2.191 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:25:38.505
    Sep  4 18:25:38.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename dns 09/04/23 18:25:38.508
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:25:38.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:25:38.539
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5049.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5049.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     09/04/23 18:25:38.545
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5049.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5049.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     09/04/23 18:25:38.545
    STEP: creating a pod to probe /etc/hosts 09/04/23 18:25:38.545
    STEP: submitting the pod to kubernetes 09/04/23 18:25:38.546
    Sep  4 18:25:38.563: INFO: Waiting up to 15m0s for pod "dns-test-1b495cd4-b2c5-4671-9b58-083a78bb548b" in namespace "dns-5049" to be "running"
    Sep  4 18:25:38.580: INFO: Pod "dns-test-1b495cd4-b2c5-4671-9b58-083a78bb548b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.095732ms
    Sep  4 18:25:40.587: INFO: Pod "dns-test-1b495cd4-b2c5-4671-9b58-083a78bb548b": Phase="Running", Reason="", readiness=true. Elapsed: 2.02348858s
    Sep  4 18:25:40.587: INFO: Pod "dns-test-1b495cd4-b2c5-4671-9b58-083a78bb548b" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 18:25:40.587
    STEP: looking for the results for each expected name from probers 09/04/23 18:25:40.595
    Sep  4 18:25:40.628: INFO: DNS probes using dns-5049/dns-test-1b495cd4-b2c5-4671-9b58-083a78bb548b succeeded

    STEP: deleting the pod 09/04/23 18:25:40.629
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:25:40.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5049" for this suite. 09/04/23 18:25:40.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:25:40.71
Sep  4 18:25:40.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename dns 09/04/23 18:25:40.711
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:25:40.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:25:40.739
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 09/04/23 18:25:40.747
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local;sleep 1; done
 09/04/23 18:25:40.755
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local;sleep 1; done
 09/04/23 18:25:40.755
STEP: creating a pod to probe DNS 09/04/23 18:25:40.756
STEP: submitting the pod to kubernetes 09/04/23 18:25:40.756
Sep  4 18:25:40.770: INFO: Waiting up to 15m0s for pod "dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a" in namespace "dns-4959" to be "running"
Sep  4 18:25:40.790: INFO: Pod "dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.297553ms
Sep  4 18:25:42.812: INFO: Pod "dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a": Phase="Running", Reason="", readiness=true. Elapsed: 2.041127243s
Sep  4 18:25:42.812: INFO: Pod "dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a" satisfied condition "running"
STEP: retrieving the pod 09/04/23 18:25:42.812
STEP: looking for the results for each expected name from probers 09/04/23 18:25:42.822
Sep  4 18:25:42.901: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:42.909: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:42.916: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:42.926: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:42.933: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:42.940: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:42.950: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:42.957: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:42.957: INFO: Lookups using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local]

Sep  4 18:25:47.967: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:47.977: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:47.985: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:47.993: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:48.002: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:48.009: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:48.016: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:48.026: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:48.026: INFO: Lookups using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local]

Sep  4 18:25:52.965: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:52.984: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:52.994: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:53.005: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:53.013: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:53.023: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:53.031: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:53.038: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:53.038: INFO: Lookups using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local]

Sep  4 18:25:57.970: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:57.978: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:57.986: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:57.995: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:58.002: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:58.010: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:58.020: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:58.027: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:25:58.027: INFO: Lookups using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local]

Sep  4 18:26:02.974: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:02.986: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:02.994: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:03.002: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:03.013: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:03.020: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:03.028: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:03.039: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:03.039: INFO: Lookups using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local]

Sep  4 18:26:07.966: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:07.973: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:07.983: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:07.990: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:07.998: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:08.007: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:08.014: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:08.022: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:08.022: INFO: Lookups using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local]

Sep  4 18:26:12.966: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:12.986: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:13.038: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
Sep  4 18:26:13.051: INFO: Lookups using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local]

Sep  4 18:26:18.026: INFO: DNS probes using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a succeeded

STEP: deleting the pod 09/04/23 18:26:18.026
STEP: deleting the test headless service 09/04/23 18:26:18.082
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 18:26:18.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4959" for this suite. 09/04/23 18:26:18.179
------------------------------
â€¢ [SLOW TEST] [37.484 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:25:40.71
    Sep  4 18:25:40.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename dns 09/04/23 18:25:40.711
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:25:40.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:25:40.739
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 09/04/23 18:25:40.747
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local;sleep 1; done
     09/04/23 18:25:40.755
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4959.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local;sleep 1; done
     09/04/23 18:25:40.755
    STEP: creating a pod to probe DNS 09/04/23 18:25:40.756
    STEP: submitting the pod to kubernetes 09/04/23 18:25:40.756
    Sep  4 18:25:40.770: INFO: Waiting up to 15m0s for pod "dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a" in namespace "dns-4959" to be "running"
    Sep  4 18:25:40.790: INFO: Pod "dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.297553ms
    Sep  4 18:25:42.812: INFO: Pod "dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a": Phase="Running", Reason="", readiness=true. Elapsed: 2.041127243s
    Sep  4 18:25:42.812: INFO: Pod "dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 18:25:42.812
    STEP: looking for the results for each expected name from probers 09/04/23 18:25:42.822
    Sep  4 18:25:42.901: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:42.909: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:42.916: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:42.926: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:42.933: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:42.940: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:42.950: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:42.957: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:42.957: INFO: Lookups using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local]

    Sep  4 18:25:47.967: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:47.977: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:47.985: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:47.993: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:48.002: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:48.009: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:48.016: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:48.026: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:48.026: INFO: Lookups using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local]

    Sep  4 18:25:52.965: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:52.984: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:52.994: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:53.005: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:53.013: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:53.023: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:53.031: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:53.038: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:53.038: INFO: Lookups using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local]

    Sep  4 18:25:57.970: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:57.978: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:57.986: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:57.995: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:58.002: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:58.010: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:58.020: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:58.027: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:25:58.027: INFO: Lookups using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local]

    Sep  4 18:26:02.974: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:02.986: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:02.994: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:03.002: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:03.013: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:03.020: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:03.028: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:03.039: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:03.039: INFO: Lookups using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local]

    Sep  4 18:26:07.966: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:07.973: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:07.983: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:07.990: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:07.998: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:08.007: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:08.014: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:08.022: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:08.022: INFO: Lookups using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4959.svc.cluster.local]

    Sep  4 18:26:12.966: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:12.986: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:13.038: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local from pod dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a: the server could not find the requested resource (get pods dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a)
    Sep  4 18:26:13.051: INFO: Lookups using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4959.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4959.svc.cluster.local jessie_udp@dns-test-service-2.dns-4959.svc.cluster.local]

    Sep  4 18:26:18.026: INFO: DNS probes using dns-4959/dns-test-25ac8f2e-d3e4-4ad3-9a7f-6fe4b75f3e2a succeeded

    STEP: deleting the pod 09/04/23 18:26:18.026
    STEP: deleting the test headless service 09/04/23 18:26:18.082
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:26:18.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4959" for this suite. 09/04/23 18:26:18.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:26:18.2
Sep  4 18:26:18.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename svcaccounts 09/04/23 18:26:18.201
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:26:18.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:26:18.233
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  09/04/23 18:26:18.24
Sep  4 18:26:18.253: INFO: Waiting up to 5m0s for pod "test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5" in namespace "svcaccounts-5975" to be "Succeeded or Failed"
Sep  4 18:26:18.269: INFO: Pod "test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5": Phase="Pending", Reason="", readiness=false. Elapsed: 15.688056ms
Sep  4 18:26:20.275: INFO: Pod "test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022112329s
Sep  4 18:26:22.276: INFO: Pod "test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02245867s
STEP: Saw pod success 09/04/23 18:26:22.276
Sep  4 18:26:22.276: INFO: Pod "test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5" satisfied condition "Succeeded or Failed"
Sep  4 18:26:22.281: INFO: Trying to get logs from node tenant-000001 pod test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 18:26:22.318
Sep  4 18:26:22.334: INFO: Waiting for pod test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5 to disappear
Sep  4 18:26:22.339: INFO: Pod test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  4 18:26:22.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5975" for this suite. 09/04/23 18:26:22.347
------------------------------
â€¢ [4.160 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:26:18.2
    Sep  4 18:26:18.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename svcaccounts 09/04/23 18:26:18.201
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:26:18.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:26:18.233
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  09/04/23 18:26:18.24
    Sep  4 18:26:18.253: INFO: Waiting up to 5m0s for pod "test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5" in namespace "svcaccounts-5975" to be "Succeeded or Failed"
    Sep  4 18:26:18.269: INFO: Pod "test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5": Phase="Pending", Reason="", readiness=false. Elapsed: 15.688056ms
    Sep  4 18:26:20.275: INFO: Pod "test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022112329s
    Sep  4 18:26:22.276: INFO: Pod "test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02245867s
    STEP: Saw pod success 09/04/23 18:26:22.276
    Sep  4 18:26:22.276: INFO: Pod "test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5" satisfied condition "Succeeded or Failed"
    Sep  4 18:26:22.281: INFO: Trying to get logs from node tenant-000001 pod test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 18:26:22.318
    Sep  4 18:26:22.334: INFO: Waiting for pod test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5 to disappear
    Sep  4 18:26:22.339: INFO: Pod test-pod-c698c8a6-8d11-4e12-8a0b-0ec4a1c03fc5 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:26:22.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5975" for this suite. 09/04/23 18:26:22.347
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:26:22.361
Sep  4 18:26:22.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-runtime 09/04/23 18:26:22.362
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:26:22.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:26:22.395
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 09/04/23 18:26:22.403
STEP: wait for the container to reach Succeeded 09/04/23 18:26:22.415
STEP: get the container status 09/04/23 18:26:26.457
STEP: the container should be terminated 09/04/23 18:26:26.466
STEP: the termination message should be set 09/04/23 18:26:26.466
Sep  4 18:26:26.466: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 09/04/23 18:26:26.466
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  4 18:26:26.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5053" for this suite. 09/04/23 18:26:26.499
------------------------------
â€¢ [4.150 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:26:22.361
    Sep  4 18:26:22.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-runtime 09/04/23 18:26:22.362
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:26:22.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:26:22.395
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 09/04/23 18:26:22.403
    STEP: wait for the container to reach Succeeded 09/04/23 18:26:22.415
    STEP: get the container status 09/04/23 18:26:26.457
    STEP: the container should be terminated 09/04/23 18:26:26.466
    STEP: the termination message should be set 09/04/23 18:26:26.466
    Sep  4 18:26:26.466: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 09/04/23 18:26:26.466
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:26:26.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5053" for this suite. 09/04/23 18:26:26.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:26:26.518
Sep  4 18:26:26.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 18:26:26.519
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:26:26.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:26:26.55
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-63a24c75-16dd-48a2-ae19-2644766ab580 09/04/23 18:26:26.558
STEP: Creating a pod to test consume configMaps 09/04/23 18:26:26.567
Sep  4 18:26:26.580: INFO: Waiting up to 5m0s for pod "pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725" in namespace "configmap-3053" to be "Succeeded or Failed"
Sep  4 18:26:26.592: INFO: Pod "pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725": Phase="Pending", Reason="", readiness=false. Elapsed: 12.077656ms
Sep  4 18:26:28.598: INFO: Pod "pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018352329s
Sep  4 18:26:30.598: INFO: Pod "pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018581575s
STEP: Saw pod success 09/04/23 18:26:30.599
Sep  4 18:26:30.599: INFO: Pod "pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725" satisfied condition "Succeeded or Failed"
Sep  4 18:26:30.605: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725 container configmap-volume-test: <nil>
STEP: delete the pod 09/04/23 18:26:30.619
Sep  4 18:26:30.638: INFO: Waiting for pod pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725 to disappear
Sep  4 18:26:30.644: INFO: Pod pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 18:26:30.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3053" for this suite. 09/04/23 18:26:30.652
------------------------------
â€¢ [4.148 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:26:26.518
    Sep  4 18:26:26.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 18:26:26.519
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:26:26.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:26:26.55
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-63a24c75-16dd-48a2-ae19-2644766ab580 09/04/23 18:26:26.558
    STEP: Creating a pod to test consume configMaps 09/04/23 18:26:26.567
    Sep  4 18:26:26.580: INFO: Waiting up to 5m0s for pod "pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725" in namespace "configmap-3053" to be "Succeeded or Failed"
    Sep  4 18:26:26.592: INFO: Pod "pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725": Phase="Pending", Reason="", readiness=false. Elapsed: 12.077656ms
    Sep  4 18:26:28.598: INFO: Pod "pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018352329s
    Sep  4 18:26:30.598: INFO: Pod "pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018581575s
    STEP: Saw pod success 09/04/23 18:26:30.599
    Sep  4 18:26:30.599: INFO: Pod "pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725" satisfied condition "Succeeded or Failed"
    Sep  4 18:26:30.605: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725 container configmap-volume-test: <nil>
    STEP: delete the pod 09/04/23 18:26:30.619
    Sep  4 18:26:30.638: INFO: Waiting for pod pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725 to disappear
    Sep  4 18:26:30.644: INFO: Pod pod-configmaps-38582a56-ae2e-4930-a616-fbf20acaa725 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:26:30.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3053" for this suite. 09/04/23 18:26:30.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:26:30.671
Sep  4 18:26:30.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename sched-pred 09/04/23 18:26:30.672
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:26:30.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:26:30.717
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  4 18:26:30.724: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  4 18:26:30.738: INFO: Waiting for terminating namespaces to be deleted...
Sep  4 18:26:30.742: INFO: 
Logging pods the apiserver thinks is on node tenant-000001 before test
Sep  4 18:26:30.752: INFO: calico-node-dj2mb from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
Sep  4 18:26:30.752: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 18:26:30.752: INFO: konnectivity-agent-k8gs4 from kube-system started at 2023-09-04 18:09:31 +0000 UTC (1 container statuses recorded)
Sep  4 18:26:30.752: INFO: 	Container konnectivity-agent ready: true, restart count 0
Sep  4 18:26:30.752: INFO: kube-proxy-959c5 from kube-system started at 2023-09-04 17:14:03 +0000 UTC (1 container statuses recorded)
Sep  4 18:26:30.752: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 18:26:30.752: INFO: sonobuoy from sonobuoy started at 2023-09-04 17:16:08 +0000 UTC (1 container statuses recorded)
Sep  4 18:26:30.752: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  4 18:26:30.753: INFO: sonobuoy-e2e-job-8da28a692bc241e2 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
Sep  4 18:26:30.753: INFO: 	Container e2e ready: true, restart count 0
Sep  4 18:26:30.753: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  4 18:26:30.753: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-64p86 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
Sep  4 18:26:30.753: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  4 18:26:30.753: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  4 18:26:30.753: INFO: 
Logging pods the apiserver thinks is on node tenant-000003 before test
Sep  4 18:26:30.766: INFO: calico-kube-controllers-5f94594857-4wpdt from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 18:26:30.766: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  4 18:26:30.766: INFO: calico-node-wlgx4 from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
Sep  4 18:26:30.766: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 18:26:30.766: INFO: coredns-6bfc4dc876-rnzz9 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 18:26:30.766: INFO: 	Container coredns ready: true, restart count 0
Sep  4 18:26:30.766: INFO: coredns-6bfc4dc876-xp5ff from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 18:26:30.766: INFO: 	Container coredns ready: true, restart count 0
Sep  4 18:26:30.767: INFO: konnectivity-agent-j62r4 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 18:26:30.767: INFO: 	Container konnectivity-agent ready: true, restart count 0
Sep  4 18:26:30.767: INFO: kube-proxy-z2kxt from kube-system started at 2023-09-04 17:15:09 +0000 UTC (1 container statuses recorded)
Sep  4 18:26:30.767: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 18:26:30.767: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-7wr4k from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
Sep  4 18:26:30.767: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  4 18:26:30.767: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/04/23 18:26:30.768
Sep  4 18:26:30.781: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6371" to be "running"
Sep  4 18:26:30.801: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 19.963783ms
Sep  4 18:26:32.818: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.036528815s
Sep  4 18:26:32.818: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/04/23 18:26:32.834
STEP: Trying to apply a random label on the found node. 09/04/23 18:26:32.876
STEP: verifying the node has the label kubernetes.io/e2e-d21ecf7a-f469-43de-ba49-8a3dc0683851 95 09/04/23 18:26:32.92
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/04/23 18:26:32.945
Sep  4 18:26:32.971: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6371" to be "not pending"
Sep  4 18:26:32.994: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.274209ms
Sep  4 18:26:35.001: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.029197709s
Sep  4 18:26:35.001: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.225.0.5 on the node which pod4 resides and expect not scheduled 09/04/23 18:26:35.001
Sep  4 18:26:35.013: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6371" to be "not pending"
Sep  4 18:26:35.046: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.919886ms
Sep  4 18:26:37.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039827181s
Sep  4 18:26:39.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041447889s
Sep  4 18:26:41.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039492503s
Sep  4 18:26:43.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039906874s
Sep  4 18:26:45.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042118432s
Sep  4 18:26:47.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039156912s
Sep  4 18:26:49.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039447967s
Sep  4 18:26:51.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.0422048s
Sep  4 18:26:53.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.039341443s
Sep  4 18:26:55.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.040745713s
Sep  4 18:26:57.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.042560506s
Sep  4 18:26:59.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.039047255s
Sep  4 18:27:01.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.039365651s
Sep  4 18:27:03.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.041462742s
Sep  4 18:27:05.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.039918639s
Sep  4 18:27:07.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.039676955s
Sep  4 18:27:09.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.041716155s
Sep  4 18:27:11.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.041562039s
Sep  4 18:27:13.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.039524325s
Sep  4 18:27:15.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.042330932s
Sep  4 18:27:17.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.039065103s
Sep  4 18:27:19.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.042638526s
Sep  4 18:27:21.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.041461263s
Sep  4 18:27:23.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.076658874s
Sep  4 18:27:25.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.042010184s
Sep  4 18:27:27.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.041135881s
Sep  4 18:27:29.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.041425138s
Sep  4 18:27:31.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.039367429s
Sep  4 18:27:33.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.041129433s
Sep  4 18:27:35.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.042424412s
Sep  4 18:27:37.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.03912905s
Sep  4 18:27:39.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.039495496s
Sep  4 18:27:41.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.042000166s
Sep  4 18:27:43.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.040201608s
Sep  4 18:27:45.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.040791577s
Sep  4 18:27:47.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.041936411s
Sep  4 18:27:49.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.039187864s
Sep  4 18:27:51.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.040042207s
Sep  4 18:27:53.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.041903103s
Sep  4 18:27:55.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.039674641s
Sep  4 18:27:57.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.041673774s
Sep  4 18:27:59.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.041310182s
Sep  4 18:28:01.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.039598719s
Sep  4 18:28:03.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.039652486s
Sep  4 18:28:05.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.042482152s
Sep  4 18:28:07.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.039068277s
Sep  4 18:28:09.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.040442257s
Sep  4 18:28:11.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.041901121s
Sep  4 18:28:13.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.039122708s
Sep  4 18:28:15.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.040875556s
Sep  4 18:28:17.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.041488568s
Sep  4 18:28:19.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.039663615s
Sep  4 18:28:21.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.03954539s
Sep  4 18:28:23.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.041649215s
Sep  4 18:28:25.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.039736869s
Sep  4 18:28:27.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.040158936s
Sep  4 18:28:29.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.041660526s
Sep  4 18:28:31.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.039206085s
Sep  4 18:28:33.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.039237991s
Sep  4 18:28:35.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.041945689s
Sep  4 18:28:37.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.039470248s
Sep  4 18:28:39.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.039175273s
Sep  4 18:28:41.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.042054839s
Sep  4 18:28:43.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.038809472s
Sep  4 18:28:45.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.040612216s
Sep  4 18:28:47.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.046023536s
Sep  4 18:28:49.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.039195122s
Sep  4 18:28:51.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.0397475s
Sep  4 18:28:53.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.041972939s
Sep  4 18:28:55.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.040883709s
Sep  4 18:28:57.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.039153927s
Sep  4 18:28:59.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.042523537s
Sep  4 18:29:01.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.038920378s
Sep  4 18:29:03.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.039528839s
Sep  4 18:29:05.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.042304166s
Sep  4 18:29:07.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.03925706s
Sep  4 18:29:09.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.039744394s
Sep  4 18:29:11.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.044587962s
Sep  4 18:29:13.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.039198048s
Sep  4 18:29:15.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.039778766s
Sep  4 18:29:17.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.039455789s
Sep  4 18:29:19.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.039174516s
Sep  4 18:29:21.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.042302307s
Sep  4 18:29:23.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.039289802s
Sep  4 18:29:25.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.04027009s
Sep  4 18:29:27.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.042588578s
Sep  4 18:29:29.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.038887012s
Sep  4 18:29:31.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.039349273s
Sep  4 18:29:33.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.041881346s
Sep  4 18:29:35.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.040068594s
Sep  4 18:29:37.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.039716252s
Sep  4 18:29:39.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.040042605s
Sep  4 18:29:41.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.040015329s
Sep  4 18:29:43.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.039410209s
Sep  4 18:29:45.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.044407906s
Sep  4 18:29:47.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.039529337s
Sep  4 18:29:49.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.039734408s
Sep  4 18:29:51.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.041354383s
Sep  4 18:29:53.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.038616638s
Sep  4 18:29:55.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.040695923s
Sep  4 18:29:57.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.043645186s
Sep  4 18:29:59.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.03875848s
Sep  4 18:30:01.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.03959539s
Sep  4 18:30:03.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.041874335s
Sep  4 18:30:05.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.039061568s
Sep  4 18:30:07.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.040137842s
Sep  4 18:30:09.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.042029952s
Sep  4 18:30:11.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.038800862s
Sep  4 18:30:13.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.040670568s
Sep  4 18:30:15.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.04201011s
Sep  4 18:30:17.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.039117943s
Sep  4 18:30:19.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.039010084s
Sep  4 18:30:21.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.045322457s
Sep  4 18:30:23.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.038804664s
Sep  4 18:30:25.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.039585442s
Sep  4 18:30:27.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.042753272s
Sep  4 18:30:29.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.039509212s
Sep  4 18:30:31.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.039029145s
Sep  4 18:30:33.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.041596593s
Sep  4 18:30:35.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.039335462s
Sep  4 18:30:37.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.039217194s
Sep  4 18:30:39.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.043172626s
Sep  4 18:30:41.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.039187573s
Sep  4 18:30:43.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.040466996s
Sep  4 18:30:45.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.041915024s
Sep  4 18:30:47.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.038958635s
Sep  4 18:30:49.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.039375788s
Sep  4 18:30:51.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.039013709s
Sep  4 18:30:53.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.039792916s
Sep  4 18:30:55.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.043562123s
Sep  4 18:30:57.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.038688844s
Sep  4 18:30:59.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.039229874s
Sep  4 18:31:01.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.041810283s
Sep  4 18:31:03.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.039168648s
Sep  4 18:31:05.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.039403766s
Sep  4 18:31:07.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.040773357s
Sep  4 18:31:09.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.039319691s
Sep  4 18:31:11.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.040513464s
Sep  4 18:31:13.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.042277389s
Sep  4 18:31:15.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.03865022s
Sep  4 18:31:17.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.039097735s
Sep  4 18:31:19.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.04214206s
Sep  4 18:31:21.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.03896358s
Sep  4 18:31:23.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.039252838s
Sep  4 18:31:25.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.042268526s
Sep  4 18:31:27.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.038773887s
Sep  4 18:31:29.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.039215567s
Sep  4 18:31:31.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.041456738s
Sep  4 18:31:33.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.041987838s
Sep  4 18:31:35.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.039876713s
Sep  4 18:31:35.061: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.048266424s
STEP: removing the label kubernetes.io/e2e-d21ecf7a-f469-43de-ba49-8a3dc0683851 off the node tenant-000001 09/04/23 18:31:35.062
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d21ecf7a-f469-43de-ba49-8a3dc0683851 09/04/23 18:31:35.084
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:31:35.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6371" for this suite. 09/04/23 18:31:35.1
------------------------------
â€¢ [SLOW TEST] [304.442 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:26:30.671
    Sep  4 18:26:30.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename sched-pred 09/04/23 18:26:30.672
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:26:30.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:26:30.717
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  4 18:26:30.724: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  4 18:26:30.738: INFO: Waiting for terminating namespaces to be deleted...
    Sep  4 18:26:30.742: INFO: 
    Logging pods the apiserver thinks is on node tenant-000001 before test
    Sep  4 18:26:30.752: INFO: calico-node-dj2mb from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
    Sep  4 18:26:30.752: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 18:26:30.752: INFO: konnectivity-agent-k8gs4 from kube-system started at 2023-09-04 18:09:31 +0000 UTC (1 container statuses recorded)
    Sep  4 18:26:30.752: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Sep  4 18:26:30.752: INFO: kube-proxy-959c5 from kube-system started at 2023-09-04 17:14:03 +0000 UTC (1 container statuses recorded)
    Sep  4 18:26:30.752: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 18:26:30.752: INFO: sonobuoy from sonobuoy started at 2023-09-04 17:16:08 +0000 UTC (1 container statuses recorded)
    Sep  4 18:26:30.752: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  4 18:26:30.753: INFO: sonobuoy-e2e-job-8da28a692bc241e2 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
    Sep  4 18:26:30.753: INFO: 	Container e2e ready: true, restart count 0
    Sep  4 18:26:30.753: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  4 18:26:30.753: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-64p86 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
    Sep  4 18:26:30.753: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  4 18:26:30.753: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  4 18:26:30.753: INFO: 
    Logging pods the apiserver thinks is on node tenant-000003 before test
    Sep  4 18:26:30.766: INFO: calico-kube-controllers-5f94594857-4wpdt from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 18:26:30.766: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  4 18:26:30.766: INFO: calico-node-wlgx4 from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
    Sep  4 18:26:30.766: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 18:26:30.766: INFO: coredns-6bfc4dc876-rnzz9 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 18:26:30.766: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 18:26:30.766: INFO: coredns-6bfc4dc876-xp5ff from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 18:26:30.766: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 18:26:30.767: INFO: konnectivity-agent-j62r4 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 18:26:30.767: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Sep  4 18:26:30.767: INFO: kube-proxy-z2kxt from kube-system started at 2023-09-04 17:15:09 +0000 UTC (1 container statuses recorded)
    Sep  4 18:26:30.767: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 18:26:30.767: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-7wr4k from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
    Sep  4 18:26:30.767: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  4 18:26:30.767: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/04/23 18:26:30.768
    Sep  4 18:26:30.781: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6371" to be "running"
    Sep  4 18:26:30.801: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 19.963783ms
    Sep  4 18:26:32.818: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.036528815s
    Sep  4 18:26:32.818: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/04/23 18:26:32.834
    STEP: Trying to apply a random label on the found node. 09/04/23 18:26:32.876
    STEP: verifying the node has the label kubernetes.io/e2e-d21ecf7a-f469-43de-ba49-8a3dc0683851 95 09/04/23 18:26:32.92
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/04/23 18:26:32.945
    Sep  4 18:26:32.971: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6371" to be "not pending"
    Sep  4 18:26:32.994: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.274209ms
    Sep  4 18:26:35.001: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.029197709s
    Sep  4 18:26:35.001: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.225.0.5 on the node which pod4 resides and expect not scheduled 09/04/23 18:26:35.001
    Sep  4 18:26:35.013: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6371" to be "not pending"
    Sep  4 18:26:35.046: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.919886ms
    Sep  4 18:26:37.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039827181s
    Sep  4 18:26:39.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041447889s
    Sep  4 18:26:41.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039492503s
    Sep  4 18:26:43.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039906874s
    Sep  4 18:26:45.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042118432s
    Sep  4 18:26:47.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039156912s
    Sep  4 18:26:49.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039447967s
    Sep  4 18:26:51.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.0422048s
    Sep  4 18:26:53.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.039341443s
    Sep  4 18:26:55.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.040745713s
    Sep  4 18:26:57.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.042560506s
    Sep  4 18:26:59.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.039047255s
    Sep  4 18:27:01.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.039365651s
    Sep  4 18:27:03.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.041462742s
    Sep  4 18:27:05.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.039918639s
    Sep  4 18:27:07.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.039676955s
    Sep  4 18:27:09.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.041716155s
    Sep  4 18:27:11.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.041562039s
    Sep  4 18:27:13.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.039524325s
    Sep  4 18:27:15.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.042330932s
    Sep  4 18:27:17.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.039065103s
    Sep  4 18:27:19.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.042638526s
    Sep  4 18:27:21.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.041461263s
    Sep  4 18:27:23.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.076658874s
    Sep  4 18:27:25.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.042010184s
    Sep  4 18:27:27.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.041135881s
    Sep  4 18:27:29.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.041425138s
    Sep  4 18:27:31.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.039367429s
    Sep  4 18:27:33.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.041129433s
    Sep  4 18:27:35.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.042424412s
    Sep  4 18:27:37.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.03912905s
    Sep  4 18:27:39.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.039495496s
    Sep  4 18:27:41.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.042000166s
    Sep  4 18:27:43.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.040201608s
    Sep  4 18:27:45.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.040791577s
    Sep  4 18:27:47.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.041936411s
    Sep  4 18:27:49.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.039187864s
    Sep  4 18:27:51.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.040042207s
    Sep  4 18:27:53.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.041903103s
    Sep  4 18:27:55.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.039674641s
    Sep  4 18:27:57.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.041673774s
    Sep  4 18:27:59.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.041310182s
    Sep  4 18:28:01.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.039598719s
    Sep  4 18:28:03.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.039652486s
    Sep  4 18:28:05.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.042482152s
    Sep  4 18:28:07.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.039068277s
    Sep  4 18:28:09.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.040442257s
    Sep  4 18:28:11.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.041901121s
    Sep  4 18:28:13.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.039122708s
    Sep  4 18:28:15.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.040875556s
    Sep  4 18:28:17.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.041488568s
    Sep  4 18:28:19.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.039663615s
    Sep  4 18:28:21.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.03954539s
    Sep  4 18:28:23.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.041649215s
    Sep  4 18:28:25.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.039736869s
    Sep  4 18:28:27.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.040158936s
    Sep  4 18:28:29.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.041660526s
    Sep  4 18:28:31.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.039206085s
    Sep  4 18:28:33.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.039237991s
    Sep  4 18:28:35.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.041945689s
    Sep  4 18:28:37.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.039470248s
    Sep  4 18:28:39.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.039175273s
    Sep  4 18:28:41.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.042054839s
    Sep  4 18:28:43.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.038809472s
    Sep  4 18:28:45.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.040612216s
    Sep  4 18:28:47.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.046023536s
    Sep  4 18:28:49.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.039195122s
    Sep  4 18:28:51.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.0397475s
    Sep  4 18:28:53.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.041972939s
    Sep  4 18:28:55.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.040883709s
    Sep  4 18:28:57.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.039153927s
    Sep  4 18:28:59.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.042523537s
    Sep  4 18:29:01.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.038920378s
    Sep  4 18:29:03.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.039528839s
    Sep  4 18:29:05.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.042304166s
    Sep  4 18:29:07.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.03925706s
    Sep  4 18:29:09.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.039744394s
    Sep  4 18:29:11.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.044587962s
    Sep  4 18:29:13.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.039198048s
    Sep  4 18:29:15.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.039778766s
    Sep  4 18:29:17.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.039455789s
    Sep  4 18:29:19.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.039174516s
    Sep  4 18:29:21.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.042302307s
    Sep  4 18:29:23.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.039289802s
    Sep  4 18:29:25.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.04027009s
    Sep  4 18:29:27.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.042588578s
    Sep  4 18:29:29.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.038887012s
    Sep  4 18:29:31.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.039349273s
    Sep  4 18:29:33.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.041881346s
    Sep  4 18:29:35.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.040068594s
    Sep  4 18:29:37.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.039716252s
    Sep  4 18:29:39.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.040042605s
    Sep  4 18:29:41.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.040015329s
    Sep  4 18:29:43.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.039410209s
    Sep  4 18:29:45.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.044407906s
    Sep  4 18:29:47.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.039529337s
    Sep  4 18:29:49.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.039734408s
    Sep  4 18:29:51.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.041354383s
    Sep  4 18:29:53.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.038616638s
    Sep  4 18:29:55.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.040695923s
    Sep  4 18:29:57.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.043645186s
    Sep  4 18:29:59.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.03875848s
    Sep  4 18:30:01.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.03959539s
    Sep  4 18:30:03.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.041874335s
    Sep  4 18:30:05.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.039061568s
    Sep  4 18:30:07.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.040137842s
    Sep  4 18:30:09.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.042029952s
    Sep  4 18:30:11.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.038800862s
    Sep  4 18:30:13.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.040670568s
    Sep  4 18:30:15.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.04201011s
    Sep  4 18:30:17.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.039117943s
    Sep  4 18:30:19.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.039010084s
    Sep  4 18:30:21.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.045322457s
    Sep  4 18:30:23.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.038804664s
    Sep  4 18:30:25.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.039585442s
    Sep  4 18:30:27.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.042753272s
    Sep  4 18:30:29.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.039509212s
    Sep  4 18:30:31.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.039029145s
    Sep  4 18:30:33.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.041596593s
    Sep  4 18:30:35.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.039335462s
    Sep  4 18:30:37.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.039217194s
    Sep  4 18:30:39.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.043172626s
    Sep  4 18:30:41.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.039187573s
    Sep  4 18:30:43.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.040466996s
    Sep  4 18:30:45.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.041915024s
    Sep  4 18:30:47.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.038958635s
    Sep  4 18:30:49.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.039375788s
    Sep  4 18:30:51.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.039013709s
    Sep  4 18:30:53.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.039792916s
    Sep  4 18:30:55.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.043562123s
    Sep  4 18:30:57.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.038688844s
    Sep  4 18:30:59.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.039229874s
    Sep  4 18:31:01.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.041810283s
    Sep  4 18:31:03.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.039168648s
    Sep  4 18:31:05.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.039403766s
    Sep  4 18:31:07.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.040773357s
    Sep  4 18:31:09.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.039319691s
    Sep  4 18:31:11.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.040513464s
    Sep  4 18:31:13.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.042277389s
    Sep  4 18:31:15.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.03865022s
    Sep  4 18:31:17.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.039097735s
    Sep  4 18:31:19.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.04214206s
    Sep  4 18:31:21.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.03896358s
    Sep  4 18:31:23.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.039252838s
    Sep  4 18:31:25.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.042268526s
    Sep  4 18:31:27.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.038773887s
    Sep  4 18:31:29.052: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.039215567s
    Sep  4 18:31:31.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.041456738s
    Sep  4 18:31:33.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.041987838s
    Sep  4 18:31:35.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.039876713s
    Sep  4 18:31:35.061: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.048266424s
    STEP: removing the label kubernetes.io/e2e-d21ecf7a-f469-43de-ba49-8a3dc0683851 off the node tenant-000001 09/04/23 18:31:35.062
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-d21ecf7a-f469-43de-ba49-8a3dc0683851 09/04/23 18:31:35.084
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:31:35.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6371" for this suite. 09/04/23 18:31:35.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:31:35.126
Sep  4 18:31:35.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename endpointslice 09/04/23 18:31:35.127
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:31:35.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:31:35.163
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Sep  4 18:31:35.193: INFO: Endpoints addresses: [10.224.0.7] , ports: [6443]
Sep  4 18:31:35.193: INFO: EndpointSlices addresses: [10.224.0.7] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  4 18:31:35.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5862" for this suite. 09/04/23 18:31:35.203
------------------------------
â€¢ [0.091 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:31:35.126
    Sep  4 18:31:35.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename endpointslice 09/04/23 18:31:35.127
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:31:35.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:31:35.163
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Sep  4 18:31:35.193: INFO: Endpoints addresses: [10.224.0.7] , ports: [6443]
    Sep  4 18:31:35.193: INFO: EndpointSlices addresses: [10.224.0.7] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:31:35.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5862" for this suite. 09/04/23 18:31:35.203
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:31:35.219
Sep  4 18:31:35.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename replicaset 09/04/23 18:31:35.22
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:31:35.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:31:35.256
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Sep  4 18:31:35.285: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  4 18:31:40.292: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/04/23 18:31:40.292
STEP: Scaling up "test-rs" replicaset  09/04/23 18:31:40.292
Sep  4 18:31:40.306: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 09/04/23 18:31:40.306
W0904 18:31:40.320355      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  4 18:31:40.324: INFO: observed ReplicaSet test-rs in namespace replicaset-7725 with ReadyReplicas 1, AvailableReplicas 1
Sep  4 18:31:40.344: INFO: observed ReplicaSet test-rs in namespace replicaset-7725 with ReadyReplicas 1, AvailableReplicas 1
Sep  4 18:31:40.378: INFO: observed ReplicaSet test-rs in namespace replicaset-7725 with ReadyReplicas 1, AvailableReplicas 1
Sep  4 18:31:40.394: INFO: observed ReplicaSet test-rs in namespace replicaset-7725 with ReadyReplicas 1, AvailableReplicas 1
Sep  4 18:31:42.088: INFO: observed ReplicaSet test-rs in namespace replicaset-7725 with ReadyReplicas 2, AvailableReplicas 2
Sep  4 18:31:42.369: INFO: observed Replicaset test-rs in namespace replicaset-7725 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  4 18:31:42.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7725" for this suite. 09/04/23 18:31:42.379
------------------------------
â€¢ [SLOW TEST] [7.169 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:31:35.219
    Sep  4 18:31:35.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename replicaset 09/04/23 18:31:35.22
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:31:35.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:31:35.256
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Sep  4 18:31:35.285: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  4 18:31:40.292: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/04/23 18:31:40.292
    STEP: Scaling up "test-rs" replicaset  09/04/23 18:31:40.292
    Sep  4 18:31:40.306: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 09/04/23 18:31:40.306
    W0904 18:31:40.320355      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  4 18:31:40.324: INFO: observed ReplicaSet test-rs in namespace replicaset-7725 with ReadyReplicas 1, AvailableReplicas 1
    Sep  4 18:31:40.344: INFO: observed ReplicaSet test-rs in namespace replicaset-7725 with ReadyReplicas 1, AvailableReplicas 1
    Sep  4 18:31:40.378: INFO: observed ReplicaSet test-rs in namespace replicaset-7725 with ReadyReplicas 1, AvailableReplicas 1
    Sep  4 18:31:40.394: INFO: observed ReplicaSet test-rs in namespace replicaset-7725 with ReadyReplicas 1, AvailableReplicas 1
    Sep  4 18:31:42.088: INFO: observed ReplicaSet test-rs in namespace replicaset-7725 with ReadyReplicas 2, AvailableReplicas 2
    Sep  4 18:31:42.369: INFO: observed Replicaset test-rs in namespace replicaset-7725 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:31:42.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7725" for this suite. 09/04/23 18:31:42.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:31:42.399
Sep  4 18:31:42.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 18:31:42.4
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:31:42.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:31:42.431
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-a49bbc76-b4dd-464f-99d7-15a5ea5d84ee 09/04/23 18:31:42.439
STEP: Creating a pod to test consume configMaps 09/04/23 18:31:42.447
Sep  4 18:31:42.460: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11" in namespace "projected-8723" to be "Succeeded or Failed"
Sep  4 18:31:42.465: INFO: Pod "pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.886281ms
Sep  4 18:31:44.472: INFO: Pod "pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011454574s
Sep  4 18:31:46.473: INFO: Pod "pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013242621s
STEP: Saw pod success 09/04/23 18:31:46.474
Sep  4 18:31:46.474: INFO: Pod "pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11" satisfied condition "Succeeded or Failed"
Sep  4 18:31:46.480: INFO: Trying to get logs from node tenant-000003 pod pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11 container projected-configmap-volume-test: <nil>
STEP: delete the pod 09/04/23 18:31:46.514
Sep  4 18:31:46.540: INFO: Waiting for pod pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11 to disappear
Sep  4 18:31:46.545: INFO: Pod pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 18:31:46.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8723" for this suite. 09/04/23 18:31:46.552
------------------------------
â€¢ [4.163 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:31:42.399
    Sep  4 18:31:42.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 18:31:42.4
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:31:42.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:31:42.431
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-a49bbc76-b4dd-464f-99d7-15a5ea5d84ee 09/04/23 18:31:42.439
    STEP: Creating a pod to test consume configMaps 09/04/23 18:31:42.447
    Sep  4 18:31:42.460: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11" in namespace "projected-8723" to be "Succeeded or Failed"
    Sep  4 18:31:42.465: INFO: Pod "pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.886281ms
    Sep  4 18:31:44.472: INFO: Pod "pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011454574s
    Sep  4 18:31:46.473: INFO: Pod "pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013242621s
    STEP: Saw pod success 09/04/23 18:31:46.474
    Sep  4 18:31:46.474: INFO: Pod "pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11" satisfied condition "Succeeded or Failed"
    Sep  4 18:31:46.480: INFO: Trying to get logs from node tenant-000003 pod pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 09/04/23 18:31:46.514
    Sep  4 18:31:46.540: INFO: Waiting for pod pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11 to disappear
    Sep  4 18:31:46.545: INFO: Pod pod-projected-configmaps-bdc017d2-02c1-4b1d-91b3-80d6e1c3be11 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:31:46.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8723" for this suite. 09/04/23 18:31:46.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:31:46.573
Sep  4 18:31:46.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename endpointslice 09/04/23 18:31:46.574
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:31:46.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:31:46.609
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 09/04/23 18:31:46.618
STEP: getting /apis/discovery.k8s.io 09/04/23 18:31:46.624
STEP: getting /apis/discovery.k8s.iov1 09/04/23 18:31:46.628
STEP: creating 09/04/23 18:31:46.631
STEP: getting 09/04/23 18:31:46.664
STEP: listing 09/04/23 18:31:46.669
STEP: watching 09/04/23 18:31:46.675
Sep  4 18:31:46.675: INFO: starting watch
STEP: cluster-wide listing 09/04/23 18:31:46.679
STEP: cluster-wide watching 09/04/23 18:31:46.686
Sep  4 18:31:46.687: INFO: starting watch
STEP: patching 09/04/23 18:31:46.69
STEP: updating 09/04/23 18:31:46.7
Sep  4 18:31:46.716: INFO: waiting for watch events with expected annotations
Sep  4 18:31:46.716: INFO: saw patched and updated annotations
STEP: deleting 09/04/23 18:31:46.717
STEP: deleting a collection 09/04/23 18:31:46.736
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  4 18:31:46.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9412" for this suite. 09/04/23 18:31:46.771
------------------------------
â€¢ [0.208 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:31:46.573
    Sep  4 18:31:46.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename endpointslice 09/04/23 18:31:46.574
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:31:46.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:31:46.609
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 09/04/23 18:31:46.618
    STEP: getting /apis/discovery.k8s.io 09/04/23 18:31:46.624
    STEP: getting /apis/discovery.k8s.iov1 09/04/23 18:31:46.628
    STEP: creating 09/04/23 18:31:46.631
    STEP: getting 09/04/23 18:31:46.664
    STEP: listing 09/04/23 18:31:46.669
    STEP: watching 09/04/23 18:31:46.675
    Sep  4 18:31:46.675: INFO: starting watch
    STEP: cluster-wide listing 09/04/23 18:31:46.679
    STEP: cluster-wide watching 09/04/23 18:31:46.686
    Sep  4 18:31:46.687: INFO: starting watch
    STEP: patching 09/04/23 18:31:46.69
    STEP: updating 09/04/23 18:31:46.7
    Sep  4 18:31:46.716: INFO: waiting for watch events with expected annotations
    Sep  4 18:31:46.716: INFO: saw patched and updated annotations
    STEP: deleting 09/04/23 18:31:46.717
    STEP: deleting a collection 09/04/23 18:31:46.736
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:31:46.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9412" for this suite. 09/04/23 18:31:46.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:31:46.797
Sep  4 18:31:46.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 18:31:46.798
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:31:46.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:31:46.827
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 09/04/23 18:31:46.835
Sep  4 18:31:46.835: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-1066 proxy --unix-socket=/tmp/kubectl-proxy-unix1693922661/test'
STEP: retrieving proxy /api/ output 09/04/23 18:31:46.888
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 18:31:46.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1066" for this suite. 09/04/23 18:31:46.896
------------------------------
â€¢ [0.108 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:31:46.797
    Sep  4 18:31:46.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 18:31:46.798
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:31:46.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:31:46.827
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 09/04/23 18:31:46.835
    Sep  4 18:31:46.835: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-1066 proxy --unix-socket=/tmp/kubectl-proxy-unix1693922661/test'
    STEP: retrieving proxy /api/ output 09/04/23 18:31:46.888
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:31:46.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1066" for this suite. 09/04/23 18:31:46.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:31:46.905
Sep  4 18:31:46.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename statefulset 09/04/23 18:31:46.906
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:31:46.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:31:46.934
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5331 09/04/23 18:31:46.941
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 09/04/23 18:31:46.949
Sep  4 18:31:46.968: INFO: Found 0 stateful pods, waiting for 3
Sep  4 18:31:56.975: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 18:31:56.976: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 18:31:56.976: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/04/23 18:31:56.994
Sep  4 18:31:57.022: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/04/23 18:31:57.022
STEP: Not applying an update when the partition is greater than the number of replicas 09/04/23 18:32:07.047
STEP: Performing a canary update 09/04/23 18:32:07.048
Sep  4 18:32:07.077: INFO: Updating stateful set ss2
Sep  4 18:32:07.115: INFO: Waiting for Pod statefulset-5331/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 09/04/23 18:32:17.132
Sep  4 18:32:17.234: INFO: Found 2 stateful pods, waiting for 3
Sep  4 18:32:27.243: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 18:32:27.243: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 18:32:27.243: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 09/04/23 18:32:27.254
Sep  4 18:32:27.284: INFO: Updating stateful set ss2
Sep  4 18:32:27.298: INFO: Waiting for Pod statefulset-5331/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Sep  4 18:32:37.343: INFO: Updating stateful set ss2
Sep  4 18:32:37.356: INFO: Waiting for StatefulSet statefulset-5331/ss2 to complete update
Sep  4 18:32:37.356: INFO: Waiting for Pod statefulset-5331/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 18:32:47.369: INFO: Deleting all statefulset in ns statefulset-5331
Sep  4 18:32:47.374: INFO: Scaling statefulset ss2 to 0
Sep  4 18:32:57.404: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 18:32:57.409: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 18:32:57.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5331" for this suite. 09/04/23 18:32:57.45
------------------------------
â€¢ [SLOW TEST] [70.560 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:31:46.905
    Sep  4 18:31:46.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename statefulset 09/04/23 18:31:46.906
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:31:46.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:31:46.934
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5331 09/04/23 18:31:46.941
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 09/04/23 18:31:46.949
    Sep  4 18:31:46.968: INFO: Found 0 stateful pods, waiting for 3
    Sep  4 18:31:56.975: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 18:31:56.976: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 18:31:56.976: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/04/23 18:31:56.994
    Sep  4 18:31:57.022: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/04/23 18:31:57.022
    STEP: Not applying an update when the partition is greater than the number of replicas 09/04/23 18:32:07.047
    STEP: Performing a canary update 09/04/23 18:32:07.048
    Sep  4 18:32:07.077: INFO: Updating stateful set ss2
    Sep  4 18:32:07.115: INFO: Waiting for Pod statefulset-5331/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 09/04/23 18:32:17.132
    Sep  4 18:32:17.234: INFO: Found 2 stateful pods, waiting for 3
    Sep  4 18:32:27.243: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 18:32:27.243: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 18:32:27.243: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 09/04/23 18:32:27.254
    Sep  4 18:32:27.284: INFO: Updating stateful set ss2
    Sep  4 18:32:27.298: INFO: Waiting for Pod statefulset-5331/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Sep  4 18:32:37.343: INFO: Updating stateful set ss2
    Sep  4 18:32:37.356: INFO: Waiting for StatefulSet statefulset-5331/ss2 to complete update
    Sep  4 18:32:37.356: INFO: Waiting for Pod statefulset-5331/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 18:32:47.369: INFO: Deleting all statefulset in ns statefulset-5331
    Sep  4 18:32:47.374: INFO: Scaling statefulset ss2 to 0
    Sep  4 18:32:57.404: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 18:32:57.409: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:32:57.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5331" for this suite. 09/04/23 18:32:57.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:32:57.465
Sep  4 18:32:57.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename resourcequota 09/04/23 18:32:57.467
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:32:57.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:32:57.495
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 09/04/23 18:32:57.502
STEP: Creating a ResourceQuota 09/04/23 18:33:02.51
STEP: Ensuring resource quota status is calculated 09/04/23 18:33:02.517
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 18:33:04.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8742" for this suite. 09/04/23 18:33:04.533
------------------------------
â€¢ [SLOW TEST] [7.079 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:32:57.465
    Sep  4 18:32:57.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename resourcequota 09/04/23 18:32:57.467
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:32:57.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:32:57.495
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 09/04/23 18:32:57.502
    STEP: Creating a ResourceQuota 09/04/23 18:33:02.51
    STEP: Ensuring resource quota status is calculated 09/04/23 18:33:02.517
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:33:04.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8742" for this suite. 09/04/23 18:33:04.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:33:04.551
Sep  4 18:33:04.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename daemonsets 09/04/23 18:33:04.553
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:33:04.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:33:04.581
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 09/04/23 18:33:04.617
STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 18:33:04.63
Sep  4 18:33:04.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:33:04.653: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:33:05.691: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 18:33:05.691: INFO: Node tenant-000003 is running 0 daemon pod, expected 1
Sep  4 18:33:06.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 18:33:06.669: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 09/04/23 18:33:06.673
Sep  4 18:33:06.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 18:33:06.714: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:33:07.731: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 18:33:07.731: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:33:08.737: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 18:33:08.737: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:33:09.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 18:33:09.736: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/04/23 18:33:09.741
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9802, will wait for the garbage collector to delete the pods 09/04/23 18:33:09.742
Sep  4 18:33:09.813: INFO: Deleting DaemonSet.extensions daemon-set took: 13.967368ms
Sep  4 18:33:09.914: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.764162ms
Sep  4 18:33:12.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:33:12.838: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  4 18:33:12.849: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30973"},"items":null}

Sep  4 18:33:12.870: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30973"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:33:12.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9802" for this suite. 09/04/23 18:33:12.914
------------------------------
â€¢ [SLOW TEST] [8.371 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:33:04.551
    Sep  4 18:33:04.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename daemonsets 09/04/23 18:33:04.553
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:33:04.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:33:04.581
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 09/04/23 18:33:04.617
    STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 18:33:04.63
    Sep  4 18:33:04.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:33:04.653: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:33:05.691: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 18:33:05.691: INFO: Node tenant-000003 is running 0 daemon pod, expected 1
    Sep  4 18:33:06.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 18:33:06.669: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 09/04/23 18:33:06.673
    Sep  4 18:33:06.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 18:33:06.714: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:33:07.731: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 18:33:07.731: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:33:08.737: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 18:33:08.737: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:33:09.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 18:33:09.736: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/04/23 18:33:09.741
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9802, will wait for the garbage collector to delete the pods 09/04/23 18:33:09.742
    Sep  4 18:33:09.813: INFO: Deleting DaemonSet.extensions daemon-set took: 13.967368ms
    Sep  4 18:33:09.914: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.764162ms
    Sep  4 18:33:12.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:33:12.838: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  4 18:33:12.849: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30973"},"items":null}

    Sep  4 18:33:12.870: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30973"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:33:12.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9802" for this suite. 09/04/23 18:33:12.914
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:33:12.926
Sep  4 18:33:12.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename dns 09/04/23 18:33:12.927
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:33:12.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:33:12.96
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/04/23 18:33:12.967
Sep  4 18:33:12.977: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3255  ac05ce6a-2cf4-4ec7-a51a-67e3289bba2f 30978 0 2023-09-04 18:33:12 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-09-04 18:33:12 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sb6lk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sb6lk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 18:33:12.977: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3255" to be "running and ready"
Sep  4 18:33:12.994: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 16.545782ms
Sep  4 18:33:12.994: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:33:15.003: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.025981221s
Sep  4 18:33:15.004: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Sep  4 18:33:15.004: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 09/04/23 18:33:15.004
Sep  4 18:33:15.004: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3255 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 18:33:15.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 18:33:15.005: INFO: ExecWithOptions: Clientset creation
Sep  4 18:33:15.005: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3255/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 09/04/23 18:33:15.151
Sep  4 18:33:15.151: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3255 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 18:33:15.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 18:33:15.152: INFO: ExecWithOptions: Clientset creation
Sep  4 18:33:15.152: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3255/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  4 18:33:15.263: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 18:33:15.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3255" for this suite. 09/04/23 18:33:15.331
------------------------------
â€¢ [2.421 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:33:12.926
    Sep  4 18:33:12.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename dns 09/04/23 18:33:12.927
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:33:12.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:33:12.96
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/04/23 18:33:12.967
    Sep  4 18:33:12.977: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3255  ac05ce6a-2cf4-4ec7-a51a-67e3289bba2f 30978 0 2023-09-04 18:33:12 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-09-04 18:33:12 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sb6lk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sb6lk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 18:33:12.977: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3255" to be "running and ready"
    Sep  4 18:33:12.994: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 16.545782ms
    Sep  4 18:33:12.994: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:33:15.003: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.025981221s
    Sep  4 18:33:15.004: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Sep  4 18:33:15.004: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 09/04/23 18:33:15.004
    Sep  4 18:33:15.004: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3255 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 18:33:15.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 18:33:15.005: INFO: ExecWithOptions: Clientset creation
    Sep  4 18:33:15.005: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3255/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 09/04/23 18:33:15.151
    Sep  4 18:33:15.151: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3255 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 18:33:15.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 18:33:15.152: INFO: ExecWithOptions: Clientset creation
    Sep  4 18:33:15.152: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3255/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  4 18:33:15.263: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:33:15.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3255" for this suite. 09/04/23 18:33:15.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:33:15.353
Sep  4 18:33:15.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 18:33:15.354
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:33:15.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:33:15.381
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 18:33:15.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4414" for this suite. 09/04/23 18:33:15.404
------------------------------
â€¢ [0.067 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:33:15.353
    Sep  4 18:33:15.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 18:33:15.354
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:33:15.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:33:15.381
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:33:15.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4414" for this suite. 09/04/23 18:33:15.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:33:15.426
Sep  4 18:33:15.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename aggregator 09/04/23 18:33:15.428
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:33:15.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:33:15.466
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Sep  4 18:33:15.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 09/04/23 18:33:15.474
Sep  4 18:33:15.822: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  4 18:33:17.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 18:33:19.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 18:33:21.939: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 18:33:23.930: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 18:33:25.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 18:33:27.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 18:33:29.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 18:33:31.926: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 18:33:33.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 18:33:35.930: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 18:33:37.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 18:33:40.073: INFO: Waited 135.833352ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 09/04/23 18:33:40.168
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/04/23 18:33:40.174
STEP: List APIServices 09/04/23 18:33:40.188
Sep  4 18:33:40.201: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Sep  4 18:33:40.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-6319" for this suite. 09/04/23 18:33:40.571
------------------------------
â€¢ [SLOW TEST] [25.173 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:33:15.426
    Sep  4 18:33:15.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename aggregator 09/04/23 18:33:15.428
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:33:15.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:33:15.466
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Sep  4 18:33:15.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 09/04/23 18:33:15.474
    Sep  4 18:33:15.822: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Sep  4 18:33:17.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 18:33:19.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 18:33:21.939: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 18:33:23.930: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 18:33:25.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 18:33:27.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 18:33:29.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 18:33:31.926: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 18:33:33.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 18:33:35.930: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 18:33:37.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 33, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 18:33:40.073: INFO: Waited 135.833352ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 09/04/23 18:33:40.168
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/04/23 18:33:40.174
    STEP: List APIServices 09/04/23 18:33:40.188
    Sep  4 18:33:40.201: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:33:40.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-6319" for this suite. 09/04/23 18:33:40.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:33:40.605
Sep  4 18:33:40.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 18:33:40.612
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:33:40.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:33:40.647
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 09/04/23 18:33:40.654
Sep  4 18:33:40.674: INFO: Waiting up to 5m0s for pod "pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501" in namespace "emptydir-9806" to be "Succeeded or Failed"
Sep  4 18:33:40.684: INFO: Pod "pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501": Phase="Pending", Reason="", readiness=false. Elapsed: 9.778601ms
Sep  4 18:33:42.690: INFO: Pod "pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015914058s
Sep  4 18:33:44.692: INFO: Pod "pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01804108s
STEP: Saw pod success 09/04/23 18:33:44.692
Sep  4 18:33:44.692: INFO: Pod "pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501" satisfied condition "Succeeded or Failed"
Sep  4 18:33:44.701: INFO: Trying to get logs from node tenant-000001 pod pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501 container test-container: <nil>
STEP: delete the pod 09/04/23 18:33:44.735
Sep  4 18:33:44.754: INFO: Waiting for pod pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501 to disappear
Sep  4 18:33:44.759: INFO: Pod pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 18:33:44.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9806" for this suite. 09/04/23 18:33:44.767
------------------------------
â€¢ [4.174 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:33:40.605
    Sep  4 18:33:40.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 18:33:40.612
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:33:40.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:33:40.647
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/04/23 18:33:40.654
    Sep  4 18:33:40.674: INFO: Waiting up to 5m0s for pod "pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501" in namespace "emptydir-9806" to be "Succeeded or Failed"
    Sep  4 18:33:40.684: INFO: Pod "pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501": Phase="Pending", Reason="", readiness=false. Elapsed: 9.778601ms
    Sep  4 18:33:42.690: INFO: Pod "pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015914058s
    Sep  4 18:33:44.692: INFO: Pod "pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01804108s
    STEP: Saw pod success 09/04/23 18:33:44.692
    Sep  4 18:33:44.692: INFO: Pod "pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501" satisfied condition "Succeeded or Failed"
    Sep  4 18:33:44.701: INFO: Trying to get logs from node tenant-000001 pod pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501 container test-container: <nil>
    STEP: delete the pod 09/04/23 18:33:44.735
    Sep  4 18:33:44.754: INFO: Waiting for pod pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501 to disappear
    Sep  4 18:33:44.759: INFO: Pod pod-2de7b0c3-cad0-4958-b18e-9eb3b6379501 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:33:44.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9806" for this suite. 09/04/23 18:33:44.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:33:44.785
Sep  4 18:33:44.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename statefulset 09/04/23 18:33:44.787
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:33:44.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:33:44.82
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4463 09/04/23 18:33:44.827
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-4463 09/04/23 18:33:44.84
Sep  4 18:33:44.870: INFO: Found 0 stateful pods, waiting for 1
Sep  4 18:33:54.877: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 09/04/23 18:33:54.889
STEP: updating a scale subresource 09/04/23 18:33:54.896
STEP: verifying the statefulset Spec.Replicas was modified 09/04/23 18:33:54.904
STEP: Patch a scale subresource 09/04/23 18:33:54.912
STEP: verifying the statefulset Spec.Replicas was modified 09/04/23 18:33:54.93
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 18:33:54.935: INFO: Deleting all statefulset in ns statefulset-4463
Sep  4 18:33:54.943: INFO: Scaling statefulset ss to 0
Sep  4 18:34:04.992: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 18:34:05.000: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 18:34:05.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4463" for this suite. 09/04/23 18:34:05.037
------------------------------
â€¢ [SLOW TEST] [20.266 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:33:44.785
    Sep  4 18:33:44.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename statefulset 09/04/23 18:33:44.787
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:33:44.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:33:44.82
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4463 09/04/23 18:33:44.827
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-4463 09/04/23 18:33:44.84
    Sep  4 18:33:44.870: INFO: Found 0 stateful pods, waiting for 1
    Sep  4 18:33:54.877: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 09/04/23 18:33:54.889
    STEP: updating a scale subresource 09/04/23 18:33:54.896
    STEP: verifying the statefulset Spec.Replicas was modified 09/04/23 18:33:54.904
    STEP: Patch a scale subresource 09/04/23 18:33:54.912
    STEP: verifying the statefulset Spec.Replicas was modified 09/04/23 18:33:54.93
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 18:33:54.935: INFO: Deleting all statefulset in ns statefulset-4463
    Sep  4 18:33:54.943: INFO: Scaling statefulset ss to 0
    Sep  4 18:34:04.992: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 18:34:05.000: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:34:05.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4463" for this suite. 09/04/23 18:34:05.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:34:05.064
Sep  4 18:34:05.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename containers 09/04/23 18:34:05.065
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:34:05.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:34:05.093
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 09/04/23 18:34:05.1
Sep  4 18:34:05.115: INFO: Waiting up to 5m0s for pod "client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5" in namespace "containers-6420" to be "Succeeded or Failed"
Sep  4 18:34:05.128: INFO: Pod "client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.129624ms
Sep  4 18:34:07.135: INFO: Pod "client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018992882s
Sep  4 18:34:09.135: INFO: Pod "client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019309295s
STEP: Saw pod success 09/04/23 18:34:09.135
Sep  4 18:34:09.135: INFO: Pod "client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5" satisfied condition "Succeeded or Failed"
Sep  4 18:34:09.144: INFO: Trying to get logs from node tenant-000001 pod client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 18:34:09.157
Sep  4 18:34:09.174: INFO: Waiting for pod client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5 to disappear
Sep  4 18:34:09.179: INFO: Pod client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  4 18:34:09.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6420" for this suite. 09/04/23 18:34:09.195
------------------------------
â€¢ [4.146 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:34:05.064
    Sep  4 18:34:05.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename containers 09/04/23 18:34:05.065
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:34:05.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:34:05.093
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 09/04/23 18:34:05.1
    Sep  4 18:34:05.115: INFO: Waiting up to 5m0s for pod "client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5" in namespace "containers-6420" to be "Succeeded or Failed"
    Sep  4 18:34:05.128: INFO: Pod "client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.129624ms
    Sep  4 18:34:07.135: INFO: Pod "client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018992882s
    Sep  4 18:34:09.135: INFO: Pod "client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019309295s
    STEP: Saw pod success 09/04/23 18:34:09.135
    Sep  4 18:34:09.135: INFO: Pod "client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5" satisfied condition "Succeeded or Failed"
    Sep  4 18:34:09.144: INFO: Trying to get logs from node tenant-000001 pod client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 18:34:09.157
    Sep  4 18:34:09.174: INFO: Waiting for pod client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5 to disappear
    Sep  4 18:34:09.179: INFO: Pod client-containers-7bdccbe0-c437-4e9a-80b6-8e36457434d5 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:34:09.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6420" for this suite. 09/04/23 18:34:09.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:34:09.214
Sep  4 18:34:09.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename security-context-test 09/04/23 18:34:09.216
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:34:09.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:34:09.248
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Sep  4 18:34:09.272: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-5b82f519-d6fb-44cf-9458-ef2e2ca81728" in namespace "security-context-test-1772" to be "Succeeded or Failed"
Sep  4 18:34:09.288: INFO: Pod "alpine-nnp-false-5b82f519-d6fb-44cf-9458-ef2e2ca81728": Phase="Pending", Reason="", readiness=false. Elapsed: 15.614661ms
Sep  4 18:34:11.297: INFO: Pod "alpine-nnp-false-5b82f519-d6fb-44cf-9458-ef2e2ca81728": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024242664s
Sep  4 18:34:13.297: INFO: Pod "alpine-nnp-false-5b82f519-d6fb-44cf-9458-ef2e2ca81728": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024435344s
Sep  4 18:34:15.297: INFO: Pod "alpine-nnp-false-5b82f519-d6fb-44cf-9458-ef2e2ca81728": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024973447s
Sep  4 18:34:15.298: INFO: Pod "alpine-nnp-false-5b82f519-d6fb-44cf-9458-ef2e2ca81728" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  4 18:34:15.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1772" for this suite. 09/04/23 18:34:15.328
------------------------------
â€¢ [SLOW TEST] [6.123 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:34:09.214
    Sep  4 18:34:09.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename security-context-test 09/04/23 18:34:09.216
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:34:09.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:34:09.248
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Sep  4 18:34:09.272: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-5b82f519-d6fb-44cf-9458-ef2e2ca81728" in namespace "security-context-test-1772" to be "Succeeded or Failed"
    Sep  4 18:34:09.288: INFO: Pod "alpine-nnp-false-5b82f519-d6fb-44cf-9458-ef2e2ca81728": Phase="Pending", Reason="", readiness=false. Elapsed: 15.614661ms
    Sep  4 18:34:11.297: INFO: Pod "alpine-nnp-false-5b82f519-d6fb-44cf-9458-ef2e2ca81728": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024242664s
    Sep  4 18:34:13.297: INFO: Pod "alpine-nnp-false-5b82f519-d6fb-44cf-9458-ef2e2ca81728": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024435344s
    Sep  4 18:34:15.297: INFO: Pod "alpine-nnp-false-5b82f519-d6fb-44cf-9458-ef2e2ca81728": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024973447s
    Sep  4 18:34:15.298: INFO: Pod "alpine-nnp-false-5b82f519-d6fb-44cf-9458-ef2e2ca81728" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:34:15.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1772" for this suite. 09/04/23 18:34:15.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:34:15.338
Sep  4 18:34:15.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename var-expansion 09/04/23 18:34:15.338
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:34:15.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:34:15.369
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 09/04/23 18:34:15.377
Sep  4 18:34:15.393: INFO: Waiting up to 2m0s for pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70" in namespace "var-expansion-1364" to be "running"
Sep  4 18:34:15.410: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 16.31421ms
Sep  4 18:34:17.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023516167s
Sep  4 18:34:19.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023851356s
Sep  4 18:34:21.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02535983s
Sep  4 18:34:23.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022832829s
Sep  4 18:34:25.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023184024s
Sep  4 18:34:27.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023659487s
Sep  4 18:34:29.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026040565s
Sep  4 18:34:31.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023964482s
Sep  4 18:34:33.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 18.026116198s
Sep  4 18:34:35.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 20.02286778s
Sep  4 18:34:37.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 22.023573562s
Sep  4 18:34:39.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 24.025757244s
Sep  4 18:34:41.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 26.023805648s
Sep  4 18:34:43.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 28.025578602s
Sep  4 18:34:45.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 30.02581622s
Sep  4 18:34:47.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 32.02322025s
Sep  4 18:34:49.418: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 34.024478735s
Sep  4 18:34:51.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 36.025475056s
Sep  4 18:34:53.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 38.023545923s
Sep  4 18:34:55.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 40.023540746s
Sep  4 18:34:57.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 42.026222672s
Sep  4 18:34:59.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 44.023320617s
Sep  4 18:35:01.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 46.024030056s
Sep  4 18:35:03.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 48.025881371s
Sep  4 18:35:05.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 50.023295492s
Sep  4 18:35:07.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 52.023943828s
Sep  4 18:35:09.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 54.026199331s
Sep  4 18:35:11.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 56.02322916s
Sep  4 18:35:13.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 58.023807043s
Sep  4 18:35:15.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.025840142s
Sep  4 18:35:17.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023770957s
Sep  4 18:35:19.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.023662505s
Sep  4 18:35:21.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.023810669s
Sep  4 18:35:23.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025552828s
Sep  4 18:35:25.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.026147678s
Sep  4 18:35:27.415: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.021948097s
Sep  4 18:35:29.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.023176492s
Sep  4 18:35:31.418: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.024607712s
Sep  4 18:35:33.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.023419721s
Sep  4 18:35:35.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.025402469s
Sep  4 18:35:37.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.025755594s
Sep  4 18:35:39.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.023105431s
Sep  4 18:35:41.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.023305964s
Sep  4 18:35:43.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.026083375s
Sep  4 18:35:45.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.022858973s
Sep  4 18:35:47.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.023690114s
Sep  4 18:35:49.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.025898752s
Sep  4 18:35:51.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.022892131s
Sep  4 18:35:53.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.023438854s
Sep  4 18:35:55.418: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.024839834s
Sep  4 18:35:57.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.023307683s
Sep  4 18:35:59.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.023349942s
Sep  4 18:36:01.418: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.024917506s
Sep  4 18:36:03.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.023652679s
Sep  4 18:36:05.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.023559644s
Sep  4 18:36:07.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.025938278s
Sep  4 18:36:09.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.023758899s
Sep  4 18:36:11.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.023960609s
Sep  4 18:36:13.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.026024511s
Sep  4 18:36:15.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.02324398s
Sep  4 18:36:15.422: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.02890727s
STEP: updating the pod 09/04/23 18:36:15.422
Sep  4 18:36:15.972: INFO: Successfully updated pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70"
STEP: waiting for pod running 09/04/23 18:36:15.972
Sep  4 18:36:15.973: INFO: Waiting up to 2m0s for pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70" in namespace "var-expansion-1364" to be "running"
Sep  4 18:36:15.979: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 5.873504ms
Sep  4 18:36:17.989: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Running", Reason="", readiness=true. Elapsed: 2.015344015s
Sep  4 18:36:17.989: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70" satisfied condition "running"
STEP: deleting the pod gracefully 09/04/23 18:36:17.989
Sep  4 18:36:17.989: INFO: Deleting pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70" in namespace "var-expansion-1364"
Sep  4 18:36:18.022: INFO: Wait up to 5m0s for pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 18:36:50.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1364" for this suite. 09/04/23 18:36:50.049
------------------------------
â€¢ [SLOW TEST] [154.724 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:34:15.338
    Sep  4 18:34:15.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename var-expansion 09/04/23 18:34:15.338
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:34:15.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:34:15.369
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 09/04/23 18:34:15.377
    Sep  4 18:34:15.393: INFO: Waiting up to 2m0s for pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70" in namespace "var-expansion-1364" to be "running"
    Sep  4 18:34:15.410: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 16.31421ms
    Sep  4 18:34:17.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023516167s
    Sep  4 18:34:19.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023851356s
    Sep  4 18:34:21.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02535983s
    Sep  4 18:34:23.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022832829s
    Sep  4 18:34:25.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023184024s
    Sep  4 18:34:27.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023659487s
    Sep  4 18:34:29.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026040565s
    Sep  4 18:34:31.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023964482s
    Sep  4 18:34:33.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 18.026116198s
    Sep  4 18:34:35.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 20.02286778s
    Sep  4 18:34:37.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 22.023573562s
    Sep  4 18:34:39.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 24.025757244s
    Sep  4 18:34:41.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 26.023805648s
    Sep  4 18:34:43.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 28.025578602s
    Sep  4 18:34:45.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 30.02581622s
    Sep  4 18:34:47.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 32.02322025s
    Sep  4 18:34:49.418: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 34.024478735s
    Sep  4 18:34:51.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 36.025475056s
    Sep  4 18:34:53.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 38.023545923s
    Sep  4 18:34:55.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 40.023540746s
    Sep  4 18:34:57.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 42.026222672s
    Sep  4 18:34:59.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 44.023320617s
    Sep  4 18:35:01.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 46.024030056s
    Sep  4 18:35:03.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 48.025881371s
    Sep  4 18:35:05.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 50.023295492s
    Sep  4 18:35:07.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 52.023943828s
    Sep  4 18:35:09.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 54.026199331s
    Sep  4 18:35:11.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 56.02322916s
    Sep  4 18:35:13.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 58.023807043s
    Sep  4 18:35:15.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.025840142s
    Sep  4 18:35:17.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023770957s
    Sep  4 18:35:19.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.023662505s
    Sep  4 18:35:21.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.023810669s
    Sep  4 18:35:23.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025552828s
    Sep  4 18:35:25.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.026147678s
    Sep  4 18:35:27.415: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.021948097s
    Sep  4 18:35:29.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.023176492s
    Sep  4 18:35:31.418: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.024607712s
    Sep  4 18:35:33.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.023419721s
    Sep  4 18:35:35.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.025402469s
    Sep  4 18:35:37.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.025755594s
    Sep  4 18:35:39.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.023105431s
    Sep  4 18:35:41.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.023305964s
    Sep  4 18:35:43.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.026083375s
    Sep  4 18:35:45.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.022858973s
    Sep  4 18:35:47.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.023690114s
    Sep  4 18:35:49.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.025898752s
    Sep  4 18:35:51.416: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.022892131s
    Sep  4 18:35:53.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.023438854s
    Sep  4 18:35:55.418: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.024839834s
    Sep  4 18:35:57.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.023307683s
    Sep  4 18:35:59.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.023349942s
    Sep  4 18:36:01.418: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.024917506s
    Sep  4 18:36:03.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.023652679s
    Sep  4 18:36:05.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.023559644s
    Sep  4 18:36:07.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.025938278s
    Sep  4 18:36:09.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.023758899s
    Sep  4 18:36:11.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.023960609s
    Sep  4 18:36:13.419: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.026024511s
    Sep  4 18:36:15.417: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.02324398s
    Sep  4 18:36:15.422: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.02890727s
    STEP: updating the pod 09/04/23 18:36:15.422
    Sep  4 18:36:15.972: INFO: Successfully updated pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70"
    STEP: waiting for pod running 09/04/23 18:36:15.972
    Sep  4 18:36:15.973: INFO: Waiting up to 2m0s for pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70" in namespace "var-expansion-1364" to be "running"
    Sep  4 18:36:15.979: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Pending", Reason="", readiness=false. Elapsed: 5.873504ms
    Sep  4 18:36:17.989: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70": Phase="Running", Reason="", readiness=true. Elapsed: 2.015344015s
    Sep  4 18:36:17.989: INFO: Pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70" satisfied condition "running"
    STEP: deleting the pod gracefully 09/04/23 18:36:17.989
    Sep  4 18:36:17.989: INFO: Deleting pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70" in namespace "var-expansion-1364"
    Sep  4 18:36:18.022: INFO: Wait up to 5m0s for pod "var-expansion-497b107a-d1bc-4d63-b7b3-40a07f964d70" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:36:50.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1364" for this suite. 09/04/23 18:36:50.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:36:50.062
Sep  4 18:36:50.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 18:36:50.063
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:36:50.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:36:50.093
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Sep  4 18:36:50.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9747 version'
Sep  4 18:36:50.168: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Sep  4 18:36:50.168: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:13:53Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:05:35Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 18:36:50.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9747" for this suite. 09/04/23 18:36:50.174
------------------------------
â€¢ [0.124 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:36:50.062
    Sep  4 18:36:50.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 18:36:50.063
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:36:50.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:36:50.093
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Sep  4 18:36:50.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9747 version'
    Sep  4 18:36:50.168: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Sep  4 18:36:50.168: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:13:53Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:05:35Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:36:50.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9747" for this suite. 09/04/23 18:36:50.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:36:50.188
Sep  4 18:36:50.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 18:36:50.189
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:36:50.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:36:50.219
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 18:36:50.247
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:36:51.17
STEP: Deploying the webhook pod 09/04/23 18:36:51.183
STEP: Wait for the deployment to be ready 09/04/23 18:36:51.209
Sep  4 18:36:51.237: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 18:36:53.255
STEP: Verifying the service has paired with the endpoint 09/04/23 18:36:53.276
Sep  4 18:36:54.277: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Sep  4 18:36:54.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3370-crds.webhook.example.com via the AdmissionRegistration API 09/04/23 18:36:54.805
STEP: Creating a custom resource that should be mutated by the webhook 09/04/23 18:36:54.835
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:36:57.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4714" for this suite. 09/04/23 18:36:57.549
STEP: Destroying namespace "webhook-4714-markers" for this suite. 09/04/23 18:36:57.577
------------------------------
â€¢ [SLOW TEST] [7.412 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:36:50.188
    Sep  4 18:36:50.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 18:36:50.189
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:36:50.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:36:50.219
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 18:36:50.247
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:36:51.17
    STEP: Deploying the webhook pod 09/04/23 18:36:51.183
    STEP: Wait for the deployment to be ready 09/04/23 18:36:51.209
    Sep  4 18:36:51.237: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 18:36:53.255
    STEP: Verifying the service has paired with the endpoint 09/04/23 18:36:53.276
    Sep  4 18:36:54.277: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Sep  4 18:36:54.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3370-crds.webhook.example.com via the AdmissionRegistration API 09/04/23 18:36:54.805
    STEP: Creating a custom resource that should be mutated by the webhook 09/04/23 18:36:54.835
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:36:57.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4714" for this suite. 09/04/23 18:36:57.549
    STEP: Destroying namespace "webhook-4714-markers" for this suite. 09/04/23 18:36:57.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:36:57.62
Sep  4 18:36:57.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 18:36:57.626
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:36:57.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:36:57.69
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-8e26e2a8-0119-4c4a-8ed4-1efef7cea000 09/04/23 18:36:57.717
STEP: Creating a pod to test consume secrets 09/04/23 18:36:57.732
Sep  4 18:36:57.759: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee" in namespace "projected-2116" to be "Succeeded or Failed"
Sep  4 18:36:57.768: INFO: Pod "pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee": Phase="Pending", Reason="", readiness=false. Elapsed: 8.538971ms
Sep  4 18:36:59.775: INFO: Pod "pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015818873s
Sep  4 18:37:01.776: INFO: Pod "pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01651075s
STEP: Saw pod success 09/04/23 18:37:01.776
Sep  4 18:37:01.776: INFO: Pod "pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee" satisfied condition "Succeeded or Failed"
Sep  4 18:37:01.781: INFO: Trying to get logs from node tenant-000001 pod pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee container projected-secret-volume-test: <nil>
STEP: delete the pod 09/04/23 18:37:01.818
Sep  4 18:37:01.843: INFO: Waiting for pod pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee to disappear
Sep  4 18:37:01.859: INFO: Pod pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  4 18:37:01.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2116" for this suite. 09/04/23 18:37:01.869
------------------------------
â€¢ [4.261 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:36:57.62
    Sep  4 18:36:57.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 18:36:57.626
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:36:57.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:36:57.69
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-8e26e2a8-0119-4c4a-8ed4-1efef7cea000 09/04/23 18:36:57.717
    STEP: Creating a pod to test consume secrets 09/04/23 18:36:57.732
    Sep  4 18:36:57.759: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee" in namespace "projected-2116" to be "Succeeded or Failed"
    Sep  4 18:36:57.768: INFO: Pod "pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee": Phase="Pending", Reason="", readiness=false. Elapsed: 8.538971ms
    Sep  4 18:36:59.775: INFO: Pod "pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015818873s
    Sep  4 18:37:01.776: INFO: Pod "pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01651075s
    STEP: Saw pod success 09/04/23 18:37:01.776
    Sep  4 18:37:01.776: INFO: Pod "pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee" satisfied condition "Succeeded or Failed"
    Sep  4 18:37:01.781: INFO: Trying to get logs from node tenant-000001 pod pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 18:37:01.818
    Sep  4 18:37:01.843: INFO: Waiting for pod pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee to disappear
    Sep  4 18:37:01.859: INFO: Pod pod-projected-secrets-6146abfd-b00f-4fa9-aa88-09395671a4ee no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:37:01.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2116" for this suite. 09/04/23 18:37:01.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:37:01.899
Sep  4 18:37:01.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename statefulset 09/04/23 18:37:01.9
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:37:01.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:37:01.935
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6294 09/04/23 18:37:01.949
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 09/04/23 18:37:01.958
Sep  4 18:37:01.976: INFO: Found 0 stateful pods, waiting for 3
Sep  4 18:37:11.987: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 18:37:11.987: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 18:37:11.987: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 18:37:12.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-6294 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 18:37:12.229: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 18:37:12.229: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 18:37:12.229: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/04/23 18:37:22.257
Sep  4 18:37:22.282: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/04/23 18:37:22.283
STEP: Updating Pods in reverse ordinal order 09/04/23 18:37:32.31
Sep  4 18:37:32.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-6294 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 18:37:32.517: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  4 18:37:32.517: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 18:37:32.517: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 09/04/23 18:37:42.557
Sep  4 18:37:42.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-6294 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 18:37:42.910: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 18:37:42.910: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 18:37:42.910: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 18:37:52.963: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 09/04/23 18:38:02.998
Sep  4 18:38:03.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-6294 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 18:38:03.201: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  4 18:38:03.201: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 18:38:03.201: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 18:38:13.248: INFO: Deleting all statefulset in ns statefulset-6294
Sep  4 18:38:13.253: INFO: Scaling statefulset ss2 to 0
Sep  4 18:38:23.280: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 18:38:23.286: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 18:38:23.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6294" for this suite. 09/04/23 18:38:23.32
------------------------------
â€¢ [SLOW TEST] [81.433 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:37:01.899
    Sep  4 18:37:01.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename statefulset 09/04/23 18:37:01.9
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:37:01.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:37:01.935
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6294 09/04/23 18:37:01.949
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 09/04/23 18:37:01.958
    Sep  4 18:37:01.976: INFO: Found 0 stateful pods, waiting for 3
    Sep  4 18:37:11.987: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 18:37:11.987: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 18:37:11.987: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 18:37:12.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-6294 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 18:37:12.229: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 18:37:12.229: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 18:37:12.229: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/04/23 18:37:22.257
    Sep  4 18:37:22.282: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/04/23 18:37:22.283
    STEP: Updating Pods in reverse ordinal order 09/04/23 18:37:32.31
    Sep  4 18:37:32.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-6294 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 18:37:32.517: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  4 18:37:32.517: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 18:37:32.517: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 09/04/23 18:37:42.557
    Sep  4 18:37:42.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-6294 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 18:37:42.910: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 18:37:42.910: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 18:37:42.910: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 18:37:52.963: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 09/04/23 18:38:02.998
    Sep  4 18:38:03.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=statefulset-6294 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 18:38:03.201: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  4 18:38:03.201: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 18:38:03.201: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 18:38:13.248: INFO: Deleting all statefulset in ns statefulset-6294
    Sep  4 18:38:13.253: INFO: Scaling statefulset ss2 to 0
    Sep  4 18:38:23.280: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 18:38:23.286: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:38:23.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6294" for this suite. 09/04/23 18:38:23.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:38:23.338
Sep  4 18:38:23.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename containers 09/04/23 18:38:23.339
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:23.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:23.368
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 09/04/23 18:38:23.375
Sep  4 18:38:23.390: INFO: Waiting up to 5m0s for pod "client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4" in namespace "containers-5122" to be "Succeeded or Failed"
Sep  4 18:38:23.395: INFO: Pod "client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.874501ms
Sep  4 18:38:25.404: INFO: Pod "client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013632244s
Sep  4 18:38:27.403: INFO: Pod "client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013309074s
STEP: Saw pod success 09/04/23 18:38:27.404
Sep  4 18:38:27.404: INFO: Pod "client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4" satisfied condition "Succeeded or Failed"
Sep  4 18:38:27.413: INFO: Trying to get logs from node tenant-000001 pod client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 18:38:27.425
Sep  4 18:38:27.445: INFO: Waiting for pod client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4 to disappear
Sep  4 18:38:27.452: INFO: Pod client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  4 18:38:27.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5122" for this suite. 09/04/23 18:38:27.464
------------------------------
â€¢ [4.137 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:38:23.338
    Sep  4 18:38:23.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename containers 09/04/23 18:38:23.339
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:23.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:23.368
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 09/04/23 18:38:23.375
    Sep  4 18:38:23.390: INFO: Waiting up to 5m0s for pod "client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4" in namespace "containers-5122" to be "Succeeded or Failed"
    Sep  4 18:38:23.395: INFO: Pod "client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.874501ms
    Sep  4 18:38:25.404: INFO: Pod "client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013632244s
    Sep  4 18:38:27.403: INFO: Pod "client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013309074s
    STEP: Saw pod success 09/04/23 18:38:27.404
    Sep  4 18:38:27.404: INFO: Pod "client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4" satisfied condition "Succeeded or Failed"
    Sep  4 18:38:27.413: INFO: Trying to get logs from node tenant-000001 pod client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 18:38:27.425
    Sep  4 18:38:27.445: INFO: Waiting for pod client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4 to disappear
    Sep  4 18:38:27.452: INFO: Pod client-containers-4dd610ea-d1e1-4d3b-b5dd-1e864084f4c4 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:38:27.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5122" for this suite. 09/04/23 18:38:27.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:38:27.479
Sep  4 18:38:27.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 18:38:27.482
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:27.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:27.507
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-8cc50c86-3124-4571-94e3-87a79ecd5f26 09/04/23 18:38:27.522
STEP: Creating the pod 09/04/23 18:38:27.532
Sep  4 18:38:27.543: INFO: Waiting up to 5m0s for pod "pod-configmaps-d33d4c25-5c19-473e-a535-68e2a6f9a7c9" in namespace "configmap-9454" to be "running and ready"
Sep  4 18:38:27.548: INFO: Pod "pod-configmaps-d33d4c25-5c19-473e-a535-68e2a6f9a7c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.82609ms
Sep  4 18:38:27.548: INFO: The phase of Pod pod-configmaps-d33d4c25-5c19-473e-a535-68e2a6f9a7c9 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:38:29.557: INFO: Pod "pod-configmaps-d33d4c25-5c19-473e-a535-68e2a6f9a7c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.013705761s
Sep  4 18:38:29.557: INFO: The phase of Pod pod-configmaps-d33d4c25-5c19-473e-a535-68e2a6f9a7c9 is Running (Ready = true)
Sep  4 18:38:29.558: INFO: Pod "pod-configmaps-d33d4c25-5c19-473e-a535-68e2a6f9a7c9" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-8cc50c86-3124-4571-94e3-87a79ecd5f26 09/04/23 18:38:29.575
STEP: waiting to observe update in volume 09/04/23 18:38:29.587
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 18:38:31.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9454" for this suite. 09/04/23 18:38:31.624
------------------------------
â€¢ [4.159 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:38:27.479
    Sep  4 18:38:27.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 18:38:27.482
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:27.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:27.507
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-8cc50c86-3124-4571-94e3-87a79ecd5f26 09/04/23 18:38:27.522
    STEP: Creating the pod 09/04/23 18:38:27.532
    Sep  4 18:38:27.543: INFO: Waiting up to 5m0s for pod "pod-configmaps-d33d4c25-5c19-473e-a535-68e2a6f9a7c9" in namespace "configmap-9454" to be "running and ready"
    Sep  4 18:38:27.548: INFO: Pod "pod-configmaps-d33d4c25-5c19-473e-a535-68e2a6f9a7c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.82609ms
    Sep  4 18:38:27.548: INFO: The phase of Pod pod-configmaps-d33d4c25-5c19-473e-a535-68e2a6f9a7c9 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:38:29.557: INFO: Pod "pod-configmaps-d33d4c25-5c19-473e-a535-68e2a6f9a7c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.013705761s
    Sep  4 18:38:29.557: INFO: The phase of Pod pod-configmaps-d33d4c25-5c19-473e-a535-68e2a6f9a7c9 is Running (Ready = true)
    Sep  4 18:38:29.558: INFO: Pod "pod-configmaps-d33d4c25-5c19-473e-a535-68e2a6f9a7c9" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-8cc50c86-3124-4571-94e3-87a79ecd5f26 09/04/23 18:38:29.575
    STEP: waiting to observe update in volume 09/04/23 18:38:29.587
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:38:31.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9454" for this suite. 09/04/23 18:38:31.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:38:31.639
Sep  4 18:38:31.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 18:38:31.644
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:31.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:31.678
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/04/23 18:38:31.685
Sep  4 18:38:31.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 18:38:33.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:38:41.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4976" for this suite. 09/04/23 18:38:41.061
------------------------------
â€¢ [SLOW TEST] [9.432 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:38:31.639
    Sep  4 18:38:31.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 18:38:31.644
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:31.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:31.678
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/04/23 18:38:31.685
    Sep  4 18:38:31.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 18:38:33.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:38:41.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4976" for this suite. 09/04/23 18:38:41.061
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:38:41.076
Sep  4 18:38:41.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename var-expansion 09/04/23 18:38:41.078
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:41.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:41.121
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Sep  4 18:38:41.142: INFO: Waiting up to 2m0s for pod "var-expansion-2ce2e9cb-7ed5-47e3-8317-5dfdde363701" in namespace "var-expansion-7057" to be "container 0 failed with reason CreateContainerConfigError"
Sep  4 18:38:41.157: INFO: Pod "var-expansion-2ce2e9cb-7ed5-47e3-8317-5dfdde363701": Phase="Pending", Reason="", readiness=false. Elapsed: 14.74591ms
Sep  4 18:38:43.164: INFO: Pod "var-expansion-2ce2e9cb-7ed5-47e3-8317-5dfdde363701": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021570634s
Sep  4 18:38:43.164: INFO: Pod "var-expansion-2ce2e9cb-7ed5-47e3-8317-5dfdde363701" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep  4 18:38:43.164: INFO: Deleting pod "var-expansion-2ce2e9cb-7ed5-47e3-8317-5dfdde363701" in namespace "var-expansion-7057"
Sep  4 18:38:43.191: INFO: Wait up to 5m0s for pod "var-expansion-2ce2e9cb-7ed5-47e3-8317-5dfdde363701" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 18:38:45.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7057" for this suite. 09/04/23 18:38:45.215
------------------------------
â€¢ [4.152 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:38:41.076
    Sep  4 18:38:41.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename var-expansion 09/04/23 18:38:41.078
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:41.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:41.121
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Sep  4 18:38:41.142: INFO: Waiting up to 2m0s for pod "var-expansion-2ce2e9cb-7ed5-47e3-8317-5dfdde363701" in namespace "var-expansion-7057" to be "container 0 failed with reason CreateContainerConfigError"
    Sep  4 18:38:41.157: INFO: Pod "var-expansion-2ce2e9cb-7ed5-47e3-8317-5dfdde363701": Phase="Pending", Reason="", readiness=false. Elapsed: 14.74591ms
    Sep  4 18:38:43.164: INFO: Pod "var-expansion-2ce2e9cb-7ed5-47e3-8317-5dfdde363701": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021570634s
    Sep  4 18:38:43.164: INFO: Pod "var-expansion-2ce2e9cb-7ed5-47e3-8317-5dfdde363701" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep  4 18:38:43.164: INFO: Deleting pod "var-expansion-2ce2e9cb-7ed5-47e3-8317-5dfdde363701" in namespace "var-expansion-7057"
    Sep  4 18:38:43.191: INFO: Wait up to 5m0s for pod "var-expansion-2ce2e9cb-7ed5-47e3-8317-5dfdde363701" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:38:45.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7057" for this suite. 09/04/23 18:38:45.215
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:38:45.232
Sep  4 18:38:45.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 18:38:45.233
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:45.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:45.259
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 09/04/23 18:38:45.267
Sep  4 18:38:45.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-5183 cluster-info'
Sep  4 18:38:45.349: INFO: stderr: ""
Sep  4 18:38:45.349: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 18:38:45.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5183" for this suite. 09/04/23 18:38:45.355
------------------------------
â€¢ [0.137 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:38:45.232
    Sep  4 18:38:45.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 18:38:45.233
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:45.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:45.259
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 09/04/23 18:38:45.267
    Sep  4 18:38:45.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-5183 cluster-info'
    Sep  4 18:38:45.349: INFO: stderr: ""
    Sep  4 18:38:45.349: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:38:45.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5183" for this suite. 09/04/23 18:38:45.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:38:45.369
Sep  4 18:38:45.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename cronjob 09/04/23 18:38:45.371
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:45.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:45.41
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 09/04/23 18:38:45.417
STEP: creating 09/04/23 18:38:45.417
STEP: getting 09/04/23 18:38:45.425
STEP: listing 09/04/23 18:38:45.433
STEP: watching 09/04/23 18:38:45.441
Sep  4 18:38:45.441: INFO: starting watch
STEP: cluster-wide listing 09/04/23 18:38:45.444
STEP: cluster-wide watching 09/04/23 18:38:45.449
Sep  4 18:38:45.449: INFO: starting watch
STEP: patching 09/04/23 18:38:45.452
STEP: updating 09/04/23 18:38:45.463
Sep  4 18:38:45.483: INFO: waiting for watch events with expected annotations
Sep  4 18:38:45.483: INFO: saw patched and updated annotations
STEP: patching /status 09/04/23 18:38:45.483
STEP: updating /status 09/04/23 18:38:45.493
STEP: get /status 09/04/23 18:38:45.51
STEP: deleting 09/04/23 18:38:45.515
STEP: deleting a collection 09/04/23 18:38:45.542
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  4 18:38:45.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5955" for this suite. 09/04/23 18:38:45.568
------------------------------
â€¢ [0.213 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:38:45.369
    Sep  4 18:38:45.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename cronjob 09/04/23 18:38:45.371
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:45.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:45.41
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 09/04/23 18:38:45.417
    STEP: creating 09/04/23 18:38:45.417
    STEP: getting 09/04/23 18:38:45.425
    STEP: listing 09/04/23 18:38:45.433
    STEP: watching 09/04/23 18:38:45.441
    Sep  4 18:38:45.441: INFO: starting watch
    STEP: cluster-wide listing 09/04/23 18:38:45.444
    STEP: cluster-wide watching 09/04/23 18:38:45.449
    Sep  4 18:38:45.449: INFO: starting watch
    STEP: patching 09/04/23 18:38:45.452
    STEP: updating 09/04/23 18:38:45.463
    Sep  4 18:38:45.483: INFO: waiting for watch events with expected annotations
    Sep  4 18:38:45.483: INFO: saw patched and updated annotations
    STEP: patching /status 09/04/23 18:38:45.483
    STEP: updating /status 09/04/23 18:38:45.493
    STEP: get /status 09/04/23 18:38:45.51
    STEP: deleting 09/04/23 18:38:45.515
    STEP: deleting a collection 09/04/23 18:38:45.542
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:38:45.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5955" for this suite. 09/04/23 18:38:45.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:38:45.589
Sep  4 18:38:45.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename secrets 09/04/23 18:38:45.591
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:45.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:45.617
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-c1b4062c-1f37-48f0-a909-692b29e0336e 09/04/23 18:38:45.629
STEP: Creating secret with name s-test-opt-upd-7ac1b631-0d9b-4552-82d5-063638146f76 09/04/23 18:38:45.637
STEP: Creating the pod 09/04/23 18:38:45.646
Sep  4 18:38:45.667: INFO: Waiting up to 5m0s for pod "pod-secrets-e3f7f937-ed12-4a27-af49-cf4a153abf81" in namespace "secrets-9540" to be "running and ready"
Sep  4 18:38:45.683: INFO: Pod "pod-secrets-e3f7f937-ed12-4a27-af49-cf4a153abf81": Phase="Pending", Reason="", readiness=false. Elapsed: 15.618776ms
Sep  4 18:38:45.683: INFO: The phase of Pod pod-secrets-e3f7f937-ed12-4a27-af49-cf4a153abf81 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:38:47.689: INFO: Pod "pod-secrets-e3f7f937-ed12-4a27-af49-cf4a153abf81": Phase="Running", Reason="", readiness=true. Elapsed: 2.022517767s
Sep  4 18:38:47.690: INFO: The phase of Pod pod-secrets-e3f7f937-ed12-4a27-af49-cf4a153abf81 is Running (Ready = true)
Sep  4 18:38:47.690: INFO: Pod "pod-secrets-e3f7f937-ed12-4a27-af49-cf4a153abf81" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-c1b4062c-1f37-48f0-a909-692b29e0336e 09/04/23 18:38:47.759
STEP: Updating secret s-test-opt-upd-7ac1b631-0d9b-4552-82d5-063638146f76 09/04/23 18:38:47.777
STEP: Creating secret with name s-test-opt-create-8230fe10-0ebf-49ae-a3d2-c00013975e33 09/04/23 18:38:47.784
STEP: waiting to observe update in volume 09/04/23 18:38:47.794
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 18:38:49.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9540" for this suite. 09/04/23 18:38:49.859
------------------------------
â€¢ [4.279 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:38:45.589
    Sep  4 18:38:45.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename secrets 09/04/23 18:38:45.591
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:45.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:45.617
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-c1b4062c-1f37-48f0-a909-692b29e0336e 09/04/23 18:38:45.629
    STEP: Creating secret with name s-test-opt-upd-7ac1b631-0d9b-4552-82d5-063638146f76 09/04/23 18:38:45.637
    STEP: Creating the pod 09/04/23 18:38:45.646
    Sep  4 18:38:45.667: INFO: Waiting up to 5m0s for pod "pod-secrets-e3f7f937-ed12-4a27-af49-cf4a153abf81" in namespace "secrets-9540" to be "running and ready"
    Sep  4 18:38:45.683: INFO: Pod "pod-secrets-e3f7f937-ed12-4a27-af49-cf4a153abf81": Phase="Pending", Reason="", readiness=false. Elapsed: 15.618776ms
    Sep  4 18:38:45.683: INFO: The phase of Pod pod-secrets-e3f7f937-ed12-4a27-af49-cf4a153abf81 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:38:47.689: INFO: Pod "pod-secrets-e3f7f937-ed12-4a27-af49-cf4a153abf81": Phase="Running", Reason="", readiness=true. Elapsed: 2.022517767s
    Sep  4 18:38:47.690: INFO: The phase of Pod pod-secrets-e3f7f937-ed12-4a27-af49-cf4a153abf81 is Running (Ready = true)
    Sep  4 18:38:47.690: INFO: Pod "pod-secrets-e3f7f937-ed12-4a27-af49-cf4a153abf81" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-c1b4062c-1f37-48f0-a909-692b29e0336e 09/04/23 18:38:47.759
    STEP: Updating secret s-test-opt-upd-7ac1b631-0d9b-4552-82d5-063638146f76 09/04/23 18:38:47.777
    STEP: Creating secret with name s-test-opt-create-8230fe10-0ebf-49ae-a3d2-c00013975e33 09/04/23 18:38:47.784
    STEP: waiting to observe update in volume 09/04/23 18:38:47.794
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:38:49.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9540" for this suite. 09/04/23 18:38:49.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:38:49.873
Sep  4 18:38:49.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubelet-test 09/04/23 18:38:49.875
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:49.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:49.903
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 09/04/23 18:38:49.919
Sep  4 18:38:49.920: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases37a58a0c-ea03-4106-b3b6-00df449c85da" in namespace "kubelet-test-6437" to be "completed"
Sep  4 18:38:49.932: INFO: Pod "agnhost-host-aliases37a58a0c-ea03-4106-b3b6-00df449c85da": Phase="Pending", Reason="", readiness=false. Elapsed: 12.061143ms
Sep  4 18:38:51.938: INFO: Pod "agnhost-host-aliases37a58a0c-ea03-4106-b3b6-00df449c85da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018429858s
Sep  4 18:38:53.940: INFO: Pod "agnhost-host-aliases37a58a0c-ea03-4106-b3b6-00df449c85da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020308344s
Sep  4 18:38:53.940: INFO: Pod "agnhost-host-aliases37a58a0c-ea03-4106-b3b6-00df449c85da" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  4 18:38:53.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6437" for this suite. 09/04/23 18:38:53.998
------------------------------
â€¢ [4.139 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:38:49.873
    Sep  4 18:38:49.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubelet-test 09/04/23 18:38:49.875
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:49.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:49.903
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 09/04/23 18:38:49.919
    Sep  4 18:38:49.920: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases37a58a0c-ea03-4106-b3b6-00df449c85da" in namespace "kubelet-test-6437" to be "completed"
    Sep  4 18:38:49.932: INFO: Pod "agnhost-host-aliases37a58a0c-ea03-4106-b3b6-00df449c85da": Phase="Pending", Reason="", readiness=false. Elapsed: 12.061143ms
    Sep  4 18:38:51.938: INFO: Pod "agnhost-host-aliases37a58a0c-ea03-4106-b3b6-00df449c85da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018429858s
    Sep  4 18:38:53.940: INFO: Pod "agnhost-host-aliases37a58a0c-ea03-4106-b3b6-00df449c85da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020308344s
    Sep  4 18:38:53.940: INFO: Pod "agnhost-host-aliases37a58a0c-ea03-4106-b3b6-00df449c85da" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:38:53.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6437" for this suite. 09/04/23 18:38:53.998
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:38:54.016
Sep  4 18:38:54.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-runtime 09/04/23 18:38:54.017
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:54.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:54.043
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/04/23 18:38:54.062
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/04/23 18:39:12.211
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/04/23 18:39:12.216
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/04/23 18:39:12.229
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/04/23 18:39:12.229
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/04/23 18:39:12.266
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/04/23 18:39:15.303
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/04/23 18:39:17.362
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/04/23 18:39:17.376
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/04/23 18:39:17.376
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/04/23 18:39:17.41
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/04/23 18:39:18.432
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/04/23 18:39:21.458
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/04/23 18:39:21.468
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/04/23 18:39:21.469
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  4 18:39:21.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9232" for this suite. 09/04/23 18:39:21.524
------------------------------
â€¢ [SLOW TEST] [27.520 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:38:54.016
    Sep  4 18:38:54.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-runtime 09/04/23 18:38:54.017
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:38:54.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:38:54.043
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/04/23 18:38:54.062
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/04/23 18:39:12.211
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/04/23 18:39:12.216
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/04/23 18:39:12.229
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/04/23 18:39:12.229
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/04/23 18:39:12.266
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/04/23 18:39:15.303
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/04/23 18:39:17.362
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/04/23 18:39:17.376
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/04/23 18:39:17.376
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/04/23 18:39:17.41
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/04/23 18:39:18.432
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/04/23 18:39:21.458
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/04/23 18:39:21.468
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/04/23 18:39:21.469
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:39:21.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9232" for this suite. 09/04/23 18:39:21.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:39:21.541
Sep  4 18:39:21.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 18:39:21.542
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:39:21.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:39:21.582
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Sep  4 18:39:21.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/04/23 18:39:23.583
Sep  4 18:39:23.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-3979 --namespace=crd-publish-openapi-3979 create -f -'
Sep  4 18:39:24.311: INFO: stderr: ""
Sep  4 18:39:24.311: INFO: stdout: "e2e-test-crd-publish-openapi-9989-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  4 18:39:24.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-3979 --namespace=crd-publish-openapi-3979 delete e2e-test-crd-publish-openapi-9989-crds test-cr'
Sep  4 18:39:24.438: INFO: stderr: ""
Sep  4 18:39:24.438: INFO: stdout: "e2e-test-crd-publish-openapi-9989-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep  4 18:39:24.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-3979 --namespace=crd-publish-openapi-3979 apply -f -'
Sep  4 18:39:25.011: INFO: stderr: ""
Sep  4 18:39:25.011: INFO: stdout: "e2e-test-crd-publish-openapi-9989-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  4 18:39:25.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-3979 --namespace=crd-publish-openapi-3979 delete e2e-test-crd-publish-openapi-9989-crds test-cr'
Sep  4 18:39:25.108: INFO: stderr: ""
Sep  4 18:39:25.108: INFO: stdout: "e2e-test-crd-publish-openapi-9989-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 09/04/23 18:39:25.108
Sep  4 18:39:25.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-3979 explain e2e-test-crd-publish-openapi-9989-crds'
Sep  4 18:39:25.373: INFO: stderr: ""
Sep  4 18:39:25.373: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9989-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:39:27.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3979" for this suite. 09/04/23 18:39:27.273
------------------------------
â€¢ [SLOW TEST] [5.742 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:39:21.541
    Sep  4 18:39:21.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 18:39:21.542
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:39:21.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:39:21.582
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Sep  4 18:39:21.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/04/23 18:39:23.583
    Sep  4 18:39:23.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-3979 --namespace=crd-publish-openapi-3979 create -f -'
    Sep  4 18:39:24.311: INFO: stderr: ""
    Sep  4 18:39:24.311: INFO: stdout: "e2e-test-crd-publish-openapi-9989-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep  4 18:39:24.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-3979 --namespace=crd-publish-openapi-3979 delete e2e-test-crd-publish-openapi-9989-crds test-cr'
    Sep  4 18:39:24.438: INFO: stderr: ""
    Sep  4 18:39:24.438: INFO: stdout: "e2e-test-crd-publish-openapi-9989-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Sep  4 18:39:24.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-3979 --namespace=crd-publish-openapi-3979 apply -f -'
    Sep  4 18:39:25.011: INFO: stderr: ""
    Sep  4 18:39:25.011: INFO: stdout: "e2e-test-crd-publish-openapi-9989-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep  4 18:39:25.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-3979 --namespace=crd-publish-openapi-3979 delete e2e-test-crd-publish-openapi-9989-crds test-cr'
    Sep  4 18:39:25.108: INFO: stderr: ""
    Sep  4 18:39:25.108: INFO: stdout: "e2e-test-crd-publish-openapi-9989-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 09/04/23 18:39:25.108
    Sep  4 18:39:25.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-3979 explain e2e-test-crd-publish-openapi-9989-crds'
    Sep  4 18:39:25.373: INFO: stderr: ""
    Sep  4 18:39:25.373: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9989-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:39:27.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3979" for this suite. 09/04/23 18:39:27.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:39:27.289
Sep  4 18:39:27.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename deployment 09/04/23 18:39:27.291
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:39:27.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:39:27.329
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Sep  4 18:39:27.336: INFO: Creating deployment "test-recreate-deployment"
Sep  4 18:39:27.345: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  4 18:39:27.381: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  4 18:39:27.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 39, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 39, 27, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-795566c5cb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 39, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 39, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Sep  4 18:39:29.437: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  4 18:39:29.458: INFO: Updating deployment test-recreate-deployment
Sep  4 18:39:29.458: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 18:39:29.596: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1387  bd29c054-32ca-4ea8-b7a7-58cea514244a 33011 2 2023-09-04 18:39:27 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a44308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-04 18:39:29 +0000 UTC,LastTransitionTime:2023-09-04 18:39:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-09-04 18:39:29 +0000 UTC,LastTransitionTime:2023-09-04 18:39:27 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep  4 18:39:29.601: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-1387  95c0b1dc-9673-4e4a-a14e-1a35e4ea04ae 33010 1 2023-09-04 18:39:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment bd29c054-32ca-4ea8-b7a7-58cea514244a 0xc003a44ec0 0xc003a44ec1}] [] [{kube-controller-manager Update apps/v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd29c054-32ca-4ea8-b7a7-58cea514244a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a44f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  4 18:39:29.601: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  4 18:39:29.601: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-1387  fc8e17bf-39a8-487e-a8a8-fbd530cb7686 32999 2 2023-09-04 18:39:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment bd29c054-32ca-4ea8-b7a7-58cea514244a 0xc003a446d7 0xc003a446d8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd29c054-32ca-4ea8-b7a7-58cea514244a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a44898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  4 18:39:29.609: INFO: Pod "test-recreate-deployment-cff6dc657-2s88l" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-2s88l test-recreate-deployment-cff6dc657- deployment-1387  68294179-30c8-4a86-9e1d-2a2d0231e1df 33009 0 2023-09-04 18:39:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 95c0b1dc-9673-4e4a-a14e-1a35e4ea04ae 0xc003a45580 0xc003a45581}] [] [{kube-controller-manager Update v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95c0b1dc-9673-4e4a-a14e-1a35e4ea04ae\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2tv82,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2tv82,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:39:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:39:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:39:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:39:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:,StartTime:2023-09-04 18:39:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 18:39:29.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1387" for this suite. 09/04/23 18:39:29.62
------------------------------
â€¢ [2.340 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:39:27.289
    Sep  4 18:39:27.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename deployment 09/04/23 18:39:27.291
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:39:27.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:39:27.329
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Sep  4 18:39:27.336: INFO: Creating deployment "test-recreate-deployment"
    Sep  4 18:39:27.345: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Sep  4 18:39:27.381: INFO: Waiting deployment "test-recreate-deployment" to complete
    Sep  4 18:39:27.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 39, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 39, 27, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-795566c5cb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 18, 39, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 39, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Sep  4 18:39:29.437: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Sep  4 18:39:29.458: INFO: Updating deployment test-recreate-deployment
    Sep  4 18:39:29.458: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 18:39:29.596: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1387  bd29c054-32ca-4ea8-b7a7-58cea514244a 33011 2 2023-09-04 18:39:27 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a44308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-04 18:39:29 +0000 UTC,LastTransitionTime:2023-09-04 18:39:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-09-04 18:39:29 +0000 UTC,LastTransitionTime:2023-09-04 18:39:27 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Sep  4 18:39:29.601: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-1387  95c0b1dc-9673-4e4a-a14e-1a35e4ea04ae 33010 1 2023-09-04 18:39:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment bd29c054-32ca-4ea8-b7a7-58cea514244a 0xc003a44ec0 0xc003a44ec1}] [] [{kube-controller-manager Update apps/v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd29c054-32ca-4ea8-b7a7-58cea514244a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a44f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 18:39:29.601: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Sep  4 18:39:29.601: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-1387  fc8e17bf-39a8-487e-a8a8-fbd530cb7686 32999 2 2023-09-04 18:39:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment bd29c054-32ca-4ea8-b7a7-58cea514244a 0xc003a446d7 0xc003a446d8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd29c054-32ca-4ea8-b7a7-58cea514244a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a44898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 18:39:29.609: INFO: Pod "test-recreate-deployment-cff6dc657-2s88l" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-2s88l test-recreate-deployment-cff6dc657- deployment-1387  68294179-30c8-4a86-9e1d-2a2d0231e1df 33009 0 2023-09-04 18:39:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 95c0b1dc-9673-4e4a-a14e-1a35e4ea04ae 0xc003a45580 0xc003a45581}] [] [{kube-controller-manager Update v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95c0b1dc-9673-4e4a-a14e-1a35e4ea04ae\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 18:39:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2tv82,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2tv82,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:39:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:39:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:39:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:39:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:,StartTime:2023-09-04 18:39:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:39:29.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1387" for this suite. 09/04/23 18:39:29.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:39:29.643
Sep  4 18:39:29.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename replication-controller 09/04/23 18:39:29.644
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:39:29.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:39:29.674
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 09/04/23 18:39:29.682
STEP: When the matched label of one of its pods change 09/04/23 18:39:29.694
Sep  4 18:39:29.699: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  4 18:39:34.706: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 09/04/23 18:39:34.747
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  4 18:39:34.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5086" for this suite. 09/04/23 18:39:34.792
------------------------------
â€¢ [SLOW TEST] [5.161 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:39:29.643
    Sep  4 18:39:29.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename replication-controller 09/04/23 18:39:29.644
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:39:29.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:39:29.674
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 09/04/23 18:39:29.682
    STEP: When the matched label of one of its pods change 09/04/23 18:39:29.694
    Sep  4 18:39:29.699: INFO: Pod name pod-release: Found 0 pods out of 1
    Sep  4 18:39:34.706: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/04/23 18:39:34.747
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:39:34.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5086" for this suite. 09/04/23 18:39:34.792
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:39:34.809
Sep  4 18:39:34.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 18:39:34.81
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:39:34.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:39:34.838
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 09/04/23 18:39:34.846
Sep  4 18:39:34.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: rename a version 09/04/23 18:39:39.017
STEP: check the new version name is served 09/04/23 18:39:39.035
STEP: check the old version name is removed 09/04/23 18:39:41.01
STEP: check the other version is not changed 09/04/23 18:39:41.796
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:39:45.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9199" for this suite. 09/04/23 18:39:45.654
------------------------------
â€¢ [SLOW TEST] [10.856 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:39:34.809
    Sep  4 18:39:34.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 18:39:34.81
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:39:34.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:39:34.838
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 09/04/23 18:39:34.846
    Sep  4 18:39:34.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: rename a version 09/04/23 18:39:39.017
    STEP: check the new version name is served 09/04/23 18:39:39.035
    STEP: check the old version name is removed 09/04/23 18:39:41.01
    STEP: check the other version is not changed 09/04/23 18:39:41.796
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:39:45.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9199" for this suite. 09/04/23 18:39:45.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:39:45.675
Sep  4 18:39:45.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename taint-multiple-pods 09/04/23 18:39:45.676
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:39:45.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:39:45.706
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Sep  4 18:39:45.711: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  4 18:40:45.740: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Sep  4 18:40:45.746: INFO: Starting informer...
STEP: Starting pods... 09/04/23 18:40:45.746
Sep  4 18:40:45.977: INFO: Pod1 is running on tenant-000001. Tainting Node
Sep  4 18:40:46.191: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8601" to be "running"
Sep  4 18:40:46.196: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.141689ms
Sep  4 18:40:48.203: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011525562s
Sep  4 18:40:48.203: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Sep  4 18:40:48.203: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8601" to be "running"
Sep  4 18:40:48.209: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 5.269397ms
Sep  4 18:40:48.209: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Sep  4 18:40:48.209: INFO: Pod2 is running on tenant-000001. Tainting Node
STEP: Trying to apply a taint on the Node 09/04/23 18:40:48.209
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 18:40:48.228
STEP: Waiting for Pod1 and Pod2 to be deleted 09/04/23 18:40:48.235
Sep  4 18:40:54.447: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep  4 18:41:14.488: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 18:41:14.508
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:41:14.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-8601" for this suite. 09/04/23 18:41:14.523
------------------------------
â€¢ [SLOW TEST] [88.864 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:39:45.675
    Sep  4 18:39:45.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename taint-multiple-pods 09/04/23 18:39:45.676
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:39:45.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:39:45.706
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Sep  4 18:39:45.711: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  4 18:40:45.740: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Sep  4 18:40:45.746: INFO: Starting informer...
    STEP: Starting pods... 09/04/23 18:40:45.746
    Sep  4 18:40:45.977: INFO: Pod1 is running on tenant-000001. Tainting Node
    Sep  4 18:40:46.191: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8601" to be "running"
    Sep  4 18:40:46.196: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.141689ms
    Sep  4 18:40:48.203: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011525562s
    Sep  4 18:40:48.203: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Sep  4 18:40:48.203: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8601" to be "running"
    Sep  4 18:40:48.209: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 5.269397ms
    Sep  4 18:40:48.209: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Sep  4 18:40:48.209: INFO: Pod2 is running on tenant-000001. Tainting Node
    STEP: Trying to apply a taint on the Node 09/04/23 18:40:48.209
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 18:40:48.228
    STEP: Waiting for Pod1 and Pod2 to be deleted 09/04/23 18:40:48.235
    Sep  4 18:40:54.447: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Sep  4 18:41:14.488: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 18:41:14.508
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:41:14.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-8601" for this suite. 09/04/23 18:41:14.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:41:14.544
Sep  4 18:41:14.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename replication-controller 09/04/23 18:41:14.546
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:41:14.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:41:14.595
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Sep  4 18:41:14.611: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/04/23 18:41:14.629
STEP: Checking rc "condition-test" has the desired failure condition set 09/04/23 18:41:14.639
STEP: Scaling down rc "condition-test" to satisfy pod quota 09/04/23 18:41:15.65
Sep  4 18:41:15.663: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 09/04/23 18:41:15.664
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  4 18:41:16.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2121" for this suite. 09/04/23 18:41:16.684
------------------------------
â€¢ [2.154 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:41:14.544
    Sep  4 18:41:14.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename replication-controller 09/04/23 18:41:14.546
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:41:14.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:41:14.595
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Sep  4 18:41:14.611: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/04/23 18:41:14.629
    STEP: Checking rc "condition-test" has the desired failure condition set 09/04/23 18:41:14.639
    STEP: Scaling down rc "condition-test" to satisfy pod quota 09/04/23 18:41:15.65
    Sep  4 18:41:15.663: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 09/04/23 18:41:15.664
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:41:16.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2121" for this suite. 09/04/23 18:41:16.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:41:16.698
Sep  4 18:41:16.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename deployment 09/04/23 18:41:16.699
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:41:16.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:41:16.726
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Sep  4 18:41:16.753: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  4 18:41:21.785: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/04/23 18:41:21.794
Sep  4 18:41:21.794: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/04/23 18:41:21.811
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 18:41:21.832: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8603  be34f27d-8998-433c-b57a-4df2e9b92e42 33495 1 2023-09-04 18:41:21 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-09-04 18:41:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047c27d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep  4 18:41:21.854: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-8603  0362b8e8-9d91-4118-b202-c6aa310e2780 33498 1 2023-09-04 18:41:21 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment be34f27d-8998-433c-b57a-4df2e9b92e42 0xc0047c2db7 0xc0047c2db8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 18:41:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"be34f27d-8998-433c-b57a-4df2e9b92e42\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047c2e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  4 18:41:21.855: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep  4 18:41:21.855: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8603  001420ad-d8d5-4b3e-baf2-b8c32ccc94a6 33497 1 2023-09-04 18:41:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment be34f27d-8998-433c-b57a-4df2e9b92e42 0xc0047c2c87 0xc0047c2c88}] [] [{e2e.test Update apps/v1 2023-09-04 18:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:41:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-04 18:41:21 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"be34f27d-8998-433c-b57a-4df2e9b92e42\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0047c2d48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  4 18:41:21.874: INFO: Pod "test-cleanup-controller-hhkm6" is available:
&Pod{ObjectMeta:{test-cleanup-controller-hhkm6 test-cleanup-controller- deployment-8603  718fa5df-9d9e-401d-8741-3645d703998e 33458 0 2023-09-04 18:41:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:be8eb4c9bd71d4face8a38e1a1adc43c7929e0c5625d07927d0c4fca66f69329 cni.projectcalico.org/podIP:10.36.55.78/32 cni.projectcalico.org/podIPs:10.36.55.78/32] [{apps/v1 ReplicaSet test-cleanup-controller 001420ad-d8d5-4b3e-baf2-b8c32ccc94a6 0xc0047c32b7 0xc0047c32b8}] [] [{kube-controller-manager Update v1 2023-09-04 18:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"001420ad-d8d5-4b3e-baf2-b8c32ccc94a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 18:41:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 18:41:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-thpmd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-thpmd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:41:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:41:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:41:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:41:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.78,StartTime:2023-09-04 18:41:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 18:41:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://49478d372311a156b1e43699c2e45d08f205576a28c3b46bd4155fa6b4bcd458,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 18:41:21.874: INFO: Pod "test-cleanup-deployment-7698ff6f6b-r9m2z" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-r9m2z test-cleanup-deployment-7698ff6f6b- deployment-8603  668e4a0d-23ff-4464-a09a-52cc83b31cc1 33505 0 2023-09-04 18:41:21 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 0362b8e8-9d91-4118-b202-c6aa310e2780 0xc0047c34c7 0xc0047c34c8}] [] [{kube-controller-manager Update v1 2023-09-04 18:41:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0362b8e8-9d91-4118-b202-c6aa310e2780\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bqb5w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bqb5w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:41:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 18:41:21.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8603" for this suite. 09/04/23 18:41:21.886
------------------------------
â€¢ [SLOW TEST] [5.205 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:41:16.698
    Sep  4 18:41:16.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename deployment 09/04/23 18:41:16.699
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:41:16.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:41:16.726
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Sep  4 18:41:16.753: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Sep  4 18:41:21.785: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/04/23 18:41:21.794
    Sep  4 18:41:21.794: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/04/23 18:41:21.811
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 18:41:21.832: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8603  be34f27d-8998-433c-b57a-4df2e9b92e42 33495 1 2023-09-04 18:41:21 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-09-04 18:41:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047c27d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Sep  4 18:41:21.854: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-8603  0362b8e8-9d91-4118-b202-c6aa310e2780 33498 1 2023-09-04 18:41:21 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment be34f27d-8998-433c-b57a-4df2e9b92e42 0xc0047c2db7 0xc0047c2db8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 18:41:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"be34f27d-8998-433c-b57a-4df2e9b92e42\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047c2e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 18:41:21.855: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Sep  4 18:41:21.855: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8603  001420ad-d8d5-4b3e-baf2-b8c32ccc94a6 33497 1 2023-09-04 18:41:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment be34f27d-8998-433c-b57a-4df2e9b92e42 0xc0047c2c87 0xc0047c2c88}] [] [{e2e.test Update apps/v1 2023-09-04 18:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:41:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-04 18:41:21 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"be34f27d-8998-433c-b57a-4df2e9b92e42\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0047c2d48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 18:41:21.874: INFO: Pod "test-cleanup-controller-hhkm6" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-hhkm6 test-cleanup-controller- deployment-8603  718fa5df-9d9e-401d-8741-3645d703998e 33458 0 2023-09-04 18:41:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:be8eb4c9bd71d4face8a38e1a1adc43c7929e0c5625d07927d0c4fca66f69329 cni.projectcalico.org/podIP:10.36.55.78/32 cni.projectcalico.org/podIPs:10.36.55.78/32] [{apps/v1 ReplicaSet test-cleanup-controller 001420ad-d8d5-4b3e-baf2-b8c32ccc94a6 0xc0047c32b7 0xc0047c32b8}] [] [{kube-controller-manager Update v1 2023-09-04 18:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"001420ad-d8d5-4b3e-baf2-b8c32ccc94a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 18:41:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 18:41:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-thpmd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-thpmd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:41:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:41:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:41:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:41:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.78,StartTime:2023-09-04 18:41:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 18:41:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://49478d372311a156b1e43699c2e45d08f205576a28c3b46bd4155fa6b4bcd458,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 18:41:21.874: INFO: Pod "test-cleanup-deployment-7698ff6f6b-r9m2z" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-r9m2z test-cleanup-deployment-7698ff6f6b- deployment-8603  668e4a0d-23ff-4464-a09a-52cc83b31cc1 33505 0 2023-09-04 18:41:21 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 0362b8e8-9d91-4118-b202-c6aa310e2780 0xc0047c34c7 0xc0047c34c8}] [] [{kube-controller-manager Update v1 2023-09-04 18:41:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0362b8e8-9d91-4118-b202-c6aa310e2780\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bqb5w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bqb5w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:41:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:41:21.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8603" for this suite. 09/04/23 18:41:21.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:41:21.913
Sep  4 18:41:21.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubelet-test 09/04/23 18:41:21.914
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:41:21.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:41:21.951
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  4 18:41:21.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4158" for this suite. 09/04/23 18:41:21.992
------------------------------
â€¢ [0.088 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:41:21.913
    Sep  4 18:41:21.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubelet-test 09/04/23 18:41:21.914
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:41:21.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:41:21.951
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:41:21.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4158" for this suite. 09/04/23 18:41:21.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:41:22.016
Sep  4 18:41:22.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename secrets 09/04/23 18:41:22.017
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:41:22.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:41:22.046
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-b3ce4e12-ad5e-45da-aa55-688fdf098047 09/04/23 18:41:22.051
STEP: Creating a pod to test consume secrets 09/04/23 18:41:22.061
Sep  4 18:41:22.079: INFO: Waiting up to 5m0s for pod "pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2" in namespace "secrets-3055" to be "Succeeded or Failed"
Sep  4 18:41:22.091: INFO: Pod "pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.856322ms
Sep  4 18:41:24.097: INFO: Pod "pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017165102s
Sep  4 18:41:26.097: INFO: Pod "pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017483592s
STEP: Saw pod success 09/04/23 18:41:26.098
Sep  4 18:41:26.098: INFO: Pod "pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2" satisfied condition "Succeeded or Failed"
Sep  4 18:41:26.103: INFO: Trying to get logs from node tenant-000003 pod pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2 container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 18:41:26.142
Sep  4 18:41:26.162: INFO: Waiting for pod pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2 to disappear
Sep  4 18:41:26.168: INFO: Pod pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 18:41:26.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3055" for this suite. 09/04/23 18:41:26.177
------------------------------
â€¢ [4.176 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:41:22.016
    Sep  4 18:41:22.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename secrets 09/04/23 18:41:22.017
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:41:22.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:41:22.046
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-b3ce4e12-ad5e-45da-aa55-688fdf098047 09/04/23 18:41:22.051
    STEP: Creating a pod to test consume secrets 09/04/23 18:41:22.061
    Sep  4 18:41:22.079: INFO: Waiting up to 5m0s for pod "pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2" in namespace "secrets-3055" to be "Succeeded or Failed"
    Sep  4 18:41:22.091: INFO: Pod "pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.856322ms
    Sep  4 18:41:24.097: INFO: Pod "pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017165102s
    Sep  4 18:41:26.097: INFO: Pod "pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017483592s
    STEP: Saw pod success 09/04/23 18:41:26.098
    Sep  4 18:41:26.098: INFO: Pod "pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2" satisfied condition "Succeeded or Failed"
    Sep  4 18:41:26.103: INFO: Trying to get logs from node tenant-000003 pod pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2 container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 18:41:26.142
    Sep  4 18:41:26.162: INFO: Waiting for pod pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2 to disappear
    Sep  4 18:41:26.168: INFO: Pod pod-secrets-e20af4b7-1007-454c-8496-93e6860d8eb2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:41:26.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3055" for this suite. 09/04/23 18:41:26.177
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:41:26.196
Sep  4 18:41:26.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename proxy 09/04/23 18:41:26.198
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:41:26.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:41:26.226
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Sep  4 18:41:26.231: INFO: Creating pod...
Sep  4 18:41:26.242: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6738" to be "running"
Sep  4 18:41:26.253: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 11.012426ms
Sep  4 18:41:28.261: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.018516791s
Sep  4 18:41:28.261: INFO: Pod "agnhost" satisfied condition "running"
Sep  4 18:41:28.261: INFO: Creating service...
Sep  4 18:41:28.284: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/pods/agnhost/proxy/some/path/with/DELETE
Sep  4 18:41:28.306: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  4 18:41:28.307: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/pods/agnhost/proxy/some/path/with/GET
Sep  4 18:41:28.317: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep  4 18:41:28.318: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/pods/agnhost/proxy/some/path/with/HEAD
Sep  4 18:41:28.330: INFO: http.Client request:HEAD | StatusCode:200
Sep  4 18:41:28.330: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/pods/agnhost/proxy/some/path/with/OPTIONS
Sep  4 18:41:28.340: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  4 18:41:28.340: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/pods/agnhost/proxy/some/path/with/PATCH
Sep  4 18:41:28.352: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  4 18:41:28.352: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/pods/agnhost/proxy/some/path/with/POST
Sep  4 18:41:28.363: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  4 18:41:28.363: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/pods/agnhost/proxy/some/path/with/PUT
Sep  4 18:41:28.377: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  4 18:41:28.377: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/services/test-service/proxy/some/path/with/DELETE
Sep  4 18:41:28.390: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  4 18:41:28.391: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/services/test-service/proxy/some/path/with/GET
Sep  4 18:41:28.402: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep  4 18:41:28.402: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/services/test-service/proxy/some/path/with/HEAD
Sep  4 18:41:28.411: INFO: http.Client request:HEAD | StatusCode:200
Sep  4 18:41:28.412: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/services/test-service/proxy/some/path/with/OPTIONS
Sep  4 18:41:28.421: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  4 18:41:28.421: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/services/test-service/proxy/some/path/with/PATCH
Sep  4 18:41:28.430: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  4 18:41:28.430: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/services/test-service/proxy/some/path/with/POST
Sep  4 18:41:28.440: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  4 18:41:28.440: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/services/test-service/proxy/some/path/with/PUT
Sep  4 18:41:28.450: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  4 18:41:28.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6738" for this suite. 09/04/23 18:41:28.458
------------------------------
â€¢ [2.277 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:41:26.196
    Sep  4 18:41:26.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename proxy 09/04/23 18:41:26.198
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:41:26.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:41:26.226
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Sep  4 18:41:26.231: INFO: Creating pod...
    Sep  4 18:41:26.242: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6738" to be "running"
    Sep  4 18:41:26.253: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 11.012426ms
    Sep  4 18:41:28.261: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.018516791s
    Sep  4 18:41:28.261: INFO: Pod "agnhost" satisfied condition "running"
    Sep  4 18:41:28.261: INFO: Creating service...
    Sep  4 18:41:28.284: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/pods/agnhost/proxy/some/path/with/DELETE
    Sep  4 18:41:28.306: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  4 18:41:28.307: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/pods/agnhost/proxy/some/path/with/GET
    Sep  4 18:41:28.317: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep  4 18:41:28.318: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/pods/agnhost/proxy/some/path/with/HEAD
    Sep  4 18:41:28.330: INFO: http.Client request:HEAD | StatusCode:200
    Sep  4 18:41:28.330: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/pods/agnhost/proxy/some/path/with/OPTIONS
    Sep  4 18:41:28.340: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  4 18:41:28.340: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/pods/agnhost/proxy/some/path/with/PATCH
    Sep  4 18:41:28.352: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  4 18:41:28.352: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/pods/agnhost/proxy/some/path/with/POST
    Sep  4 18:41:28.363: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  4 18:41:28.363: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/pods/agnhost/proxy/some/path/with/PUT
    Sep  4 18:41:28.377: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  4 18:41:28.377: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/services/test-service/proxy/some/path/with/DELETE
    Sep  4 18:41:28.390: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  4 18:41:28.391: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/services/test-service/proxy/some/path/with/GET
    Sep  4 18:41:28.402: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep  4 18:41:28.402: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/services/test-service/proxy/some/path/with/HEAD
    Sep  4 18:41:28.411: INFO: http.Client request:HEAD | StatusCode:200
    Sep  4 18:41:28.412: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/services/test-service/proxy/some/path/with/OPTIONS
    Sep  4 18:41:28.421: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  4 18:41:28.421: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/services/test-service/proxy/some/path/with/PATCH
    Sep  4 18:41:28.430: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  4 18:41:28.430: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/services/test-service/proxy/some/path/with/POST
    Sep  4 18:41:28.440: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  4 18:41:28.440: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6738/services/test-service/proxy/some/path/with/PUT
    Sep  4 18:41:28.450: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:41:28.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6738" for this suite. 09/04/23 18:41:28.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:41:28.482
Sep  4 18:41:28.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 18:41:28.484
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:41:28.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:41:28.514
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 18:41:28.55
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:41:29.446
STEP: Deploying the webhook pod 09/04/23 18:41:29.458
STEP: Wait for the deployment to be ready 09/04/23 18:41:29.483
Sep  4 18:41:29.502: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 18:41:31.517
STEP: Verifying the service has paired with the endpoint 09/04/23 18:41:31.541
Sep  4 18:41:32.542: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/04/23 18:41:32.549
Sep  4 18:41:32.580: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/04/23 18:41:32.702
STEP: Creating a dummy validating-webhook-configuration object 09/04/23 18:41:32.81
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/04/23 18:41:32.893
STEP: Creating a dummy mutating-webhook-configuration object 09/04/23 18:41:32.905
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/04/23 18:41:32.922
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:41:32.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5773" for this suite. 09/04/23 18:41:33.039
STEP: Destroying namespace "webhook-5773-markers" for this suite. 09/04/23 18:41:33.08
------------------------------
â€¢ [4.609 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:41:28.482
    Sep  4 18:41:28.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 18:41:28.484
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:41:28.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:41:28.514
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 18:41:28.55
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:41:29.446
    STEP: Deploying the webhook pod 09/04/23 18:41:29.458
    STEP: Wait for the deployment to be ready 09/04/23 18:41:29.483
    Sep  4 18:41:29.502: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 18:41:31.517
    STEP: Verifying the service has paired with the endpoint 09/04/23 18:41:31.541
    Sep  4 18:41:32.542: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/04/23 18:41:32.549
    Sep  4 18:41:32.580: INFO: Waiting for webhook configuration to be ready...
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/04/23 18:41:32.702
    STEP: Creating a dummy validating-webhook-configuration object 09/04/23 18:41:32.81
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/04/23 18:41:32.893
    STEP: Creating a dummy mutating-webhook-configuration object 09/04/23 18:41:32.905
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/04/23 18:41:32.922
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:41:32.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5773" for this suite. 09/04/23 18:41:33.039
    STEP: Destroying namespace "webhook-5773-markers" for this suite. 09/04/23 18:41:33.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:41:33.113
Sep  4 18:41:33.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename svcaccounts 09/04/23 18:41:33.114
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:41:33.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:41:33.155
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Sep  4 18:41:33.236: INFO: created pod
Sep  4 18:41:33.236: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2988" to be "Succeeded or Failed"
Sep  4 18:41:33.242: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.452428ms
Sep  4 18:41:35.249: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012413109s
Sep  4 18:41:37.249: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01273239s
STEP: Saw pod success 09/04/23 18:41:37.25
Sep  4 18:41:37.250: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Sep  4 18:42:07.251: INFO: polling logs
Sep  4 18:42:07.289: INFO: Pod logs: 
I0904 18:41:34.338975       1 log.go:198] OK: Got token
I0904 18:41:34.339191       1 log.go:198] validating with in-cluster discovery
I0904 18:41:34.339640       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0904 18:41:34.339786       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2988:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693853493, NotBefore:1693852893, IssuedAt:1693852893, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2988", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"61f73cd8-0f85-4ff3-8ed6-2e24436c390d"}}}
I0904 18:41:34.375908       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0904 18:41:34.387765       1 log.go:198] OK: Validated signature on JWT
I0904 18:41:34.388042       1 log.go:198] OK: Got valid claims from token!
I0904 18:41:34.388184       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2988:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693853493, NotBefore:1693852893, IssuedAt:1693852893, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2988", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"61f73cd8-0f85-4ff3-8ed6-2e24436c390d"}}}

Sep  4 18:42:07.289: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  4 18:42:07.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2988" for this suite. 09/04/23 18:42:07.306
------------------------------
â€¢ [SLOW TEST] [34.203 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:41:33.113
    Sep  4 18:41:33.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename svcaccounts 09/04/23 18:41:33.114
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:41:33.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:41:33.155
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Sep  4 18:41:33.236: INFO: created pod
    Sep  4 18:41:33.236: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2988" to be "Succeeded or Failed"
    Sep  4 18:41:33.242: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.452428ms
    Sep  4 18:41:35.249: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012413109s
    Sep  4 18:41:37.249: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01273239s
    STEP: Saw pod success 09/04/23 18:41:37.25
    Sep  4 18:41:37.250: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Sep  4 18:42:07.251: INFO: polling logs
    Sep  4 18:42:07.289: INFO: Pod logs: 
    I0904 18:41:34.338975       1 log.go:198] OK: Got token
    I0904 18:41:34.339191       1 log.go:198] validating with in-cluster discovery
    I0904 18:41:34.339640       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0904 18:41:34.339786       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2988:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693853493, NotBefore:1693852893, IssuedAt:1693852893, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2988", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"61f73cd8-0f85-4ff3-8ed6-2e24436c390d"}}}
    I0904 18:41:34.375908       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0904 18:41:34.387765       1 log.go:198] OK: Validated signature on JWT
    I0904 18:41:34.388042       1 log.go:198] OK: Got valid claims from token!
    I0904 18:41:34.388184       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2988:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693853493, NotBefore:1693852893, IssuedAt:1693852893, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2988", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"61f73cd8-0f85-4ff3-8ed6-2e24436c390d"}}}

    Sep  4 18:42:07.289: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:42:07.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2988" for this suite. 09/04/23 18:42:07.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:42:07.327
Sep  4 18:42:07.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename hostport 09/04/23 18:42:07.328
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:07.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:07.364
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/04/23 18:42:07.375
Sep  4 18:42:07.393: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2095" to be "running and ready"
Sep  4 18:42:07.399: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.609907ms
Sep  4 18:42:07.399: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:42:09.407: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01295551s
Sep  4 18:42:09.407: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  4 18:42:09.407: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.225.0.5 on the node which pod1 resides and expect scheduled 09/04/23 18:42:09.407
Sep  4 18:42:09.425: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2095" to be "running and ready"
Sep  4 18:42:09.430: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.811918ms
Sep  4 18:42:09.431: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:42:11.437: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012767797s
Sep  4 18:42:11.438: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  4 18:42:11.438: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.225.0.5 but use UDP protocol on the node which pod2 resides 09/04/23 18:42:11.438
Sep  4 18:42:11.453: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2095" to be "running and ready"
Sep  4 18:42:11.458: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.85087ms
Sep  4 18:42:11.458: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:42:13.464: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011146621s
Sep  4 18:42:13.464: INFO: The phase of Pod pod3 is Running (Ready = true)
Sep  4 18:42:13.464: INFO: Pod "pod3" satisfied condition "running and ready"
Sep  4 18:42:13.481: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2095" to be "running and ready"
Sep  4 18:42:13.488: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.220347ms
Sep  4 18:42:13.488: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:42:15.494: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.012321087s
Sep  4 18:42:15.494: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Sep  4 18:42:15.494: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/04/23 18:42:15.499
Sep  4 18:42:15.499: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.225.0.5 http://127.0.0.1:54323/hostname] Namespace:hostport-2095 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 18:42:15.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 18:42:15.501: INFO: ExecWithOptions: Clientset creation
Sep  4 18:42:15.501: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2095/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.225.0.5+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.225.0.5, port: 54323 09/04/23 18:42:15.613
Sep  4 18:42:15.614: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.225.0.5:54323/hostname] Namespace:hostport-2095 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 18:42:15.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 18:42:15.614: INFO: ExecWithOptions: Clientset creation
Sep  4 18:42:15.615: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2095/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.225.0.5%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.225.0.5, port: 54323 UDP 09/04/23 18:42:15.717
Sep  4 18:42:15.718: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.225.0.5 54323] Namespace:hostport-2095 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 18:42:15.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 18:42:15.719: INFO: ExecWithOptions: Clientset creation
Sep  4 18:42:15.719: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2095/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.225.0.5+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Sep  4 18:42:20.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-2095" for this suite. 09/04/23 18:42:20.835
------------------------------
â€¢ [SLOW TEST] [13.518 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:42:07.327
    Sep  4 18:42:07.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename hostport 09/04/23 18:42:07.328
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:07.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:07.364
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/04/23 18:42:07.375
    Sep  4 18:42:07.393: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2095" to be "running and ready"
    Sep  4 18:42:07.399: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.609907ms
    Sep  4 18:42:07.399: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:42:09.407: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01295551s
    Sep  4 18:42:09.407: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  4 18:42:09.407: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.225.0.5 on the node which pod1 resides and expect scheduled 09/04/23 18:42:09.407
    Sep  4 18:42:09.425: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2095" to be "running and ready"
    Sep  4 18:42:09.430: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.811918ms
    Sep  4 18:42:09.431: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:42:11.437: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012767797s
    Sep  4 18:42:11.438: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  4 18:42:11.438: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.225.0.5 but use UDP protocol on the node which pod2 resides 09/04/23 18:42:11.438
    Sep  4 18:42:11.453: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2095" to be "running and ready"
    Sep  4 18:42:11.458: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.85087ms
    Sep  4 18:42:11.458: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:42:13.464: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011146621s
    Sep  4 18:42:13.464: INFO: The phase of Pod pod3 is Running (Ready = true)
    Sep  4 18:42:13.464: INFO: Pod "pod3" satisfied condition "running and ready"
    Sep  4 18:42:13.481: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2095" to be "running and ready"
    Sep  4 18:42:13.488: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.220347ms
    Sep  4 18:42:13.488: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:42:15.494: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.012321087s
    Sep  4 18:42:15.494: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Sep  4 18:42:15.494: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/04/23 18:42:15.499
    Sep  4 18:42:15.499: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.225.0.5 http://127.0.0.1:54323/hostname] Namespace:hostport-2095 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 18:42:15.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 18:42:15.501: INFO: ExecWithOptions: Clientset creation
    Sep  4 18:42:15.501: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2095/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.225.0.5+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.225.0.5, port: 54323 09/04/23 18:42:15.613
    Sep  4 18:42:15.614: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.225.0.5:54323/hostname] Namespace:hostport-2095 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 18:42:15.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 18:42:15.614: INFO: ExecWithOptions: Clientset creation
    Sep  4 18:42:15.615: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2095/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.225.0.5%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.225.0.5, port: 54323 UDP 09/04/23 18:42:15.717
    Sep  4 18:42:15.718: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.225.0.5 54323] Namespace:hostport-2095 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 18:42:15.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 18:42:15.719: INFO: ExecWithOptions: Clientset creation
    Sep  4 18:42:15.719: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2095/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.225.0.5+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:42:20.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-2095" for this suite. 09/04/23 18:42:20.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:42:20.851
Sep  4 18:42:20.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 18:42:20.853
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:20.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:20.883
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Sep  4 18:42:20.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8029 create -f -'
Sep  4 18:42:21.458: INFO: stderr: ""
Sep  4 18:42:21.458: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep  4 18:42:21.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8029 create -f -'
Sep  4 18:42:21.695: INFO: stderr: ""
Sep  4 18:42:21.695: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/04/23 18:42:21.695
Sep  4 18:42:22.702: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 18:42:22.702: INFO: Found 1 / 1
Sep  4 18:42:22.702: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  4 18:42:22.708: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 18:42:22.708: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  4 18:42:22.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8029 describe pod agnhost-primary-mxzp5'
Sep  4 18:42:23.039: INFO: stderr: ""
Sep  4 18:42:23.039: INFO: stdout: "Name:             agnhost-primary-mxzp5\nNamespace:        kubectl-8029\nPriority:         0\nService Account:  default\nNode:             tenant-000003/10.225.0.7\nStart Time:       Mon, 04 Sep 2023 18:42:21 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: abd0d402f97af532804879358cdf965cd650df69c6157f99994f049cb869194d\n                  cni.projectcalico.org/podIP: 10.36.217.219/32\n                  cni.projectcalico.org/podIPs: 10.36.217.219/32\nStatus:           Running\nIP:               10.36.217.219\nIPs:\n  IP:           10.36.217.219\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://e47b68cbb354f0f614d15bda94b4012b7b11861dfa4dbd73f37adf493fc8b27e\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 04 Sep 2023 18:42:22 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-z9b6w (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-z9b6w:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-8029/agnhost-primary-mxzp5 to tenant-000003\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Sep  4 18:42:23.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8029 describe rc agnhost-primary'
Sep  4 18:42:23.144: INFO: stderr: ""
Sep  4 18:42:23.145: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8029\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-mxzp5\n"
Sep  4 18:42:23.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8029 describe service agnhost-primary'
Sep  4 18:42:23.271: INFO: stderr: ""
Sep  4 18:42:23.271: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8029\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.8.177\nIPs:               10.96.8.177\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.36.217.219:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  4 18:42:23.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8029 describe node tenant-000001'
Sep  4 18:42:23.385: INFO: stderr: ""
Sep  4 18:42:23.385: INFO: stdout: "Name:               tenant-000001\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=tenant-000001\n                    kubernetes.io/os=linux\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.225.0.5/16\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.36.55.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 04 Sep 2023 17:13:55 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  tenant-000001\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 04 Sep 2023 18:42:22 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 04 Sep 2023 17:16:16 +0000   Mon, 04 Sep 2023 17:16:16 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 04 Sep 2023 18:39:28 +0000   Mon, 04 Sep 2023 17:13:55 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 04 Sep 2023 18:39:28 +0000   Mon, 04 Sep 2023 17:13:55 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 04 Sep 2023 18:39:28 +0000   Mon, 04 Sep 2023 17:13:55 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 04 Sep 2023 18:39:28 +0000   Mon, 04 Sep 2023 17:16:07 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.225.0.5\n  Hostname:    tenant-000001\nCapacity:\n  cpu:                  1\n  ephemeral-storage:    30298176Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               3491324Ki\n  pods:                 110\nAllocatable:\n  cpu:                  1\n  ephemeral-storage:    30298176Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               3491324Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 269dedb123ea45c9a791b6ea02966a6a\n  System UUID:                da51e38a-d64b-4629-91fb-dd53c75dec05\n  Boot ID:                    e5f6b5e3-7888-4e76-be71-d1b092de1755\n  Kernel Version:             5.15.0-1041-azure\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.2\n  Kubelet Version:            v1.26.4\n  Kube-Proxy Version:         v1.26.4\nPodCIDR:                      10.36.0.0/24\nPodCIDRs:                     10.36.0.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  hostport-2095               e2e-host-exec                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         10s\n  hostport-2095               pod1                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         16s\n  hostport-2095               pod2                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         14s\n  hostport-2095               pod3                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         12s\n  kube-system                 calico-node-dj2mb                                          250m (25%)    0 (0%)      0 (0%)           0 (0%)         86m\n  kube-system                 konnectivity-agent-4d7rv                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         69s\n  kube-system                 kube-proxy-959c5                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         88m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         86m\n  sonobuoy                    sonobuoy-e2e-job-8da28a692bc241e2                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         85m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-64p86    0 (0%)        0 (0%)      0 (0%)           0 (0%)         85m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests    Limits\n  --------             --------    ------\n  cpu                  250m (25%)  0 (0%)\n  memory               0 (0%)      0 (0%)\n  ephemeral-storage    0 (0%)      0 (0%)\n  hugepages-1Gi        0 (0%)      0 (0%)\n  hugepages-2Mi        0 (0%)      0 (0%)\n  example.com/fakecpu  0           0\nEvents:                <none>\n"
Sep  4 18:42:23.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8029 describe namespace kubectl-8029'
Sep  4 18:42:23.489: INFO: stderr: ""
Sep  4 18:42:23.489: INFO: stdout: "Name:         kubectl-8029\nLabels:       e2e-framework=kubectl\n              e2e-run=8135def3-9470-4ff0-8b37-b89f5d81782b\n              kubernetes.io/metadata.name=kubectl-8029\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 18:42:23.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8029" for this suite. 09/04/23 18:42:23.496
------------------------------
â€¢ [2.654 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:42:20.851
    Sep  4 18:42:20.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 18:42:20.853
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:20.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:20.883
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Sep  4 18:42:20.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8029 create -f -'
    Sep  4 18:42:21.458: INFO: stderr: ""
    Sep  4 18:42:21.458: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Sep  4 18:42:21.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8029 create -f -'
    Sep  4 18:42:21.695: INFO: stderr: ""
    Sep  4 18:42:21.695: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/04/23 18:42:21.695
    Sep  4 18:42:22.702: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 18:42:22.702: INFO: Found 1 / 1
    Sep  4 18:42:22.702: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep  4 18:42:22.708: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 18:42:22.708: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  4 18:42:22.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8029 describe pod agnhost-primary-mxzp5'
    Sep  4 18:42:23.039: INFO: stderr: ""
    Sep  4 18:42:23.039: INFO: stdout: "Name:             agnhost-primary-mxzp5\nNamespace:        kubectl-8029\nPriority:         0\nService Account:  default\nNode:             tenant-000003/10.225.0.7\nStart Time:       Mon, 04 Sep 2023 18:42:21 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: abd0d402f97af532804879358cdf965cd650df69c6157f99994f049cb869194d\n                  cni.projectcalico.org/podIP: 10.36.217.219/32\n                  cni.projectcalico.org/podIPs: 10.36.217.219/32\nStatus:           Running\nIP:               10.36.217.219\nIPs:\n  IP:           10.36.217.219\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://e47b68cbb354f0f614d15bda94b4012b7b11861dfa4dbd73f37adf493fc8b27e\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 04 Sep 2023 18:42:22 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-z9b6w (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-z9b6w:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-8029/agnhost-primary-mxzp5 to tenant-000003\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Sep  4 18:42:23.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8029 describe rc agnhost-primary'
    Sep  4 18:42:23.144: INFO: stderr: ""
    Sep  4 18:42:23.145: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8029\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-mxzp5\n"
    Sep  4 18:42:23.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8029 describe service agnhost-primary'
    Sep  4 18:42:23.271: INFO: stderr: ""
    Sep  4 18:42:23.271: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8029\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.8.177\nIPs:               10.96.8.177\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.36.217.219:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Sep  4 18:42:23.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8029 describe node tenant-000001'
    Sep  4 18:42:23.385: INFO: stderr: ""
    Sep  4 18:42:23.385: INFO: stdout: "Name:               tenant-000001\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=tenant-000001\n                    kubernetes.io/os=linux\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.225.0.5/16\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.36.55.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 04 Sep 2023 17:13:55 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  tenant-000001\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 04 Sep 2023 18:42:22 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 04 Sep 2023 17:16:16 +0000   Mon, 04 Sep 2023 17:16:16 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 04 Sep 2023 18:39:28 +0000   Mon, 04 Sep 2023 17:13:55 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 04 Sep 2023 18:39:28 +0000   Mon, 04 Sep 2023 17:13:55 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 04 Sep 2023 18:39:28 +0000   Mon, 04 Sep 2023 17:13:55 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 04 Sep 2023 18:39:28 +0000   Mon, 04 Sep 2023 17:16:07 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.225.0.5\n  Hostname:    tenant-000001\nCapacity:\n  cpu:                  1\n  ephemeral-storage:    30298176Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               3491324Ki\n  pods:                 110\nAllocatable:\n  cpu:                  1\n  ephemeral-storage:    30298176Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               3491324Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 269dedb123ea45c9a791b6ea02966a6a\n  System UUID:                da51e38a-d64b-4629-91fb-dd53c75dec05\n  Boot ID:                    e5f6b5e3-7888-4e76-be71-d1b092de1755\n  Kernel Version:             5.15.0-1041-azure\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.2\n  Kubelet Version:            v1.26.4\n  Kube-Proxy Version:         v1.26.4\nPodCIDR:                      10.36.0.0/24\nPodCIDRs:                     10.36.0.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  hostport-2095               e2e-host-exec                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         10s\n  hostport-2095               pod1                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         16s\n  hostport-2095               pod2                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         14s\n  hostport-2095               pod3                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         12s\n  kube-system                 calico-node-dj2mb                                          250m (25%)    0 (0%)      0 (0%)           0 (0%)         86m\n  kube-system                 konnectivity-agent-4d7rv                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         69s\n  kube-system                 kube-proxy-959c5                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         88m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         86m\n  sonobuoy                    sonobuoy-e2e-job-8da28a692bc241e2                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         85m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-64p86    0 (0%)        0 (0%)      0 (0%)           0 (0%)         85m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests    Limits\n  --------             --------    ------\n  cpu                  250m (25%)  0 (0%)\n  memory               0 (0%)      0 (0%)\n  ephemeral-storage    0 (0%)      0 (0%)\n  hugepages-1Gi        0 (0%)      0 (0%)\n  hugepages-2Mi        0 (0%)      0 (0%)\n  example.com/fakecpu  0           0\nEvents:                <none>\n"
    Sep  4 18:42:23.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8029 describe namespace kubectl-8029'
    Sep  4 18:42:23.489: INFO: stderr: ""
    Sep  4 18:42:23.489: INFO: stdout: "Name:         kubectl-8029\nLabels:       e2e-framework=kubectl\n              e2e-run=8135def3-9470-4ff0-8b37-b89f5d81782b\n              kubernetes.io/metadata.name=kubectl-8029\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:42:23.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8029" for this suite. 09/04/23 18:42:23.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:42:23.505
Sep  4 18:42:23.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 18:42:23.507
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:23.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:23.535
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 09/04/23 18:42:23.54
Sep  4 18:42:23.551: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264" in namespace "downward-api-8080" to be "Succeeded or Failed"
Sep  4 18:42:23.559: INFO: Pod "downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264": Phase="Pending", Reason="", readiness=false. Elapsed: 8.536375ms
Sep  4 18:42:25.565: INFO: Pod "downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014935959s
Sep  4 18:42:27.567: INFO: Pod "downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016346617s
STEP: Saw pod success 09/04/23 18:42:27.567
Sep  4 18:42:27.568: INFO: Pod "downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264" satisfied condition "Succeeded or Failed"
Sep  4 18:42:27.573: INFO: Trying to get logs from node tenant-000003 pod downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264 container client-container: <nil>
STEP: delete the pod 09/04/23 18:42:27.587
Sep  4 18:42:27.607: INFO: Waiting for pod downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264 to disappear
Sep  4 18:42:27.611: INFO: Pod downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 18:42:27.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8080" for this suite. 09/04/23 18:42:27.62
------------------------------
â€¢ [4.125 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:42:23.505
    Sep  4 18:42:23.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 18:42:23.507
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:23.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:23.535
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 09/04/23 18:42:23.54
    Sep  4 18:42:23.551: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264" in namespace "downward-api-8080" to be "Succeeded or Failed"
    Sep  4 18:42:23.559: INFO: Pod "downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264": Phase="Pending", Reason="", readiness=false. Elapsed: 8.536375ms
    Sep  4 18:42:25.565: INFO: Pod "downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014935959s
    Sep  4 18:42:27.567: INFO: Pod "downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016346617s
    STEP: Saw pod success 09/04/23 18:42:27.567
    Sep  4 18:42:27.568: INFO: Pod "downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264" satisfied condition "Succeeded or Failed"
    Sep  4 18:42:27.573: INFO: Trying to get logs from node tenant-000003 pod downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264 container client-container: <nil>
    STEP: delete the pod 09/04/23 18:42:27.587
    Sep  4 18:42:27.607: INFO: Waiting for pod downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264 to disappear
    Sep  4 18:42:27.611: INFO: Pod downwardapi-volume-e74ef217-21d7-4b27-b132-41ad398e2264 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:42:27.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8080" for this suite. 09/04/23 18:42:27.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:42:27.635
Sep  4 18:42:27.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename watch 09/04/23 18:42:27.637
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:27.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:27.663
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 09/04/23 18:42:27.668
STEP: starting a background goroutine to produce watch events 09/04/23 18:42:27.674
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/04/23 18:42:27.674
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  4 18:42:30.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1922" for this suite. 09/04/23 18:42:30.498
------------------------------
â€¢ [2.917 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:42:27.635
    Sep  4 18:42:27.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename watch 09/04/23 18:42:27.637
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:27.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:27.663
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 09/04/23 18:42:27.668
    STEP: starting a background goroutine to produce watch events 09/04/23 18:42:27.674
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/04/23 18:42:27.674
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:42:30.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1922" for this suite. 09/04/23 18:42:30.498
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:42:30.559
Sep  4 18:42:30.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 18:42:30.56
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:30.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:30.595
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/04/23 18:42:30.61
Sep  4 18:42:30.622: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7629" to be "running and ready"
Sep  4 18:42:30.628: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.935726ms
Sep  4 18:42:30.629: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:42:32.636: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013117523s
Sep  4 18:42:32.636: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  4 18:42:32.636: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 09/04/23 18:42:32.642
Sep  4 18:42:32.650: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-7629" to be "running and ready"
Sep  4 18:42:32.655: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.481701ms
Sep  4 18:42:32.656: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:42:34.662: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012175571s
Sep  4 18:42:34.662: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Sep  4 18:42:34.662: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/04/23 18:42:34.667
STEP: delete the pod with lifecycle hook 09/04/23 18:42:34.682
Sep  4 18:42:34.709: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  4 18:42:34.717: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  4 18:42:36.718: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  4 18:42:36.736: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  4 18:42:38.718: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  4 18:42:38.726: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  4 18:42:38.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7629" for this suite. 09/04/23 18:42:38.733
------------------------------
â€¢ [SLOW TEST] [8.186 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:42:30.559
    Sep  4 18:42:30.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 18:42:30.56
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:30.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:30.595
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/04/23 18:42:30.61
    Sep  4 18:42:30.622: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7629" to be "running and ready"
    Sep  4 18:42:30.628: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.935726ms
    Sep  4 18:42:30.629: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:42:32.636: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013117523s
    Sep  4 18:42:32.636: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  4 18:42:32.636: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 09/04/23 18:42:32.642
    Sep  4 18:42:32.650: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-7629" to be "running and ready"
    Sep  4 18:42:32.655: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.481701ms
    Sep  4 18:42:32.656: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:42:34.662: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012175571s
    Sep  4 18:42:34.662: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Sep  4 18:42:34.662: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/04/23 18:42:34.667
    STEP: delete the pod with lifecycle hook 09/04/23 18:42:34.682
    Sep  4 18:42:34.709: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  4 18:42:34.717: INFO: Pod pod-with-poststart-exec-hook still exists
    Sep  4 18:42:36.718: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  4 18:42:36.736: INFO: Pod pod-with-poststart-exec-hook still exists
    Sep  4 18:42:38.718: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  4 18:42:38.726: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:42:38.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7629" for this suite. 09/04/23 18:42:38.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:42:38.755
Sep  4 18:42:38.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename runtimeclass 09/04/23 18:42:38.756
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:38.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:38.788
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-8735-delete-me 09/04/23 18:42:38.807
STEP: Waiting for the RuntimeClass to disappear 09/04/23 18:42:38.859
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  4 18:42:38.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8735" for this suite. 09/04/23 18:42:38.889
------------------------------
â€¢ [0.150 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:42:38.755
    Sep  4 18:42:38.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename runtimeclass 09/04/23 18:42:38.756
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:38.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:38.788
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-8735-delete-me 09/04/23 18:42:38.807
    STEP: Waiting for the RuntimeClass to disappear 09/04/23 18:42:38.859
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:42:38.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8735" for this suite. 09/04/23 18:42:38.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:42:38.905
Sep  4 18:42:38.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename events 09/04/23 18:42:38.906
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:38.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:38.939
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 09/04/23 18:42:38.944
STEP: listing events in all namespaces 09/04/23 18:42:38.961
STEP: listing events in test namespace 09/04/23 18:42:38.968
STEP: listing events with field selection filtering on source 09/04/23 18:42:38.973
STEP: listing events with field selection filtering on reportingController 09/04/23 18:42:38.979
STEP: getting the test event 09/04/23 18:42:38.983
STEP: patching the test event 09/04/23 18:42:38.991
STEP: getting the test event 09/04/23 18:42:39.006
STEP: updating the test event 09/04/23 18:42:39.011
STEP: getting the test event 09/04/23 18:42:39.021
STEP: deleting the test event 09/04/23 18:42:39.026
STEP: listing events in all namespaces 09/04/23 18:42:39.037
STEP: listing events in test namespace 09/04/23 18:42:39.042
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Sep  4 18:42:39.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3637" for this suite. 09/04/23 18:42:39.054
------------------------------
â€¢ [0.164 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:42:38.905
    Sep  4 18:42:38.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename events 09/04/23 18:42:38.906
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:38.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:38.939
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 09/04/23 18:42:38.944
    STEP: listing events in all namespaces 09/04/23 18:42:38.961
    STEP: listing events in test namespace 09/04/23 18:42:38.968
    STEP: listing events with field selection filtering on source 09/04/23 18:42:38.973
    STEP: listing events with field selection filtering on reportingController 09/04/23 18:42:38.979
    STEP: getting the test event 09/04/23 18:42:38.983
    STEP: patching the test event 09/04/23 18:42:38.991
    STEP: getting the test event 09/04/23 18:42:39.006
    STEP: updating the test event 09/04/23 18:42:39.011
    STEP: getting the test event 09/04/23 18:42:39.021
    STEP: deleting the test event 09/04/23 18:42:39.026
    STEP: listing events in all namespaces 09/04/23 18:42:39.037
    STEP: listing events in test namespace 09/04/23 18:42:39.042
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:42:39.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3637" for this suite. 09/04/23 18:42:39.054
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:42:39.074
Sep  4 18:42:39.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename ingressclass 09/04/23 18:42:39.076
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:39.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:39.104
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 09/04/23 18:42:39.109
STEP: getting /apis/networking.k8s.io 09/04/23 18:42:39.114
STEP: getting /apis/networking.k8s.iov1 09/04/23 18:42:39.116
STEP: creating 09/04/23 18:42:39.118
STEP: getting 09/04/23 18:42:39.14
STEP: listing 09/04/23 18:42:39.145
STEP: watching 09/04/23 18:42:39.15
Sep  4 18:42:39.150: INFO: starting watch
STEP: patching 09/04/23 18:42:39.152
STEP: updating 09/04/23 18:42:39.166
Sep  4 18:42:39.174: INFO: waiting for watch events with expected annotations
Sep  4 18:42:39.174: INFO: saw patched and updated annotations
STEP: deleting 09/04/23 18:42:39.175
STEP: deleting a collection 09/04/23 18:42:39.196
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Sep  4 18:42:39.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-5798" for this suite. 09/04/23 18:42:39.225
------------------------------
â€¢ [0.161 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:42:39.074
    Sep  4 18:42:39.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename ingressclass 09/04/23 18:42:39.076
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:39.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:39.104
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 09/04/23 18:42:39.109
    STEP: getting /apis/networking.k8s.io 09/04/23 18:42:39.114
    STEP: getting /apis/networking.k8s.iov1 09/04/23 18:42:39.116
    STEP: creating 09/04/23 18:42:39.118
    STEP: getting 09/04/23 18:42:39.14
    STEP: listing 09/04/23 18:42:39.145
    STEP: watching 09/04/23 18:42:39.15
    Sep  4 18:42:39.150: INFO: starting watch
    STEP: patching 09/04/23 18:42:39.152
    STEP: updating 09/04/23 18:42:39.166
    Sep  4 18:42:39.174: INFO: waiting for watch events with expected annotations
    Sep  4 18:42:39.174: INFO: saw patched and updated annotations
    STEP: deleting 09/04/23 18:42:39.175
    STEP: deleting a collection 09/04/23 18:42:39.196
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:42:39.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-5798" for this suite. 09/04/23 18:42:39.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:42:39.241
Sep  4 18:42:39.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 18:42:39.243
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:39.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:39.281
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Sep  4 18:42:39.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:42:42.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4867" for this suite. 09/04/23 18:42:42.445
------------------------------
â€¢ [3.220 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:42:39.241
    Sep  4 18:42:39.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 18:42:39.243
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:39.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:39.281
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Sep  4 18:42:39.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:42:42.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4867" for this suite. 09/04/23 18:42:42.445
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:42:42.465
Sep  4 18:42:42.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename replicaset 09/04/23 18:42:42.467
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:42.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:42.496
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Sep  4 18:42:42.501: INFO: Creating ReplicaSet my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff
Sep  4 18:42:42.515: INFO: Pod name my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff: Found 0 pods out of 1
Sep  4 18:42:47.520: INFO: Pod name my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff: Found 1 pods out of 1
Sep  4 18:42:47.521: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff" is running
Sep  4 18:42:47.521: INFO: Waiting up to 5m0s for pod "my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff-cmx46" in namespace "replicaset-4523" to be "running"
Sep  4 18:42:47.529: INFO: Pod "my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff-cmx46": Phase="Running", Reason="", readiness=true. Elapsed: 8.258342ms
Sep  4 18:42:47.530: INFO: Pod "my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff-cmx46" satisfied condition "running"
Sep  4 18:42:47.530: INFO: Pod "my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff-cmx46" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 18:42:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 18:42:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 18:42:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 18:42:42 +0000 UTC Reason: Message:}])
Sep  4 18:42:47.530: INFO: Trying to dial the pod
Sep  4 18:42:52.554: INFO: Controller my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff: Got expected result from replica 1 [my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff-cmx46]: "my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff-cmx46", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  4 18:42:52.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4523" for this suite. 09/04/23 18:42:52.577
------------------------------
â€¢ [SLOW TEST] [10.123 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:42:42.465
    Sep  4 18:42:42.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename replicaset 09/04/23 18:42:42.467
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:42.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:42.496
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Sep  4 18:42:42.501: INFO: Creating ReplicaSet my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff
    Sep  4 18:42:42.515: INFO: Pod name my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff: Found 0 pods out of 1
    Sep  4 18:42:47.520: INFO: Pod name my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff: Found 1 pods out of 1
    Sep  4 18:42:47.521: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff" is running
    Sep  4 18:42:47.521: INFO: Waiting up to 5m0s for pod "my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff-cmx46" in namespace "replicaset-4523" to be "running"
    Sep  4 18:42:47.529: INFO: Pod "my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff-cmx46": Phase="Running", Reason="", readiness=true. Elapsed: 8.258342ms
    Sep  4 18:42:47.530: INFO: Pod "my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff-cmx46" satisfied condition "running"
    Sep  4 18:42:47.530: INFO: Pod "my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff-cmx46" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 18:42:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 18:42:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 18:42:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-04 18:42:42 +0000 UTC Reason: Message:}])
    Sep  4 18:42:47.530: INFO: Trying to dial the pod
    Sep  4 18:42:52.554: INFO: Controller my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff: Got expected result from replica 1 [my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff-cmx46]: "my-hostname-basic-22375f5f-43d8-40ef-a327-0222d6d62eff-cmx46", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:42:52.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4523" for this suite. 09/04/23 18:42:52.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:42:52.592
Sep  4 18:42:52.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 18:42:52.594
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:52.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:52.628
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 09/04/23 18:42:52.633
Sep  4 18:42:52.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 create -f -'
Sep  4 18:42:53.519: INFO: stderr: ""
Sep  4 18:42:53.519: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 18:42:53.519
Sep  4 18:42:53.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  4 18:42:53.632: INFO: stderr: ""
Sep  4 18:42:53.632: INFO: stdout: "update-demo-nautilus-9wdrw update-demo-nautilus-pn89m "
Sep  4 18:42:53.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods update-demo-nautilus-9wdrw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 18:42:53.715: INFO: stderr: ""
Sep  4 18:42:53.715: INFO: stdout: ""
Sep  4 18:42:53.715: INFO: update-demo-nautilus-9wdrw is created but not running
Sep  4 18:42:58.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  4 18:42:58.826: INFO: stderr: ""
Sep  4 18:42:58.826: INFO: stdout: "update-demo-nautilus-9wdrw update-demo-nautilus-pn89m "
Sep  4 18:42:58.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods update-demo-nautilus-9wdrw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 18:42:59.013: INFO: stderr: ""
Sep  4 18:42:59.013: INFO: stdout: "true"
Sep  4 18:42:59.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods update-demo-nautilus-9wdrw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  4 18:42:59.224: INFO: stderr: ""
Sep  4 18:42:59.224: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  4 18:42:59.224: INFO: validating pod update-demo-nautilus-9wdrw
Sep  4 18:42:59.251: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  4 18:42:59.251: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  4 18:42:59.251: INFO: update-demo-nautilus-9wdrw is verified up and running
Sep  4 18:42:59.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods update-demo-nautilus-pn89m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 18:42:59.348: INFO: stderr: ""
Sep  4 18:42:59.348: INFO: stdout: "true"
Sep  4 18:42:59.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods update-demo-nautilus-pn89m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  4 18:42:59.445: INFO: stderr: ""
Sep  4 18:42:59.445: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  4 18:42:59.445: INFO: validating pod update-demo-nautilus-pn89m
Sep  4 18:42:59.460: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  4 18:42:59.460: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  4 18:42:59.460: INFO: update-demo-nautilus-pn89m is verified up and running
STEP: using delete to clean up resources 09/04/23 18:42:59.46
Sep  4 18:42:59.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 delete --grace-period=0 --force -f -'
Sep  4 18:42:59.550: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 18:42:59.550: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  4 18:42:59.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get rc,svc -l name=update-demo --no-headers'
Sep  4 18:42:59.779: INFO: stderr: "No resources found in kubectl-9757 namespace.\n"
Sep  4 18:42:59.779: INFO: stdout: ""
Sep  4 18:42:59.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  4 18:43:00.054: INFO: stderr: ""
Sep  4 18:43:00.054: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 18:43:00.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9757" for this suite. 09/04/23 18:43:00.062
------------------------------
â€¢ [SLOW TEST] [7.484 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:42:52.592
    Sep  4 18:42:52.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 18:42:52.594
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:42:52.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:42:52.628
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 09/04/23 18:42:52.633
    Sep  4 18:42:52.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 create -f -'
    Sep  4 18:42:53.519: INFO: stderr: ""
    Sep  4 18:42:53.519: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 18:42:53.519
    Sep  4 18:42:53.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  4 18:42:53.632: INFO: stderr: ""
    Sep  4 18:42:53.632: INFO: stdout: "update-demo-nautilus-9wdrw update-demo-nautilus-pn89m "
    Sep  4 18:42:53.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods update-demo-nautilus-9wdrw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 18:42:53.715: INFO: stderr: ""
    Sep  4 18:42:53.715: INFO: stdout: ""
    Sep  4 18:42:53.715: INFO: update-demo-nautilus-9wdrw is created but not running
    Sep  4 18:42:58.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  4 18:42:58.826: INFO: stderr: ""
    Sep  4 18:42:58.826: INFO: stdout: "update-demo-nautilus-9wdrw update-demo-nautilus-pn89m "
    Sep  4 18:42:58.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods update-demo-nautilus-9wdrw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 18:42:59.013: INFO: stderr: ""
    Sep  4 18:42:59.013: INFO: stdout: "true"
    Sep  4 18:42:59.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods update-demo-nautilus-9wdrw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  4 18:42:59.224: INFO: stderr: ""
    Sep  4 18:42:59.224: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  4 18:42:59.224: INFO: validating pod update-demo-nautilus-9wdrw
    Sep  4 18:42:59.251: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  4 18:42:59.251: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  4 18:42:59.251: INFO: update-demo-nautilus-9wdrw is verified up and running
    Sep  4 18:42:59.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods update-demo-nautilus-pn89m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 18:42:59.348: INFO: stderr: ""
    Sep  4 18:42:59.348: INFO: stdout: "true"
    Sep  4 18:42:59.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods update-demo-nautilus-pn89m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  4 18:42:59.445: INFO: stderr: ""
    Sep  4 18:42:59.445: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  4 18:42:59.445: INFO: validating pod update-demo-nautilus-pn89m
    Sep  4 18:42:59.460: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  4 18:42:59.460: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  4 18:42:59.460: INFO: update-demo-nautilus-pn89m is verified up and running
    STEP: using delete to clean up resources 09/04/23 18:42:59.46
    Sep  4 18:42:59.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 delete --grace-period=0 --force -f -'
    Sep  4 18:42:59.550: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 18:42:59.550: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep  4 18:42:59.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get rc,svc -l name=update-demo --no-headers'
    Sep  4 18:42:59.779: INFO: stderr: "No resources found in kubectl-9757 namespace.\n"
    Sep  4 18:42:59.779: INFO: stdout: ""
    Sep  4 18:42:59.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-9757 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  4 18:43:00.054: INFO: stderr: ""
    Sep  4 18:43:00.054: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:43:00.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9757" for this suite. 09/04/23 18:43:00.062
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:43:00.076
Sep  4 18:43:00.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pod-network-test 09/04/23 18:43:00.077
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:43:00.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:43:00.108
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-1180 09/04/23 18:43:00.113
STEP: creating a selector 09/04/23 18:43:00.113
STEP: Creating the service pods in kubernetes 09/04/23 18:43:00.113
Sep  4 18:43:00.113: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  4 18:43:00.157: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1180" to be "running and ready"
Sep  4 18:43:00.169: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.418522ms
Sep  4 18:43:00.169: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:43:02.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.017961913s
Sep  4 18:43:02.176: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:43:04.175: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017767236s
Sep  4 18:43:04.175: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:43:06.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017891076s
Sep  4 18:43:06.176: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:43:08.177: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019646092s
Sep  4 18:43:08.178: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:43:10.177: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.019249611s
Sep  4 18:43:10.177: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:43:12.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.018086159s
Sep  4 18:43:12.176: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:43:14.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.018077074s
Sep  4 18:43:14.176: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:43:16.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.018783216s
Sep  4 18:43:16.177: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:43:18.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018274899s
Sep  4 18:43:18.176: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:43:20.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.018278209s
Sep  4 18:43:20.176: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 18:43:22.177: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.019748099s
Sep  4 18:43:22.178: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  4 18:43:22.178: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  4 18:43:22.184: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1180" to be "running and ready"
Sep  4 18:43:22.188: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.513546ms
Sep  4 18:43:22.189: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  4 18:43:22.189: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/04/23 18:43:22.194
Sep  4 18:43:22.218: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1180" to be "running"
Sep  4 18:43:22.227: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.33351ms
Sep  4 18:43:24.234: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015964399s
Sep  4 18:43:24.234: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  4 18:43:24.239: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1180" to be "running"
Sep  4 18:43:24.245: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.789351ms
Sep  4 18:43:24.245: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep  4 18:43:24.250: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  4 18:43:24.250: INFO: Going to poll 10.36.55.103 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Sep  4 18:43:24.255: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.36.55.103:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1180 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 18:43:24.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 18:43:24.256: INFO: ExecWithOptions: Clientset creation
Sep  4 18:43:24.256: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1180/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.36.55.103%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  4 18:43:24.372: INFO: Found all 1 expected endpoints: [netserver-0]
Sep  4 18:43:24.372: INFO: Going to poll 10.36.217.231 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Sep  4 18:43:24.378: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.36.217.231:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1180 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 18:43:24.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 18:43:24.379: INFO: ExecWithOptions: Clientset creation
Sep  4 18:43:24.379: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1180/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.36.217.231%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  4 18:43:24.502: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  4 18:43:24.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1180" for this suite. 09/04/23 18:43:24.512
------------------------------
â€¢ [SLOW TEST] [24.445 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:43:00.076
    Sep  4 18:43:00.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pod-network-test 09/04/23 18:43:00.077
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:43:00.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:43:00.108
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-1180 09/04/23 18:43:00.113
    STEP: creating a selector 09/04/23 18:43:00.113
    STEP: Creating the service pods in kubernetes 09/04/23 18:43:00.113
    Sep  4 18:43:00.113: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  4 18:43:00.157: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1180" to be "running and ready"
    Sep  4 18:43:00.169: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.418522ms
    Sep  4 18:43:00.169: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:43:02.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.017961913s
    Sep  4 18:43:02.176: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:43:04.175: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017767236s
    Sep  4 18:43:04.175: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:43:06.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017891076s
    Sep  4 18:43:06.176: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:43:08.177: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019646092s
    Sep  4 18:43:08.178: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:43:10.177: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.019249611s
    Sep  4 18:43:10.177: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:43:12.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.018086159s
    Sep  4 18:43:12.176: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:43:14.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.018077074s
    Sep  4 18:43:14.176: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:43:16.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.018783216s
    Sep  4 18:43:16.177: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:43:18.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018274899s
    Sep  4 18:43:18.176: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:43:20.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.018278209s
    Sep  4 18:43:20.176: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 18:43:22.177: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.019748099s
    Sep  4 18:43:22.178: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  4 18:43:22.178: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  4 18:43:22.184: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1180" to be "running and ready"
    Sep  4 18:43:22.188: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.513546ms
    Sep  4 18:43:22.189: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  4 18:43:22.189: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/04/23 18:43:22.194
    Sep  4 18:43:22.218: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1180" to be "running"
    Sep  4 18:43:22.227: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.33351ms
    Sep  4 18:43:24.234: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015964399s
    Sep  4 18:43:24.234: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  4 18:43:24.239: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1180" to be "running"
    Sep  4 18:43:24.245: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.789351ms
    Sep  4 18:43:24.245: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep  4 18:43:24.250: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  4 18:43:24.250: INFO: Going to poll 10.36.55.103 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Sep  4 18:43:24.255: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.36.55.103:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1180 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 18:43:24.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 18:43:24.256: INFO: ExecWithOptions: Clientset creation
    Sep  4 18:43:24.256: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1180/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.36.55.103%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  4 18:43:24.372: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep  4 18:43:24.372: INFO: Going to poll 10.36.217.231 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Sep  4 18:43:24.378: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.36.217.231:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1180 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 18:43:24.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 18:43:24.379: INFO: ExecWithOptions: Clientset creation
    Sep  4 18:43:24.379: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1180/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.36.217.231%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  4 18:43:24.502: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:43:24.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1180" for this suite. 09/04/23 18:43:24.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:43:24.532
Sep  4 18:43:24.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-runtime 09/04/23 18:43:24.534
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:43:24.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:43:24.563
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 09/04/23 18:43:24.568
STEP: wait for the container to reach Succeeded 09/04/23 18:43:24.58
STEP: get the container status 09/04/23 18:43:28.609
STEP: the container should be terminated 09/04/23 18:43:28.615
STEP: the termination message should be set 09/04/23 18:43:28.615
Sep  4 18:43:28.615: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 09/04/23 18:43:28.615
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  4 18:43:28.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1591" for this suite. 09/04/23 18:43:28.648
------------------------------
â€¢ [4.126 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:43:24.532
    Sep  4 18:43:24.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-runtime 09/04/23 18:43:24.534
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:43:24.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:43:24.563
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 09/04/23 18:43:24.568
    STEP: wait for the container to reach Succeeded 09/04/23 18:43:24.58
    STEP: get the container status 09/04/23 18:43:28.609
    STEP: the container should be terminated 09/04/23 18:43:28.615
    STEP: the termination message should be set 09/04/23 18:43:28.615
    Sep  4 18:43:28.615: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 09/04/23 18:43:28.615
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:43:28.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1591" for this suite. 09/04/23 18:43:28.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:43:28.66
Sep  4 18:43:28.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 18:43:28.663
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:43:28.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:43:28.7
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Sep  4 18:43:28.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/04/23 18:43:31.195
Sep  4 18:43:31.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 create -f -'
Sep  4 18:43:31.879: INFO: stderr: ""
Sep  4 18:43:31.879: INFO: stdout: "e2e-test-crd-publish-openapi-8014-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  4 18:43:31.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 delete e2e-test-crd-publish-openapi-8014-crds test-cr'
Sep  4 18:43:32.010: INFO: stderr: ""
Sep  4 18:43:32.010: INFO: stdout: "e2e-test-crd-publish-openapi-8014-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep  4 18:43:32.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 apply -f -'
Sep  4 18:43:32.652: INFO: stderr: ""
Sep  4 18:43:32.652: INFO: stdout: "e2e-test-crd-publish-openapi-8014-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  4 18:43:32.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 delete e2e-test-crd-publish-openapi-8014-crds test-cr'
Sep  4 18:43:32.902: INFO: stderr: ""
Sep  4 18:43:32.902: INFO: stdout: "e2e-test-crd-publish-openapi-8014-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/04/23 18:43:32.902
Sep  4 18:43:32.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-8803 explain e2e-test-crd-publish-openapi-8014-crds'
Sep  4 18:43:33.160: INFO: stderr: ""
Sep  4 18:43:33.160: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8014-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:43:35.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8803" for this suite. 09/04/23 18:43:35.149
------------------------------
â€¢ [SLOW TEST] [6.500 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:43:28.66
    Sep  4 18:43:28.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 18:43:28.663
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:43:28.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:43:28.7
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Sep  4 18:43:28.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/04/23 18:43:31.195
    Sep  4 18:43:31.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 create -f -'
    Sep  4 18:43:31.879: INFO: stderr: ""
    Sep  4 18:43:31.879: INFO: stdout: "e2e-test-crd-publish-openapi-8014-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep  4 18:43:31.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 delete e2e-test-crd-publish-openapi-8014-crds test-cr'
    Sep  4 18:43:32.010: INFO: stderr: ""
    Sep  4 18:43:32.010: INFO: stdout: "e2e-test-crd-publish-openapi-8014-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Sep  4 18:43:32.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 apply -f -'
    Sep  4 18:43:32.652: INFO: stderr: ""
    Sep  4 18:43:32.652: INFO: stdout: "e2e-test-crd-publish-openapi-8014-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep  4 18:43:32.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 delete e2e-test-crd-publish-openapi-8014-crds test-cr'
    Sep  4 18:43:32.902: INFO: stderr: ""
    Sep  4 18:43:32.902: INFO: stdout: "e2e-test-crd-publish-openapi-8014-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/04/23 18:43:32.902
    Sep  4 18:43:32.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=crd-publish-openapi-8803 explain e2e-test-crd-publish-openapi-8014-crds'
    Sep  4 18:43:33.160: INFO: stderr: ""
    Sep  4 18:43:33.160: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8014-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:43:35.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8803" for this suite. 09/04/23 18:43:35.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:43:35.171
Sep  4 18:43:35.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename daemonsets 09/04/23 18:43:35.173
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:43:35.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:43:35.212
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 09/04/23 18:43:35.239
STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 18:43:35.246
Sep  4 18:43:35.259: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:43:35.259: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:43:36.272: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:43:36.273: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
Sep  4 18:43:37.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 18:43:37.267: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 09/04/23 18:43:37.27
Sep  4 18:43:37.273: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 09/04/23 18:43:37.273
Sep  4 18:43:37.285: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 09/04/23 18:43:37.286
Sep  4 18:43:37.288: INFO: Observed &DaemonSet event: ADDED
Sep  4 18:43:37.289: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 18:43:37.289: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 18:43:37.289: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 18:43:37.290: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 18:43:37.290: INFO: Found daemon set daemon-set in namespace daemonsets-6325 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  4 18:43:37.290: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 09/04/23 18:43:37.291
STEP: watching for the daemon set status to be patched 09/04/23 18:43:37.302
Sep  4 18:43:37.306: INFO: Observed &DaemonSet event: ADDED
Sep  4 18:43:37.306: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 18:43:37.307: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 18:43:37.307: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 18:43:37.307: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 18:43:37.307: INFO: Observed daemon set daemon-set in namespace daemonsets-6325 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  4 18:43:37.308: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 18:43:37.308: INFO: Found daemon set daemon-set in namespace daemonsets-6325 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Sep  4 18:43:37.308: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/04/23 18:43:37.317
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6325, will wait for the garbage collector to delete the pods 09/04/23 18:43:37.317
Sep  4 18:43:37.382: INFO: Deleting DaemonSet.extensions daemon-set took: 9.673026ms
Sep  4 18:43:37.482: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.692972ms
Sep  4 18:43:39.985: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 18:43:39.985: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  4 18:43:39.990: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34723"},"items":null}

Sep  4 18:43:39.993: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34723"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:43:40.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6325" for this suite. 09/04/23 18:43:40.015
------------------------------
â€¢ [4.850 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:43:35.171
    Sep  4 18:43:35.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename daemonsets 09/04/23 18:43:35.173
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:43:35.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:43:35.212
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 09/04/23 18:43:35.239
    STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 18:43:35.246
    Sep  4 18:43:35.259: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:43:35.259: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:43:36.272: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:43:36.273: INFO: Node tenant-000001 is running 0 daemon pod, expected 1
    Sep  4 18:43:37.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 18:43:37.267: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 09/04/23 18:43:37.27
    Sep  4 18:43:37.273: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 09/04/23 18:43:37.273
    Sep  4 18:43:37.285: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 09/04/23 18:43:37.286
    Sep  4 18:43:37.288: INFO: Observed &DaemonSet event: ADDED
    Sep  4 18:43:37.289: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 18:43:37.289: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 18:43:37.289: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 18:43:37.290: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 18:43:37.290: INFO: Found daemon set daemon-set in namespace daemonsets-6325 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  4 18:43:37.290: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 09/04/23 18:43:37.291
    STEP: watching for the daemon set status to be patched 09/04/23 18:43:37.302
    Sep  4 18:43:37.306: INFO: Observed &DaemonSet event: ADDED
    Sep  4 18:43:37.306: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 18:43:37.307: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 18:43:37.307: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 18:43:37.307: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 18:43:37.307: INFO: Observed daemon set daemon-set in namespace daemonsets-6325 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  4 18:43:37.308: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 18:43:37.308: INFO: Found daemon set daemon-set in namespace daemonsets-6325 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Sep  4 18:43:37.308: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/04/23 18:43:37.317
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6325, will wait for the garbage collector to delete the pods 09/04/23 18:43:37.317
    Sep  4 18:43:37.382: INFO: Deleting DaemonSet.extensions daemon-set took: 9.673026ms
    Sep  4 18:43:37.482: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.692972ms
    Sep  4 18:43:39.985: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 18:43:39.985: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  4 18:43:39.990: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34723"},"items":null}

    Sep  4 18:43:39.993: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34723"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:43:40.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6325" for this suite. 09/04/23 18:43:40.015
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:43:40.026
Sep  4 18:43:40.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 18:43:40.028
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:43:40.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:43:40.051
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-4996 09/04/23 18:43:40.053
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4996 to expose endpoints map[] 09/04/23 18:43:40.073
Sep  4 18:43:40.090: INFO: successfully validated that service multi-endpoint-test in namespace services-4996 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4996 09/04/23 18:43:40.09
Sep  4 18:43:40.107: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4996" to be "running and ready"
Sep  4 18:43:40.122: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.944808ms
Sep  4 18:43:40.122: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:43:42.127: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.019707427s
Sep  4 18:43:42.127: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  4 18:43:42.127: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4996 to expose endpoints map[pod1:[100]] 09/04/23 18:43:42.133
Sep  4 18:43:42.144: INFO: successfully validated that service multi-endpoint-test in namespace services-4996 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4996 09/04/23 18:43:42.144
Sep  4 18:43:42.150: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4996" to be "running and ready"
Sep  4 18:43:42.161: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.207552ms
Sep  4 18:43:42.161: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:43:44.168: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017017881s
Sep  4 18:43:44.168: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  4 18:43:44.168: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4996 to expose endpoints map[pod1:[100] pod2:[101]] 09/04/23 18:43:44.173
Sep  4 18:43:44.189: INFO: successfully validated that service multi-endpoint-test in namespace services-4996 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 09/04/23 18:43:44.189
Sep  4 18:43:44.190: INFO: Creating new exec pod
Sep  4 18:43:44.198: INFO: Waiting up to 5m0s for pod "execpodtf6rj" in namespace "services-4996" to be "running"
Sep  4 18:43:44.207: INFO: Pod "execpodtf6rj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.637667ms
Sep  4 18:43:46.213: INFO: Pod "execpodtf6rj": Phase="Running", Reason="", readiness=true. Elapsed: 2.01531709s
Sep  4 18:43:46.214: INFO: Pod "execpodtf6rj" satisfied condition "running"
Sep  4 18:43:47.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-4996 exec execpodtf6rj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Sep  4 18:43:47.415: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Sep  4 18:43:47.415: INFO: stdout: ""
Sep  4 18:43:47.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-4996 exec execpodtf6rj -- /bin/sh -x -c nc -v -z -w 2 10.96.85.103 80'
Sep  4 18:43:47.612: INFO: stderr: "+ nc -v -z -w 2 10.96.85.103 80\nConnection to 10.96.85.103 80 port [tcp/http] succeeded!\n"
Sep  4 18:43:47.612: INFO: stdout: ""
Sep  4 18:43:47.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-4996 exec execpodtf6rj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Sep  4 18:43:47.831: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Sep  4 18:43:47.831: INFO: stdout: ""
Sep  4 18:43:47.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-4996 exec execpodtf6rj -- /bin/sh -x -c nc -v -z -w 2 10.96.85.103 81'
Sep  4 18:43:48.014: INFO: stderr: "+ nc -v -z -w 2 10.96.85.103 81\nConnection to 10.96.85.103 81 port [tcp/*] succeeded!\n"
Sep  4 18:43:48.014: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-4996 09/04/23 18:43:48.014
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4996 to expose endpoints map[pod2:[101]] 09/04/23 18:43:48.075
Sep  4 18:43:48.122: INFO: successfully validated that service multi-endpoint-test in namespace services-4996 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4996 09/04/23 18:43:48.123
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4996 to expose endpoints map[] 09/04/23 18:43:48.155
Sep  4 18:43:48.193: INFO: successfully validated that service multi-endpoint-test in namespace services-4996 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 18:43:48.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4996" for this suite. 09/04/23 18:43:48.237
------------------------------
â€¢ [SLOW TEST] [8.228 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:43:40.026
    Sep  4 18:43:40.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 18:43:40.028
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:43:40.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:43:40.051
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-4996 09/04/23 18:43:40.053
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4996 to expose endpoints map[] 09/04/23 18:43:40.073
    Sep  4 18:43:40.090: INFO: successfully validated that service multi-endpoint-test in namespace services-4996 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4996 09/04/23 18:43:40.09
    Sep  4 18:43:40.107: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4996" to be "running and ready"
    Sep  4 18:43:40.122: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.944808ms
    Sep  4 18:43:40.122: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:43:42.127: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.019707427s
    Sep  4 18:43:42.127: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  4 18:43:42.127: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4996 to expose endpoints map[pod1:[100]] 09/04/23 18:43:42.133
    Sep  4 18:43:42.144: INFO: successfully validated that service multi-endpoint-test in namespace services-4996 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-4996 09/04/23 18:43:42.144
    Sep  4 18:43:42.150: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4996" to be "running and ready"
    Sep  4 18:43:42.161: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.207552ms
    Sep  4 18:43:42.161: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:43:44.168: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017017881s
    Sep  4 18:43:44.168: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  4 18:43:44.168: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4996 to expose endpoints map[pod1:[100] pod2:[101]] 09/04/23 18:43:44.173
    Sep  4 18:43:44.189: INFO: successfully validated that service multi-endpoint-test in namespace services-4996 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 09/04/23 18:43:44.189
    Sep  4 18:43:44.190: INFO: Creating new exec pod
    Sep  4 18:43:44.198: INFO: Waiting up to 5m0s for pod "execpodtf6rj" in namespace "services-4996" to be "running"
    Sep  4 18:43:44.207: INFO: Pod "execpodtf6rj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.637667ms
    Sep  4 18:43:46.213: INFO: Pod "execpodtf6rj": Phase="Running", Reason="", readiness=true. Elapsed: 2.01531709s
    Sep  4 18:43:46.214: INFO: Pod "execpodtf6rj" satisfied condition "running"
    Sep  4 18:43:47.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-4996 exec execpodtf6rj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Sep  4 18:43:47.415: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Sep  4 18:43:47.415: INFO: stdout: ""
    Sep  4 18:43:47.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-4996 exec execpodtf6rj -- /bin/sh -x -c nc -v -z -w 2 10.96.85.103 80'
    Sep  4 18:43:47.612: INFO: stderr: "+ nc -v -z -w 2 10.96.85.103 80\nConnection to 10.96.85.103 80 port [tcp/http] succeeded!\n"
    Sep  4 18:43:47.612: INFO: stdout: ""
    Sep  4 18:43:47.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-4996 exec execpodtf6rj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Sep  4 18:43:47.831: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Sep  4 18:43:47.831: INFO: stdout: ""
    Sep  4 18:43:47.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-4996 exec execpodtf6rj -- /bin/sh -x -c nc -v -z -w 2 10.96.85.103 81'
    Sep  4 18:43:48.014: INFO: stderr: "+ nc -v -z -w 2 10.96.85.103 81\nConnection to 10.96.85.103 81 port [tcp/*] succeeded!\n"
    Sep  4 18:43:48.014: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-4996 09/04/23 18:43:48.014
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4996 to expose endpoints map[pod2:[101]] 09/04/23 18:43:48.075
    Sep  4 18:43:48.122: INFO: successfully validated that service multi-endpoint-test in namespace services-4996 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-4996 09/04/23 18:43:48.123
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4996 to expose endpoints map[] 09/04/23 18:43:48.155
    Sep  4 18:43:48.193: INFO: successfully validated that service multi-endpoint-test in namespace services-4996 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:43:48.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4996" for this suite. 09/04/23 18:43:48.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:43:48.266
Sep  4 18:43:48.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename statefulset 09/04/23 18:43:48.267
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:43:48.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:43:48.308
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4253 09/04/23 18:43:48.326
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 09/04/23 18:43:48.348
STEP: Creating pod with conflicting port in namespace statefulset-4253 09/04/23 18:43:48.358
STEP: Waiting until pod test-pod will start running in namespace statefulset-4253 09/04/23 18:43:48.372
Sep  4 18:43:48.372: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4253" to be "running"
Sep  4 18:43:48.376: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07492ms
Sep  4 18:43:50.381: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008504401s
Sep  4 18:43:50.381: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-4253 09/04/23 18:43:50.381
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4253 09/04/23 18:43:50.389
Sep  4 18:43:50.404: INFO: Observed stateful pod in namespace: statefulset-4253, name: ss-0, uid: d69e593f-d324-499a-83b9-cbec38297fbb, status phase: Pending. Waiting for statefulset controller to delete.
Sep  4 18:43:50.428: INFO: Observed stateful pod in namespace: statefulset-4253, name: ss-0, uid: d69e593f-d324-499a-83b9-cbec38297fbb, status phase: Failed. Waiting for statefulset controller to delete.
Sep  4 18:43:50.449: INFO: Observed stateful pod in namespace: statefulset-4253, name: ss-0, uid: d69e593f-d324-499a-83b9-cbec38297fbb, status phase: Failed. Waiting for statefulset controller to delete.
Sep  4 18:43:50.455: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4253
STEP: Removing pod with conflicting port in namespace statefulset-4253 09/04/23 18:43:50.455
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4253 and will be in running state 09/04/23 18:43:50.495
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 18:43:52.513: INFO: Deleting all statefulset in ns statefulset-4253
Sep  4 18:43:52.517: INFO: Scaling statefulset ss to 0
Sep  4 18:44:02.535: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 18:44:02.541: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 18:44:02.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4253" for this suite. 09/04/23 18:44:02.56
------------------------------
â€¢ [SLOW TEST] [14.305 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:43:48.266
    Sep  4 18:43:48.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename statefulset 09/04/23 18:43:48.267
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:43:48.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:43:48.308
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4253 09/04/23 18:43:48.326
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 09/04/23 18:43:48.348
    STEP: Creating pod with conflicting port in namespace statefulset-4253 09/04/23 18:43:48.358
    STEP: Waiting until pod test-pod will start running in namespace statefulset-4253 09/04/23 18:43:48.372
    Sep  4 18:43:48.372: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4253" to be "running"
    Sep  4 18:43:48.376: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07492ms
    Sep  4 18:43:50.381: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008504401s
    Sep  4 18:43:50.381: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-4253 09/04/23 18:43:50.381
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4253 09/04/23 18:43:50.389
    Sep  4 18:43:50.404: INFO: Observed stateful pod in namespace: statefulset-4253, name: ss-0, uid: d69e593f-d324-499a-83b9-cbec38297fbb, status phase: Pending. Waiting for statefulset controller to delete.
    Sep  4 18:43:50.428: INFO: Observed stateful pod in namespace: statefulset-4253, name: ss-0, uid: d69e593f-d324-499a-83b9-cbec38297fbb, status phase: Failed. Waiting for statefulset controller to delete.
    Sep  4 18:43:50.449: INFO: Observed stateful pod in namespace: statefulset-4253, name: ss-0, uid: d69e593f-d324-499a-83b9-cbec38297fbb, status phase: Failed. Waiting for statefulset controller to delete.
    Sep  4 18:43:50.455: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4253
    STEP: Removing pod with conflicting port in namespace statefulset-4253 09/04/23 18:43:50.455
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4253 and will be in running state 09/04/23 18:43:50.495
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 18:43:52.513: INFO: Deleting all statefulset in ns statefulset-4253
    Sep  4 18:43:52.517: INFO: Scaling statefulset ss to 0
    Sep  4 18:44:02.535: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 18:44:02.541: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:44:02.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4253" for this suite. 09/04/23 18:44:02.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:44:02.578
Sep  4 18:44:02.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename replication-controller 09/04/23 18:44:02.579
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:02.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:02.597
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 09/04/23 18:44:02.602
STEP: waiting for RC to be added 09/04/23 18:44:02.616
STEP: waiting for available Replicas 09/04/23 18:44:02.616
STEP: patching ReplicationController 09/04/23 18:44:04.006
STEP: waiting for RC to be modified 09/04/23 18:44:04.019
STEP: patching ReplicationController status 09/04/23 18:44:04.02
STEP: waiting for RC to be modified 09/04/23 18:44:04.025
STEP: waiting for available Replicas 09/04/23 18:44:04.028
STEP: fetching ReplicationController status 09/04/23 18:44:04.04
STEP: patching ReplicationController scale 09/04/23 18:44:04.045
STEP: waiting for RC to be modified 09/04/23 18:44:04.053
STEP: waiting for ReplicationController's scale to be the max amount 09/04/23 18:44:04.054
STEP: fetching ReplicationController; ensuring that it's patched 09/04/23 18:44:05.92
STEP: updating ReplicationController status 09/04/23 18:44:05.923
STEP: waiting for RC to be modified 09/04/23 18:44:05.936
STEP: listing all ReplicationControllers 09/04/23 18:44:05.937
STEP: checking that ReplicationController has expected values 09/04/23 18:44:05.944
STEP: deleting ReplicationControllers by collection 09/04/23 18:44:05.945
STEP: waiting for ReplicationController to have a DELETED watchEvent 09/04/23 18:44:05.957
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  4 18:44:05.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2991" for this suite. 09/04/23 18:44:06.006
------------------------------
â€¢ [3.437 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:44:02.578
    Sep  4 18:44:02.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename replication-controller 09/04/23 18:44:02.579
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:02.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:02.597
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 09/04/23 18:44:02.602
    STEP: waiting for RC to be added 09/04/23 18:44:02.616
    STEP: waiting for available Replicas 09/04/23 18:44:02.616
    STEP: patching ReplicationController 09/04/23 18:44:04.006
    STEP: waiting for RC to be modified 09/04/23 18:44:04.019
    STEP: patching ReplicationController status 09/04/23 18:44:04.02
    STEP: waiting for RC to be modified 09/04/23 18:44:04.025
    STEP: waiting for available Replicas 09/04/23 18:44:04.028
    STEP: fetching ReplicationController status 09/04/23 18:44:04.04
    STEP: patching ReplicationController scale 09/04/23 18:44:04.045
    STEP: waiting for RC to be modified 09/04/23 18:44:04.053
    STEP: waiting for ReplicationController's scale to be the max amount 09/04/23 18:44:04.054
    STEP: fetching ReplicationController; ensuring that it's patched 09/04/23 18:44:05.92
    STEP: updating ReplicationController status 09/04/23 18:44:05.923
    STEP: waiting for RC to be modified 09/04/23 18:44:05.936
    STEP: listing all ReplicationControllers 09/04/23 18:44:05.937
    STEP: checking that ReplicationController has expected values 09/04/23 18:44:05.944
    STEP: deleting ReplicationControllers by collection 09/04/23 18:44:05.945
    STEP: waiting for ReplicationController to have a DELETED watchEvent 09/04/23 18:44:05.957
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:44:05.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2991" for this suite. 09/04/23 18:44:06.006
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:44:06.019
Sep  4 18:44:06.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 18:44:06.021
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:06.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:06.042
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-316396df-e2b5-4579-beea-b1d3c0fb2263 09/04/23 18:44:06.044
STEP: Creating a pod to test consume configMaps 09/04/23 18:44:06.049
Sep  4 18:44:06.059: INFO: Waiting up to 5m0s for pod "pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67" in namespace "configmap-1413" to be "Succeeded or Failed"
Sep  4 18:44:06.063: INFO: Pod "pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67": Phase="Pending", Reason="", readiness=false. Elapsed: 3.029661ms
Sep  4 18:44:08.070: INFO: Pod "pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67": Phase="Running", Reason="", readiness=false. Elapsed: 2.009918948s
Sep  4 18:44:10.070: INFO: Pod "pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009973071s
STEP: Saw pod success 09/04/23 18:44:10.07
Sep  4 18:44:10.070: INFO: Pod "pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67" satisfied condition "Succeeded or Failed"
Sep  4 18:44:10.073: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 18:44:10.107
Sep  4 18:44:10.134: INFO: Waiting for pod pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67 to disappear
Sep  4 18:44:10.137: INFO: Pod pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 18:44:10.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1413" for this suite. 09/04/23 18:44:10.145
------------------------------
â€¢ [4.138 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:44:06.019
    Sep  4 18:44:06.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 18:44:06.021
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:06.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:06.042
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-316396df-e2b5-4579-beea-b1d3c0fb2263 09/04/23 18:44:06.044
    STEP: Creating a pod to test consume configMaps 09/04/23 18:44:06.049
    Sep  4 18:44:06.059: INFO: Waiting up to 5m0s for pod "pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67" in namespace "configmap-1413" to be "Succeeded or Failed"
    Sep  4 18:44:06.063: INFO: Pod "pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67": Phase="Pending", Reason="", readiness=false. Elapsed: 3.029661ms
    Sep  4 18:44:08.070: INFO: Pod "pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67": Phase="Running", Reason="", readiness=false. Elapsed: 2.009918948s
    Sep  4 18:44:10.070: INFO: Pod "pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009973071s
    STEP: Saw pod success 09/04/23 18:44:10.07
    Sep  4 18:44:10.070: INFO: Pod "pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67" satisfied condition "Succeeded or Failed"
    Sep  4 18:44:10.073: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 18:44:10.107
    Sep  4 18:44:10.134: INFO: Waiting for pod pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67 to disappear
    Sep  4 18:44:10.137: INFO: Pod pod-configmaps-33daac51-93f4-42f5-b10e-cbff45203e67 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:44:10.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1413" for this suite. 09/04/23 18:44:10.145
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:44:10.161
Sep  4 18:44:10.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename secrets 09/04/23 18:44:10.162
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:10.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:10.188
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-79864ae8-3931-41d9-b9ff-bacc7d31377a 09/04/23 18:44:10.19
STEP: Creating a pod to test consume secrets 09/04/23 18:44:10.195
Sep  4 18:44:10.204: INFO: Waiting up to 5m0s for pod "pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265" in namespace "secrets-3392" to be "Succeeded or Failed"
Sep  4 18:44:10.221: INFO: Pod "pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265": Phase="Pending", Reason="", readiness=false. Elapsed: 16.988404ms
Sep  4 18:44:12.228: INFO: Pod "pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023027716s
Sep  4 18:44:14.228: INFO: Pod "pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023666301s
STEP: Saw pod success 09/04/23 18:44:14.229
Sep  4 18:44:14.229: INFO: Pod "pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265" satisfied condition "Succeeded or Failed"
Sep  4 18:44:14.232: INFO: Trying to get logs from node tenant-000001 pod pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265 container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 18:44:14.239
Sep  4 18:44:14.261: INFO: Waiting for pod pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265 to disappear
Sep  4 18:44:14.264: INFO: Pod pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 18:44:14.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3392" for this suite. 09/04/23 18:44:14.269
------------------------------
â€¢ [4.119 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:44:10.161
    Sep  4 18:44:10.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename secrets 09/04/23 18:44:10.162
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:10.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:10.188
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-79864ae8-3931-41d9-b9ff-bacc7d31377a 09/04/23 18:44:10.19
    STEP: Creating a pod to test consume secrets 09/04/23 18:44:10.195
    Sep  4 18:44:10.204: INFO: Waiting up to 5m0s for pod "pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265" in namespace "secrets-3392" to be "Succeeded or Failed"
    Sep  4 18:44:10.221: INFO: Pod "pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265": Phase="Pending", Reason="", readiness=false. Elapsed: 16.988404ms
    Sep  4 18:44:12.228: INFO: Pod "pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023027716s
    Sep  4 18:44:14.228: INFO: Pod "pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023666301s
    STEP: Saw pod success 09/04/23 18:44:14.229
    Sep  4 18:44:14.229: INFO: Pod "pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265" satisfied condition "Succeeded or Failed"
    Sep  4 18:44:14.232: INFO: Trying to get logs from node tenant-000001 pod pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265 container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 18:44:14.239
    Sep  4 18:44:14.261: INFO: Waiting for pod pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265 to disappear
    Sep  4 18:44:14.264: INFO: Pod pod-secrets-5e86559c-2de8-46d6-9cdf-2f420011f265 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:44:14.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3392" for this suite. 09/04/23 18:44:14.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:44:14.286
Sep  4 18:44:14.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename subpath 09/04/23 18:44:14.288
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:14.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:14.309
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/04/23 18:44:14.312
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-xg9s 09/04/23 18:44:14.326
STEP: Creating a pod to test atomic-volume-subpath 09/04/23 18:44:14.326
Sep  4 18:44:14.336: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-xg9s" in namespace "subpath-3635" to be "Succeeded or Failed"
Sep  4 18:44:14.340: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.272377ms
Sep  4 18:44:16.344: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 2.00795698s
Sep  4 18:44:18.346: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 4.00979013s
Sep  4 18:44:20.344: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 6.007943962s
Sep  4 18:44:22.346: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 8.009868553s
Sep  4 18:44:24.345: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 10.008271965s
Sep  4 18:44:26.347: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 12.010485871s
Sep  4 18:44:28.345: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 14.008955829s
Sep  4 18:44:30.344: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 16.007802931s
Sep  4 18:44:32.346: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 18.009317383s
Sep  4 18:44:34.344: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 20.007974574s
Sep  4 18:44:36.347: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=false. Elapsed: 22.010928676s
Sep  4 18:44:38.345: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00840532s
STEP: Saw pod success 09/04/23 18:44:38.345
Sep  4 18:44:38.346: INFO: Pod "pod-subpath-test-secret-xg9s" satisfied condition "Succeeded or Failed"
Sep  4 18:44:38.350: INFO: Trying to get logs from node tenant-000001 pod pod-subpath-test-secret-xg9s container test-container-subpath-secret-xg9s: <nil>
STEP: delete the pod 09/04/23 18:44:38.36
Sep  4 18:44:38.378: INFO: Waiting for pod pod-subpath-test-secret-xg9s to disappear
Sep  4 18:44:38.381: INFO: Pod pod-subpath-test-secret-xg9s no longer exists
STEP: Deleting pod pod-subpath-test-secret-xg9s 09/04/23 18:44:38.381
Sep  4 18:44:38.382: INFO: Deleting pod "pod-subpath-test-secret-xg9s" in namespace "subpath-3635"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  4 18:44:38.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3635" for this suite. 09/04/23 18:44:38.394
------------------------------
â€¢ [SLOW TEST] [24.119 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:44:14.286
    Sep  4 18:44:14.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename subpath 09/04/23 18:44:14.288
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:14.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:14.309
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/04/23 18:44:14.312
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-xg9s 09/04/23 18:44:14.326
    STEP: Creating a pod to test atomic-volume-subpath 09/04/23 18:44:14.326
    Sep  4 18:44:14.336: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-xg9s" in namespace "subpath-3635" to be "Succeeded or Failed"
    Sep  4 18:44:14.340: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.272377ms
    Sep  4 18:44:16.344: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 2.00795698s
    Sep  4 18:44:18.346: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 4.00979013s
    Sep  4 18:44:20.344: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 6.007943962s
    Sep  4 18:44:22.346: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 8.009868553s
    Sep  4 18:44:24.345: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 10.008271965s
    Sep  4 18:44:26.347: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 12.010485871s
    Sep  4 18:44:28.345: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 14.008955829s
    Sep  4 18:44:30.344: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 16.007802931s
    Sep  4 18:44:32.346: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 18.009317383s
    Sep  4 18:44:34.344: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=true. Elapsed: 20.007974574s
    Sep  4 18:44:36.347: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Running", Reason="", readiness=false. Elapsed: 22.010928676s
    Sep  4 18:44:38.345: INFO: Pod "pod-subpath-test-secret-xg9s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00840532s
    STEP: Saw pod success 09/04/23 18:44:38.345
    Sep  4 18:44:38.346: INFO: Pod "pod-subpath-test-secret-xg9s" satisfied condition "Succeeded or Failed"
    Sep  4 18:44:38.350: INFO: Trying to get logs from node tenant-000001 pod pod-subpath-test-secret-xg9s container test-container-subpath-secret-xg9s: <nil>
    STEP: delete the pod 09/04/23 18:44:38.36
    Sep  4 18:44:38.378: INFO: Waiting for pod pod-subpath-test-secret-xg9s to disappear
    Sep  4 18:44:38.381: INFO: Pod pod-subpath-test-secret-xg9s no longer exists
    STEP: Deleting pod pod-subpath-test-secret-xg9s 09/04/23 18:44:38.381
    Sep  4 18:44:38.382: INFO: Deleting pod "pod-subpath-test-secret-xg9s" in namespace "subpath-3635"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:44:38.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3635" for this suite. 09/04/23 18:44:38.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:44:38.414
Sep  4 18:44:38.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubelet-test 09/04/23 18:44:38.415
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:38.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:38.485
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  4 18:44:42.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7213" for this suite. 09/04/23 18:44:42.512
------------------------------
â€¢ [4.106 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:44:38.414
    Sep  4 18:44:38.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubelet-test 09/04/23 18:44:38.415
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:38.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:38.485
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:44:42.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7213" for this suite. 09/04/23 18:44:42.512
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:44:42.525
Sep  4 18:44:42.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 18:44:42.526
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:42.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:42.548
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-a06c9c52-e539-4d24-9339-71a00d8c6266 09/04/23 18:44:42.55
STEP: Creating a pod to test consume configMaps 09/04/23 18:44:42.559
Sep  4 18:44:42.573: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e" in namespace "projected-9868" to be "Succeeded or Failed"
Sep  4 18:44:42.583: INFO: Pod "pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.589821ms
Sep  4 18:44:44.587: INFO: Pod "pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014203713s
Sep  4 18:44:46.587: INFO: Pod "pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013730967s
STEP: Saw pod success 09/04/23 18:44:46.587
Sep  4 18:44:46.588: INFO: Pod "pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e" satisfied condition "Succeeded or Failed"
Sep  4 18:44:46.591: INFO: Trying to get logs from node tenant-000001 pod pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e container agnhost-container: <nil>
STEP: delete the pod 09/04/23 18:44:46.601
Sep  4 18:44:46.616: INFO: Waiting for pod pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e to disappear
Sep  4 18:44:46.619: INFO: Pod pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 18:44:46.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9868" for this suite. 09/04/23 18:44:46.623
------------------------------
â€¢ [4.106 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:44:42.525
    Sep  4 18:44:42.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 18:44:42.526
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:42.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:42.548
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-a06c9c52-e539-4d24-9339-71a00d8c6266 09/04/23 18:44:42.55
    STEP: Creating a pod to test consume configMaps 09/04/23 18:44:42.559
    Sep  4 18:44:42.573: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e" in namespace "projected-9868" to be "Succeeded or Failed"
    Sep  4 18:44:42.583: INFO: Pod "pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.589821ms
    Sep  4 18:44:44.587: INFO: Pod "pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014203713s
    Sep  4 18:44:46.587: INFO: Pod "pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013730967s
    STEP: Saw pod success 09/04/23 18:44:46.587
    Sep  4 18:44:46.588: INFO: Pod "pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e" satisfied condition "Succeeded or Failed"
    Sep  4 18:44:46.591: INFO: Trying to get logs from node tenant-000001 pod pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 18:44:46.601
    Sep  4 18:44:46.616: INFO: Waiting for pod pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e to disappear
    Sep  4 18:44:46.619: INFO: Pod pod-projected-configmaps-0948ddd6-1a1b-4e07-9e52-db4cc5e1164e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:44:46.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9868" for this suite. 09/04/23 18:44:46.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:44:46.636
Sep  4 18:44:46.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename replicaset 09/04/23 18:44:46.638
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:46.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:46.659
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 09/04/23 18:44:46.666
STEP: Verify that the required pods have come up. 09/04/23 18:44:46.675
Sep  4 18:44:46.680: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  4 18:44:51.686: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/04/23 18:44:51.686
STEP: Getting /status 09/04/23 18:44:51.687
Sep  4 18:44:51.690: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 09/04/23 18:44:51.69
Sep  4 18:44:51.709: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 09/04/23 18:44:51.709
Sep  4 18:44:51.713: INFO: Observed &ReplicaSet event: ADDED
Sep  4 18:44:51.713: INFO: Observed &ReplicaSet event: MODIFIED
Sep  4 18:44:51.713: INFO: Observed &ReplicaSet event: MODIFIED
Sep  4 18:44:51.714: INFO: Observed &ReplicaSet event: MODIFIED
Sep  4 18:44:51.714: INFO: Found replicaset test-rs in namespace replicaset-8680 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  4 18:44:51.714: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 09/04/23 18:44:51.715
Sep  4 18:44:51.715: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  4 18:44:51.724: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 09/04/23 18:44:51.724
Sep  4 18:44:51.726: INFO: Observed &ReplicaSet event: ADDED
Sep  4 18:44:51.726: INFO: Observed &ReplicaSet event: MODIFIED
Sep  4 18:44:51.727: INFO: Observed &ReplicaSet event: MODIFIED
Sep  4 18:44:51.727: INFO: Observed &ReplicaSet event: MODIFIED
Sep  4 18:44:51.727: INFO: Observed replicaset test-rs in namespace replicaset-8680 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  4 18:44:51.727: INFO: Observed &ReplicaSet event: MODIFIED
Sep  4 18:44:51.727: INFO: Found replicaset test-rs in namespace replicaset-8680 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Sep  4 18:44:51.728: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  4 18:44:51.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8680" for this suite. 09/04/23 18:44:51.742
------------------------------
â€¢ [SLOW TEST] [5.139 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:44:46.636
    Sep  4 18:44:46.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename replicaset 09/04/23 18:44:46.638
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:46.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:46.659
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 09/04/23 18:44:46.666
    STEP: Verify that the required pods have come up. 09/04/23 18:44:46.675
    Sep  4 18:44:46.680: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  4 18:44:51.686: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/04/23 18:44:51.686
    STEP: Getting /status 09/04/23 18:44:51.687
    Sep  4 18:44:51.690: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 09/04/23 18:44:51.69
    Sep  4 18:44:51.709: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 09/04/23 18:44:51.709
    Sep  4 18:44:51.713: INFO: Observed &ReplicaSet event: ADDED
    Sep  4 18:44:51.713: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  4 18:44:51.713: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  4 18:44:51.714: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  4 18:44:51.714: INFO: Found replicaset test-rs in namespace replicaset-8680 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  4 18:44:51.714: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 09/04/23 18:44:51.715
    Sep  4 18:44:51.715: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  4 18:44:51.724: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 09/04/23 18:44:51.724
    Sep  4 18:44:51.726: INFO: Observed &ReplicaSet event: ADDED
    Sep  4 18:44:51.726: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  4 18:44:51.727: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  4 18:44:51.727: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  4 18:44:51.727: INFO: Observed replicaset test-rs in namespace replicaset-8680 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  4 18:44:51.727: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  4 18:44:51.727: INFO: Found replicaset test-rs in namespace replicaset-8680 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Sep  4 18:44:51.728: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:44:51.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8680" for this suite. 09/04/23 18:44:51.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:44:51.781
Sep  4 18:44:51.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename ephemeral-containers-test 09/04/23 18:44:51.782
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:51.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:51.818
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 09/04/23 18:44:51.821
Sep  4 18:44:51.833: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3348" to be "running and ready"
Sep  4 18:44:51.849: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.33369ms
Sep  4 18:44:51.849: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:44:53.854: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021012088s
Sep  4 18:44:53.855: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Sep  4 18:44:53.855: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 09/04/23 18:44:53.859
Sep  4 18:44:53.875: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3348" to be "container debugger running"
Sep  4 18:44:53.889: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 13.303087ms
Sep  4 18:44:55.897: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021185662s
Sep  4 18:44:57.894: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018287857s
Sep  4 18:44:57.894: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 09/04/23 18:44:57.894
Sep  4 18:44:57.894: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3348 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 18:44:57.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 18:44:57.895: INFO: ExecWithOptions: Clientset creation
Sep  4 18:44:57.895: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-3348/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Sep  4 18:44:57.979: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:44:57.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-3348" for this suite. 09/04/23 18:44:58.001
------------------------------
â€¢ [SLOW TEST] [6.228 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:44:51.781
    Sep  4 18:44:51.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename ephemeral-containers-test 09/04/23 18:44:51.782
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:51.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:51.818
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 09/04/23 18:44:51.821
    Sep  4 18:44:51.833: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3348" to be "running and ready"
    Sep  4 18:44:51.849: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.33369ms
    Sep  4 18:44:51.849: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:44:53.854: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021012088s
    Sep  4 18:44:53.855: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Sep  4 18:44:53.855: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 09/04/23 18:44:53.859
    Sep  4 18:44:53.875: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3348" to be "container debugger running"
    Sep  4 18:44:53.889: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 13.303087ms
    Sep  4 18:44:55.897: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021185662s
    Sep  4 18:44:57.894: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018287857s
    Sep  4 18:44:57.894: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 09/04/23 18:44:57.894
    Sep  4 18:44:57.894: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3348 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 18:44:57.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 18:44:57.895: INFO: ExecWithOptions: Clientset creation
    Sep  4 18:44:57.895: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-3348/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Sep  4 18:44:57.979: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:44:57.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-3348" for this suite. 09/04/23 18:44:58.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:44:58.014
Sep  4 18:44:58.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename conformance-tests 09/04/23 18:44:58.016
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:58.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:58.034
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 09/04/23 18:44:58.037
Sep  4 18:44:58.037: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Sep  4 18:44:58.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-4935" for this suite. 09/04/23 18:44:58.059
------------------------------
â€¢ [0.056 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:44:58.014
    Sep  4 18:44:58.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename conformance-tests 09/04/23 18:44:58.016
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:58.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:58.034
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 09/04/23 18:44:58.037
    Sep  4 18:44:58.037: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:44:58.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-4935" for this suite. 09/04/23 18:44:58.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:44:58.075
Sep  4 18:44:58.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename resourcequota 09/04/23 18:44:58.076
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:58.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:58.098
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 09/04/23 18:45:15.113
STEP: Creating a ResourceQuota 09/04/23 18:45:20.117
STEP: Ensuring resource quota status is calculated 09/04/23 18:45:20.135
STEP: Creating a ConfigMap 09/04/23 18:45:22.14
STEP: Ensuring resource quota status captures configMap creation 09/04/23 18:45:22.155
STEP: Deleting a ConfigMap 09/04/23 18:45:24.162
STEP: Ensuring resource quota status released usage 09/04/23 18:45:24.177
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 18:45:26.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7495" for this suite. 09/04/23 18:45:26.187
------------------------------
â€¢ [SLOW TEST] [28.121 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:44:58.075
    Sep  4 18:44:58.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename resourcequota 09/04/23 18:44:58.076
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:44:58.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:44:58.098
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 09/04/23 18:45:15.113
    STEP: Creating a ResourceQuota 09/04/23 18:45:20.117
    STEP: Ensuring resource quota status is calculated 09/04/23 18:45:20.135
    STEP: Creating a ConfigMap 09/04/23 18:45:22.14
    STEP: Ensuring resource quota status captures configMap creation 09/04/23 18:45:22.155
    STEP: Deleting a ConfigMap 09/04/23 18:45:24.162
    STEP: Ensuring resource quota status released usage 09/04/23 18:45:24.177
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:45:26.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7495" for this suite. 09/04/23 18:45:26.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:45:26.211
Sep  4 18:45:26.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename discovery 09/04/23 18:45:26.212
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:26.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:26.238
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 09/04/23 18:45:26.242
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Sep  4 18:45:26.739: INFO: Checking APIGroup: apiregistration.k8s.io
Sep  4 18:45:26.740: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep  4 18:45:26.741: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Sep  4 18:45:26.741: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep  4 18:45:26.741: INFO: Checking APIGroup: apps
Sep  4 18:45:26.742: INFO: PreferredVersion.GroupVersion: apps/v1
Sep  4 18:45:26.742: INFO: Versions found [{apps/v1 v1}]
Sep  4 18:45:26.742: INFO: apps/v1 matches apps/v1
Sep  4 18:45:26.742: INFO: Checking APIGroup: events.k8s.io
Sep  4 18:45:26.743: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep  4 18:45:26.743: INFO: Versions found [{events.k8s.io/v1 v1}]
Sep  4 18:45:26.743: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep  4 18:45:26.743: INFO: Checking APIGroup: authentication.k8s.io
Sep  4 18:45:26.743: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep  4 18:45:26.743: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Sep  4 18:45:26.743: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep  4 18:45:26.743: INFO: Checking APIGroup: authorization.k8s.io
Sep  4 18:45:26.745: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep  4 18:45:26.745: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Sep  4 18:45:26.745: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep  4 18:45:26.745: INFO: Checking APIGroup: autoscaling
Sep  4 18:45:26.746: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Sep  4 18:45:26.746: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Sep  4 18:45:26.746: INFO: autoscaling/v2 matches autoscaling/v2
Sep  4 18:45:26.746: INFO: Checking APIGroup: batch
Sep  4 18:45:26.746: INFO: PreferredVersion.GroupVersion: batch/v1
Sep  4 18:45:26.746: INFO: Versions found [{batch/v1 v1}]
Sep  4 18:45:26.746: INFO: batch/v1 matches batch/v1
Sep  4 18:45:26.746: INFO: Checking APIGroup: certificates.k8s.io
Sep  4 18:45:26.747: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep  4 18:45:26.747: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Sep  4 18:45:26.747: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep  4 18:45:26.747: INFO: Checking APIGroup: networking.k8s.io
Sep  4 18:45:26.748: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep  4 18:45:26.748: INFO: Versions found [{networking.k8s.io/v1 v1}]
Sep  4 18:45:26.748: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep  4 18:45:26.749: INFO: Checking APIGroup: policy
Sep  4 18:45:26.750: INFO: PreferredVersion.GroupVersion: policy/v1
Sep  4 18:45:26.750: INFO: Versions found [{policy/v1 v1}]
Sep  4 18:45:26.750: INFO: policy/v1 matches policy/v1
Sep  4 18:45:26.750: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep  4 18:45:26.752: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep  4 18:45:26.752: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Sep  4 18:45:26.752: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep  4 18:45:26.752: INFO: Checking APIGroup: storage.k8s.io
Sep  4 18:45:26.753: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep  4 18:45:26.753: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep  4 18:45:26.753: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep  4 18:45:26.753: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep  4 18:45:26.754: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep  4 18:45:26.754: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Sep  4 18:45:26.755: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep  4 18:45:26.755: INFO: Checking APIGroup: apiextensions.k8s.io
Sep  4 18:45:26.756: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep  4 18:45:26.756: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Sep  4 18:45:26.756: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep  4 18:45:26.756: INFO: Checking APIGroup: scheduling.k8s.io
Sep  4 18:45:26.757: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep  4 18:45:26.757: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Sep  4 18:45:26.757: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep  4 18:45:26.757: INFO: Checking APIGroup: coordination.k8s.io
Sep  4 18:45:26.758: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep  4 18:45:26.758: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Sep  4 18:45:26.758: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep  4 18:45:26.759: INFO: Checking APIGroup: node.k8s.io
Sep  4 18:45:26.760: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Sep  4 18:45:26.760: INFO: Versions found [{node.k8s.io/v1 v1}]
Sep  4 18:45:26.760: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Sep  4 18:45:26.760: INFO: Checking APIGroup: discovery.k8s.io
Sep  4 18:45:26.761: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Sep  4 18:45:26.762: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Sep  4 18:45:26.762: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Sep  4 18:45:26.762: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep  4 18:45:26.763: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Sep  4 18:45:26.763: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Sep  4 18:45:26.763: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Sep  4 18:45:26.763: INFO: Checking APIGroup: crd.projectcalico.org
Sep  4 18:45:26.764: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Sep  4 18:45:26.764: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Sep  4 18:45:26.765: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Sep  4 18:45:26.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-2851" for this suite. 09/04/23 18:45:26.772
------------------------------
â€¢ [0.569 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:45:26.211
    Sep  4 18:45:26.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename discovery 09/04/23 18:45:26.212
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:26.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:26.238
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 09/04/23 18:45:26.242
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Sep  4 18:45:26.739: INFO: Checking APIGroup: apiregistration.k8s.io
    Sep  4 18:45:26.740: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Sep  4 18:45:26.741: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Sep  4 18:45:26.741: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Sep  4 18:45:26.741: INFO: Checking APIGroup: apps
    Sep  4 18:45:26.742: INFO: PreferredVersion.GroupVersion: apps/v1
    Sep  4 18:45:26.742: INFO: Versions found [{apps/v1 v1}]
    Sep  4 18:45:26.742: INFO: apps/v1 matches apps/v1
    Sep  4 18:45:26.742: INFO: Checking APIGroup: events.k8s.io
    Sep  4 18:45:26.743: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Sep  4 18:45:26.743: INFO: Versions found [{events.k8s.io/v1 v1}]
    Sep  4 18:45:26.743: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Sep  4 18:45:26.743: INFO: Checking APIGroup: authentication.k8s.io
    Sep  4 18:45:26.743: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Sep  4 18:45:26.743: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Sep  4 18:45:26.743: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Sep  4 18:45:26.743: INFO: Checking APIGroup: authorization.k8s.io
    Sep  4 18:45:26.745: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Sep  4 18:45:26.745: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Sep  4 18:45:26.745: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Sep  4 18:45:26.745: INFO: Checking APIGroup: autoscaling
    Sep  4 18:45:26.746: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Sep  4 18:45:26.746: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Sep  4 18:45:26.746: INFO: autoscaling/v2 matches autoscaling/v2
    Sep  4 18:45:26.746: INFO: Checking APIGroup: batch
    Sep  4 18:45:26.746: INFO: PreferredVersion.GroupVersion: batch/v1
    Sep  4 18:45:26.746: INFO: Versions found [{batch/v1 v1}]
    Sep  4 18:45:26.746: INFO: batch/v1 matches batch/v1
    Sep  4 18:45:26.746: INFO: Checking APIGroup: certificates.k8s.io
    Sep  4 18:45:26.747: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Sep  4 18:45:26.747: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Sep  4 18:45:26.747: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Sep  4 18:45:26.747: INFO: Checking APIGroup: networking.k8s.io
    Sep  4 18:45:26.748: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Sep  4 18:45:26.748: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Sep  4 18:45:26.748: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Sep  4 18:45:26.749: INFO: Checking APIGroup: policy
    Sep  4 18:45:26.750: INFO: PreferredVersion.GroupVersion: policy/v1
    Sep  4 18:45:26.750: INFO: Versions found [{policy/v1 v1}]
    Sep  4 18:45:26.750: INFO: policy/v1 matches policy/v1
    Sep  4 18:45:26.750: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Sep  4 18:45:26.752: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Sep  4 18:45:26.752: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Sep  4 18:45:26.752: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Sep  4 18:45:26.752: INFO: Checking APIGroup: storage.k8s.io
    Sep  4 18:45:26.753: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Sep  4 18:45:26.753: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Sep  4 18:45:26.753: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Sep  4 18:45:26.753: INFO: Checking APIGroup: admissionregistration.k8s.io
    Sep  4 18:45:26.754: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Sep  4 18:45:26.754: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Sep  4 18:45:26.755: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Sep  4 18:45:26.755: INFO: Checking APIGroup: apiextensions.k8s.io
    Sep  4 18:45:26.756: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Sep  4 18:45:26.756: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Sep  4 18:45:26.756: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Sep  4 18:45:26.756: INFO: Checking APIGroup: scheduling.k8s.io
    Sep  4 18:45:26.757: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Sep  4 18:45:26.757: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Sep  4 18:45:26.757: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Sep  4 18:45:26.757: INFO: Checking APIGroup: coordination.k8s.io
    Sep  4 18:45:26.758: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Sep  4 18:45:26.758: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Sep  4 18:45:26.758: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Sep  4 18:45:26.759: INFO: Checking APIGroup: node.k8s.io
    Sep  4 18:45:26.760: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Sep  4 18:45:26.760: INFO: Versions found [{node.k8s.io/v1 v1}]
    Sep  4 18:45:26.760: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Sep  4 18:45:26.760: INFO: Checking APIGroup: discovery.k8s.io
    Sep  4 18:45:26.761: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Sep  4 18:45:26.762: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Sep  4 18:45:26.762: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Sep  4 18:45:26.762: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Sep  4 18:45:26.763: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Sep  4 18:45:26.763: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Sep  4 18:45:26.763: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Sep  4 18:45:26.763: INFO: Checking APIGroup: crd.projectcalico.org
    Sep  4 18:45:26.764: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Sep  4 18:45:26.764: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Sep  4 18:45:26.765: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:45:26.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-2851" for this suite. 09/04/23 18:45:26.772
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:45:26.785
Sep  4 18:45:26.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 18:45:26.787
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:26.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:26.806
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 18:45:26.825
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:45:27.349
STEP: Deploying the webhook pod 09/04/23 18:45:27.361
STEP: Wait for the deployment to be ready 09/04/23 18:45:27.389
Sep  4 18:45:27.417: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 18:45:29.433
STEP: Verifying the service has paired with the endpoint 09/04/23 18:45:29.452
Sep  4 18:45:30.453: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/04/23 18:45:30.46
STEP: create a namespace for the webhook 09/04/23 18:45:30.486
STEP: create a configmap should be unconditionally rejected by the webhook 09/04/23 18:45:30.499
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:45:30.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1454" for this suite. 09/04/23 18:45:30.606
STEP: Destroying namespace "webhook-1454-markers" for this suite. 09/04/23 18:45:30.628
------------------------------
â€¢ [3.875 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:45:26.785
    Sep  4 18:45:26.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 18:45:26.787
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:26.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:26.806
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 18:45:26.825
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:45:27.349
    STEP: Deploying the webhook pod 09/04/23 18:45:27.361
    STEP: Wait for the deployment to be ready 09/04/23 18:45:27.389
    Sep  4 18:45:27.417: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 18:45:29.433
    STEP: Verifying the service has paired with the endpoint 09/04/23 18:45:29.452
    Sep  4 18:45:30.453: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/04/23 18:45:30.46
    STEP: create a namespace for the webhook 09/04/23 18:45:30.486
    STEP: create a configmap should be unconditionally rejected by the webhook 09/04/23 18:45:30.499
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:45:30.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1454" for this suite. 09/04/23 18:45:30.606
    STEP: Destroying namespace "webhook-1454-markers" for this suite. 09/04/23 18:45:30.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:45:30.681
Sep  4 18:45:30.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 18:45:30.683
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:30.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:30.7
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 09/04/23 18:45:30.703
Sep  4 18:45:30.718: INFO: Waiting up to 5m0s for pod "annotationupdatea31e9502-99d5-4162-964d-756031c5bf34" in namespace "downward-api-1563" to be "running and ready"
Sep  4 18:45:30.731: INFO: Pod "annotationupdatea31e9502-99d5-4162-964d-756031c5bf34": Phase="Pending", Reason="", readiness=false. Elapsed: 13.478772ms
Sep  4 18:45:30.731: INFO: The phase of Pod annotationupdatea31e9502-99d5-4162-964d-756031c5bf34 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:45:32.735: INFO: Pod "annotationupdatea31e9502-99d5-4162-964d-756031c5bf34": Phase="Running", Reason="", readiness=true. Elapsed: 2.017536284s
Sep  4 18:45:32.735: INFO: The phase of Pod annotationupdatea31e9502-99d5-4162-964d-756031c5bf34 is Running (Ready = true)
Sep  4 18:45:32.736: INFO: Pod "annotationupdatea31e9502-99d5-4162-964d-756031c5bf34" satisfied condition "running and ready"
Sep  4 18:45:33.361: INFO: Successfully updated pod "annotationupdatea31e9502-99d5-4162-964d-756031c5bf34"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 18:45:37.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1563" for this suite. 09/04/23 18:45:37.454
------------------------------
â€¢ [SLOW TEST] [6.784 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:45:30.681
    Sep  4 18:45:30.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 18:45:30.683
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:30.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:30.7
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 09/04/23 18:45:30.703
    Sep  4 18:45:30.718: INFO: Waiting up to 5m0s for pod "annotationupdatea31e9502-99d5-4162-964d-756031c5bf34" in namespace "downward-api-1563" to be "running and ready"
    Sep  4 18:45:30.731: INFO: Pod "annotationupdatea31e9502-99d5-4162-964d-756031c5bf34": Phase="Pending", Reason="", readiness=false. Elapsed: 13.478772ms
    Sep  4 18:45:30.731: INFO: The phase of Pod annotationupdatea31e9502-99d5-4162-964d-756031c5bf34 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:45:32.735: INFO: Pod "annotationupdatea31e9502-99d5-4162-964d-756031c5bf34": Phase="Running", Reason="", readiness=true. Elapsed: 2.017536284s
    Sep  4 18:45:32.735: INFO: The phase of Pod annotationupdatea31e9502-99d5-4162-964d-756031c5bf34 is Running (Ready = true)
    Sep  4 18:45:32.736: INFO: Pod "annotationupdatea31e9502-99d5-4162-964d-756031c5bf34" satisfied condition "running and ready"
    Sep  4 18:45:33.361: INFO: Successfully updated pod "annotationupdatea31e9502-99d5-4162-964d-756031c5bf34"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:45:37.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1563" for this suite. 09/04/23 18:45:37.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:45:37.466
Sep  4 18:45:37.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 18:45:37.466
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:37.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:37.489
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 09/04/23 18:45:37.492
Sep  4 18:45:37.504: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a" in namespace "downward-api-1978" to be "Succeeded or Failed"
Sep  4 18:45:37.517: INFO: Pod "downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.579727ms
Sep  4 18:45:39.522: INFO: Pod "downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016951855s
Sep  4 18:45:41.522: INFO: Pod "downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017042599s
STEP: Saw pod success 09/04/23 18:45:41.522
Sep  4 18:45:41.522: INFO: Pod "downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a" satisfied condition "Succeeded or Failed"
Sep  4 18:45:41.527: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a container client-container: <nil>
STEP: delete the pod 09/04/23 18:45:41.536
Sep  4 18:45:41.561: INFO: Waiting for pod downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a to disappear
Sep  4 18:45:41.565: INFO: Pod downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 18:45:41.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1978" for this suite. 09/04/23 18:45:41.569
------------------------------
â€¢ [4.111 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:45:37.466
    Sep  4 18:45:37.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 18:45:37.466
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:37.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:37.489
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 09/04/23 18:45:37.492
    Sep  4 18:45:37.504: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a" in namespace "downward-api-1978" to be "Succeeded or Failed"
    Sep  4 18:45:37.517: INFO: Pod "downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.579727ms
    Sep  4 18:45:39.522: INFO: Pod "downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016951855s
    Sep  4 18:45:41.522: INFO: Pod "downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017042599s
    STEP: Saw pod success 09/04/23 18:45:41.522
    Sep  4 18:45:41.522: INFO: Pod "downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a" satisfied condition "Succeeded or Failed"
    Sep  4 18:45:41.527: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a container client-container: <nil>
    STEP: delete the pod 09/04/23 18:45:41.536
    Sep  4 18:45:41.561: INFO: Waiting for pod downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a to disappear
    Sep  4 18:45:41.565: INFO: Pod downwardapi-volume-6d0c99f9-1e10-44c7-a92e-aa460d0c0b5a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:45:41.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1978" for this suite. 09/04/23 18:45:41.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:45:41.582
Sep  4 18:45:41.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename limitrange 09/04/23 18:45:41.586
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:41.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:41.607
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-f7ql5" in namespace "limitrange-579" 09/04/23 18:45:41.61
STEP: Creating another limitRange in another namespace 09/04/23 18:45:41.618
Sep  4 18:45:41.637: INFO: Namespace "e2e-limitrange-f7ql5-7850" created
Sep  4 18:45:41.637: INFO: Creating LimitRange "e2e-limitrange-f7ql5" in namespace "e2e-limitrange-f7ql5-7850"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-f7ql5" 09/04/23 18:45:41.645
Sep  4 18:45:41.647: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-f7ql5" in "limitrange-579" namespace 09/04/23 18:45:41.648
Sep  4 18:45:41.657: INFO: LimitRange "e2e-limitrange-f7ql5" has been patched
STEP: Delete LimitRange "e2e-limitrange-f7ql5" by Collection with labelSelector: "e2e-limitrange-f7ql5=patched" 09/04/23 18:45:41.657
STEP: Confirm that the limitRange "e2e-limitrange-f7ql5" has been deleted 09/04/23 18:45:41.669
Sep  4 18:45:41.669: INFO: Requesting list of LimitRange to confirm quantity
Sep  4 18:45:41.674: INFO: Found 0 LimitRange with label "e2e-limitrange-f7ql5=patched"
Sep  4 18:45:41.674: INFO: LimitRange "e2e-limitrange-f7ql5" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-f7ql5" 09/04/23 18:45:41.674
Sep  4 18:45:41.677: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Sep  4 18:45:41.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-579" for this suite. 09/04/23 18:45:41.681
STEP: Destroying namespace "e2e-limitrange-f7ql5-7850" for this suite. 09/04/23 18:45:41.69
------------------------------
â€¢ [0.120 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:45:41.582
    Sep  4 18:45:41.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename limitrange 09/04/23 18:45:41.586
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:41.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:41.607
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-f7ql5" in namespace "limitrange-579" 09/04/23 18:45:41.61
    STEP: Creating another limitRange in another namespace 09/04/23 18:45:41.618
    Sep  4 18:45:41.637: INFO: Namespace "e2e-limitrange-f7ql5-7850" created
    Sep  4 18:45:41.637: INFO: Creating LimitRange "e2e-limitrange-f7ql5" in namespace "e2e-limitrange-f7ql5-7850"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-f7ql5" 09/04/23 18:45:41.645
    Sep  4 18:45:41.647: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-f7ql5" in "limitrange-579" namespace 09/04/23 18:45:41.648
    Sep  4 18:45:41.657: INFO: LimitRange "e2e-limitrange-f7ql5" has been patched
    STEP: Delete LimitRange "e2e-limitrange-f7ql5" by Collection with labelSelector: "e2e-limitrange-f7ql5=patched" 09/04/23 18:45:41.657
    STEP: Confirm that the limitRange "e2e-limitrange-f7ql5" has been deleted 09/04/23 18:45:41.669
    Sep  4 18:45:41.669: INFO: Requesting list of LimitRange to confirm quantity
    Sep  4 18:45:41.674: INFO: Found 0 LimitRange with label "e2e-limitrange-f7ql5=patched"
    Sep  4 18:45:41.674: INFO: LimitRange "e2e-limitrange-f7ql5" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-f7ql5" 09/04/23 18:45:41.674
    Sep  4 18:45:41.677: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:45:41.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-579" for this suite. 09/04/23 18:45:41.681
    STEP: Destroying namespace "e2e-limitrange-f7ql5-7850" for this suite. 09/04/23 18:45:41.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:45:41.705
Sep  4 18:45:41.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename svcaccounts 09/04/23 18:45:41.706
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:41.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:41.74
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-s6l2h"  09/04/23 18:45:41.743
Sep  4 18:45:41.750: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-s6l2h"  09/04/23 18:45:41.75
Sep  4 18:45:41.776: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  4 18:45:41.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2105" for this suite. 09/04/23 18:45:41.784
------------------------------
â€¢ [0.089 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:45:41.705
    Sep  4 18:45:41.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename svcaccounts 09/04/23 18:45:41.706
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:41.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:41.74
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-s6l2h"  09/04/23 18:45:41.743
    Sep  4 18:45:41.750: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-s6l2h"  09/04/23 18:45:41.75
    Sep  4 18:45:41.776: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:45:41.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2105" for this suite. 09/04/23 18:45:41.784
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:45:41.795
Sep  4 18:45:41.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename gc 09/04/23 18:45:41.795
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:41.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:41.819
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 09/04/23 18:45:41.824
STEP: delete the rc 09/04/23 18:45:46.847
STEP: wait for all pods to be garbage collected 09/04/23 18:45:46.856
STEP: Gathering metrics 09/04/23 18:45:51.864
W0904 18:45:51.875028      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep  4 18:45:51.875: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  4 18:45:51.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2451" for this suite. 09/04/23 18:45:51.883
------------------------------
â€¢ [SLOW TEST] [10.097 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:45:41.795
    Sep  4 18:45:41.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename gc 09/04/23 18:45:41.795
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:41.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:41.819
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 09/04/23 18:45:41.824
    STEP: delete the rc 09/04/23 18:45:46.847
    STEP: wait for all pods to be garbage collected 09/04/23 18:45:46.856
    STEP: Gathering metrics 09/04/23 18:45:51.864
    W0904 18:45:51.875028      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep  4 18:45:51.875: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:45:51.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2451" for this suite. 09/04/23 18:45:51.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:45:51.902
Sep  4 18:45:51.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename dns 09/04/23 18:45:51.904
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:51.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:51.925
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 09/04/23 18:45:51.928
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-170 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-170;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-170 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-170;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-170.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-170.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-170.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-170.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-170.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-170.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-170.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-170.svc;check="$$(dig +notcp +noall +answer +search 234.22.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.22.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.22.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.22.234_tcp@PTR;sleep 1; done
 09/04/23 18:45:51.954
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-170 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-170;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-170 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-170;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-170.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-170.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-170.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-170.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-170.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-170.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-170.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-170.svc;check="$$(dig +notcp +noall +answer +search 234.22.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.22.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.22.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.22.234_tcp@PTR;sleep 1; done
 09/04/23 18:45:51.954
STEP: creating a pod to probe DNS 09/04/23 18:45:51.955
STEP: submitting the pod to kubernetes 09/04/23 18:45:51.955
Sep  4 18:45:51.976: INFO: Waiting up to 15m0s for pod "dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739" in namespace "dns-170" to be "running"
Sep  4 18:45:51.988: INFO: Pod "dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739": Phase="Pending", Reason="", readiness=false. Elapsed: 12.158893ms
Sep  4 18:45:53.995: INFO: Pod "dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739": Phase="Running", Reason="", readiness=true. Elapsed: 2.019021206s
Sep  4 18:45:53.995: INFO: Pod "dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739" satisfied condition "running"
STEP: retrieving the pod 09/04/23 18:45:53.995
STEP: looking for the results for each expected name from probers 09/04/23 18:45:54.006
Sep  4 18:45:54.020: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:54.026: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:54.038: INFO: Unable to read wheezy_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:54.043: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:54.052: INFO: Unable to read wheezy_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:54.064: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:54.120: INFO: Unable to read jessie_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:54.125: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:54.137: INFO: Unable to read jessie_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:54.143: INFO: Unable to read jessie_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:54.148: INFO: Unable to read jessie_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:54.156: INFO: Unable to read jessie_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:54.197: INFO: Lookups using dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-170 wheezy_tcp@dns-test-service.dns-170 wheezy_udp@dns-test-service.dns-170.svc wheezy_tcp@dns-test-service.dns-170.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-170 jessie_tcp@dns-test-service.dns-170 jessie_udp@dns-test-service.dns-170.svc jessie_tcp@dns-test-service.dns-170.svc]

Sep  4 18:45:59.209: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:59.218: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:59.228: INFO: Unable to read wheezy_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:59.237: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:59.241: INFO: Unable to read wheezy_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:59.252: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:59.304: INFO: Unable to read jessie_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:59.310: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:59.321: INFO: Unable to read jessie_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:59.328: INFO: Unable to read jessie_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:59.332: INFO: Unable to read jessie_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:59.337: INFO: Unable to read jessie_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:45:59.372: INFO: Lookups using dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-170 wheezy_tcp@dns-test-service.dns-170 wheezy_udp@dns-test-service.dns-170.svc wheezy_tcp@dns-test-service.dns-170.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-170 jessie_tcp@dns-test-service.dns-170 jessie_udp@dns-test-service.dns-170.svc jessie_tcp@dns-test-service.dns-170.svc]

Sep  4 18:46:04.205: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:04.210: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:04.217: INFO: Unable to read wheezy_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:04.224: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:04.229: INFO: Unable to read wheezy_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:04.236: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:04.292: INFO: Unable to read jessie_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:04.298: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:04.305: INFO: Unable to read jessie_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:04.311: INFO: Unable to read jessie_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:04.319: INFO: Unable to read jessie_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:04.326: INFO: Unable to read jessie_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:04.375: INFO: Lookups using dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-170 wheezy_tcp@dns-test-service.dns-170 wheezy_udp@dns-test-service.dns-170.svc wheezy_tcp@dns-test-service.dns-170.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-170 jessie_tcp@dns-test-service.dns-170 jessie_udp@dns-test-service.dns-170.svc jessie_tcp@dns-test-service.dns-170.svc]

Sep  4 18:46:09.208: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:09.213: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:09.219: INFO: Unable to read wheezy_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:09.224: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:09.228: INFO: Unable to read wheezy_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:09.236: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:09.276: INFO: Unable to read jessie_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:09.281: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:09.289: INFO: Unable to read jessie_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:09.293: INFO: Unable to read jessie_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:09.301: INFO: Unable to read jessie_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:09.309: INFO: Unable to read jessie_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:09.356: INFO: Lookups using dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-170 wheezy_tcp@dns-test-service.dns-170 wheezy_udp@dns-test-service.dns-170.svc wheezy_tcp@dns-test-service.dns-170.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-170 jessie_tcp@dns-test-service.dns-170 jessie_udp@dns-test-service.dns-170.svc jessie_tcp@dns-test-service.dns-170.svc]

Sep  4 18:46:14.210: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:14.219: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:14.228: INFO: Unable to read wheezy_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:14.234: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:14.244: INFO: Unable to read wheezy_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:14.254: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:14.312: INFO: Unable to read jessie_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:14.322: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:14.328: INFO: Unable to read jessie_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:14.336: INFO: Unable to read jessie_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:14.344: INFO: Unable to read jessie_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:14.349: INFO: Unable to read jessie_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:14.383: INFO: Lookups using dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-170 wheezy_tcp@dns-test-service.dns-170 wheezy_udp@dns-test-service.dns-170.svc wheezy_tcp@dns-test-service.dns-170.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-170 jessie_tcp@dns-test-service.dns-170 jessie_udp@dns-test-service.dns-170.svc jessie_tcp@dns-test-service.dns-170.svc]

Sep  4 18:46:19.207: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:19.220: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:19.229: INFO: Unable to read wheezy_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:19.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:19.245: INFO: Unable to read wheezy_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:19.254: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:19.300: INFO: Unable to read jessie_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:19.307: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:19.313: INFO: Unable to read jessie_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:19.323: INFO: Unable to read jessie_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:19.330: INFO: Unable to read jessie_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:19.336: INFO: Unable to read jessie_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
Sep  4 18:46:19.372: INFO: Lookups using dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-170 wheezy_tcp@dns-test-service.dns-170 wheezy_udp@dns-test-service.dns-170.svc wheezy_tcp@dns-test-service.dns-170.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-170 jessie_tcp@dns-test-service.dns-170 jessie_udp@dns-test-service.dns-170.svc jessie_tcp@dns-test-service.dns-170.svc]

Sep  4 18:46:24.333: INFO: DNS probes using dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739 succeeded

STEP: deleting the pod 09/04/23 18:46:24.333
STEP: deleting the test service 09/04/23 18:46:24.401
STEP: deleting the test headless service 09/04/23 18:46:24.494
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 18:46:24.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-170" for this suite. 09/04/23 18:46:24.546
------------------------------
â€¢ [SLOW TEST] [32.654 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:45:51.902
    Sep  4 18:45:51.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename dns 09/04/23 18:45:51.904
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:45:51.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:45:51.925
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 09/04/23 18:45:51.928
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-170 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-170;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-170 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-170;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-170.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-170.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-170.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-170.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-170.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-170.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-170.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-170.svc;check="$$(dig +notcp +noall +answer +search 234.22.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.22.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.22.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.22.234_tcp@PTR;sleep 1; done
     09/04/23 18:45:51.954
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-170 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-170;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-170 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-170;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-170.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-170.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-170.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-170.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-170.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-170.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-170.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-170.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-170.svc;check="$$(dig +notcp +noall +answer +search 234.22.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.22.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.22.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.22.234_tcp@PTR;sleep 1; done
     09/04/23 18:45:51.954
    STEP: creating a pod to probe DNS 09/04/23 18:45:51.955
    STEP: submitting the pod to kubernetes 09/04/23 18:45:51.955
    Sep  4 18:45:51.976: INFO: Waiting up to 15m0s for pod "dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739" in namespace "dns-170" to be "running"
    Sep  4 18:45:51.988: INFO: Pod "dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739": Phase="Pending", Reason="", readiness=false. Elapsed: 12.158893ms
    Sep  4 18:45:53.995: INFO: Pod "dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739": Phase="Running", Reason="", readiness=true. Elapsed: 2.019021206s
    Sep  4 18:45:53.995: INFO: Pod "dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 18:45:53.995
    STEP: looking for the results for each expected name from probers 09/04/23 18:45:54.006
    Sep  4 18:45:54.020: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:54.026: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:54.038: INFO: Unable to read wheezy_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:54.043: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:54.052: INFO: Unable to read wheezy_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:54.064: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:54.120: INFO: Unable to read jessie_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:54.125: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:54.137: INFO: Unable to read jessie_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:54.143: INFO: Unable to read jessie_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:54.148: INFO: Unable to read jessie_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:54.156: INFO: Unable to read jessie_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:54.197: INFO: Lookups using dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-170 wheezy_tcp@dns-test-service.dns-170 wheezy_udp@dns-test-service.dns-170.svc wheezy_tcp@dns-test-service.dns-170.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-170 jessie_tcp@dns-test-service.dns-170 jessie_udp@dns-test-service.dns-170.svc jessie_tcp@dns-test-service.dns-170.svc]

    Sep  4 18:45:59.209: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:59.218: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:59.228: INFO: Unable to read wheezy_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:59.237: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:59.241: INFO: Unable to read wheezy_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:59.252: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:59.304: INFO: Unable to read jessie_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:59.310: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:59.321: INFO: Unable to read jessie_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:59.328: INFO: Unable to read jessie_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:59.332: INFO: Unable to read jessie_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:59.337: INFO: Unable to read jessie_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:45:59.372: INFO: Lookups using dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-170 wheezy_tcp@dns-test-service.dns-170 wheezy_udp@dns-test-service.dns-170.svc wheezy_tcp@dns-test-service.dns-170.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-170 jessie_tcp@dns-test-service.dns-170 jessie_udp@dns-test-service.dns-170.svc jessie_tcp@dns-test-service.dns-170.svc]

    Sep  4 18:46:04.205: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:04.210: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:04.217: INFO: Unable to read wheezy_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:04.224: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:04.229: INFO: Unable to read wheezy_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:04.236: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:04.292: INFO: Unable to read jessie_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:04.298: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:04.305: INFO: Unable to read jessie_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:04.311: INFO: Unable to read jessie_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:04.319: INFO: Unable to read jessie_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:04.326: INFO: Unable to read jessie_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:04.375: INFO: Lookups using dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-170 wheezy_tcp@dns-test-service.dns-170 wheezy_udp@dns-test-service.dns-170.svc wheezy_tcp@dns-test-service.dns-170.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-170 jessie_tcp@dns-test-service.dns-170 jessie_udp@dns-test-service.dns-170.svc jessie_tcp@dns-test-service.dns-170.svc]

    Sep  4 18:46:09.208: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:09.213: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:09.219: INFO: Unable to read wheezy_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:09.224: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:09.228: INFO: Unable to read wheezy_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:09.236: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:09.276: INFO: Unable to read jessie_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:09.281: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:09.289: INFO: Unable to read jessie_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:09.293: INFO: Unable to read jessie_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:09.301: INFO: Unable to read jessie_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:09.309: INFO: Unable to read jessie_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:09.356: INFO: Lookups using dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-170 wheezy_tcp@dns-test-service.dns-170 wheezy_udp@dns-test-service.dns-170.svc wheezy_tcp@dns-test-service.dns-170.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-170 jessie_tcp@dns-test-service.dns-170 jessie_udp@dns-test-service.dns-170.svc jessie_tcp@dns-test-service.dns-170.svc]

    Sep  4 18:46:14.210: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:14.219: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:14.228: INFO: Unable to read wheezy_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:14.234: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:14.244: INFO: Unable to read wheezy_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:14.254: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:14.312: INFO: Unable to read jessie_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:14.322: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:14.328: INFO: Unable to read jessie_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:14.336: INFO: Unable to read jessie_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:14.344: INFO: Unable to read jessie_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:14.349: INFO: Unable to read jessie_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:14.383: INFO: Lookups using dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-170 wheezy_tcp@dns-test-service.dns-170 wheezy_udp@dns-test-service.dns-170.svc wheezy_tcp@dns-test-service.dns-170.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-170 jessie_tcp@dns-test-service.dns-170 jessie_udp@dns-test-service.dns-170.svc jessie_tcp@dns-test-service.dns-170.svc]

    Sep  4 18:46:19.207: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:19.220: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:19.229: INFO: Unable to read wheezy_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:19.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:19.245: INFO: Unable to read wheezy_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:19.254: INFO: Unable to read wheezy_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:19.300: INFO: Unable to read jessie_udp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:19.307: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:19.313: INFO: Unable to read jessie_udp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:19.323: INFO: Unable to read jessie_tcp@dns-test-service.dns-170 from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:19.330: INFO: Unable to read jessie_udp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:19.336: INFO: Unable to read jessie_tcp@dns-test-service.dns-170.svc from pod dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739: the server could not find the requested resource (get pods dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739)
    Sep  4 18:46:19.372: INFO: Lookups using dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-170 wheezy_tcp@dns-test-service.dns-170 wheezy_udp@dns-test-service.dns-170.svc wheezy_tcp@dns-test-service.dns-170.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-170 jessie_tcp@dns-test-service.dns-170 jessie_udp@dns-test-service.dns-170.svc jessie_tcp@dns-test-service.dns-170.svc]

    Sep  4 18:46:24.333: INFO: DNS probes using dns-170/dns-test-cf16e956-cfaa-4dfd-b9ed-32d1b7519739 succeeded

    STEP: deleting the pod 09/04/23 18:46:24.333
    STEP: deleting the test service 09/04/23 18:46:24.401
    STEP: deleting the test headless service 09/04/23 18:46:24.494
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:46:24.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-170" for this suite. 09/04/23 18:46:24.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:46:24.568
Sep  4 18:46:24.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 18:46:24.569
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:46:24.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:46:24.603
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-97525429-8dbe-40e3-add9-5e94832ec285 09/04/23 18:46:24.61
STEP: Creating a pod to test consume configMaps 09/04/23 18:46:24.617
Sep  4 18:46:24.628: INFO: Waiting up to 5m0s for pod "pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd" in namespace "configmap-3062" to be "Succeeded or Failed"
Sep  4 18:46:24.632: INFO: Pod "pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990191ms
Sep  4 18:46:26.640: INFO: Pod "pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012079023s
Sep  4 18:46:28.639: INFO: Pod "pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010755923s
STEP: Saw pod success 09/04/23 18:46:28.639
Sep  4 18:46:28.640: INFO: Pod "pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd" satisfied condition "Succeeded or Failed"
Sep  4 18:46:28.643: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd container agnhost-container: <nil>
STEP: delete the pod 09/04/23 18:46:28.65
Sep  4 18:46:28.674: INFO: Waiting for pod pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd to disappear
Sep  4 18:46:28.676: INFO: Pod pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 18:46:28.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3062" for this suite. 09/04/23 18:46:28.685
------------------------------
â€¢ [4.128 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:46:24.568
    Sep  4 18:46:24.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 18:46:24.569
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:46:24.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:46:24.603
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-97525429-8dbe-40e3-add9-5e94832ec285 09/04/23 18:46:24.61
    STEP: Creating a pod to test consume configMaps 09/04/23 18:46:24.617
    Sep  4 18:46:24.628: INFO: Waiting up to 5m0s for pod "pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd" in namespace "configmap-3062" to be "Succeeded or Failed"
    Sep  4 18:46:24.632: INFO: Pod "pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990191ms
    Sep  4 18:46:26.640: INFO: Pod "pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012079023s
    Sep  4 18:46:28.639: INFO: Pod "pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010755923s
    STEP: Saw pod success 09/04/23 18:46:28.639
    Sep  4 18:46:28.640: INFO: Pod "pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd" satisfied condition "Succeeded or Failed"
    Sep  4 18:46:28.643: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 18:46:28.65
    Sep  4 18:46:28.674: INFO: Waiting for pod pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd to disappear
    Sep  4 18:46:28.676: INFO: Pod pod-configmaps-f60944b6-271d-410f-ac1d-dee5b6f3a5cd no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:46:28.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3062" for this suite. 09/04/23 18:46:28.685
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:46:28.701
Sep  4 18:46:28.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename pods 09/04/23 18:46:28.703
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:46:28.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:46:28.733
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 09/04/23 18:46:28.736
STEP: setting up watch 09/04/23 18:46:28.736
STEP: submitting the pod to kubernetes 09/04/23 18:46:28.84
STEP: verifying the pod is in kubernetes 09/04/23 18:46:28.856
STEP: verifying pod creation was observed 09/04/23 18:46:28.872
Sep  4 18:46:28.872: INFO: Waiting up to 5m0s for pod "pod-submit-remove-4b6b2f71-590a-424d-a05c-422a029f1b94" in namespace "pods-5281" to be "running"
Sep  4 18:46:28.875: INFO: Pod "pod-submit-remove-4b6b2f71-590a-424d-a05c-422a029f1b94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.573356ms
Sep  4 18:46:30.882: INFO: Pod "pod-submit-remove-4b6b2f71-590a-424d-a05c-422a029f1b94": Phase="Running", Reason="", readiness=true. Elapsed: 2.009756463s
Sep  4 18:46:30.882: INFO: Pod "pod-submit-remove-4b6b2f71-590a-424d-a05c-422a029f1b94" satisfied condition "running"
STEP: deleting the pod gracefully 09/04/23 18:46:30.885
STEP: verifying pod deletion was observed 09/04/23 18:46:30.898
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 18:46:33.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5281" for this suite. 09/04/23 18:46:33.577
------------------------------
â€¢ [4.891 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:46:28.701
    Sep  4 18:46:28.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename pods 09/04/23 18:46:28.703
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:46:28.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:46:28.733
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 09/04/23 18:46:28.736
    STEP: setting up watch 09/04/23 18:46:28.736
    STEP: submitting the pod to kubernetes 09/04/23 18:46:28.84
    STEP: verifying the pod is in kubernetes 09/04/23 18:46:28.856
    STEP: verifying pod creation was observed 09/04/23 18:46:28.872
    Sep  4 18:46:28.872: INFO: Waiting up to 5m0s for pod "pod-submit-remove-4b6b2f71-590a-424d-a05c-422a029f1b94" in namespace "pods-5281" to be "running"
    Sep  4 18:46:28.875: INFO: Pod "pod-submit-remove-4b6b2f71-590a-424d-a05c-422a029f1b94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.573356ms
    Sep  4 18:46:30.882: INFO: Pod "pod-submit-remove-4b6b2f71-590a-424d-a05c-422a029f1b94": Phase="Running", Reason="", readiness=true. Elapsed: 2.009756463s
    Sep  4 18:46:30.882: INFO: Pod "pod-submit-remove-4b6b2f71-590a-424d-a05c-422a029f1b94" satisfied condition "running"
    STEP: deleting the pod gracefully 09/04/23 18:46:30.885
    STEP: verifying pod deletion was observed 09/04/23 18:46:30.898
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:46:33.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5281" for this suite. 09/04/23 18:46:33.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:46:33.605
Sep  4 18:46:33.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename resourcequota 09/04/23 18:46:33.606
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:46:33.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:46:33.634
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 09/04/23 18:46:33.636
STEP: Creating a ResourceQuota 09/04/23 18:46:38.639
STEP: Ensuring resource quota status is calculated 09/04/23 18:46:38.646
STEP: Creating a ReplicationController 09/04/23 18:46:40.652
STEP: Ensuring resource quota status captures replication controller creation 09/04/23 18:46:40.67
STEP: Deleting a ReplicationController 09/04/23 18:46:42.676
STEP: Ensuring resource quota status released usage 09/04/23 18:46:42.686
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 18:46:44.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1270" for this suite. 09/04/23 18:46:44.698
------------------------------
â€¢ [SLOW TEST] [11.101 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:46:33.605
    Sep  4 18:46:33.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename resourcequota 09/04/23 18:46:33.606
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:46:33.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:46:33.634
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 09/04/23 18:46:33.636
    STEP: Creating a ResourceQuota 09/04/23 18:46:38.639
    STEP: Ensuring resource quota status is calculated 09/04/23 18:46:38.646
    STEP: Creating a ReplicationController 09/04/23 18:46:40.652
    STEP: Ensuring resource quota status captures replication controller creation 09/04/23 18:46:40.67
    STEP: Deleting a ReplicationController 09/04/23 18:46:42.676
    STEP: Ensuring resource quota status released usage 09/04/23 18:46:42.686
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:46:44.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1270" for this suite. 09/04/23 18:46:44.698
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:46:44.706
Sep  4 18:46:44.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 18:46:44.707
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:46:44.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:46:44.737
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-543/configmap-test-9aec51cf-d855-4f6e-9c25-276fffe10d66 09/04/23 18:46:44.74
STEP: Creating a pod to test consume configMaps 09/04/23 18:46:44.746
Sep  4 18:46:44.757: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de" in namespace "configmap-543" to be "Succeeded or Failed"
Sep  4 18:46:44.762: INFO: Pod "pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.504219ms
Sep  4 18:46:46.766: INFO: Pod "pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008784072s
Sep  4 18:46:48.767: INFO: Pod "pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010251526s
STEP: Saw pod success 09/04/23 18:46:48.768
Sep  4 18:46:48.768: INFO: Pod "pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de" satisfied condition "Succeeded or Failed"
Sep  4 18:46:48.771: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de container env-test: <nil>
STEP: delete the pod 09/04/23 18:46:48.779
Sep  4 18:46:48.798: INFO: Waiting for pod pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de to disappear
Sep  4 18:46:48.801: INFO: Pod pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 18:46:48.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-543" for this suite. 09/04/23 18:46:48.806
------------------------------
â€¢ [4.109 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:46:44.706
    Sep  4 18:46:44.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 18:46:44.707
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:46:44.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:46:44.737
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-543/configmap-test-9aec51cf-d855-4f6e-9c25-276fffe10d66 09/04/23 18:46:44.74
    STEP: Creating a pod to test consume configMaps 09/04/23 18:46:44.746
    Sep  4 18:46:44.757: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de" in namespace "configmap-543" to be "Succeeded or Failed"
    Sep  4 18:46:44.762: INFO: Pod "pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.504219ms
    Sep  4 18:46:46.766: INFO: Pod "pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008784072s
    Sep  4 18:46:48.767: INFO: Pod "pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010251526s
    STEP: Saw pod success 09/04/23 18:46:48.768
    Sep  4 18:46:48.768: INFO: Pod "pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de" satisfied condition "Succeeded or Failed"
    Sep  4 18:46:48.771: INFO: Trying to get logs from node tenant-000001 pod pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de container env-test: <nil>
    STEP: delete the pod 09/04/23 18:46:48.779
    Sep  4 18:46:48.798: INFO: Waiting for pod pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de to disappear
    Sep  4 18:46:48.801: INFO: Pod pod-configmaps-2ce89c9d-16ca-47e8-9857-214b353350de no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:46:48.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-543" for this suite. 09/04/23 18:46:48.806
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:46:48.822
Sep  4 18:46:48.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 18:46:48.823
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:46:48.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:46:48.844
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-0d6ed2c4-cb68-4f31-af96-c6949c3b4694 09/04/23 18:46:48.855
STEP: Creating configMap with name cm-test-opt-upd-22acc1c2-e6af-4d12-9f45-3321e4aec8b8 09/04/23 18:46:48.86
STEP: Creating the pod 09/04/23 18:46:48.867
Sep  4 18:46:48.881: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-918ec8b8-5af6-497f-b52e-d6b40d686bdf" in namespace "projected-4628" to be "running and ready"
Sep  4 18:46:48.884: INFO: Pod "pod-projected-configmaps-918ec8b8-5af6-497f-b52e-d6b40d686bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.450773ms
Sep  4 18:46:48.885: INFO: The phase of Pod pod-projected-configmaps-918ec8b8-5af6-497f-b52e-d6b40d686bdf is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:46:50.889: INFO: Pod "pod-projected-configmaps-918ec8b8-5af6-497f-b52e-d6b40d686bdf": Phase="Running", Reason="", readiness=true. Elapsed: 2.007274565s
Sep  4 18:46:50.890: INFO: The phase of Pod pod-projected-configmaps-918ec8b8-5af6-497f-b52e-d6b40d686bdf is Running (Ready = true)
Sep  4 18:46:50.890: INFO: Pod "pod-projected-configmaps-918ec8b8-5af6-497f-b52e-d6b40d686bdf" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-0d6ed2c4-cb68-4f31-af96-c6949c3b4694 09/04/23 18:46:50.919
STEP: Updating configmap cm-test-opt-upd-22acc1c2-e6af-4d12-9f45-3321e4aec8b8 09/04/23 18:46:50.926
STEP: Creating configMap with name cm-test-opt-create-8d09928a-17c5-42af-9253-48c05bf6402a 09/04/23 18:46:50.933
STEP: waiting to observe update in volume 09/04/23 18:46:50.937
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 18:46:52.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4628" for this suite. 09/04/23 18:46:52.981
------------------------------
â€¢ [4.172 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:46:48.822
    Sep  4 18:46:48.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 18:46:48.823
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:46:48.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:46:48.844
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-0d6ed2c4-cb68-4f31-af96-c6949c3b4694 09/04/23 18:46:48.855
    STEP: Creating configMap with name cm-test-opt-upd-22acc1c2-e6af-4d12-9f45-3321e4aec8b8 09/04/23 18:46:48.86
    STEP: Creating the pod 09/04/23 18:46:48.867
    Sep  4 18:46:48.881: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-918ec8b8-5af6-497f-b52e-d6b40d686bdf" in namespace "projected-4628" to be "running and ready"
    Sep  4 18:46:48.884: INFO: Pod "pod-projected-configmaps-918ec8b8-5af6-497f-b52e-d6b40d686bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.450773ms
    Sep  4 18:46:48.885: INFO: The phase of Pod pod-projected-configmaps-918ec8b8-5af6-497f-b52e-d6b40d686bdf is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:46:50.889: INFO: Pod "pod-projected-configmaps-918ec8b8-5af6-497f-b52e-d6b40d686bdf": Phase="Running", Reason="", readiness=true. Elapsed: 2.007274565s
    Sep  4 18:46:50.890: INFO: The phase of Pod pod-projected-configmaps-918ec8b8-5af6-497f-b52e-d6b40d686bdf is Running (Ready = true)
    Sep  4 18:46:50.890: INFO: Pod "pod-projected-configmaps-918ec8b8-5af6-497f-b52e-d6b40d686bdf" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-0d6ed2c4-cb68-4f31-af96-c6949c3b4694 09/04/23 18:46:50.919
    STEP: Updating configmap cm-test-opt-upd-22acc1c2-e6af-4d12-9f45-3321e4aec8b8 09/04/23 18:46:50.926
    STEP: Creating configMap with name cm-test-opt-create-8d09928a-17c5-42af-9253-48c05bf6402a 09/04/23 18:46:50.933
    STEP: waiting to observe update in volume 09/04/23 18:46:50.937
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:46:52.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4628" for this suite. 09/04/23 18:46:52.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:46:52.997
Sep  4 18:46:52.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-probe 09/04/23 18:46:52.999
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:46:53.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:46:53.019
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 18:47:53.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5104" for this suite. 09/04/23 18:47:53.041
------------------------------
â€¢ [SLOW TEST] [60.069 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:46:52.997
    Sep  4 18:46:52.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-probe 09/04/23 18:46:52.999
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:46:53.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:46:53.019
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:47:53.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5104" for this suite. 09/04/23 18:47:53.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:47:53.072
Sep  4 18:47:53.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename namespaces 09/04/23 18:47:53.073
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:47:53.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:47:53.096
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-9qshn" 09/04/23 18:47:53.098
Sep  4 18:47:53.119: INFO: Namespace "e2e-ns-9qshn-402" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-9qshn-402" 09/04/23 18:47:53.12
Sep  4 18:47:53.137: INFO: Namespace "e2e-ns-9qshn-402" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-9qshn-402" 09/04/23 18:47:53.137
Sep  4 18:47:53.150: INFO: Namespace "e2e-ns-9qshn-402" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:47:53.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4666" for this suite. 09/04/23 18:47:53.159
STEP: Destroying namespace "e2e-ns-9qshn-402" for this suite. 09/04/23 18:47:53.167
------------------------------
â€¢ [0.106 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:47:53.072
    Sep  4 18:47:53.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename namespaces 09/04/23 18:47:53.073
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:47:53.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:47:53.096
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-9qshn" 09/04/23 18:47:53.098
    Sep  4 18:47:53.119: INFO: Namespace "e2e-ns-9qshn-402" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-9qshn-402" 09/04/23 18:47:53.12
    Sep  4 18:47:53.137: INFO: Namespace "e2e-ns-9qshn-402" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-9qshn-402" 09/04/23 18:47:53.137
    Sep  4 18:47:53.150: INFO: Namespace "e2e-ns-9qshn-402" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:47:53.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4666" for this suite. 09/04/23 18:47:53.159
    STEP: Destroying namespace "e2e-ns-9qshn-402" for this suite. 09/04/23 18:47:53.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:47:53.189
Sep  4 18:47:53.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 18:47:53.19
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:47:53.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:47:53.212
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 09/04/23 18:47:53.215
Sep  4 18:47:53.215: INFO: Creating e2e-svc-a-8grcz
Sep  4 18:47:53.230: INFO: Creating e2e-svc-b-mvtjg
Sep  4 18:47:53.246: INFO: Creating e2e-svc-c-p6z4x
STEP: deleting service collection 09/04/23 18:47:53.272
Sep  4 18:47:53.320: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 18:47:53.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3677" for this suite. 09/04/23 18:47:53.325
------------------------------
â€¢ [0.150 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:47:53.189
    Sep  4 18:47:53.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 18:47:53.19
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:47:53.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:47:53.212
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 09/04/23 18:47:53.215
    Sep  4 18:47:53.215: INFO: Creating e2e-svc-a-8grcz
    Sep  4 18:47:53.230: INFO: Creating e2e-svc-b-mvtjg
    Sep  4 18:47:53.246: INFO: Creating e2e-svc-c-p6z4x
    STEP: deleting service collection 09/04/23 18:47:53.272
    Sep  4 18:47:53.320: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:47:53.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3677" for this suite. 09/04/23 18:47:53.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:47:53.349
Sep  4 18:47:53.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 18:47:53.351
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:47:53.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:47:53.373
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-b0161667-92b0-4f33-ad29-32bf0be41aaf 09/04/23 18:47:53.375
STEP: Creating secret with name secret-projected-all-test-volume-da366597-fe75-4254-957a-7e445863f2fd 09/04/23 18:47:53.384
STEP: Creating a pod to test Check all projections for projected volume plugin 09/04/23 18:47:53.389
Sep  4 18:47:53.402: INFO: Waiting up to 5m0s for pod "projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9" in namespace "projected-7000" to be "Succeeded or Failed"
Sep  4 18:47:53.408: INFO: Pod "projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.14186ms
Sep  4 18:47:55.414: INFO: Pod "projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011676507s
Sep  4 18:47:57.414: INFO: Pod "projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012027168s
STEP: Saw pod success 09/04/23 18:47:57.415
Sep  4 18:47:57.415: INFO: Pod "projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9" satisfied condition "Succeeded or Failed"
Sep  4 18:47:57.419: INFO: Trying to get logs from node tenant-000001 pod projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9 container projected-all-volume-test: <nil>
STEP: delete the pod 09/04/23 18:47:57.43
Sep  4 18:47:57.456: INFO: Waiting for pod projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9 to disappear
Sep  4 18:47:57.458: INFO: Pod projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Sep  4 18:47:57.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7000" for this suite. 09/04/23 18:47:57.462
------------------------------
â€¢ [4.121 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:47:53.349
    Sep  4 18:47:53.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 18:47:53.351
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:47:53.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:47:53.373
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-b0161667-92b0-4f33-ad29-32bf0be41aaf 09/04/23 18:47:53.375
    STEP: Creating secret with name secret-projected-all-test-volume-da366597-fe75-4254-957a-7e445863f2fd 09/04/23 18:47:53.384
    STEP: Creating a pod to test Check all projections for projected volume plugin 09/04/23 18:47:53.389
    Sep  4 18:47:53.402: INFO: Waiting up to 5m0s for pod "projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9" in namespace "projected-7000" to be "Succeeded or Failed"
    Sep  4 18:47:53.408: INFO: Pod "projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.14186ms
    Sep  4 18:47:55.414: INFO: Pod "projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011676507s
    Sep  4 18:47:57.414: INFO: Pod "projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012027168s
    STEP: Saw pod success 09/04/23 18:47:57.415
    Sep  4 18:47:57.415: INFO: Pod "projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9" satisfied condition "Succeeded or Failed"
    Sep  4 18:47:57.419: INFO: Trying to get logs from node tenant-000001 pod projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9 container projected-all-volume-test: <nil>
    STEP: delete the pod 09/04/23 18:47:57.43
    Sep  4 18:47:57.456: INFO: Waiting for pod projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9 to disappear
    Sep  4 18:47:57.458: INFO: Pod projected-volume-0eb7cd6b-849b-41dd-ad1b-dd8c68f0f3d9 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:47:57.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7000" for this suite. 09/04/23 18:47:57.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:47:57.477
Sep  4 18:47:57.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename security-context-test 09/04/23 18:47:57.478
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:47:57.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:47:57.506
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Sep  4 18:47:57.521: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5b5ec363-c5ea-4eb0-ae43-0394fdba309a" in namespace "security-context-test-3349" to be "Succeeded or Failed"
Sep  4 18:47:57.524: INFO: Pod "busybox-privileged-false-5b5ec363-c5ea-4eb0-ae43-0394fdba309a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.504154ms
Sep  4 18:47:59.531: INFO: Pod "busybox-privileged-false-5b5ec363-c5ea-4eb0-ae43-0394fdba309a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010270908s
Sep  4 18:48:01.529: INFO: Pod "busybox-privileged-false-5b5ec363-c5ea-4eb0-ae43-0394fdba309a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007887798s
Sep  4 18:48:01.529: INFO: Pod "busybox-privileged-false-5b5ec363-c5ea-4eb0-ae43-0394fdba309a" satisfied condition "Succeeded or Failed"
Sep  4 18:48:01.536: INFO: Got logs for pod "busybox-privileged-false-5b5ec363-c5ea-4eb0-ae43-0394fdba309a": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  4 18:48:01.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3349" for this suite. 09/04/23 18:48:01.541
------------------------------
â€¢ [4.073 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:47:57.477
    Sep  4 18:47:57.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename security-context-test 09/04/23 18:47:57.478
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:47:57.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:47:57.506
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Sep  4 18:47:57.521: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5b5ec363-c5ea-4eb0-ae43-0394fdba309a" in namespace "security-context-test-3349" to be "Succeeded or Failed"
    Sep  4 18:47:57.524: INFO: Pod "busybox-privileged-false-5b5ec363-c5ea-4eb0-ae43-0394fdba309a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.504154ms
    Sep  4 18:47:59.531: INFO: Pod "busybox-privileged-false-5b5ec363-c5ea-4eb0-ae43-0394fdba309a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010270908s
    Sep  4 18:48:01.529: INFO: Pod "busybox-privileged-false-5b5ec363-c5ea-4eb0-ae43-0394fdba309a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007887798s
    Sep  4 18:48:01.529: INFO: Pod "busybox-privileged-false-5b5ec363-c5ea-4eb0-ae43-0394fdba309a" satisfied condition "Succeeded or Failed"
    Sep  4 18:48:01.536: INFO: Got logs for pod "busybox-privileged-false-5b5ec363-c5ea-4eb0-ae43-0394fdba309a": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:48:01.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3349" for this suite. 09/04/23 18:48:01.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:48:01.565
Sep  4 18:48:01.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-probe 09/04/23 18:48:01.566
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:48:01.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:48:01.587
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 in namespace container-probe-6724 09/04/23 18:48:01.591
Sep  4 18:48:01.604: INFO: Waiting up to 5m0s for pod "liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34" in namespace "container-probe-6724" to be "not pending"
Sep  4 18:48:01.620: INFO: Pod "liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34": Phase="Pending", Reason="", readiness=false. Elapsed: 16.364486ms
Sep  4 18:48:03.625: INFO: Pod "liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34": Phase="Running", Reason="", readiness=true. Elapsed: 2.020854228s
Sep  4 18:48:03.625: INFO: Pod "liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34" satisfied condition "not pending"
Sep  4 18:48:03.625: INFO: Started pod liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 in namespace container-probe-6724
STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 18:48:03.625
Sep  4 18:48:03.632: INFO: Initial restart count of pod liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 is 0
Sep  4 18:48:23.694: INFO: Restart count of pod container-probe-6724/liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 is now 1 (20.062064984s elapsed)
Sep  4 18:48:43.746: INFO: Restart count of pod container-probe-6724/liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 is now 2 (40.11370132s elapsed)
Sep  4 18:49:03.802: INFO: Restart count of pod container-probe-6724/liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 is now 3 (1m0.170054156s elapsed)
Sep  4 18:49:23.860: INFO: Restart count of pod container-probe-6724/liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 is now 4 (1m20.227743419s elapsed)
Sep  4 18:50:24.012: INFO: Restart count of pod container-probe-6724/liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 is now 5 (2m20.380209653s elapsed)
STEP: deleting the pod 09/04/23 18:50:24.012
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 18:50:24.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6724" for this suite. 09/04/23 18:50:24.082
------------------------------
â€¢ [SLOW TEST] [142.549 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:48:01.565
    Sep  4 18:48:01.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-probe 09/04/23 18:48:01.566
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:48:01.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:48:01.587
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 in namespace container-probe-6724 09/04/23 18:48:01.591
    Sep  4 18:48:01.604: INFO: Waiting up to 5m0s for pod "liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34" in namespace "container-probe-6724" to be "not pending"
    Sep  4 18:48:01.620: INFO: Pod "liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34": Phase="Pending", Reason="", readiness=false. Elapsed: 16.364486ms
    Sep  4 18:48:03.625: INFO: Pod "liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34": Phase="Running", Reason="", readiness=true. Elapsed: 2.020854228s
    Sep  4 18:48:03.625: INFO: Pod "liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34" satisfied condition "not pending"
    Sep  4 18:48:03.625: INFO: Started pod liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 in namespace container-probe-6724
    STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 18:48:03.625
    Sep  4 18:48:03.632: INFO: Initial restart count of pod liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 is 0
    Sep  4 18:48:23.694: INFO: Restart count of pod container-probe-6724/liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 is now 1 (20.062064984s elapsed)
    Sep  4 18:48:43.746: INFO: Restart count of pod container-probe-6724/liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 is now 2 (40.11370132s elapsed)
    Sep  4 18:49:03.802: INFO: Restart count of pod container-probe-6724/liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 is now 3 (1m0.170054156s elapsed)
    Sep  4 18:49:23.860: INFO: Restart count of pod container-probe-6724/liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 is now 4 (1m20.227743419s elapsed)
    Sep  4 18:50:24.012: INFO: Restart count of pod container-probe-6724/liveness-67cfb2a4-aff1-4aad-a8bc-e7f1c285af34 is now 5 (2m20.380209653s elapsed)
    STEP: deleting the pod 09/04/23 18:50:24.012
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:50:24.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6724" for this suite. 09/04/23 18:50:24.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:50:24.114
Sep  4 18:50:24.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 18:50:24.115
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:50:24.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:50:24.134
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-448 09/04/23 18:50:24.137
STEP: creating service affinity-nodeport in namespace services-448 09/04/23 18:50:24.137
STEP: creating replication controller affinity-nodeport in namespace services-448 09/04/23 18:50:24.167
I0904 18:50:24.177772      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-448, replica count: 3
I0904 18:50:27.234258      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 18:50:27.245: INFO: Creating new exec pod
Sep  4 18:50:27.255: INFO: Waiting up to 5m0s for pod "execpod-affinityx5n8s" in namespace "services-448" to be "running"
Sep  4 18:50:27.271: INFO: Pod "execpod-affinityx5n8s": Phase="Pending", Reason="", readiness=false. Elapsed: 16.720173ms
Sep  4 18:50:29.275: INFO: Pod "execpod-affinityx5n8s": Phase="Running", Reason="", readiness=true. Elapsed: 2.020408364s
Sep  4 18:50:29.275: INFO: Pod "execpod-affinityx5n8s" satisfied condition "running"
Sep  4 18:50:30.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-448 exec execpod-affinityx5n8s -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Sep  4 18:50:30.485: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep  4 18:50:30.485: INFO: stdout: ""
Sep  4 18:50:30.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-448 exec execpod-affinityx5n8s -- /bin/sh -x -c nc -v -z -w 2 10.96.37.188 80'
Sep  4 18:50:30.666: INFO: stderr: "+ nc -v -z -w 2 10.96.37.188 80\nConnection to 10.96.37.188 80 port [tcp/http] succeeded!\n"
Sep  4 18:50:30.666: INFO: stdout: ""
Sep  4 18:50:30.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-448 exec execpod-affinityx5n8s -- /bin/sh -x -c nc -v -z -w 2 10.225.0.5 30733'
Sep  4 18:50:30.875: INFO: stderr: "+ nc -v -z -w 2 10.225.0.5 30733\nConnection to 10.225.0.5 30733 port [tcp/*] succeeded!\n"
Sep  4 18:50:30.875: INFO: stdout: ""
Sep  4 18:50:30.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-448 exec execpod-affinityx5n8s -- /bin/sh -x -c nc -v -z -w 2 10.225.0.7 30733'
Sep  4 18:50:31.053: INFO: stderr: "+ nc -v -z -w 2 10.225.0.7 30733\nConnection to 10.225.0.7 30733 port [tcp/*] succeeded!\n"
Sep  4 18:50:31.053: INFO: stdout: ""
Sep  4 18:50:31.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-448 exec execpod-affinityx5n8s -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.225.0.5:30733/ ; done'
Sep  4 18:50:31.356: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n"
Sep  4 18:50:31.356: INFO: stdout: "\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p"
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
Sep  4 18:50:31.356: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-448, will wait for the garbage collector to delete the pods 09/04/23 18:50:31.402
Sep  4 18:50:31.477: INFO: Deleting ReplicationController affinity-nodeport took: 8.989262ms
Sep  4 18:50:31.610: INFO: Terminating ReplicationController affinity-nodeport pods took: 132.179265ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 18:50:34.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-448" for this suite. 09/04/23 18:50:34.36
------------------------------
â€¢ [SLOW TEST] [10.255 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:50:24.114
    Sep  4 18:50:24.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 18:50:24.115
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:50:24.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:50:24.134
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-448 09/04/23 18:50:24.137
    STEP: creating service affinity-nodeport in namespace services-448 09/04/23 18:50:24.137
    STEP: creating replication controller affinity-nodeport in namespace services-448 09/04/23 18:50:24.167
    I0904 18:50:24.177772      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-448, replica count: 3
    I0904 18:50:27.234258      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 18:50:27.245: INFO: Creating new exec pod
    Sep  4 18:50:27.255: INFO: Waiting up to 5m0s for pod "execpod-affinityx5n8s" in namespace "services-448" to be "running"
    Sep  4 18:50:27.271: INFO: Pod "execpod-affinityx5n8s": Phase="Pending", Reason="", readiness=false. Elapsed: 16.720173ms
    Sep  4 18:50:29.275: INFO: Pod "execpod-affinityx5n8s": Phase="Running", Reason="", readiness=true. Elapsed: 2.020408364s
    Sep  4 18:50:29.275: INFO: Pod "execpod-affinityx5n8s" satisfied condition "running"
    Sep  4 18:50:30.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-448 exec execpod-affinityx5n8s -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Sep  4 18:50:30.485: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Sep  4 18:50:30.485: INFO: stdout: ""
    Sep  4 18:50:30.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-448 exec execpod-affinityx5n8s -- /bin/sh -x -c nc -v -z -w 2 10.96.37.188 80'
    Sep  4 18:50:30.666: INFO: stderr: "+ nc -v -z -w 2 10.96.37.188 80\nConnection to 10.96.37.188 80 port [tcp/http] succeeded!\n"
    Sep  4 18:50:30.666: INFO: stdout: ""
    Sep  4 18:50:30.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-448 exec execpod-affinityx5n8s -- /bin/sh -x -c nc -v -z -w 2 10.225.0.5 30733'
    Sep  4 18:50:30.875: INFO: stderr: "+ nc -v -z -w 2 10.225.0.5 30733\nConnection to 10.225.0.5 30733 port [tcp/*] succeeded!\n"
    Sep  4 18:50:30.875: INFO: stdout: ""
    Sep  4 18:50:30.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-448 exec execpod-affinityx5n8s -- /bin/sh -x -c nc -v -z -w 2 10.225.0.7 30733'
    Sep  4 18:50:31.053: INFO: stderr: "+ nc -v -z -w 2 10.225.0.7 30733\nConnection to 10.225.0.7 30733 port [tcp/*] succeeded!\n"
    Sep  4 18:50:31.053: INFO: stdout: ""
    Sep  4 18:50:31.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-448 exec execpod-affinityx5n8s -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.225.0.5:30733/ ; done'
    Sep  4 18:50:31.356: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.225.0.5:30733/\n"
    Sep  4 18:50:31.356: INFO: stdout: "\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p\naffinity-nodeport-sw76p"
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Received response from host: affinity-nodeport-sw76p
    Sep  4 18:50:31.356: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-448, will wait for the garbage collector to delete the pods 09/04/23 18:50:31.402
    Sep  4 18:50:31.477: INFO: Deleting ReplicationController affinity-nodeport took: 8.989262ms
    Sep  4 18:50:31.610: INFO: Terminating ReplicationController affinity-nodeport pods took: 132.179265ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:50:34.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-448" for this suite. 09/04/23 18:50:34.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:50:34.375
Sep  4 18:50:34.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename crd-webhook 09/04/23 18:50:34.377
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:50:34.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:50:34.402
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/04/23 18:50:34.405
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/04/23 18:50:34.886
STEP: Deploying the custom resource conversion webhook pod 09/04/23 18:50:34.906
STEP: Wait for the deployment to be ready 09/04/23 18:50:34.926
Sep  4 18:50:34.946: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 18:50:36.957
STEP: Verifying the service has paired with the endpoint 09/04/23 18:50:36.972
Sep  4 18:50:37.973: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Sep  4 18:50:37.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Creating a v1 custom resource 09/04/23 18:50:40.586
STEP: Create a v2 custom resource 09/04/23 18:50:40.625
STEP: List CRs in v1 09/04/23 18:50:40.686
STEP: List CRs in v2 09/04/23 18:50:40.71
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:50:41.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-3168" for this suite. 09/04/23 18:50:41.295
------------------------------
â€¢ [SLOW TEST] [6.938 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:50:34.375
    Sep  4 18:50:34.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename crd-webhook 09/04/23 18:50:34.377
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:50:34.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:50:34.402
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/04/23 18:50:34.405
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/04/23 18:50:34.886
    STEP: Deploying the custom resource conversion webhook pod 09/04/23 18:50:34.906
    STEP: Wait for the deployment to be ready 09/04/23 18:50:34.926
    Sep  4 18:50:34.946: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 18:50:36.957
    STEP: Verifying the service has paired with the endpoint 09/04/23 18:50:36.972
    Sep  4 18:50:37.973: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Sep  4 18:50:37.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Creating a v1 custom resource 09/04/23 18:50:40.586
    STEP: Create a v2 custom resource 09/04/23 18:50:40.625
    STEP: List CRs in v1 09/04/23 18:50:40.686
    STEP: List CRs in v2 09/04/23 18:50:40.71
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:50:41.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-3168" for this suite. 09/04/23 18:50:41.295
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:50:41.35
Sep  4 18:50:41.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 18:50:41.351
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:50:41.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:50:41.405
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 09/04/23 18:50:41.412
Sep  4 18:50:41.421: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15" in namespace "projected-9341" to be "Succeeded or Failed"
Sep  4 18:50:41.424: INFO: Pod "downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144352ms
Sep  4 18:50:43.430: INFO: Pod "downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008665835s
Sep  4 18:50:45.431: INFO: Pod "downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009189429s
STEP: Saw pod success 09/04/23 18:50:45.431
Sep  4 18:50:45.431: INFO: Pod "downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15" satisfied condition "Succeeded or Failed"
Sep  4 18:50:45.437: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15 container client-container: <nil>
STEP: delete the pod 09/04/23 18:50:45.468
Sep  4 18:50:45.488: INFO: Waiting for pod downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15 to disappear
Sep  4 18:50:45.491: INFO: Pod downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 18:50:45.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9341" for this suite. 09/04/23 18:50:45.499
------------------------------
â€¢ [4.193 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:50:41.35
    Sep  4 18:50:41.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 18:50:41.351
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:50:41.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:50:41.405
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 09/04/23 18:50:41.412
    Sep  4 18:50:41.421: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15" in namespace "projected-9341" to be "Succeeded or Failed"
    Sep  4 18:50:41.424: INFO: Pod "downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144352ms
    Sep  4 18:50:43.430: INFO: Pod "downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008665835s
    Sep  4 18:50:45.431: INFO: Pod "downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009189429s
    STEP: Saw pod success 09/04/23 18:50:45.431
    Sep  4 18:50:45.431: INFO: Pod "downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15" satisfied condition "Succeeded or Failed"
    Sep  4 18:50:45.437: INFO: Trying to get logs from node tenant-000001 pod downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15 container client-container: <nil>
    STEP: delete the pod 09/04/23 18:50:45.468
    Sep  4 18:50:45.488: INFO: Waiting for pod downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15 to disappear
    Sep  4 18:50:45.491: INFO: Pod downwardapi-volume-94500194-4370-4788-bd53-255cb7449c15 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:50:45.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9341" for this suite. 09/04/23 18:50:45.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:50:45.518
Sep  4 18:50:45.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename proxy 09/04/23 18:50:45.519
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:50:45.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:50:45.541
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 09/04/23 18:50:45.571
STEP: creating replication controller proxy-service-x8f9r in namespace proxy-402 09/04/23 18:50:45.574
I0904 18:50:45.594438      19 runners.go:193] Created replication controller with name: proxy-service-x8f9r, namespace: proxy-402, replica count: 1
I0904 18:50:46.649694      19 runners.go:193] proxy-service-x8f9r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0904 18:50:47.650434      19 runners.go:193] proxy-service-x8f9r Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 18:50:47.654: INFO: setup took 2.107722418s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/04/23 18:50:47.654
Sep  4 18:50:47.670: INFO: (0) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 15.414141ms)
Sep  4 18:50:47.675: INFO: (0) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 20.42118ms)
Sep  4 18:50:47.677: INFO: (0) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 21.32134ms)
Sep  4 18:50:47.688: INFO: (0) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 32.402889ms)
Sep  4 18:50:47.688: INFO: (0) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 33.067934ms)
Sep  4 18:50:47.694: INFO: (0) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 39.547373ms)
Sep  4 18:50:47.694: INFO: (0) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 40.087708ms)
Sep  4 18:50:47.697: INFO: (0) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 42.063042ms)
Sep  4 18:50:47.698: INFO: (0) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 42.974203ms)
Sep  4 18:50:47.699: INFO: (0) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 44.066077ms)
Sep  4 18:50:47.699: INFO: (0) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 43.984972ms)
Sep  4 18:50:47.699: INFO: (0) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 44.153383ms)
Sep  4 18:50:47.699: INFO: (0) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 44.940537ms)
Sep  4 18:50:47.700: INFO: (0) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 44.69142ms)
Sep  4 18:50:47.700: INFO: (0) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 44.318794ms)
Sep  4 18:50:47.703: INFO: (0) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 48.10925ms)
Sep  4 18:50:47.710: INFO: (1) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 6.117113ms)
Sep  4 18:50:47.714: INFO: (1) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 10.183688ms)
Sep  4 18:50:47.714: INFO: (1) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 10.94904ms)
Sep  4 18:50:47.716: INFO: (1) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 12.417139ms)
Sep  4 18:50:47.716: INFO: (1) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 12.471442ms)
Sep  4 18:50:47.718: INFO: (1) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 13.940942ms)
Sep  4 18:50:47.718: INFO: (1) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 13.924441ms)
Sep  4 18:50:47.718: INFO: (1) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 13.90414ms)
Sep  4 18:50:47.718: INFO: (1) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 14.375671ms)
Sep  4 18:50:47.721: INFO: (1) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 17.277768ms)
Sep  4 18:50:47.721: INFO: (1) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 17.186962ms)
Sep  4 18:50:47.722: INFO: (1) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 17.587988ms)
Sep  4 18:50:47.722: INFO: (1) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 17.903509ms)
Sep  4 18:50:47.722: INFO: (1) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 17.723798ms)
Sep  4 18:50:47.722: INFO: (1) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 17.940412ms)
Sep  4 18:50:47.727: INFO: (1) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 23.428583ms)
Sep  4 18:50:47.735: INFO: (2) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 6.939069ms)
Sep  4 18:50:47.735: INFO: (2) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 6.780058ms)
Sep  4 18:50:47.737: INFO: (2) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 8.658085ms)
Sep  4 18:50:47.740: INFO: (2) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 12.267229ms)
Sep  4 18:50:47.741: INFO: (2) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 12.621653ms)
Sep  4 18:50:47.741: INFO: (2) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 12.309032ms)
Sep  4 18:50:47.741: INFO: (2) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 12.451641ms)
Sep  4 18:50:47.741: INFO: (2) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 12.830267ms)
Sep  4 18:50:47.741: INFO: (2) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 12.597451ms)
Sep  4 18:50:47.744: INFO: (2) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 15.23203ms)
Sep  4 18:50:47.744: INFO: (2) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 15.486446ms)
Sep  4 18:50:47.746: INFO: (2) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 18.115324ms)
Sep  4 18:50:47.747: INFO: (2) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 17.867607ms)
Sep  4 18:50:47.747: INFO: (2) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 18.416844ms)
Sep  4 18:50:47.747: INFO: (2) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 18.105123ms)
Sep  4 18:50:47.748: INFO: (2) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 19.720632ms)
Sep  4 18:50:47.754: INFO: (3) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 6.203419ms)
Sep  4 18:50:47.755: INFO: (3) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 5.77329ms)
Sep  4 18:50:47.758: INFO: (3) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 9.321429ms)
Sep  4 18:50:47.759: INFO: (3) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 10.06808ms)
Sep  4 18:50:47.759: INFO: (3) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 10.440106ms)
Sep  4 18:50:47.759: INFO: (3) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 10.143585ms)
Sep  4 18:50:47.763: INFO: (3) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 13.949943ms)
Sep  4 18:50:47.763: INFO: (3) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 13.714326ms)
Sep  4 18:50:47.763: INFO: (3) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 14.022347ms)
Sep  4 18:50:47.764: INFO: (3) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 15.360638ms)
Sep  4 18:50:47.764: INFO: (3) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 15.230329ms)
Sep  4 18:50:47.765: INFO: (3) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 15.885274ms)
Sep  4 18:50:47.765: INFO: (3) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 16.623423ms)
Sep  4 18:50:47.765: INFO: (3) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 16.734731ms)
Sep  4 18:50:47.765: INFO: (3) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 16.180093ms)
Sep  4 18:50:47.766: INFO: (3) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 17.336272ms)
Sep  4 18:50:47.775: INFO: (4) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 7.953637ms)
Sep  4 18:50:47.775: INFO: (4) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 8.228356ms)
Sep  4 18:50:47.776: INFO: (4) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 8.229156ms)
Sep  4 18:50:47.781: INFO: (4) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 13.597919ms)
Sep  4 18:50:47.784: INFO: (4) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 17.034951ms)
Sep  4 18:50:47.786: INFO: (4) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 18.811171ms)
Sep  4 18:50:47.786: INFO: (4) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 19.585723ms)
Sep  4 18:50:47.787: INFO: (4) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 19.482516ms)
Sep  4 18:50:47.787: INFO: (4) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 20.223566ms)
Sep  4 18:50:47.788: INFO: (4) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 21.120727ms)
Sep  4 18:50:47.789: INFO: (4) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 21.174331ms)
Sep  4 18:50:47.789: INFO: (4) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 21.750269ms)
Sep  4 18:50:47.789: INFO: (4) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 21.351943ms)
Sep  4 18:50:47.789: INFO: (4) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 22.304907ms)
Sep  4 18:50:47.790: INFO: (4) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 23.284573ms)
Sep  4 18:50:47.791: INFO: (4) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 24.298741ms)
Sep  4 18:50:47.797: INFO: (5) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 5.966403ms)
Sep  4 18:50:47.802: INFO: (5) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 10.327797ms)
Sep  4 18:50:47.802: INFO: (5) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 10.145286ms)
Sep  4 18:50:47.802: INFO: (5) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 9.056412ms)
Sep  4 18:50:47.804: INFO: (5) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 12.115118ms)
Sep  4 18:50:47.804: INFO: (5) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 12.033714ms)
Sep  4 18:50:47.804: INFO: (5) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 12.762062ms)
Sep  4 18:50:47.805: INFO: (5) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 12.671156ms)
Sep  4 18:50:47.806: INFO: (5) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 14.216161ms)
Sep  4 18:50:47.807: INFO: (5) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 15.795367ms)
Sep  4 18:50:47.816: INFO: (5) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 24.626364ms)
Sep  4 18:50:47.817: INFO: (5) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 24.725371ms)
Sep  4 18:50:47.817: INFO: (5) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 25.177901ms)
Sep  4 18:50:47.817: INFO: (5) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 25.712137ms)
Sep  4 18:50:47.817: INFO: (5) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 25.136098ms)
Sep  4 18:50:47.817: INFO: (5) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 25.520324ms)
Sep  4 18:50:47.827: INFO: (6) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 9.051311ms)
Sep  4 18:50:47.831: INFO: (6) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 13.307099ms)
Sep  4 18:50:47.832: INFO: (6) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 13.216893ms)
Sep  4 18:50:47.832: INFO: (6) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 13.892838ms)
Sep  4 18:50:47.836: INFO: (6) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 18.114624ms)
Sep  4 18:50:47.838: INFO: (6) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 20.292771ms)
Sep  4 18:50:47.838: INFO: (6) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 19.731033ms)
Sep  4 18:50:47.838: INFO: (6) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 20.452482ms)
Sep  4 18:50:47.838: INFO: (6) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 20.12156ms)
Sep  4 18:50:47.839: INFO: (6) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 20.768603ms)
Sep  4 18:50:47.839: INFO: (6) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 21.866378ms)
Sep  4 18:50:47.840: INFO: (6) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 21.118627ms)
Sep  4 18:50:47.840: INFO: (6) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 21.729569ms)
Sep  4 18:50:47.840: INFO: (6) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 21.519554ms)
Sep  4 18:50:47.840: INFO: (6) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 21.685565ms)
Sep  4 18:50:47.840: INFO: (6) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 22.563724ms)
Sep  4 18:50:47.848: INFO: (7) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 7.699121ms)
Sep  4 18:50:47.849: INFO: (7) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 8.111148ms)
Sep  4 18:50:47.850: INFO: (7) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 7.291593ms)
Sep  4 18:50:47.851: INFO: (7) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 10.175088ms)
Sep  4 18:50:47.852: INFO: (7) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 9.776961ms)
Sep  4 18:50:47.854: INFO: (7) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 11.182856ms)
Sep  4 18:50:47.854: INFO: (7) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 13.164489ms)
Sep  4 18:50:47.854: INFO: (7) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 11.706091ms)
Sep  4 18:50:47.855: INFO: (7) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 13.818134ms)
Sep  4 18:50:47.856: INFO: (7) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 13.665123ms)
Sep  4 18:50:47.860: INFO: (7) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 17.654293ms)
Sep  4 18:50:47.860: INFO: (7) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 17.844706ms)
Sep  4 18:50:47.861: INFO: (7) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 18.899177ms)
Sep  4 18:50:47.861: INFO: (7) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 20.332574ms)
Sep  4 18:50:47.861: INFO: (7) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 18.365641ms)
Sep  4 18:50:47.863: INFO: (7) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 20.038054ms)
Sep  4 18:50:47.870: INFO: (8) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 6.95687ms)
Sep  4 18:50:47.872: INFO: (8) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 9.158819ms)
Sep  4 18:50:47.873: INFO: (8) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 9.792261ms)
Sep  4 18:50:47.875: INFO: (8) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 11.205357ms)
Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 14.415375ms)
Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 15.138722ms)
Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 15.24313ms)
Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 15.116621ms)
Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 15.111521ms)
Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 15.698961ms)
Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 15.319735ms)
Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 15.376239ms)
Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 15.261532ms)
Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 15.313035ms)
Sep  4 18:50:47.882: INFO: (8) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 18.616658ms)
Sep  4 18:50:47.882: INFO: (8) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 18.851774ms)
Sep  4 18:50:47.891: INFO: (9) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 8.341164ms)
Sep  4 18:50:47.891: INFO: (9) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 8.925103ms)
Sep  4 18:50:47.892: INFO: (9) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 9.499041ms)
Sep  4 18:50:47.892: INFO: (9) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 9.777361ms)
Sep  4 18:50:47.892: INFO: (9) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 10.442506ms)
Sep  4 18:50:47.893: INFO: (9) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 10.409704ms)
Sep  4 18:50:47.895: INFO: (9) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 12.858869ms)
Sep  4 18:50:47.898: INFO: (9) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 15.230529ms)
Sep  4 18:50:47.899: INFO: (9) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 16.091387ms)
Sep  4 18:50:47.899: INFO: (9) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 16.325503ms)
Sep  4 18:50:47.899: INFO: (9) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 16.696228ms)
Sep  4 18:50:47.900: INFO: (9) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 16.991848ms)
Sep  4 18:50:47.900: INFO: (9) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 17.416176ms)
Sep  4 18:50:47.900: INFO: (9) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 17.704596ms)
Sep  4 18:50:47.900: INFO: (9) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 17.187161ms)
Sep  4 18:50:47.900: INFO: (9) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 17.354172ms)
Sep  4 18:50:47.910: INFO: (10) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 8.8776ms)
Sep  4 18:50:47.910: INFO: (10) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 9.437738ms)
Sep  4 18:50:47.914: INFO: (10) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 12.328933ms)
Sep  4 18:50:47.914: INFO: (10) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 12.961776ms)
Sep  4 18:50:47.914: INFO: (10) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 12.946275ms)
Sep  4 18:50:47.914: INFO: (10) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 13.821834ms)
Sep  4 18:50:47.914: INFO: (10) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 13.498013ms)
Sep  4 18:50:47.915: INFO: (10) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 13.904339ms)
Sep  4 18:50:47.916: INFO: (10) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 14.915208ms)
Sep  4 18:50:47.916: INFO: (10) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 15.613055ms)
Sep  4 18:50:47.916: INFO: (10) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 15.479046ms)
Sep  4 18:50:47.916: INFO: (10) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 14.986013ms)
Sep  4 18:50:47.917: INFO: (10) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 15.128223ms)
Sep  4 18:50:47.917: INFO: (10) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 16.313702ms)
Sep  4 18:50:47.917: INFO: (10) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 16.117189ms)
Sep  4 18:50:47.920: INFO: (10) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 18.511751ms)
Sep  4 18:50:47.930: INFO: (11) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 9.258725ms)
Sep  4 18:50:47.930: INFO: (11) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 10.334898ms)
Sep  4 18:50:47.930: INFO: (11) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 10.087982ms)
Sep  4 18:50:47.931: INFO: (11) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 10.108783ms)
Sep  4 18:50:47.935: INFO: (11) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 14.881306ms)
Sep  4 18:50:47.935: INFO: (11) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 15.021915ms)
Sep  4 18:50:47.935: INFO: (11) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 15.430943ms)
Sep  4 18:50:47.936: INFO: (11) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 15.179025ms)
Sep  4 18:50:47.938: INFO: (11) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 17.954013ms)
Sep  4 18:50:47.938: INFO: (11) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 18.293836ms)
Sep  4 18:50:47.938: INFO: (11) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 17.780602ms)
Sep  4 18:50:47.938: INFO: (11) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 17.741398ms)
Sep  4 18:50:47.939: INFO: (11) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 18.464848ms)
Sep  4 18:50:47.941: INFO: (11) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 19.967949ms)
Sep  4 18:50:47.941: INFO: (11) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 20.608692ms)
Sep  4 18:50:47.941: INFO: (11) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 14.559283ms)
Sep  4 18:50:47.953: INFO: (12) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 10.850434ms)
Sep  4 18:50:47.953: INFO: (12) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 11.253061ms)
Sep  4 18:50:47.953: INFO: (12) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 11.372368ms)
Sep  4 18:50:47.955: INFO: (12) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 11.97991ms)
Sep  4 18:50:47.955: INFO: (12) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 12.570049ms)
Sep  4 18:50:47.958: INFO: (12) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 15.99168ms)
Sep  4 18:50:47.959: INFO: (12) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 17.236065ms)
Sep  4 18:50:47.960: INFO: (12) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 17.212963ms)
Sep  4 18:50:47.962: INFO: (12) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 20.435481ms)
Sep  4 18:50:47.962: INFO: (12) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 20.506685ms)
Sep  4 18:50:47.962: INFO: (12) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 20.097358ms)
Sep  4 18:50:47.963: INFO: (12) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 21.538055ms)
Sep  4 18:50:47.963: INFO: (12) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 20.816807ms)
Sep  4 18:50:47.964: INFO: (12) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 21.734668ms)
Sep  4 18:50:47.964: INFO: (12) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 21.802174ms)
Sep  4 18:50:47.966: INFO: (12) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 23.753805ms)
Sep  4 18:50:47.972: INFO: (13) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 6.05691ms)
Sep  4 18:50:47.974: INFO: (13) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 7.900334ms)
Sep  4 18:50:47.980: INFO: (13) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 12.918373ms)
Sep  4 18:50:47.983: INFO: (13) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 16.036484ms)
Sep  4 18:50:47.983: INFO: (13) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 16.912343ms)
Sep  4 18:50:47.983: INFO: (13) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 16.678627ms)
Sep  4 18:50:47.983: INFO: (13) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 16.214196ms)
Sep  4 18:50:47.983: INFO: (13) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 16.907042ms)
Sep  4 18:50:47.984: INFO: (13) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 16.354705ms)
Sep  4 18:50:47.985: INFO: (13) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 18.390243ms)
Sep  4 18:50:47.985: INFO: (13) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 18.257734ms)
Sep  4 18:50:47.986: INFO: (13) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 19.031386ms)
Sep  4 18:50:47.986: INFO: (13) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 19.524919ms)
Sep  4 18:50:47.986: INFO: (13) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 19.988151ms)
Sep  4 18:50:47.986: INFO: (13) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 20.13606ms)
Sep  4 18:50:47.986: INFO: (13) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 19.68023ms)
Sep  4 18:50:47.994: INFO: (14) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 6.817461ms)
Sep  4 18:50:47.997: INFO: (14) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 9.016409ms)
Sep  4 18:50:47.997: INFO: (14) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 8.975306ms)
Sep  4 18:50:48.008: INFO: (14) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 19.775736ms)
Sep  4 18:50:48.008: INFO: (14) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 20.029553ms)
Sep  4 18:50:48.008: INFO: (14) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 19.995852ms)
Sep  4 18:50:48.009: INFO: (14) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 21.129928ms)
Sep  4 18:50:48.009: INFO: (14) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 21.105226ms)
Sep  4 18:50:48.009: INFO: (14) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 21.60736ms)
Sep  4 18:50:48.009: INFO: (14) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 21.040522ms)
Sep  4 18:50:48.009: INFO: (14) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 21.122527ms)
Sep  4 18:50:48.009: INFO: (14) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 21.441249ms)
Sep  4 18:50:48.011: INFO: (14) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 22.236503ms)
Sep  4 18:50:48.013: INFO: (14) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 25.232005ms)
Sep  4 18:50:48.013: INFO: (14) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 25.331611ms)
Sep  4 18:50:48.014: INFO: (14) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 26.005857ms)
Sep  4 18:50:48.025: INFO: (15) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 10.769828ms)
Sep  4 18:50:48.026: INFO: (15) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 10.50561ms)
Sep  4 18:50:48.029: INFO: (15) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 13.848935ms)
Sep  4 18:50:48.029: INFO: (15) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 14.428474ms)
Sep  4 18:50:48.029: INFO: (15) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 15.100021ms)
Sep  4 18:50:48.029: INFO: (15) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 15.250731ms)
Sep  4 18:50:48.030: INFO: (15) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 15.887773ms)
Sep  4 18:50:48.030: INFO: (15) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 15.652657ms)
Sep  4 18:50:48.030: INFO: (15) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 15.505548ms)
Sep  4 18:50:48.030: INFO: (15) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 15.617455ms)
Sep  4 18:50:48.030: INFO: (15) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 16.57742ms)
Sep  4 18:50:48.031: INFO: (15) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 15.879373ms)
Sep  4 18:50:48.033: INFO: (15) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 18.020817ms)
Sep  4 18:50:48.035: INFO: (15) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 20.12526ms)
Sep  4 18:50:48.035: INFO: (15) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 20.143561ms)
Sep  4 18:50:48.035: INFO: (15) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 20.606493ms)
Sep  4 18:50:48.042: INFO: (16) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 6.205919ms)
Sep  4 18:50:48.045: INFO: (16) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 9.402136ms)
Sep  4 18:50:48.045: INFO: (16) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 9.577747ms)
Sep  4 18:50:48.046: INFO: (16) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 9.665653ms)
Sep  4 18:50:48.048: INFO: (16) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 12.007711ms)
Sep  4 18:50:48.048: INFO: (16) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 12.863069ms)
Sep  4 18:50:48.049: INFO: (16) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 12.864169ms)
Sep  4 18:50:48.050: INFO: (16) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 13.522314ms)
Sep  4 18:50:48.051: INFO: (16) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 16.052785ms)
Sep  4 18:50:48.052: INFO: (16) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 15.894274ms)
Sep  4 18:50:48.052: INFO: (16) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 14.846103ms)
Sep  4 18:50:48.054: INFO: (16) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 18.033718ms)
Sep  4 18:50:48.054: INFO: (16) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 18.35584ms)
Sep  4 18:50:48.054: INFO: (16) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 18.413644ms)
Sep  4 18:50:48.055: INFO: (16) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 19.025485ms)
Sep  4 18:50:48.056: INFO: (16) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 18.600756ms)
Sep  4 18:50:48.062: INFO: (17) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 6.150516ms)
Sep  4 18:50:48.064: INFO: (17) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 7.313494ms)
Sep  4 18:50:48.064: INFO: (17) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 7.973939ms)
Sep  4 18:50:48.068: INFO: (17) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 12.517545ms)
Sep  4 18:50:48.069: INFO: (17) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 11.846301ms)
Sep  4 18:50:48.069: INFO: (17) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 12.449142ms)
Sep  4 18:50:48.072: INFO: (17) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 16.092987ms)
Sep  4 18:50:48.073: INFO: (17) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 15.929977ms)
Sep  4 18:50:48.073: INFO: (17) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 16.414709ms)
Sep  4 18:50:48.074: INFO: (17) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 17.114556ms)
Sep  4 18:50:48.074: INFO: (17) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 17.660893ms)
Sep  4 18:50:48.074: INFO: (17) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 18.129325ms)
Sep  4 18:50:48.074: INFO: (17) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 17.688996ms)
Sep  4 18:50:48.075: INFO: (17) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 18.697764ms)
Sep  4 18:50:48.076: INFO: (17) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 19.698631ms)
Sep  4 18:50:48.078: INFO: (17) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 22.507021ms)
Sep  4 18:50:48.089: INFO: (18) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 9.500042ms)
Sep  4 18:50:48.089: INFO: (18) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 9.053911ms)
Sep  4 18:50:48.089: INFO: (18) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 9.707456ms)
Sep  4 18:50:48.089: INFO: (18) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 9.654652ms)
Sep  4 18:50:48.093: INFO: (18) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 14.043249ms)
Sep  4 18:50:48.093: INFO: (18) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 13.882038ms)
Sep  4 18:50:48.094: INFO: (18) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 14.708594ms)
Sep  4 18:50:48.094: INFO: (18) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 14.339569ms)
Sep  4 18:50:48.094: INFO: (18) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 14.907907ms)
Sep  4 18:50:48.096: INFO: (18) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 16.080886ms)
Sep  4 18:50:48.099: INFO: (18) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 19.410811ms)
Sep  4 18:50:48.099: INFO: (18) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 20.342075ms)
Sep  4 18:50:48.100: INFO: (18) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 20.228967ms)
Sep  4 18:50:48.100: INFO: (18) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 20.639695ms)
Sep  4 18:50:48.100: INFO: (18) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 21.237435ms)
Sep  4 18:50:48.100: INFO: (18) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 20.769704ms)
Sep  4 18:50:48.105: INFO: (19) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 5.123246ms)
Sep  4 18:50:48.108: INFO: (19) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 7.000473ms)
Sep  4 18:50:48.113: INFO: (19) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 12.029112ms)
Sep  4 18:50:48.113: INFO: (19) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 12.986878ms)
Sep  4 18:50:48.114: INFO: (19) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 12.917273ms)
Sep  4 18:50:48.117: INFO: (19) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 15.678959ms)
Sep  4 18:50:48.117: INFO: (19) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 15.617955ms)
Sep  4 18:50:48.117: INFO: (19) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 15.990881ms)
Sep  4 18:50:48.117: INFO: (19) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 16.780234ms)
Sep  4 18:50:48.117: INFO: (19) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 16.145491ms)
Sep  4 18:50:48.121: INFO: (19) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 19.498118ms)
Sep  4 18:50:48.121: INFO: (19) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 20.182663ms)
Sep  4 18:50:48.121: INFO: (19) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 20.28987ms)
Sep  4 18:50:48.121: INFO: (19) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 20.7148ms)
Sep  4 18:50:48.121: INFO: (19) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 20.194564ms)
Sep  4 18:50:48.122: INFO: (19) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 20.698598ms)
STEP: deleting ReplicationController proxy-service-x8f9r in namespace proxy-402, will wait for the garbage collector to delete the pods 09/04/23 18:50:48.122
Sep  4 18:50:48.188: INFO: Deleting ReplicationController proxy-service-x8f9r took: 11.925006ms
Sep  4 18:50:48.289: INFO: Terminating ReplicationController proxy-service-x8f9r pods took: 100.958622ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  4 18:50:50.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-402" for this suite. 09/04/23 18:50:50.297
------------------------------
â€¢ [4.790 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:50:45.518
    Sep  4 18:50:45.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename proxy 09/04/23 18:50:45.519
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:50:45.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:50:45.541
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 09/04/23 18:50:45.571
    STEP: creating replication controller proxy-service-x8f9r in namespace proxy-402 09/04/23 18:50:45.574
    I0904 18:50:45.594438      19 runners.go:193] Created replication controller with name: proxy-service-x8f9r, namespace: proxy-402, replica count: 1
    I0904 18:50:46.649694      19 runners.go:193] proxy-service-x8f9r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0904 18:50:47.650434      19 runners.go:193] proxy-service-x8f9r Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 18:50:47.654: INFO: setup took 2.107722418s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/04/23 18:50:47.654
    Sep  4 18:50:47.670: INFO: (0) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 15.414141ms)
    Sep  4 18:50:47.675: INFO: (0) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 20.42118ms)
    Sep  4 18:50:47.677: INFO: (0) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 21.32134ms)
    Sep  4 18:50:47.688: INFO: (0) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 32.402889ms)
    Sep  4 18:50:47.688: INFO: (0) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 33.067934ms)
    Sep  4 18:50:47.694: INFO: (0) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 39.547373ms)
    Sep  4 18:50:47.694: INFO: (0) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 40.087708ms)
    Sep  4 18:50:47.697: INFO: (0) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 42.063042ms)
    Sep  4 18:50:47.698: INFO: (0) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 42.974203ms)
    Sep  4 18:50:47.699: INFO: (0) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 44.066077ms)
    Sep  4 18:50:47.699: INFO: (0) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 43.984972ms)
    Sep  4 18:50:47.699: INFO: (0) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 44.153383ms)
    Sep  4 18:50:47.699: INFO: (0) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 44.940537ms)
    Sep  4 18:50:47.700: INFO: (0) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 44.69142ms)
    Sep  4 18:50:47.700: INFO: (0) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 44.318794ms)
    Sep  4 18:50:47.703: INFO: (0) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 48.10925ms)
    Sep  4 18:50:47.710: INFO: (1) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 6.117113ms)
    Sep  4 18:50:47.714: INFO: (1) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 10.183688ms)
    Sep  4 18:50:47.714: INFO: (1) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 10.94904ms)
    Sep  4 18:50:47.716: INFO: (1) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 12.417139ms)
    Sep  4 18:50:47.716: INFO: (1) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 12.471442ms)
    Sep  4 18:50:47.718: INFO: (1) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 13.940942ms)
    Sep  4 18:50:47.718: INFO: (1) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 13.924441ms)
    Sep  4 18:50:47.718: INFO: (1) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 13.90414ms)
    Sep  4 18:50:47.718: INFO: (1) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 14.375671ms)
    Sep  4 18:50:47.721: INFO: (1) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 17.277768ms)
    Sep  4 18:50:47.721: INFO: (1) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 17.186962ms)
    Sep  4 18:50:47.722: INFO: (1) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 17.587988ms)
    Sep  4 18:50:47.722: INFO: (1) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 17.903509ms)
    Sep  4 18:50:47.722: INFO: (1) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 17.723798ms)
    Sep  4 18:50:47.722: INFO: (1) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 17.940412ms)
    Sep  4 18:50:47.727: INFO: (1) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 23.428583ms)
    Sep  4 18:50:47.735: INFO: (2) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 6.939069ms)
    Sep  4 18:50:47.735: INFO: (2) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 6.780058ms)
    Sep  4 18:50:47.737: INFO: (2) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 8.658085ms)
    Sep  4 18:50:47.740: INFO: (2) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 12.267229ms)
    Sep  4 18:50:47.741: INFO: (2) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 12.621653ms)
    Sep  4 18:50:47.741: INFO: (2) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 12.309032ms)
    Sep  4 18:50:47.741: INFO: (2) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 12.451641ms)
    Sep  4 18:50:47.741: INFO: (2) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 12.830267ms)
    Sep  4 18:50:47.741: INFO: (2) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 12.597451ms)
    Sep  4 18:50:47.744: INFO: (2) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 15.23203ms)
    Sep  4 18:50:47.744: INFO: (2) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 15.486446ms)
    Sep  4 18:50:47.746: INFO: (2) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 18.115324ms)
    Sep  4 18:50:47.747: INFO: (2) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 17.867607ms)
    Sep  4 18:50:47.747: INFO: (2) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 18.416844ms)
    Sep  4 18:50:47.747: INFO: (2) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 18.105123ms)
    Sep  4 18:50:47.748: INFO: (2) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 19.720632ms)
    Sep  4 18:50:47.754: INFO: (3) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 6.203419ms)
    Sep  4 18:50:47.755: INFO: (3) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 5.77329ms)
    Sep  4 18:50:47.758: INFO: (3) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 9.321429ms)
    Sep  4 18:50:47.759: INFO: (3) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 10.06808ms)
    Sep  4 18:50:47.759: INFO: (3) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 10.440106ms)
    Sep  4 18:50:47.759: INFO: (3) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 10.143585ms)
    Sep  4 18:50:47.763: INFO: (3) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 13.949943ms)
    Sep  4 18:50:47.763: INFO: (3) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 13.714326ms)
    Sep  4 18:50:47.763: INFO: (3) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 14.022347ms)
    Sep  4 18:50:47.764: INFO: (3) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 15.360638ms)
    Sep  4 18:50:47.764: INFO: (3) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 15.230329ms)
    Sep  4 18:50:47.765: INFO: (3) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 15.885274ms)
    Sep  4 18:50:47.765: INFO: (3) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 16.623423ms)
    Sep  4 18:50:47.765: INFO: (3) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 16.734731ms)
    Sep  4 18:50:47.765: INFO: (3) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 16.180093ms)
    Sep  4 18:50:47.766: INFO: (3) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 17.336272ms)
    Sep  4 18:50:47.775: INFO: (4) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 7.953637ms)
    Sep  4 18:50:47.775: INFO: (4) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 8.228356ms)
    Sep  4 18:50:47.776: INFO: (4) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 8.229156ms)
    Sep  4 18:50:47.781: INFO: (4) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 13.597919ms)
    Sep  4 18:50:47.784: INFO: (4) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 17.034951ms)
    Sep  4 18:50:47.786: INFO: (4) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 18.811171ms)
    Sep  4 18:50:47.786: INFO: (4) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 19.585723ms)
    Sep  4 18:50:47.787: INFO: (4) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 19.482516ms)
    Sep  4 18:50:47.787: INFO: (4) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 20.223566ms)
    Sep  4 18:50:47.788: INFO: (4) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 21.120727ms)
    Sep  4 18:50:47.789: INFO: (4) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 21.174331ms)
    Sep  4 18:50:47.789: INFO: (4) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 21.750269ms)
    Sep  4 18:50:47.789: INFO: (4) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 21.351943ms)
    Sep  4 18:50:47.789: INFO: (4) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 22.304907ms)
    Sep  4 18:50:47.790: INFO: (4) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 23.284573ms)
    Sep  4 18:50:47.791: INFO: (4) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 24.298741ms)
    Sep  4 18:50:47.797: INFO: (5) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 5.966403ms)
    Sep  4 18:50:47.802: INFO: (5) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 10.327797ms)
    Sep  4 18:50:47.802: INFO: (5) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 10.145286ms)
    Sep  4 18:50:47.802: INFO: (5) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 9.056412ms)
    Sep  4 18:50:47.804: INFO: (5) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 12.115118ms)
    Sep  4 18:50:47.804: INFO: (5) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 12.033714ms)
    Sep  4 18:50:47.804: INFO: (5) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 12.762062ms)
    Sep  4 18:50:47.805: INFO: (5) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 12.671156ms)
    Sep  4 18:50:47.806: INFO: (5) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 14.216161ms)
    Sep  4 18:50:47.807: INFO: (5) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 15.795367ms)
    Sep  4 18:50:47.816: INFO: (5) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 24.626364ms)
    Sep  4 18:50:47.817: INFO: (5) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 24.725371ms)
    Sep  4 18:50:47.817: INFO: (5) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 25.177901ms)
    Sep  4 18:50:47.817: INFO: (5) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 25.712137ms)
    Sep  4 18:50:47.817: INFO: (5) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 25.136098ms)
    Sep  4 18:50:47.817: INFO: (5) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 25.520324ms)
    Sep  4 18:50:47.827: INFO: (6) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 9.051311ms)
    Sep  4 18:50:47.831: INFO: (6) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 13.307099ms)
    Sep  4 18:50:47.832: INFO: (6) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 13.216893ms)
    Sep  4 18:50:47.832: INFO: (6) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 13.892838ms)
    Sep  4 18:50:47.836: INFO: (6) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 18.114624ms)
    Sep  4 18:50:47.838: INFO: (6) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 20.292771ms)
    Sep  4 18:50:47.838: INFO: (6) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 19.731033ms)
    Sep  4 18:50:47.838: INFO: (6) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 20.452482ms)
    Sep  4 18:50:47.838: INFO: (6) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 20.12156ms)
    Sep  4 18:50:47.839: INFO: (6) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 20.768603ms)
    Sep  4 18:50:47.839: INFO: (6) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 21.866378ms)
    Sep  4 18:50:47.840: INFO: (6) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 21.118627ms)
    Sep  4 18:50:47.840: INFO: (6) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 21.729569ms)
    Sep  4 18:50:47.840: INFO: (6) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 21.519554ms)
    Sep  4 18:50:47.840: INFO: (6) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 21.685565ms)
    Sep  4 18:50:47.840: INFO: (6) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 22.563724ms)
    Sep  4 18:50:47.848: INFO: (7) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 7.699121ms)
    Sep  4 18:50:47.849: INFO: (7) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 8.111148ms)
    Sep  4 18:50:47.850: INFO: (7) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 7.291593ms)
    Sep  4 18:50:47.851: INFO: (7) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 10.175088ms)
    Sep  4 18:50:47.852: INFO: (7) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 9.776961ms)
    Sep  4 18:50:47.854: INFO: (7) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 11.182856ms)
    Sep  4 18:50:47.854: INFO: (7) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 13.164489ms)
    Sep  4 18:50:47.854: INFO: (7) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 11.706091ms)
    Sep  4 18:50:47.855: INFO: (7) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 13.818134ms)
    Sep  4 18:50:47.856: INFO: (7) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 13.665123ms)
    Sep  4 18:50:47.860: INFO: (7) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 17.654293ms)
    Sep  4 18:50:47.860: INFO: (7) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 17.844706ms)
    Sep  4 18:50:47.861: INFO: (7) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 18.899177ms)
    Sep  4 18:50:47.861: INFO: (7) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 20.332574ms)
    Sep  4 18:50:47.861: INFO: (7) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 18.365641ms)
    Sep  4 18:50:47.863: INFO: (7) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 20.038054ms)
    Sep  4 18:50:47.870: INFO: (8) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 6.95687ms)
    Sep  4 18:50:47.872: INFO: (8) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 9.158819ms)
    Sep  4 18:50:47.873: INFO: (8) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 9.792261ms)
    Sep  4 18:50:47.875: INFO: (8) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 11.205357ms)
    Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 14.415375ms)
    Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 15.138722ms)
    Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 15.24313ms)
    Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 15.116621ms)
    Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 15.111521ms)
    Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 15.698961ms)
    Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 15.319735ms)
    Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 15.376239ms)
    Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 15.261532ms)
    Sep  4 18:50:47.878: INFO: (8) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 15.313035ms)
    Sep  4 18:50:47.882: INFO: (8) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 18.616658ms)
    Sep  4 18:50:47.882: INFO: (8) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 18.851774ms)
    Sep  4 18:50:47.891: INFO: (9) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 8.341164ms)
    Sep  4 18:50:47.891: INFO: (9) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 8.925103ms)
    Sep  4 18:50:47.892: INFO: (9) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 9.499041ms)
    Sep  4 18:50:47.892: INFO: (9) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 9.777361ms)
    Sep  4 18:50:47.892: INFO: (9) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 10.442506ms)
    Sep  4 18:50:47.893: INFO: (9) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 10.409704ms)
    Sep  4 18:50:47.895: INFO: (9) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 12.858869ms)
    Sep  4 18:50:47.898: INFO: (9) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 15.230529ms)
    Sep  4 18:50:47.899: INFO: (9) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 16.091387ms)
    Sep  4 18:50:47.899: INFO: (9) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 16.325503ms)
    Sep  4 18:50:47.899: INFO: (9) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 16.696228ms)
    Sep  4 18:50:47.900: INFO: (9) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 16.991848ms)
    Sep  4 18:50:47.900: INFO: (9) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 17.416176ms)
    Sep  4 18:50:47.900: INFO: (9) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 17.704596ms)
    Sep  4 18:50:47.900: INFO: (9) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 17.187161ms)
    Sep  4 18:50:47.900: INFO: (9) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 17.354172ms)
    Sep  4 18:50:47.910: INFO: (10) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 8.8776ms)
    Sep  4 18:50:47.910: INFO: (10) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 9.437738ms)
    Sep  4 18:50:47.914: INFO: (10) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 12.328933ms)
    Sep  4 18:50:47.914: INFO: (10) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 12.961776ms)
    Sep  4 18:50:47.914: INFO: (10) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 12.946275ms)
    Sep  4 18:50:47.914: INFO: (10) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 13.821834ms)
    Sep  4 18:50:47.914: INFO: (10) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 13.498013ms)
    Sep  4 18:50:47.915: INFO: (10) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 13.904339ms)
    Sep  4 18:50:47.916: INFO: (10) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 14.915208ms)
    Sep  4 18:50:47.916: INFO: (10) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 15.613055ms)
    Sep  4 18:50:47.916: INFO: (10) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 15.479046ms)
    Sep  4 18:50:47.916: INFO: (10) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 14.986013ms)
    Sep  4 18:50:47.917: INFO: (10) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 15.128223ms)
    Sep  4 18:50:47.917: INFO: (10) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 16.313702ms)
    Sep  4 18:50:47.917: INFO: (10) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 16.117189ms)
    Sep  4 18:50:47.920: INFO: (10) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 18.511751ms)
    Sep  4 18:50:47.930: INFO: (11) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 9.258725ms)
    Sep  4 18:50:47.930: INFO: (11) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 10.334898ms)
    Sep  4 18:50:47.930: INFO: (11) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 10.087982ms)
    Sep  4 18:50:47.931: INFO: (11) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 10.108783ms)
    Sep  4 18:50:47.935: INFO: (11) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 14.881306ms)
    Sep  4 18:50:47.935: INFO: (11) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 15.021915ms)
    Sep  4 18:50:47.935: INFO: (11) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 15.430943ms)
    Sep  4 18:50:47.936: INFO: (11) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 15.179025ms)
    Sep  4 18:50:47.938: INFO: (11) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 17.954013ms)
    Sep  4 18:50:47.938: INFO: (11) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 18.293836ms)
    Sep  4 18:50:47.938: INFO: (11) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 17.780602ms)
    Sep  4 18:50:47.938: INFO: (11) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 17.741398ms)
    Sep  4 18:50:47.939: INFO: (11) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 18.464848ms)
    Sep  4 18:50:47.941: INFO: (11) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 19.967949ms)
    Sep  4 18:50:47.941: INFO: (11) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 20.608692ms)
    Sep  4 18:50:47.941: INFO: (11) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 14.559283ms)
    Sep  4 18:50:47.953: INFO: (12) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 10.850434ms)
    Sep  4 18:50:47.953: INFO: (12) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 11.253061ms)
    Sep  4 18:50:47.953: INFO: (12) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 11.372368ms)
    Sep  4 18:50:47.955: INFO: (12) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 11.97991ms)
    Sep  4 18:50:47.955: INFO: (12) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 12.570049ms)
    Sep  4 18:50:47.958: INFO: (12) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 15.99168ms)
    Sep  4 18:50:47.959: INFO: (12) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 17.236065ms)
    Sep  4 18:50:47.960: INFO: (12) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 17.212963ms)
    Sep  4 18:50:47.962: INFO: (12) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 20.435481ms)
    Sep  4 18:50:47.962: INFO: (12) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 20.506685ms)
    Sep  4 18:50:47.962: INFO: (12) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 20.097358ms)
    Sep  4 18:50:47.963: INFO: (12) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 21.538055ms)
    Sep  4 18:50:47.963: INFO: (12) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 20.816807ms)
    Sep  4 18:50:47.964: INFO: (12) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 21.734668ms)
    Sep  4 18:50:47.964: INFO: (12) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 21.802174ms)
    Sep  4 18:50:47.966: INFO: (12) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 23.753805ms)
    Sep  4 18:50:47.972: INFO: (13) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 6.05691ms)
    Sep  4 18:50:47.974: INFO: (13) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 7.900334ms)
    Sep  4 18:50:47.980: INFO: (13) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 12.918373ms)
    Sep  4 18:50:47.983: INFO: (13) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 16.036484ms)
    Sep  4 18:50:47.983: INFO: (13) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 16.912343ms)
    Sep  4 18:50:47.983: INFO: (13) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 16.678627ms)
    Sep  4 18:50:47.983: INFO: (13) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 16.214196ms)
    Sep  4 18:50:47.983: INFO: (13) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 16.907042ms)
    Sep  4 18:50:47.984: INFO: (13) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 16.354705ms)
    Sep  4 18:50:47.985: INFO: (13) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 18.390243ms)
    Sep  4 18:50:47.985: INFO: (13) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 18.257734ms)
    Sep  4 18:50:47.986: INFO: (13) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 19.031386ms)
    Sep  4 18:50:47.986: INFO: (13) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 19.524919ms)
    Sep  4 18:50:47.986: INFO: (13) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 19.988151ms)
    Sep  4 18:50:47.986: INFO: (13) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 20.13606ms)
    Sep  4 18:50:47.986: INFO: (13) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 19.68023ms)
    Sep  4 18:50:47.994: INFO: (14) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 6.817461ms)
    Sep  4 18:50:47.997: INFO: (14) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 9.016409ms)
    Sep  4 18:50:47.997: INFO: (14) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 8.975306ms)
    Sep  4 18:50:48.008: INFO: (14) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 19.775736ms)
    Sep  4 18:50:48.008: INFO: (14) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 20.029553ms)
    Sep  4 18:50:48.008: INFO: (14) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 19.995852ms)
    Sep  4 18:50:48.009: INFO: (14) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 21.129928ms)
    Sep  4 18:50:48.009: INFO: (14) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 21.105226ms)
    Sep  4 18:50:48.009: INFO: (14) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 21.60736ms)
    Sep  4 18:50:48.009: INFO: (14) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 21.040522ms)
    Sep  4 18:50:48.009: INFO: (14) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 21.122527ms)
    Sep  4 18:50:48.009: INFO: (14) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 21.441249ms)
    Sep  4 18:50:48.011: INFO: (14) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 22.236503ms)
    Sep  4 18:50:48.013: INFO: (14) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 25.232005ms)
    Sep  4 18:50:48.013: INFO: (14) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 25.331611ms)
    Sep  4 18:50:48.014: INFO: (14) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 26.005857ms)
    Sep  4 18:50:48.025: INFO: (15) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 10.769828ms)
    Sep  4 18:50:48.026: INFO: (15) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 10.50561ms)
    Sep  4 18:50:48.029: INFO: (15) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 13.848935ms)
    Sep  4 18:50:48.029: INFO: (15) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 14.428474ms)
    Sep  4 18:50:48.029: INFO: (15) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 15.100021ms)
    Sep  4 18:50:48.029: INFO: (15) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 15.250731ms)
    Sep  4 18:50:48.030: INFO: (15) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 15.887773ms)
    Sep  4 18:50:48.030: INFO: (15) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 15.652657ms)
    Sep  4 18:50:48.030: INFO: (15) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 15.505548ms)
    Sep  4 18:50:48.030: INFO: (15) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 15.617455ms)
    Sep  4 18:50:48.030: INFO: (15) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 16.57742ms)
    Sep  4 18:50:48.031: INFO: (15) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 15.879373ms)
    Sep  4 18:50:48.033: INFO: (15) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 18.020817ms)
    Sep  4 18:50:48.035: INFO: (15) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 20.12526ms)
    Sep  4 18:50:48.035: INFO: (15) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 20.143561ms)
    Sep  4 18:50:48.035: INFO: (15) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 20.606493ms)
    Sep  4 18:50:48.042: INFO: (16) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 6.205919ms)
    Sep  4 18:50:48.045: INFO: (16) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 9.402136ms)
    Sep  4 18:50:48.045: INFO: (16) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 9.577747ms)
    Sep  4 18:50:48.046: INFO: (16) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 9.665653ms)
    Sep  4 18:50:48.048: INFO: (16) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 12.007711ms)
    Sep  4 18:50:48.048: INFO: (16) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 12.863069ms)
    Sep  4 18:50:48.049: INFO: (16) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 12.864169ms)
    Sep  4 18:50:48.050: INFO: (16) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 13.522314ms)
    Sep  4 18:50:48.051: INFO: (16) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 16.052785ms)
    Sep  4 18:50:48.052: INFO: (16) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 15.894274ms)
    Sep  4 18:50:48.052: INFO: (16) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 14.846103ms)
    Sep  4 18:50:48.054: INFO: (16) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 18.033718ms)
    Sep  4 18:50:48.054: INFO: (16) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 18.35584ms)
    Sep  4 18:50:48.054: INFO: (16) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 18.413644ms)
    Sep  4 18:50:48.055: INFO: (16) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 19.025485ms)
    Sep  4 18:50:48.056: INFO: (16) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 18.600756ms)
    Sep  4 18:50:48.062: INFO: (17) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 6.150516ms)
    Sep  4 18:50:48.064: INFO: (17) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 7.313494ms)
    Sep  4 18:50:48.064: INFO: (17) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 7.973939ms)
    Sep  4 18:50:48.068: INFO: (17) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 12.517545ms)
    Sep  4 18:50:48.069: INFO: (17) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 11.846301ms)
    Sep  4 18:50:48.069: INFO: (17) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 12.449142ms)
    Sep  4 18:50:48.072: INFO: (17) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 16.092987ms)
    Sep  4 18:50:48.073: INFO: (17) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 15.929977ms)
    Sep  4 18:50:48.073: INFO: (17) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 16.414709ms)
    Sep  4 18:50:48.074: INFO: (17) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 17.114556ms)
    Sep  4 18:50:48.074: INFO: (17) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 17.660893ms)
    Sep  4 18:50:48.074: INFO: (17) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 18.129325ms)
    Sep  4 18:50:48.074: INFO: (17) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 17.688996ms)
    Sep  4 18:50:48.075: INFO: (17) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 18.697764ms)
    Sep  4 18:50:48.076: INFO: (17) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 19.698631ms)
    Sep  4 18:50:48.078: INFO: (17) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 22.507021ms)
    Sep  4 18:50:48.089: INFO: (18) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 9.500042ms)
    Sep  4 18:50:48.089: INFO: (18) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 9.053911ms)
    Sep  4 18:50:48.089: INFO: (18) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 9.707456ms)
    Sep  4 18:50:48.089: INFO: (18) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 9.654652ms)
    Sep  4 18:50:48.093: INFO: (18) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 14.043249ms)
    Sep  4 18:50:48.093: INFO: (18) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 13.882038ms)
    Sep  4 18:50:48.094: INFO: (18) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 14.708594ms)
    Sep  4 18:50:48.094: INFO: (18) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 14.339569ms)
    Sep  4 18:50:48.094: INFO: (18) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 14.907907ms)
    Sep  4 18:50:48.096: INFO: (18) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 16.080886ms)
    Sep  4 18:50:48.099: INFO: (18) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 19.410811ms)
    Sep  4 18:50:48.099: INFO: (18) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 20.342075ms)
    Sep  4 18:50:48.100: INFO: (18) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 20.228967ms)
    Sep  4 18:50:48.100: INFO: (18) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 20.639695ms)
    Sep  4 18:50:48.100: INFO: (18) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 21.237435ms)
    Sep  4 18:50:48.100: INFO: (18) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 20.769704ms)
    Sep  4 18:50:48.105: INFO: (19) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">t... (200; 5.123246ms)
    Sep  4 18:50:48.108: INFO: (19) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:460/proxy/: tls baz (200; 7.000473ms)
    Sep  4 18:50:48.113: INFO: (19) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:1080/proxy/rewriteme">test</... (200; 12.029112ms)
    Sep  4 18:50:48.113: INFO: (19) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 12.986878ms)
    Sep  4 18:50:48.114: INFO: (19) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname2/proxy/: bar (200; 12.917273ms)
    Sep  4 18:50:48.117: INFO: (19) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm/proxy/rewriteme">test</a> (200; 15.678959ms)
    Sep  4 18:50:48.117: INFO: (19) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:462/proxy/: tls qux (200; 15.617955ms)
    Sep  4 18:50:48.117: INFO: (19) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:160/proxy/: foo (200; 15.990881ms)
    Sep  4 18:50:48.117: INFO: (19) /api/v1/namespaces/proxy-402/pods/proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 16.780234ms)
    Sep  4 18:50:48.117: INFO: (19) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname1/proxy/: tls baz (200; 16.145491ms)
    Sep  4 18:50:48.121: INFO: (19) /api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/: <a href="/api/v1/namespaces/proxy-402/pods/https:proxy-service-x8f9r-lthdm:443/proxy/tlsrewriteme... (200; 19.498118ms)
    Sep  4 18:50:48.121: INFO: (19) /api/v1/namespaces/proxy-402/services/http:proxy-service-x8f9r:portname1/proxy/: foo (200; 20.182663ms)
    Sep  4 18:50:48.121: INFO: (19) /api/v1/namespaces/proxy-402/services/https:proxy-service-x8f9r:tlsportname2/proxy/: tls qux (200; 20.28987ms)
    Sep  4 18:50:48.121: INFO: (19) /api/v1/namespaces/proxy-402/pods/http:proxy-service-x8f9r-lthdm:162/proxy/: bar (200; 20.7148ms)
    Sep  4 18:50:48.121: INFO: (19) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname1/proxy/: foo (200; 20.194564ms)
    Sep  4 18:50:48.122: INFO: (19) /api/v1/namespaces/proxy-402/services/proxy-service-x8f9r:portname2/proxy/: bar (200; 20.698598ms)
    STEP: deleting ReplicationController proxy-service-x8f9r in namespace proxy-402, will wait for the garbage collector to delete the pods 09/04/23 18:50:48.122
    Sep  4 18:50:48.188: INFO: Deleting ReplicationController proxy-service-x8f9r took: 11.925006ms
    Sep  4 18:50:48.289: INFO: Terminating ReplicationController proxy-service-x8f9r pods took: 100.958622ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:50:50.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-402" for this suite. 09/04/23 18:50:50.297
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:50:50.314
Sep  4 18:50:50.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename init-container 09/04/23 18:50:50.317
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:50:50.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:50:50.34
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 09/04/23 18:50:50.344
Sep  4 18:50:50.344: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:50:56.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9002" for this suite. 09/04/23 18:50:56.308
------------------------------
â€¢ [SLOW TEST] [6.006 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:50:50.314
    Sep  4 18:50:50.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename init-container 09/04/23 18:50:50.317
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:50:50.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:50:50.34
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 09/04/23 18:50:50.344
    Sep  4 18:50:50.344: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:50:56.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9002" for this suite. 09/04/23 18:50:56.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:50:56.334
Sep  4 18:50:56.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename sched-pred 09/04/23 18:50:56.335
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:50:56.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:50:56.363
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  4 18:50:56.367: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  4 18:50:56.373: INFO: Waiting for terminating namespaces to be deleted...
Sep  4 18:50:56.376: INFO: 
Logging pods the apiserver thinks is on node tenant-000001 before test
Sep  4 18:50:56.385: INFO: pod-init-931b9c2a-c22d-467c-98f8-f1f21058f565 from init-container-9002 started at 2023-09-04 18:50:50 +0000 UTC (1 container statuses recorded)
Sep  4 18:50:56.385: INFO: 	Container run1 ready: false, restart count 0
Sep  4 18:50:56.386: INFO: calico-node-dj2mb from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
Sep  4 18:50:56.386: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 18:50:56.386: INFO: konnectivity-agent-4d7rv from kube-system started at 2023-09-04 18:41:14 +0000 UTC (1 container statuses recorded)
Sep  4 18:50:56.386: INFO: 	Container konnectivity-agent ready: true, restart count 0
Sep  4 18:50:56.386: INFO: kube-proxy-959c5 from kube-system started at 2023-09-04 17:14:03 +0000 UTC (1 container statuses recorded)
Sep  4 18:50:56.386: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 18:50:56.386: INFO: sonobuoy from sonobuoy started at 2023-09-04 17:16:08 +0000 UTC (1 container statuses recorded)
Sep  4 18:50:56.386: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  4 18:50:56.387: INFO: sonobuoy-e2e-job-8da28a692bc241e2 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
Sep  4 18:50:56.387: INFO: 	Container e2e ready: true, restart count 0
Sep  4 18:50:56.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  4 18:50:56.387: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-64p86 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
Sep  4 18:50:56.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  4 18:50:56.387: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  4 18:50:56.387: INFO: 
Logging pods the apiserver thinks is on node tenant-000003 before test
Sep  4 18:50:56.392: INFO: calico-kube-controllers-5f94594857-4wpdt from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 18:50:56.392: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  4 18:50:56.392: INFO: calico-node-wlgx4 from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
Sep  4 18:50:56.392: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 18:50:56.392: INFO: coredns-6bfc4dc876-rnzz9 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 18:50:56.392: INFO: 	Container coredns ready: true, restart count 0
Sep  4 18:50:56.392: INFO: coredns-6bfc4dc876-xp5ff from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 18:50:56.392: INFO: 	Container coredns ready: true, restart count 0
Sep  4 18:50:56.392: INFO: konnectivity-agent-j62r4 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
Sep  4 18:50:56.392: INFO: 	Container konnectivity-agent ready: true, restart count 0
Sep  4 18:50:56.392: INFO: kube-proxy-z2kxt from kube-system started at 2023-09-04 17:15:09 +0000 UTC (1 container statuses recorded)
Sep  4 18:50:56.392: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 18:50:56.392: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-7wr4k from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
Sep  4 18:50:56.392: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  4 18:50:56.392: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/04/23 18:50:56.392
Sep  4 18:50:56.400: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7395" to be "running"
Sep  4 18:50:56.418: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 17.163187ms
Sep  4 18:50:58.424: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.023276559s
Sep  4 18:50:58.424: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/04/23 18:50:58.427
STEP: Trying to apply a random label on the found node. 09/04/23 18:50:58.456
STEP: verifying the node has the label kubernetes.io/e2e-f481bf37-c553-46b6-a85c-db8a021271f0 42 09/04/23 18:50:58.493
STEP: Trying to relaunch the pod, now with labels. 09/04/23 18:50:58.502
Sep  4 18:50:58.512: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7395" to be "not pending"
Sep  4 18:50:58.519: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 7.139794ms
Sep  4 18:51:00.523: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.011469461s
Sep  4 18:51:00.524: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-f481bf37-c553-46b6-a85c-db8a021271f0 off the node tenant-000001 09/04/23 18:51:00.529
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f481bf37-c553-46b6-a85c-db8a021271f0 09/04/23 18:51:00.55
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:51:00.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7395" for this suite. 09/04/23 18:51:00.579
------------------------------
â€¢ [4.257 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:50:56.334
    Sep  4 18:50:56.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename sched-pred 09/04/23 18:50:56.335
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:50:56.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:50:56.363
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  4 18:50:56.367: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  4 18:50:56.373: INFO: Waiting for terminating namespaces to be deleted...
    Sep  4 18:50:56.376: INFO: 
    Logging pods the apiserver thinks is on node tenant-000001 before test
    Sep  4 18:50:56.385: INFO: pod-init-931b9c2a-c22d-467c-98f8-f1f21058f565 from init-container-9002 started at 2023-09-04 18:50:50 +0000 UTC (1 container statuses recorded)
    Sep  4 18:50:56.385: INFO: 	Container run1 ready: false, restart count 0
    Sep  4 18:50:56.386: INFO: calico-node-dj2mb from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
    Sep  4 18:50:56.386: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 18:50:56.386: INFO: konnectivity-agent-4d7rv from kube-system started at 2023-09-04 18:41:14 +0000 UTC (1 container statuses recorded)
    Sep  4 18:50:56.386: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Sep  4 18:50:56.386: INFO: kube-proxy-959c5 from kube-system started at 2023-09-04 17:14:03 +0000 UTC (1 container statuses recorded)
    Sep  4 18:50:56.386: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 18:50:56.386: INFO: sonobuoy from sonobuoy started at 2023-09-04 17:16:08 +0000 UTC (1 container statuses recorded)
    Sep  4 18:50:56.386: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  4 18:50:56.387: INFO: sonobuoy-e2e-job-8da28a692bc241e2 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
    Sep  4 18:50:56.387: INFO: 	Container e2e ready: true, restart count 0
    Sep  4 18:50:56.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  4 18:50:56.387: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-64p86 from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
    Sep  4 18:50:56.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  4 18:50:56.387: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  4 18:50:56.387: INFO: 
    Logging pods the apiserver thinks is on node tenant-000003 before test
    Sep  4 18:50:56.392: INFO: calico-kube-controllers-5f94594857-4wpdt from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 18:50:56.392: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  4 18:50:56.392: INFO: calico-node-wlgx4 from kube-system started at 2023-09-04 17:15:52 +0000 UTC (1 container statuses recorded)
    Sep  4 18:50:56.392: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 18:50:56.392: INFO: coredns-6bfc4dc876-rnzz9 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 18:50:56.392: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 18:50:56.392: INFO: coredns-6bfc4dc876-xp5ff from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 18:50:56.392: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 18:50:56.392: INFO: konnectivity-agent-j62r4 from kube-system started at 2023-09-04 17:16:05 +0000 UTC (1 container statuses recorded)
    Sep  4 18:50:56.392: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Sep  4 18:50:56.392: INFO: kube-proxy-z2kxt from kube-system started at 2023-09-04 17:15:09 +0000 UTC (1 container statuses recorded)
    Sep  4 18:50:56.392: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 18:50:56.392: INFO: sonobuoy-systemd-logs-daemon-set-a84595d899be4b4c-7wr4k from sonobuoy started at 2023-09-04 17:16:28 +0000 UTC (2 container statuses recorded)
    Sep  4 18:50:56.392: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  4 18:50:56.392: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/04/23 18:50:56.392
    Sep  4 18:50:56.400: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7395" to be "running"
    Sep  4 18:50:56.418: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 17.163187ms
    Sep  4 18:50:58.424: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.023276559s
    Sep  4 18:50:58.424: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/04/23 18:50:58.427
    STEP: Trying to apply a random label on the found node. 09/04/23 18:50:58.456
    STEP: verifying the node has the label kubernetes.io/e2e-f481bf37-c553-46b6-a85c-db8a021271f0 42 09/04/23 18:50:58.493
    STEP: Trying to relaunch the pod, now with labels. 09/04/23 18:50:58.502
    Sep  4 18:50:58.512: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7395" to be "not pending"
    Sep  4 18:50:58.519: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 7.139794ms
    Sep  4 18:51:00.523: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.011469461s
    Sep  4 18:51:00.524: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-f481bf37-c553-46b6-a85c-db8a021271f0 off the node tenant-000001 09/04/23 18:51:00.529
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-f481bf37-c553-46b6-a85c-db8a021271f0 09/04/23 18:51:00.55
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:51:00.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7395" for this suite. 09/04/23 18:51:00.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:51:00.611
Sep  4 18:51:00.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename events 09/04/23 18:51:00.622
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:00.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:00.675
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 09/04/23 18:51:00.682
STEP: listing all events in all namespaces 09/04/23 18:51:00.691
STEP: patching the test event 09/04/23 18:51:00.697
STEP: fetching the test event 09/04/23 18:51:00.739
STEP: updating the test event 09/04/23 18:51:00.744
STEP: getting the test event 09/04/23 18:51:00.769
STEP: deleting the test event 09/04/23 18:51:00.775
STEP: listing all events in all namespaces 09/04/23 18:51:00.786
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Sep  4 18:51:00.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4092" for this suite. 09/04/23 18:51:00.799
------------------------------
â€¢ [0.201 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:51:00.611
    Sep  4 18:51:00.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename events 09/04/23 18:51:00.622
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:00.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:00.675
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 09/04/23 18:51:00.682
    STEP: listing all events in all namespaces 09/04/23 18:51:00.691
    STEP: patching the test event 09/04/23 18:51:00.697
    STEP: fetching the test event 09/04/23 18:51:00.739
    STEP: updating the test event 09/04/23 18:51:00.744
    STEP: getting the test event 09/04/23 18:51:00.769
    STEP: deleting the test event 09/04/23 18:51:00.775
    STEP: listing all events in all namespaces 09/04/23 18:51:00.786
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:51:00.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4092" for this suite. 09/04/23 18:51:00.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:51:00.823
Sep  4 18:51:00.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 18:51:00.825
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:00.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:00.849
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 09/04/23 18:51:00.852
Sep  4 18:51:00.868: INFO: Waiting up to 5m0s for pod "labelsupdate0b264e29-03b9-4fc4-a1b3-f89f11d25a7d" in namespace "projected-9883" to be "running and ready"
Sep  4 18:51:00.880: INFO: Pod "labelsupdate0b264e29-03b9-4fc4-a1b3-f89f11d25a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.718617ms
Sep  4 18:51:00.880: INFO: The phase of Pod labelsupdate0b264e29-03b9-4fc4-a1b3-f89f11d25a7d is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:51:02.897: INFO: Pod "labelsupdate0b264e29-03b9-4fc4-a1b3-f89f11d25a7d": Phase="Running", Reason="", readiness=true. Elapsed: 2.028633583s
Sep  4 18:51:02.897: INFO: The phase of Pod labelsupdate0b264e29-03b9-4fc4-a1b3-f89f11d25a7d is Running (Ready = true)
Sep  4 18:51:02.898: INFO: Pod "labelsupdate0b264e29-03b9-4fc4-a1b3-f89f11d25a7d" satisfied condition "running and ready"
Sep  4 18:51:03.435: INFO: Successfully updated pod "labelsupdate0b264e29-03b9-4fc4-a1b3-f89f11d25a7d"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 18:51:07.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9883" for this suite. 09/04/23 18:51:07.481
------------------------------
â€¢ [SLOW TEST] [6.667 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:51:00.823
    Sep  4 18:51:00.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 18:51:00.825
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:00.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:00.849
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 09/04/23 18:51:00.852
    Sep  4 18:51:00.868: INFO: Waiting up to 5m0s for pod "labelsupdate0b264e29-03b9-4fc4-a1b3-f89f11d25a7d" in namespace "projected-9883" to be "running and ready"
    Sep  4 18:51:00.880: INFO: Pod "labelsupdate0b264e29-03b9-4fc4-a1b3-f89f11d25a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.718617ms
    Sep  4 18:51:00.880: INFO: The phase of Pod labelsupdate0b264e29-03b9-4fc4-a1b3-f89f11d25a7d is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:51:02.897: INFO: Pod "labelsupdate0b264e29-03b9-4fc4-a1b3-f89f11d25a7d": Phase="Running", Reason="", readiness=true. Elapsed: 2.028633583s
    Sep  4 18:51:02.897: INFO: The phase of Pod labelsupdate0b264e29-03b9-4fc4-a1b3-f89f11d25a7d is Running (Ready = true)
    Sep  4 18:51:02.898: INFO: Pod "labelsupdate0b264e29-03b9-4fc4-a1b3-f89f11d25a7d" satisfied condition "running and ready"
    Sep  4 18:51:03.435: INFO: Successfully updated pod "labelsupdate0b264e29-03b9-4fc4-a1b3-f89f11d25a7d"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:51:07.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9883" for this suite. 09/04/23 18:51:07.481
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:51:07.494
Sep  4 18:51:07.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename kubectl 09/04/23 18:51:07.496
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:07.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:07.52
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 09/04/23 18:51:07.523
Sep  4 18:51:07.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8483 create -f -'
Sep  4 18:51:08.368: INFO: stderr: ""
Sep  4 18:51:08.368: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 09/04/23 18:51:08.368
Sep  4 18:51:08.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8483 diff -f -'
Sep  4 18:51:09.441: INFO: rc: 1
Sep  4 18:51:09.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8483 delete -f -'
Sep  4 18:51:09.529: INFO: stderr: ""
Sep  4 18:51:09.529: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 18:51:09.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8483" for this suite. 09/04/23 18:51:09.537
------------------------------
â€¢ [2.050 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:51:07.494
    Sep  4 18:51:07.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename kubectl 09/04/23 18:51:07.496
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:07.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:07.52
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 09/04/23 18:51:07.523
    Sep  4 18:51:07.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8483 create -f -'
    Sep  4 18:51:08.368: INFO: stderr: ""
    Sep  4 18:51:08.368: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 09/04/23 18:51:08.368
    Sep  4 18:51:08.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8483 diff -f -'
    Sep  4 18:51:09.441: INFO: rc: 1
    Sep  4 18:51:09.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=kubectl-8483 delete -f -'
    Sep  4 18:51:09.529: INFO: stderr: ""
    Sep  4 18:51:09.529: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:51:09.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8483" for this suite. 09/04/23 18:51:09.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:51:09.545
Sep  4 18:51:09.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 18:51:09.546
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:09.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:09.582
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-0e8eda84-4959-48b2-979d-81ecd3caa05e 09/04/23 18:51:09.585
STEP: Creating a pod to test consume secrets 09/04/23 18:51:09.591
Sep  4 18:51:09.606: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2" in namespace "projected-3759" to be "Succeeded or Failed"
Sep  4 18:51:09.610: INFO: Pod "pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.428525ms
Sep  4 18:51:11.616: INFO: Pod "pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009797362s
Sep  4 18:51:13.619: INFO: Pod "pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012292946s
STEP: Saw pod success 09/04/23 18:51:13.619
Sep  4 18:51:13.619: INFO: Pod "pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2" satisfied condition "Succeeded or Failed"
Sep  4 18:51:13.624: INFO: Trying to get logs from node tenant-000003 pod pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/04/23 18:51:13.69
Sep  4 18:51:13.711: INFO: Waiting for pod pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2 to disappear
Sep  4 18:51:13.715: INFO: Pod pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  4 18:51:13.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3759" for this suite. 09/04/23 18:51:13.722
------------------------------
â€¢ [4.185 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:51:09.545
    Sep  4 18:51:09.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 18:51:09.546
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:09.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:09.582
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-0e8eda84-4959-48b2-979d-81ecd3caa05e 09/04/23 18:51:09.585
    STEP: Creating a pod to test consume secrets 09/04/23 18:51:09.591
    Sep  4 18:51:09.606: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2" in namespace "projected-3759" to be "Succeeded or Failed"
    Sep  4 18:51:09.610: INFO: Pod "pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.428525ms
    Sep  4 18:51:11.616: INFO: Pod "pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009797362s
    Sep  4 18:51:13.619: INFO: Pod "pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012292946s
    STEP: Saw pod success 09/04/23 18:51:13.619
    Sep  4 18:51:13.619: INFO: Pod "pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2" satisfied condition "Succeeded or Failed"
    Sep  4 18:51:13.624: INFO: Trying to get logs from node tenant-000003 pod pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 18:51:13.69
    Sep  4 18:51:13.711: INFO: Waiting for pod pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2 to disappear
    Sep  4 18:51:13.715: INFO: Pod pod-projected-secrets-e27928cf-ef87-404b-9c7f-37b0547e81a2 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:51:13.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3759" for this suite. 09/04/23 18:51:13.722
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:51:13.733
Sep  4 18:51:13.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename configmap 09/04/23 18:51:13.735
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:13.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:13.755
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 18:51:13.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9269" for this suite. 09/04/23 18:51:13.812
------------------------------
â€¢ [0.085 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:51:13.733
    Sep  4 18:51:13.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename configmap 09/04/23 18:51:13.735
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:13.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:13.755
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:51:13.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9269" for this suite. 09/04/23 18:51:13.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:51:13.824
Sep  4 18:51:13.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename deployment 09/04/23 18:51:13.826
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:13.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:13.852
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 09/04/23 18:51:13.857
Sep  4 18:51:13.857: INFO: Creating simple deployment test-deployment-mlj2l
Sep  4 18:51:13.884: INFO: deployment "test-deployment-mlj2l" doesn't have the required revision set
STEP: Getting /status 09/04/23 18:51:15.901
Sep  4 18:51:15.905: INFO: Deployment test-deployment-mlj2l has Conditions: [{Available True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlj2l-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 09/04/23 18:51:15.905
Sep  4 18:51:15.916: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 51, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 51, 13, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-mlj2l-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 09/04/23 18:51:15.916
Sep  4 18:51:15.918: INFO: Observed &Deployment event: ADDED
Sep  4 18:51:15.919: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlj2l-54bc444df"}
Sep  4 18:51:15.919: INFO: Observed &Deployment event: MODIFIED
Sep  4 18:51:15.919: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlj2l-54bc444df"}
Sep  4 18:51:15.919: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  4 18:51:15.920: INFO: Observed &Deployment event: MODIFIED
Sep  4 18:51:15.920: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  4 18:51:15.920: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mlj2l-54bc444df" is progressing.}
Sep  4 18:51:15.920: INFO: Observed &Deployment event: MODIFIED
Sep  4 18:51:15.921: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  4 18:51:15.921: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlj2l-54bc444df" has successfully progressed.}
Sep  4 18:51:15.921: INFO: Observed &Deployment event: MODIFIED
Sep  4 18:51:15.921: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  4 18:51:15.921: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlj2l-54bc444df" has successfully progressed.}
Sep  4 18:51:15.921: INFO: Found Deployment test-deployment-mlj2l in namespace deployment-5183 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  4 18:51:15.922: INFO: Deployment test-deployment-mlj2l has an updated status
STEP: patching the Statefulset Status 09/04/23 18:51:15.922
Sep  4 18:51:15.922: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  4 18:51:15.934: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 09/04/23 18:51:15.934
Sep  4 18:51:15.936: INFO: Observed &Deployment event: ADDED
Sep  4 18:51:15.936: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlj2l-54bc444df"}
Sep  4 18:51:15.937: INFO: Observed &Deployment event: MODIFIED
Sep  4 18:51:15.937: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlj2l-54bc444df"}
Sep  4 18:51:15.937: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  4 18:51:15.937: INFO: Observed &Deployment event: MODIFIED
Sep  4 18:51:15.938: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  4 18:51:15.938: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mlj2l-54bc444df" is progressing.}
Sep  4 18:51:15.938: INFO: Observed &Deployment event: MODIFIED
Sep  4 18:51:15.939: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  4 18:51:15.939: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlj2l-54bc444df" has successfully progressed.}
Sep  4 18:51:15.939: INFO: Observed &Deployment event: MODIFIED
Sep  4 18:51:15.939: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  4 18:51:15.939: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlj2l-54bc444df" has successfully progressed.}
Sep  4 18:51:15.940: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  4 18:51:15.940: INFO: Observed &Deployment event: MODIFIED
Sep  4 18:51:15.940: INFO: Found deployment test-deployment-mlj2l in namespace deployment-5183 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Sep  4 18:51:15.940: INFO: Deployment test-deployment-mlj2l has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 18:51:15.943: INFO: Deployment "test-deployment-mlj2l":
&Deployment{ObjectMeta:{test-deployment-mlj2l  deployment-5183  9b2fdc01-42a6-4029-bd06-c63ccc3580a8 37346 1 2023-09-04 18:51:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-04 18:51:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-09-04 18:51:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-09-04 18:51:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e53018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-mlj2l-54bc444df",LastUpdateTime:2023-09-04 18:51:15 +0000 UTC,LastTransitionTime:2023-09-04 18:51:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  4 18:51:15.947: INFO: New ReplicaSet "test-deployment-mlj2l-54bc444df" of Deployment "test-deployment-mlj2l":
&ReplicaSet{ObjectMeta:{test-deployment-mlj2l-54bc444df  deployment-5183  f10ea6e7-c101-4e5b-bf26-f0880f328f88 37341 1 2023-09-04 18:51:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-mlj2l 9b2fdc01-42a6-4029-bd06-c63ccc3580a8 0xc003e53400 0xc003e53401}] [] [{kube-controller-manager Update apps/v1 2023-09-04 18:51:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9b2fdc01-42a6-4029-bd06-c63ccc3580a8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:51:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e534a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  4 18:51:15.950: INFO: Pod "test-deployment-mlj2l-54bc444df-ppbbv" is available:
&Pod{ObjectMeta:{test-deployment-mlj2l-54bc444df-ppbbv test-deployment-mlj2l-54bc444df- deployment-5183  d8822fa5-3b50-4f87-ba61-eec5add528f9 37340 0 2023-09-04 18:51:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:34110c3471186bbb056bcb96ba99bc8431814e9984e3298fd35666b8f73c9b84 cni.projectcalico.org/podIP:10.36.55.119/32 cni.projectcalico.org/podIPs:10.36.55.119/32] [{apps/v1 ReplicaSet test-deployment-mlj2l-54bc444df f10ea6e7-c101-4e5b-bf26-f0880f328f88 0xc003e53870 0xc003e53871}] [] [{kube-controller-manager Update v1 2023-09-04 18:51:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f10ea6e7-c101-4e5b-bf26-f0880f328f88\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 18:51:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 18:51:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p6rvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p6rvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:51:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:51:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:51:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:51:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.119,StartTime:2023-09-04 18:51:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 18:51:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://492a238d54d43f4807749dc0b7c8ba29b7f058576f8a8472f556d7ea6ce95075,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 18:51:15.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5183" for this suite. 09/04/23 18:51:15.956
------------------------------
â€¢ [2.138 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:51:13.824
    Sep  4 18:51:13.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename deployment 09/04/23 18:51:13.826
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:13.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:13.852
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 09/04/23 18:51:13.857
    Sep  4 18:51:13.857: INFO: Creating simple deployment test-deployment-mlj2l
    Sep  4 18:51:13.884: INFO: deployment "test-deployment-mlj2l" doesn't have the required revision set
    STEP: Getting /status 09/04/23 18:51:15.901
    Sep  4 18:51:15.905: INFO: Deployment test-deployment-mlj2l has Conditions: [{Available True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlj2l-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 09/04/23 18:51:15.905
    Sep  4 18:51:15.916: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 51, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 18, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 18, 51, 13, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-mlj2l-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 09/04/23 18:51:15.916
    Sep  4 18:51:15.918: INFO: Observed &Deployment event: ADDED
    Sep  4 18:51:15.919: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlj2l-54bc444df"}
    Sep  4 18:51:15.919: INFO: Observed &Deployment event: MODIFIED
    Sep  4 18:51:15.919: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlj2l-54bc444df"}
    Sep  4 18:51:15.919: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  4 18:51:15.920: INFO: Observed &Deployment event: MODIFIED
    Sep  4 18:51:15.920: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  4 18:51:15.920: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mlj2l-54bc444df" is progressing.}
    Sep  4 18:51:15.920: INFO: Observed &Deployment event: MODIFIED
    Sep  4 18:51:15.921: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  4 18:51:15.921: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlj2l-54bc444df" has successfully progressed.}
    Sep  4 18:51:15.921: INFO: Observed &Deployment event: MODIFIED
    Sep  4 18:51:15.921: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  4 18:51:15.921: INFO: Observed Deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlj2l-54bc444df" has successfully progressed.}
    Sep  4 18:51:15.921: INFO: Found Deployment test-deployment-mlj2l in namespace deployment-5183 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  4 18:51:15.922: INFO: Deployment test-deployment-mlj2l has an updated status
    STEP: patching the Statefulset Status 09/04/23 18:51:15.922
    Sep  4 18:51:15.922: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  4 18:51:15.934: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 09/04/23 18:51:15.934
    Sep  4 18:51:15.936: INFO: Observed &Deployment event: ADDED
    Sep  4 18:51:15.936: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlj2l-54bc444df"}
    Sep  4 18:51:15.937: INFO: Observed &Deployment event: MODIFIED
    Sep  4 18:51:15.937: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mlj2l-54bc444df"}
    Sep  4 18:51:15.937: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  4 18:51:15.937: INFO: Observed &Deployment event: MODIFIED
    Sep  4 18:51:15.938: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  4 18:51:15.938: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:13 +0000 UTC 2023-09-04 18:51:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mlj2l-54bc444df" is progressing.}
    Sep  4 18:51:15.938: INFO: Observed &Deployment event: MODIFIED
    Sep  4 18:51:15.939: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  4 18:51:15.939: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlj2l-54bc444df" has successfully progressed.}
    Sep  4 18:51:15.939: INFO: Observed &Deployment event: MODIFIED
    Sep  4 18:51:15.939: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  4 18:51:15.939: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 18:51:15 +0000 UTC 2023-09-04 18:51:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mlj2l-54bc444df" has successfully progressed.}
    Sep  4 18:51:15.940: INFO: Observed deployment test-deployment-mlj2l in namespace deployment-5183 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  4 18:51:15.940: INFO: Observed &Deployment event: MODIFIED
    Sep  4 18:51:15.940: INFO: Found deployment test-deployment-mlj2l in namespace deployment-5183 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Sep  4 18:51:15.940: INFO: Deployment test-deployment-mlj2l has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 18:51:15.943: INFO: Deployment "test-deployment-mlj2l":
    &Deployment{ObjectMeta:{test-deployment-mlj2l  deployment-5183  9b2fdc01-42a6-4029-bd06-c63ccc3580a8 37346 1 2023-09-04 18:51:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-04 18:51:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-09-04 18:51:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-09-04 18:51:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e53018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-mlj2l-54bc444df",LastUpdateTime:2023-09-04 18:51:15 +0000 UTC,LastTransitionTime:2023-09-04 18:51:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  4 18:51:15.947: INFO: New ReplicaSet "test-deployment-mlj2l-54bc444df" of Deployment "test-deployment-mlj2l":
    &ReplicaSet{ObjectMeta:{test-deployment-mlj2l-54bc444df  deployment-5183  f10ea6e7-c101-4e5b-bf26-f0880f328f88 37341 1 2023-09-04 18:51:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-mlj2l 9b2fdc01-42a6-4029-bd06-c63ccc3580a8 0xc003e53400 0xc003e53401}] [] [{kube-controller-manager Update apps/v1 2023-09-04 18:51:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9b2fdc01-42a6-4029-bd06-c63ccc3580a8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 18:51:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e534a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 18:51:15.950: INFO: Pod "test-deployment-mlj2l-54bc444df-ppbbv" is available:
    &Pod{ObjectMeta:{test-deployment-mlj2l-54bc444df-ppbbv test-deployment-mlj2l-54bc444df- deployment-5183  d8822fa5-3b50-4f87-ba61-eec5add528f9 37340 0 2023-09-04 18:51:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:34110c3471186bbb056bcb96ba99bc8431814e9984e3298fd35666b8f73c9b84 cni.projectcalico.org/podIP:10.36.55.119/32 cni.projectcalico.org/podIPs:10.36.55.119/32] [{apps/v1 ReplicaSet test-deployment-mlj2l-54bc444df f10ea6e7-c101-4e5b-bf26-f0880f328f88 0xc003e53870 0xc003e53871}] [] [{kube-controller-manager Update v1 2023-09-04 18:51:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f10ea6e7-c101-4e5b-bf26-f0880f328f88\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 18:51:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 18:51:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.36.55.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p6rvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p6rvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tenant-000001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:51:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:51:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:51:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 18:51:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.225.0.5,PodIP:10.36.55.119,StartTime:2023-09-04 18:51:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 18:51:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://492a238d54d43f4807749dc0b7c8ba29b7f058576f8a8472f556d7ea6ce95075,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.36.55.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:51:15.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5183" for this suite. 09/04/23 18:51:15.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:51:15.97
Sep  4 18:51:15.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename projected 09/04/23 18:51:15.971
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:15.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:15.998
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-21295868-ebd9-4492-ac33-afedb63048e4 09/04/23 18:51:16.008
STEP: Creating the pod 09/04/23 18:51:16.013
Sep  4 18:51:16.028: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-436cfc59-8dce-4e5d-bb81-6780462ed21a" in namespace "projected-819" to be "running and ready"
Sep  4 18:51:16.041: INFO: Pod "pod-projected-configmaps-436cfc59-8dce-4e5d-bb81-6780462ed21a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.899502ms
Sep  4 18:51:16.041: INFO: The phase of Pod pod-projected-configmaps-436cfc59-8dce-4e5d-bb81-6780462ed21a is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:51:18.046: INFO: Pod "pod-projected-configmaps-436cfc59-8dce-4e5d-bb81-6780462ed21a": Phase="Running", Reason="", readiness=true. Elapsed: 2.01743739s
Sep  4 18:51:18.046: INFO: The phase of Pod pod-projected-configmaps-436cfc59-8dce-4e5d-bb81-6780462ed21a is Running (Ready = true)
Sep  4 18:51:18.047: INFO: Pod "pod-projected-configmaps-436cfc59-8dce-4e5d-bb81-6780462ed21a" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-21295868-ebd9-4492-ac33-afedb63048e4 09/04/23 18:51:18.062
STEP: waiting to observe update in volume 09/04/23 18:51:18.067
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 18:51:20.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-819" for this suite. 09/04/23 18:51:20.094
------------------------------
â€¢ [4.136 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:51:15.97
    Sep  4 18:51:15.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename projected 09/04/23 18:51:15.971
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:15.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:15.998
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-21295868-ebd9-4492-ac33-afedb63048e4 09/04/23 18:51:16.008
    STEP: Creating the pod 09/04/23 18:51:16.013
    Sep  4 18:51:16.028: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-436cfc59-8dce-4e5d-bb81-6780462ed21a" in namespace "projected-819" to be "running and ready"
    Sep  4 18:51:16.041: INFO: Pod "pod-projected-configmaps-436cfc59-8dce-4e5d-bb81-6780462ed21a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.899502ms
    Sep  4 18:51:16.041: INFO: The phase of Pod pod-projected-configmaps-436cfc59-8dce-4e5d-bb81-6780462ed21a is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:51:18.046: INFO: Pod "pod-projected-configmaps-436cfc59-8dce-4e5d-bb81-6780462ed21a": Phase="Running", Reason="", readiness=true. Elapsed: 2.01743739s
    Sep  4 18:51:18.046: INFO: The phase of Pod pod-projected-configmaps-436cfc59-8dce-4e5d-bb81-6780462ed21a is Running (Ready = true)
    Sep  4 18:51:18.047: INFO: Pod "pod-projected-configmaps-436cfc59-8dce-4e5d-bb81-6780462ed21a" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-21295868-ebd9-4492-ac33-afedb63048e4 09/04/23 18:51:18.062
    STEP: waiting to observe update in volume 09/04/23 18:51:18.067
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:51:20.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-819" for this suite. 09/04/23 18:51:20.094
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:51:20.113
Sep  4 18:51:20.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename gc 09/04/23 18:51:20.114
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:20.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:20.163
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 09/04/23 18:51:20.166
STEP: Wait for the Deployment to create new ReplicaSet 09/04/23 18:51:20.174
STEP: delete the deployment 09/04/23 18:51:20.295
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/04/23 18:51:20.306
STEP: Gathering metrics 09/04/23 18:51:20.935
W0904 18:51:20.943488      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep  4 18:51:20.944: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  4 18:51:20.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3162" for this suite. 09/04/23 18:51:20.951
------------------------------
â€¢ [0.855 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:51:20.113
    Sep  4 18:51:20.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename gc 09/04/23 18:51:20.114
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:20.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:20.163
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 09/04/23 18:51:20.166
    STEP: Wait for the Deployment to create new ReplicaSet 09/04/23 18:51:20.174
    STEP: delete the deployment 09/04/23 18:51:20.295
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/04/23 18:51:20.306
    STEP: Gathering metrics 09/04/23 18:51:20.935
    W0904 18:51:20.943488      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep  4 18:51:20.944: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:51:20.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3162" for this suite. 09/04/23 18:51:20.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:51:20.974
Sep  4 18:51:20.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 18:51:20.978
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:21.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:21.011
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 18:51:21.034
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:51:21.845
STEP: Deploying the webhook pod 09/04/23 18:51:21.856
STEP: Wait for the deployment to be ready 09/04/23 18:51:21.884
Sep  4 18:51:21.902: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 18:51:23.917
STEP: Verifying the service has paired with the endpoint 09/04/23 18:51:23.934
Sep  4 18:51:24.937: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/04/23 18:51:24.944
Sep  4 18:51:24.971: INFO: Waiting for webhook configuration to be ready...
STEP: create a configmap that should be updated by the webhook 09/04/23 18:51:25.09
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:51:25.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2920" for this suite. 09/04/23 18:51:25.193
STEP: Destroying namespace "webhook-2920-markers" for this suite. 09/04/23 18:51:25.215
------------------------------
â€¢ [4.264 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:51:20.974
    Sep  4 18:51:20.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 18:51:20.978
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:21.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:21.011
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 18:51:21.034
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:51:21.845
    STEP: Deploying the webhook pod 09/04/23 18:51:21.856
    STEP: Wait for the deployment to be ready 09/04/23 18:51:21.884
    Sep  4 18:51:21.902: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 18:51:23.917
    STEP: Verifying the service has paired with the endpoint 09/04/23 18:51:23.934
    Sep  4 18:51:24.937: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/04/23 18:51:24.944
    Sep  4 18:51:24.971: INFO: Waiting for webhook configuration to be ready...
    STEP: create a configmap that should be updated by the webhook 09/04/23 18:51:25.09
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:51:25.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2920" for this suite. 09/04/23 18:51:25.193
    STEP: Destroying namespace "webhook-2920-markers" for this suite. 09/04/23 18:51:25.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:51:25.259
Sep  4 18:51:25.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename disruption 09/04/23 18:51:25.26
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:25.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:25.298
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 09/04/23 18:51:25.3
STEP: Waiting for the pdb to be processed 09/04/23 18:51:25.306
STEP: updating the pdb 09/04/23 18:51:25.321
STEP: Waiting for the pdb to be processed 09/04/23 18:51:25.344
STEP: patching the pdb 09/04/23 18:51:25.347
STEP: Waiting for the pdb to be processed 09/04/23 18:51:25.4
STEP: Waiting for the pdb to be deleted 09/04/23 18:51:27.421
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  4 18:51:27.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5251" for this suite. 09/04/23 18:51:27.428
------------------------------
â€¢ [2.180 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:51:25.259
    Sep  4 18:51:25.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename disruption 09/04/23 18:51:25.26
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:25.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:25.298
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 09/04/23 18:51:25.3
    STEP: Waiting for the pdb to be processed 09/04/23 18:51:25.306
    STEP: updating the pdb 09/04/23 18:51:25.321
    STEP: Waiting for the pdb to be processed 09/04/23 18:51:25.344
    STEP: patching the pdb 09/04/23 18:51:25.347
    STEP: Waiting for the pdb to be processed 09/04/23 18:51:25.4
    STEP: Waiting for the pdb to be deleted 09/04/23 18:51:27.421
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:51:27.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5251" for this suite. 09/04/23 18:51:27.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:51:27.442
Sep  4 18:51:27.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename downward-api 09/04/23 18:51:27.444
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:27.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:27.47
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 09/04/23 18:51:27.473
Sep  4 18:51:27.484: INFO: Waiting up to 5m0s for pod "downward-api-63442272-02bb-47e3-80da-1f11f4f6e875" in namespace "downward-api-7041" to be "Succeeded or Failed"
Sep  4 18:51:27.491: INFO: Pod "downward-api-63442272-02bb-47e3-80da-1f11f4f6e875": Phase="Pending", Reason="", readiness=false. Elapsed: 6.716852ms
Sep  4 18:51:29.495: INFO: Pod "downward-api-63442272-02bb-47e3-80da-1f11f4f6e875": Phase="Running", Reason="", readiness=false. Elapsed: 2.011503863s
Sep  4 18:51:31.497: INFO: Pod "downward-api-63442272-02bb-47e3-80da-1f11f4f6e875": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013393279s
STEP: Saw pod success 09/04/23 18:51:31.497
Sep  4 18:51:31.498: INFO: Pod "downward-api-63442272-02bb-47e3-80da-1f11f4f6e875" satisfied condition "Succeeded or Failed"
Sep  4 18:51:31.501: INFO: Trying to get logs from node tenant-000001 pod downward-api-63442272-02bb-47e3-80da-1f11f4f6e875 container dapi-container: <nil>
STEP: delete the pod 09/04/23 18:51:31.512
Sep  4 18:51:31.534: INFO: Waiting for pod downward-api-63442272-02bb-47e3-80da-1f11f4f6e875 to disappear
Sep  4 18:51:31.537: INFO: Pod downward-api-63442272-02bb-47e3-80da-1f11f4f6e875 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  4 18:51:31.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7041" for this suite. 09/04/23 18:51:31.545
------------------------------
â€¢ [4.113 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:51:27.442
    Sep  4 18:51:27.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename downward-api 09/04/23 18:51:27.444
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:27.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:27.47
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 09/04/23 18:51:27.473
    Sep  4 18:51:27.484: INFO: Waiting up to 5m0s for pod "downward-api-63442272-02bb-47e3-80da-1f11f4f6e875" in namespace "downward-api-7041" to be "Succeeded or Failed"
    Sep  4 18:51:27.491: INFO: Pod "downward-api-63442272-02bb-47e3-80da-1f11f4f6e875": Phase="Pending", Reason="", readiness=false. Elapsed: 6.716852ms
    Sep  4 18:51:29.495: INFO: Pod "downward-api-63442272-02bb-47e3-80da-1f11f4f6e875": Phase="Running", Reason="", readiness=false. Elapsed: 2.011503863s
    Sep  4 18:51:31.497: INFO: Pod "downward-api-63442272-02bb-47e3-80da-1f11f4f6e875": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013393279s
    STEP: Saw pod success 09/04/23 18:51:31.497
    Sep  4 18:51:31.498: INFO: Pod "downward-api-63442272-02bb-47e3-80da-1f11f4f6e875" satisfied condition "Succeeded or Failed"
    Sep  4 18:51:31.501: INFO: Trying to get logs from node tenant-000001 pod downward-api-63442272-02bb-47e3-80da-1f11f4f6e875 container dapi-container: <nil>
    STEP: delete the pod 09/04/23 18:51:31.512
    Sep  4 18:51:31.534: INFO: Waiting for pod downward-api-63442272-02bb-47e3-80da-1f11f4f6e875 to disappear
    Sep  4 18:51:31.537: INFO: Pod downward-api-63442272-02bb-47e3-80da-1f11f4f6e875 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:51:31.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7041" for this suite. 09/04/23 18:51:31.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:51:31.562
Sep  4 18:51:31.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename job 09/04/23 18:51:31.563
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:31.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:31.587
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 09/04/23 18:51:31.589
STEP: Ensuring active pods == parallelism 09/04/23 18:51:31.596
STEP: delete a job 09/04/23 18:51:33.6
STEP: deleting Job.batch foo in namespace job-1902, will wait for the garbage collector to delete the pods 09/04/23 18:51:33.601
Sep  4 18:51:33.665: INFO: Deleting Job.batch foo took: 9.207801ms
Sep  4 18:51:33.766: INFO: Terminating Job.batch foo pods took: 101.116102ms
STEP: Ensuring job was deleted 09/04/23 18:52:06.667
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  4 18:52:06.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1902" for this suite. 09/04/23 18:52:06.682
------------------------------
â€¢ [SLOW TEST] [35.132 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:51:31.562
    Sep  4 18:51:31.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename job 09/04/23 18:51:31.563
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:51:31.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:51:31.587
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 09/04/23 18:51:31.589
    STEP: Ensuring active pods == parallelism 09/04/23 18:51:31.596
    STEP: delete a job 09/04/23 18:51:33.6
    STEP: deleting Job.batch foo in namespace job-1902, will wait for the garbage collector to delete the pods 09/04/23 18:51:33.601
    Sep  4 18:51:33.665: INFO: Deleting Job.batch foo took: 9.207801ms
    Sep  4 18:51:33.766: INFO: Terminating Job.batch foo pods took: 101.116102ms
    STEP: Ensuring job was deleted 09/04/23 18:52:06.667
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:52:06.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1902" for this suite. 09/04/23 18:52:06.682
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:52:06.697
Sep  4 18:52:06.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename svcaccounts 09/04/23 18:52:06.699
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:52:06.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:52:06.719
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 09/04/23 18:52:06.721
STEP: watching for the ServiceAccount to be added 09/04/23 18:52:06.738
STEP: patching the ServiceAccount 09/04/23 18:52:06.74
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/04/23 18:52:06.746
STEP: deleting the ServiceAccount 09/04/23 18:52:06.752
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  4 18:52:06.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8023" for this suite. 09/04/23 18:52:06.793
------------------------------
â€¢ [0.106 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:52:06.697
    Sep  4 18:52:06.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename svcaccounts 09/04/23 18:52:06.699
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:52:06.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:52:06.719
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 09/04/23 18:52:06.721
    STEP: watching for the ServiceAccount to be added 09/04/23 18:52:06.738
    STEP: patching the ServiceAccount 09/04/23 18:52:06.74
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/04/23 18:52:06.746
    STEP: deleting the ServiceAccount 09/04/23 18:52:06.752
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:52:06.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8023" for this suite. 09/04/23 18:52:06.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:52:06.855
Sep  4 18:52:06.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename services 09/04/23 18:52:06.856
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:52:06.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:52:06.892
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-7500 09/04/23 18:52:06.896
STEP: creating service affinity-clusterip in namespace services-7500 09/04/23 18:52:06.896
STEP: creating replication controller affinity-clusterip in namespace services-7500 09/04/23 18:52:06.921
I0904 18:52:06.934455      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7500, replica count: 3
I0904 18:52:09.995940      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 18:52:10.005: INFO: Creating new exec pod
Sep  4 18:52:10.014: INFO: Waiting up to 5m0s for pod "execpod-affinitydwxv4" in namespace "services-7500" to be "running"
Sep  4 18:52:10.018: INFO: Pod "execpod-affinitydwxv4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.245818ms
Sep  4 18:52:12.026: INFO: Pod "execpod-affinitydwxv4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011332718s
Sep  4 18:52:12.026: INFO: Pod "execpod-affinitydwxv4" satisfied condition "running"
Sep  4 18:52:13.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7500 exec execpod-affinitydwxv4 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Sep  4 18:52:13.217: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep  4 18:52:13.217: INFO: stdout: ""
Sep  4 18:52:13.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7500 exec execpod-affinitydwxv4 -- /bin/sh -x -c nc -v -z -w 2 10.96.137.180 80'
Sep  4 18:52:13.386: INFO: stderr: "+ nc -v -z -w 2 10.96.137.180 80\nConnection to 10.96.137.180 80 port [tcp/http] succeeded!\n"
Sep  4 18:52:13.386: INFO: stdout: ""
Sep  4 18:52:13.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7500 exec execpod-affinitydwxv4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.137.180:80/ ; done'
Sep  4 18:52:13.738: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n"
Sep  4 18:52:13.738: INFO: stdout: "\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc"
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
Sep  4 18:52:13.738: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7500, will wait for the garbage collector to delete the pods 09/04/23 18:52:13.766
Sep  4 18:52:13.854: INFO: Deleting ReplicationController affinity-clusterip took: 7.055301ms
Sep  4 18:52:14.058: INFO: Terminating ReplicationController affinity-clusterip pods took: 204.127307ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 18:52:16.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7500" for this suite. 09/04/23 18:52:16.788
------------------------------
â€¢ [SLOW TEST] [9.945 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:52:06.855
    Sep  4 18:52:06.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename services 09/04/23 18:52:06.856
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:52:06.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:52:06.892
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-7500 09/04/23 18:52:06.896
    STEP: creating service affinity-clusterip in namespace services-7500 09/04/23 18:52:06.896
    STEP: creating replication controller affinity-clusterip in namespace services-7500 09/04/23 18:52:06.921
    I0904 18:52:06.934455      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7500, replica count: 3
    I0904 18:52:09.995940      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 18:52:10.005: INFO: Creating new exec pod
    Sep  4 18:52:10.014: INFO: Waiting up to 5m0s for pod "execpod-affinitydwxv4" in namespace "services-7500" to be "running"
    Sep  4 18:52:10.018: INFO: Pod "execpod-affinitydwxv4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.245818ms
    Sep  4 18:52:12.026: INFO: Pod "execpod-affinitydwxv4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011332718s
    Sep  4 18:52:12.026: INFO: Pod "execpod-affinitydwxv4" satisfied condition "running"
    Sep  4 18:52:13.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7500 exec execpod-affinitydwxv4 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Sep  4 18:52:13.217: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Sep  4 18:52:13.217: INFO: stdout: ""
    Sep  4 18:52:13.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7500 exec execpod-affinitydwxv4 -- /bin/sh -x -c nc -v -z -w 2 10.96.137.180 80'
    Sep  4 18:52:13.386: INFO: stderr: "+ nc -v -z -w 2 10.96.137.180 80\nConnection to 10.96.137.180 80 port [tcp/http] succeeded!\n"
    Sep  4 18:52:13.386: INFO: stdout: ""
    Sep  4 18:52:13.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3116509839 --namespace=services-7500 exec execpod-affinitydwxv4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.137.180:80/ ; done'
    Sep  4 18:52:13.738: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.137.180:80/\n"
    Sep  4 18:52:13.738: INFO: stdout: "\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc\naffinity-clusterip-gl4zc"
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Received response from host: affinity-clusterip-gl4zc
    Sep  4 18:52:13.738: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-7500, will wait for the garbage collector to delete the pods 09/04/23 18:52:13.766
    Sep  4 18:52:13.854: INFO: Deleting ReplicationController affinity-clusterip took: 7.055301ms
    Sep  4 18:52:14.058: INFO: Terminating ReplicationController affinity-clusterip pods took: 204.127307ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:52:16.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7500" for this suite. 09/04/23 18:52:16.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:52:16.805
Sep  4 18:52:16.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename emptydir 09/04/23 18:52:16.807
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:52:16.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:52:16.834
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 09/04/23 18:52:16.837
Sep  4 18:52:16.847: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-b097b919-aebb-466b-9dff-9b7d824c65be" in namespace "emptydir-8438" to be "running"
Sep  4 18:52:16.871: INFO: Pod "pod-sharedvolume-b097b919-aebb-466b-9dff-9b7d824c65be": Phase="Pending", Reason="", readiness=false. Elapsed: 23.707385ms
Sep  4 18:52:18.875: INFO: Pod "pod-sharedvolume-b097b919-aebb-466b-9dff-9b7d824c65be": Phase="Running", Reason="", readiness=false. Elapsed: 2.028084034s
Sep  4 18:52:18.875: INFO: Pod "pod-sharedvolume-b097b919-aebb-466b-9dff-9b7d824c65be" satisfied condition "running"
STEP: Reading file content from the nginx-container 09/04/23 18:52:18.875
Sep  4 18:52:18.876: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8438 PodName:pod-sharedvolume-b097b919-aebb-466b-9dff-9b7d824c65be ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 18:52:18.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
Sep  4 18:52:18.876: INFO: ExecWithOptions: Clientset creation
Sep  4 18:52:18.876: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8438/pods/pod-sharedvolume-b097b919-aebb-466b-9dff-9b7d824c65be/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Sep  4 18:52:18.982: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 18:52:18.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8438" for this suite. 09/04/23 18:52:18.987
------------------------------
â€¢ [2.195 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:52:16.805
    Sep  4 18:52:16.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename emptydir 09/04/23 18:52:16.807
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:52:16.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:52:16.834
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 09/04/23 18:52:16.837
    Sep  4 18:52:16.847: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-b097b919-aebb-466b-9dff-9b7d824c65be" in namespace "emptydir-8438" to be "running"
    Sep  4 18:52:16.871: INFO: Pod "pod-sharedvolume-b097b919-aebb-466b-9dff-9b7d824c65be": Phase="Pending", Reason="", readiness=false. Elapsed: 23.707385ms
    Sep  4 18:52:18.875: INFO: Pod "pod-sharedvolume-b097b919-aebb-466b-9dff-9b7d824c65be": Phase="Running", Reason="", readiness=false. Elapsed: 2.028084034s
    Sep  4 18:52:18.875: INFO: Pod "pod-sharedvolume-b097b919-aebb-466b-9dff-9b7d824c65be" satisfied condition "running"
    STEP: Reading file content from the nginx-container 09/04/23 18:52:18.875
    Sep  4 18:52:18.876: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8438 PodName:pod-sharedvolume-b097b919-aebb-466b-9dff-9b7d824c65be ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 18:52:18.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    Sep  4 18:52:18.876: INFO: ExecWithOptions: Clientset creation
    Sep  4 18:52:18.876: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8438/pods/pod-sharedvolume-b097b919-aebb-466b-9dff-9b7d824c65be/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Sep  4 18:52:18.982: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:52:18.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8438" for this suite. 09/04/23 18:52:18.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:52:19.004
Sep  4 18:52:19.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename events 09/04/23 18:52:19.005
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:52:19.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:52:19.029
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 09/04/23 18:52:19.031
STEP: get a list of Events with a label in the current namespace 09/04/23 18:52:19.057
STEP: delete a list of events 09/04/23 18:52:19.06
Sep  4 18:52:19.061: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/04/23 18:52:19.092
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Sep  4 18:52:19.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2895" for this suite. 09/04/23 18:52:19.102
------------------------------
â€¢ [0.108 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:52:19.004
    Sep  4 18:52:19.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename events 09/04/23 18:52:19.005
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:52:19.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:52:19.029
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 09/04/23 18:52:19.031
    STEP: get a list of Events with a label in the current namespace 09/04/23 18:52:19.057
    STEP: delete a list of events 09/04/23 18:52:19.06
    Sep  4 18:52:19.061: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/04/23 18:52:19.092
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:52:19.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2895" for this suite. 09/04/23 18:52:19.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:52:19.112
Sep  4 18:52:19.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename webhook 09/04/23 18:52:19.112
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:52:19.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:52:19.128
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 18:52:19.153
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:52:20.296
STEP: Deploying the webhook pod 09/04/23 18:52:20.307
STEP: Wait for the deployment to be ready 09/04/23 18:52:20.328
Sep  4 18:52:20.349: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 18:52:22.366
STEP: Verifying the service has paired with the endpoint 09/04/23 18:52:22.384
Sep  4 18:52:23.384: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Sep  4 18:52:23.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8680-crds.webhook.example.com via the AdmissionRegistration API 09/04/23 18:52:23.905
STEP: Creating a custom resource while v1 is storage version 09/04/23 18:52:23.93
STEP: Patching Custom Resource Definition to set v2 as storage 09/04/23 18:52:26.001
STEP: Patching the custom resource while v2 is storage version 09/04/23 18:52:26.023
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 18:52:26.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9930" for this suite. 09/04/23 18:52:26.687
STEP: Destroying namespace "webhook-9930-markers" for this suite. 09/04/23 18:52:26.766
------------------------------
â€¢ [SLOW TEST] [7.733 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:52:19.112
    Sep  4 18:52:19.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename webhook 09/04/23 18:52:19.112
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:52:19.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:52:19.128
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 18:52:19.153
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 18:52:20.296
    STEP: Deploying the webhook pod 09/04/23 18:52:20.307
    STEP: Wait for the deployment to be ready 09/04/23 18:52:20.328
    Sep  4 18:52:20.349: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 18:52:22.366
    STEP: Verifying the service has paired with the endpoint 09/04/23 18:52:22.384
    Sep  4 18:52:23.384: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Sep  4 18:52:23.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8680-crds.webhook.example.com via the AdmissionRegistration API 09/04/23 18:52:23.905
    STEP: Creating a custom resource while v1 is storage version 09/04/23 18:52:23.93
    STEP: Patching Custom Resource Definition to set v2 as storage 09/04/23 18:52:26.001
    STEP: Patching the custom resource while v2 is storage version 09/04/23 18:52:26.023
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:52:26.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9930" for this suite. 09/04/23 18:52:26.687
    STEP: Destroying namespace "webhook-9930-markers" for this suite. 09/04/23 18:52:26.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 18:52:26.853
Sep  4 18:52:26.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 18:52:26.854
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:52:26.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:52:26.898
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/04/23 18:52:26.904
Sep  4 18:52:26.921: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2470" to be "running and ready"
Sep  4 18:52:26.925: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.670846ms
Sep  4 18:52:26.925: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:52:28.932: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010818446s
Sep  4 18:52:28.932: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  4 18:52:28.932: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 09/04/23 18:52:28.936
Sep  4 18:52:28.951: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-2470" to be "running and ready"
Sep  4 18:52:28.954: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.774789ms
Sep  4 18:52:28.954: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  4 18:52:30.962: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010640036s
Sep  4 18:52:30.962: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Sep  4 18:52:30.962: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/04/23 18:52:30.966
Sep  4 18:52:30.979: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  4 18:52:30.982: INFO: Pod pod-with-prestop-http-hook still exists
Sep  4 18:52:32.983: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  4 18:52:32.989: INFO: Pod pod-with-prestop-http-hook still exists
Sep  4 18:52:34.983: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  4 18:52:34.989: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 09/04/23 18:52:34.99
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  4 18:52:35.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2470" for this suite. 09/04/23 18:52:35.01
------------------------------
â€¢ [SLOW TEST] [8.169 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 18:52:26.853
    Sep  4 18:52:26.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3116509839
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 18:52:26.854
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 18:52:26.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 18:52:26.898
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/04/23 18:52:26.904
    Sep  4 18:52:26.921: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2470" to be "running and ready"
    Sep  4 18:52:26.925: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.670846ms
    Sep  4 18:52:26.925: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:52:28.932: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010818446s
    Sep  4 18:52:28.932: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  4 18:52:28.932: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 09/04/23 18:52:28.936
    Sep  4 18:52:28.951: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-2470" to be "running and ready"
    Sep  4 18:52:28.954: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.774789ms
    Sep  4 18:52:28.954: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 18:52:30.962: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010640036s
    Sep  4 18:52:30.962: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Sep  4 18:52:30.962: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/04/23 18:52:30.966
    Sep  4 18:52:30.979: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  4 18:52:30.982: INFO: Pod pod-with-prestop-http-hook still exists
    Sep  4 18:52:32.983: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  4 18:52:32.989: INFO: Pod pod-with-prestop-http-hook still exists
    Sep  4 18:52:34.983: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  4 18:52:34.989: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 09/04/23 18:52:34.99
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  4 18:52:35.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2470" for this suite. 09/04/23 18:52:35.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Sep  4 18:52:35.032: INFO: Running AfterSuite actions on node 1
Sep  4 18:52:35.032: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.001 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Sep  4 18:52:35.032: INFO: Running AfterSuite actions on node 1
    Sep  4 18:52:35.032: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.118 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5744.887 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h35m45.420825061s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m


Conformance test: not doing test setup.
I0904 14:43:16.129463    7754 e2e.go:126] Starting e2e run "0be53abf-8976-49ee-9130-04032f304ae9" on Ginkgo node 1
Sep  4 14:43:16.141: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /go/src/k8s.io/kubernetes/platforms/linux/amd64
=====================================================================================
Random Seed: 1693838596 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Sep  4 14:43:16.316: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 14:43:16.319: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep  4 14:43:16.383: INFO: Waiting up to 10m0s for all pods (need at least 1) in namespace 'kube-system' to be running and ready
Sep  4 14:43:16.462: INFO: The status of Pod addons-nginx-ingress-controller-56b5dc8f6c-zwwh4 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  4 14:43:16.462: INFO: 35 / 36 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  4 14:43:16.462: INFO: expected 15 pod replicas in namespace 'kube-system', 14 are Running and Ready.
Sep  4 14:43:16.462: INFO: POD                                               NODE                                          PHASE    GRACE  CONDITIONS
Sep  4 14:43:16.462: INFO: addons-nginx-ingress-controller-56b5dc8f6c-zwwh4  shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC ContainersNotReady containers with unready status: [nginx-ingress-controller]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC ContainersNotReady containers with unready status: [nginx-ingress-controller]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC  }]
Sep  4 14:43:16.462: INFO: 
Sep  4 14:43:18.552: INFO: The status of Pod addons-nginx-ingress-controller-56b5dc8f6c-zwwh4 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  4 14:43:18.552: INFO: 35 / 36 pods in namespace 'kube-system' are running and ready (2 seconds elapsed)
Sep  4 14:43:18.552: INFO: expected 15 pod replicas in namespace 'kube-system', 14 are Running and Ready.
Sep  4 14:43:18.552: INFO: POD                                               NODE                                          PHASE    GRACE  CONDITIONS
Sep  4 14:43:18.552: INFO: addons-nginx-ingress-controller-56b5dc8f6c-zwwh4  shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC ContainersNotReady containers with unready status: [nginx-ingress-controller]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC ContainersNotReady containers with unready status: [nginx-ingress-controller]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC  }]
Sep  4 14:43:18.552: INFO: 
Sep  4 14:43:20.552: INFO: The status of Pod addons-nginx-ingress-controller-56b5dc8f6c-zwwh4 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  4 14:43:20.552: INFO: 35 / 36 pods in namespace 'kube-system' are running and ready (4 seconds elapsed)
Sep  4 14:43:20.552: INFO: expected 15 pod replicas in namespace 'kube-system', 14 are Running and Ready.
Sep  4 14:43:20.552: INFO: POD                                               NODE                                          PHASE    GRACE  CONDITIONS
Sep  4 14:43:20.552: INFO: addons-nginx-ingress-controller-56b5dc8f6c-zwwh4  shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC ContainersNotReady containers with unready status: [nginx-ingress-controller]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC ContainersNotReady containers with unready status: [nginx-ingress-controller]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC  }]
Sep  4 14:43:20.552: INFO: 
Sep  4 14:43:22.547: INFO: 35 / 35 pods in namespace 'kube-system' are running and ready (6 seconds elapsed)
Sep  4 14:43:22.547: INFO: expected 15 pod replicas in namespace 'kube-system', 15 are Running and Ready.
Sep  4 14:43:22.547: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-driver-node' (0 seconds elapsed)
Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'egress-filter-applier' (0 seconds elapsed)
Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-1-v1.26.8' (0 seconds elapsed)
Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-host' (0 seconds elapsed)
Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-pod' (0 seconds elapsed)
Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Sep  4 14:43:22.566: INFO: e2e test version: v1.26.8
Sep  4 14:43:22.576: INFO: kube-apiserver version: v1.26.8
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Sep  4 14:43:22.576: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 14:43:22.590: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [6.274 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Sep  4 14:43:16.316: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 14:43:16.319: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Sep  4 14:43:16.383: INFO: Waiting up to 10m0s for all pods (need at least 1) in namespace 'kube-system' to be running and ready
    Sep  4 14:43:16.462: INFO: The status of Pod addons-nginx-ingress-controller-56b5dc8f6c-zwwh4 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  4 14:43:16.462: INFO: 35 / 36 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Sep  4 14:43:16.462: INFO: expected 15 pod replicas in namespace 'kube-system', 14 are Running and Ready.
    Sep  4 14:43:16.462: INFO: POD                                               NODE                                          PHASE    GRACE  CONDITIONS
    Sep  4 14:43:16.462: INFO: addons-nginx-ingress-controller-56b5dc8f6c-zwwh4  shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC ContainersNotReady containers with unready status: [nginx-ingress-controller]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC ContainersNotReady containers with unready status: [nginx-ingress-controller]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC  }]
    Sep  4 14:43:16.462: INFO: 
    Sep  4 14:43:18.552: INFO: The status of Pod addons-nginx-ingress-controller-56b5dc8f6c-zwwh4 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  4 14:43:18.552: INFO: 35 / 36 pods in namespace 'kube-system' are running and ready (2 seconds elapsed)
    Sep  4 14:43:18.552: INFO: expected 15 pod replicas in namespace 'kube-system', 14 are Running and Ready.
    Sep  4 14:43:18.552: INFO: POD                                               NODE                                          PHASE    GRACE  CONDITIONS
    Sep  4 14:43:18.552: INFO: addons-nginx-ingress-controller-56b5dc8f6c-zwwh4  shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC ContainersNotReady containers with unready status: [nginx-ingress-controller]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC ContainersNotReady containers with unready status: [nginx-ingress-controller]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC  }]
    Sep  4 14:43:18.552: INFO: 
    Sep  4 14:43:20.552: INFO: The status of Pod addons-nginx-ingress-controller-56b5dc8f6c-zwwh4 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  4 14:43:20.552: INFO: 35 / 36 pods in namespace 'kube-system' are running and ready (4 seconds elapsed)
    Sep  4 14:43:20.552: INFO: expected 15 pod replicas in namespace 'kube-system', 14 are Running and Ready.
    Sep  4 14:43:20.552: INFO: POD                                               NODE                                          PHASE    GRACE  CONDITIONS
    Sep  4 14:43:20.552: INFO: addons-nginx-ingress-controller-56b5dc8f6c-zwwh4  shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC ContainersNotReady containers with unready status: [nginx-ingress-controller]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC ContainersNotReady containers with unready status: [nginx-ingress-controller]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 14:43:10 +0000 UTC  }]
    Sep  4 14:43:20.552: INFO: 
    Sep  4 14:43:22.547: INFO: 35 / 35 pods in namespace 'kube-system' are running and ready (6 seconds elapsed)
    Sep  4 14:43:22.547: INFO: expected 15 pod replicas in namespace 'kube-system', 15 are Running and Ready.
    Sep  4 14:43:22.547: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
    Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-driver-node' (0 seconds elapsed)
    Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'egress-filter-applier' (0 seconds elapsed)
    Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-1-v1.26.8' (0 seconds elapsed)
    Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-host' (0 seconds elapsed)
    Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-pod' (0 seconds elapsed)
    Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
    Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Sep  4 14:43:22.566: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
    Sep  4 14:43:22.566: INFO: e2e test version: v1.26.8
    Sep  4 14:43:22.576: INFO: kube-apiserver version: v1.26.8
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Sep  4 14:43:22.576: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 14:43:22.590: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:43:22.624
Sep  4 14:43:22.624: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 09/04/23 14:43:22.625
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:43:22.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:43:22.682
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4013 09/04/23 14:43:22.704
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-4013 09/04/23 14:43:22.728
Sep  4 14:43:22.753: INFO: Found 0 stateful pods, waiting for 1
Sep  4 14:43:32.767: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 09/04/23 14:43:32.79
STEP: Getting /status 09/04/23 14:43:32.807
Sep  4 14:43:32.821: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 09/04/23 14:43:32.821
Sep  4 14:43:32.846: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 09/04/23 14:43:32.846
Sep  4 14:43:32.856: INFO: Observed &StatefulSet event: ADDED
Sep  4 14:43:32.856: INFO: Found Statefulset ss in namespace statefulset-4013 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  4 14:43:32.856: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 09/04/23 14:43:32.856
Sep  4 14:43:32.856: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  4 14:43:32.870: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 09/04/23 14:43:32.87
Sep  4 14:43:32.880: INFO: Observed &StatefulSet event: ADDED
Sep  4 14:43:32.880: INFO: Observed Statefulset ss in namespace statefulset-4013 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  4 14:43:32.880: INFO: Observed &StatefulSet event: MODIFIED
Sep  4 14:43:32.880: INFO: Found Statefulset ss in namespace statefulset-4013 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 14:43:32.880: INFO: Deleting all statefulset in ns statefulset-4013
Sep  4 14:43:32.892: INFO: Scaling statefulset ss to 0
Sep  4 14:43:42.940: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 14:43:42.951: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 14:43:42.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4013" for this suite. 09/04/23 14:43:43.005
------------------------------
• [20.393 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:43:22.624
    Sep  4 14:43:22.624: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 09/04/23 14:43:22.625
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:43:22.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:43:22.682
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4013 09/04/23 14:43:22.704
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-4013 09/04/23 14:43:22.728
    Sep  4 14:43:22.753: INFO: Found 0 stateful pods, waiting for 1
    Sep  4 14:43:32.767: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 09/04/23 14:43:32.79
    STEP: Getting /status 09/04/23 14:43:32.807
    Sep  4 14:43:32.821: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 09/04/23 14:43:32.821
    Sep  4 14:43:32.846: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 09/04/23 14:43:32.846
    Sep  4 14:43:32.856: INFO: Observed &StatefulSet event: ADDED
    Sep  4 14:43:32.856: INFO: Found Statefulset ss in namespace statefulset-4013 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  4 14:43:32.856: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 09/04/23 14:43:32.856
    Sep  4 14:43:32.856: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  4 14:43:32.870: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 09/04/23 14:43:32.87
    Sep  4 14:43:32.880: INFO: Observed &StatefulSet event: ADDED
    Sep  4 14:43:32.880: INFO: Observed Statefulset ss in namespace statefulset-4013 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  4 14:43:32.880: INFO: Observed &StatefulSet event: MODIFIED
    Sep  4 14:43:32.880: INFO: Found Statefulset ss in namespace statefulset-4013 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 14:43:32.880: INFO: Deleting all statefulset in ns statefulset-4013
    Sep  4 14:43:32.892: INFO: Scaling statefulset ss to 0
    Sep  4 14:43:42.940: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 14:43:42.951: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:43:42.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4013" for this suite. 09/04/23 14:43:43.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:43:43.018
Sep  4 14:43:43.018: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 09/04/23 14:43:43.019
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:43:43.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:43:43.071
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 09/04/23 14:43:43.103
STEP: delete the rc 09/04/23 14:43:48.127
STEP: wait for the rc to be deleted 09/04/23 14:43:48.139
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/04/23 14:43:53.164
STEP: Gathering metrics 09/04/23 14:44:23.195
W0904 14:44:23.216176    7754 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep  4 14:44:23.216: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  4 14:44:23.216: INFO: Deleting pod "simpletest.rc-28lpb" in namespace "gc-3659"
Sep  4 14:44:23.232: INFO: Deleting pod "simpletest.rc-2k2zr" in namespace "gc-3659"
Sep  4 14:44:23.248: INFO: Deleting pod "simpletest.rc-2kl7w" in namespace "gc-3659"
Sep  4 14:44:23.266: INFO: Deleting pod "simpletest.rc-2p8pk" in namespace "gc-3659"
Sep  4 14:44:23.281: INFO: Deleting pod "simpletest.rc-44rpc" in namespace "gc-3659"
Sep  4 14:44:23.296: INFO: Deleting pod "simpletest.rc-49xnz" in namespace "gc-3659"
Sep  4 14:44:23.311: INFO: Deleting pod "simpletest.rc-4ds6j" in namespace "gc-3659"
Sep  4 14:44:23.327: INFO: Deleting pod "simpletest.rc-4tqtz" in namespace "gc-3659"
Sep  4 14:44:23.341: INFO: Deleting pod "simpletest.rc-4vrbh" in namespace "gc-3659"
Sep  4 14:44:23.357: INFO: Deleting pod "simpletest.rc-5r5ln" in namespace "gc-3659"
Sep  4 14:44:23.372: INFO: Deleting pod "simpletest.rc-5rfhx" in namespace "gc-3659"
Sep  4 14:44:23.387: INFO: Deleting pod "simpletest.rc-5wh2j" in namespace "gc-3659"
Sep  4 14:44:23.402: INFO: Deleting pod "simpletest.rc-6995c" in namespace "gc-3659"
Sep  4 14:44:23.417: INFO: Deleting pod "simpletest.rc-6gbkr" in namespace "gc-3659"
Sep  4 14:44:23.433: INFO: Deleting pod "simpletest.rc-6hdnl" in namespace "gc-3659"
Sep  4 14:44:23.448: INFO: Deleting pod "simpletest.rc-6mbdw" in namespace "gc-3659"
Sep  4 14:44:23.462: INFO: Deleting pod "simpletest.rc-6t659" in namespace "gc-3659"
Sep  4 14:44:23.478: INFO: Deleting pod "simpletest.rc-7n6v6" in namespace "gc-3659"
Sep  4 14:44:23.492: INFO: Deleting pod "simpletest.rc-7nppm" in namespace "gc-3659"
Sep  4 14:44:23.507: INFO: Deleting pod "simpletest.rc-9c8t5" in namespace "gc-3659"
Sep  4 14:44:23.528: INFO: Deleting pod "simpletest.rc-9qrcc" in namespace "gc-3659"
Sep  4 14:44:23.543: INFO: Deleting pod "simpletest.rc-9w58x" in namespace "gc-3659"
Sep  4 14:44:23.558: INFO: Deleting pod "simpletest.rc-b7ng9" in namespace "gc-3659"
Sep  4 14:44:23.572: INFO: Deleting pod "simpletest.rc-b8snk" in namespace "gc-3659"
Sep  4 14:44:23.586: INFO: Deleting pod "simpletest.rc-bc75b" in namespace "gc-3659"
Sep  4 14:44:23.600: INFO: Deleting pod "simpletest.rc-bxrg4" in namespace "gc-3659"
Sep  4 14:44:23.613: INFO: Deleting pod "simpletest.rc-c9rdc" in namespace "gc-3659"
Sep  4 14:44:23.627: INFO: Deleting pod "simpletest.rc-cd9n6" in namespace "gc-3659"
Sep  4 14:44:23.641: INFO: Deleting pod "simpletest.rc-chbcg" in namespace "gc-3659"
Sep  4 14:44:23.655: INFO: Deleting pod "simpletest.rc-chmkn" in namespace "gc-3659"
Sep  4 14:44:23.669: INFO: Deleting pod "simpletest.rc-d2v4k" in namespace "gc-3659"
Sep  4 14:44:23.683: INFO: Deleting pod "simpletest.rc-d4jrp" in namespace "gc-3659"
Sep  4 14:44:23.698: INFO: Deleting pod "simpletest.rc-dftg5" in namespace "gc-3659"
Sep  4 14:44:23.712: INFO: Deleting pod "simpletest.rc-dlgz4" in namespace "gc-3659"
Sep  4 14:44:23.726: INFO: Deleting pod "simpletest.rc-dsq9q" in namespace "gc-3659"
Sep  4 14:44:23.741: INFO: Deleting pod "simpletest.rc-dw82q" in namespace "gc-3659"
Sep  4 14:44:23.755: INFO: Deleting pod "simpletest.rc-fjk6l" in namespace "gc-3659"
Sep  4 14:44:23.770: INFO: Deleting pod "simpletest.rc-fpth5" in namespace "gc-3659"
Sep  4 14:44:23.785: INFO: Deleting pod "simpletest.rc-frsh6" in namespace "gc-3659"
Sep  4 14:44:23.801: INFO: Deleting pod "simpletest.rc-fs6dk" in namespace "gc-3659"
Sep  4 14:44:23.817: INFO: Deleting pod "simpletest.rc-g2kmg" in namespace "gc-3659"
Sep  4 14:44:23.833: INFO: Deleting pod "simpletest.rc-hcz4l" in namespace "gc-3659"
Sep  4 14:44:23.848: INFO: Deleting pod "simpletest.rc-hrhn4" in namespace "gc-3659"
Sep  4 14:44:23.863: INFO: Deleting pod "simpletest.rc-hwzhw" in namespace "gc-3659"
Sep  4 14:44:23.878: INFO: Deleting pod "simpletest.rc-j4sfq" in namespace "gc-3659"
Sep  4 14:44:23.893: INFO: Deleting pod "simpletest.rc-jcg7d" in namespace "gc-3659"
Sep  4 14:44:23.908: INFO: Deleting pod "simpletest.rc-kdf2p" in namespace "gc-3659"
Sep  4 14:44:23.924: INFO: Deleting pod "simpletest.rc-kdt2c" in namespace "gc-3659"
Sep  4 14:44:23.940: INFO: Deleting pod "simpletest.rc-ksmt9" in namespace "gc-3659"
Sep  4 14:44:23.956: INFO: Deleting pod "simpletest.rc-l7jp7" in namespace "gc-3659"
Sep  4 14:44:23.971: INFO: Deleting pod "simpletest.rc-ld4nm" in namespace "gc-3659"
Sep  4 14:44:23.986: INFO: Deleting pod "simpletest.rc-lw6zm" in namespace "gc-3659"
Sep  4 14:44:24.001: INFO: Deleting pod "simpletest.rc-mfdts" in namespace "gc-3659"
Sep  4 14:44:24.016: INFO: Deleting pod "simpletest.rc-mjbtc" in namespace "gc-3659"
Sep  4 14:44:24.030: INFO: Deleting pod "simpletest.rc-mkbrz" in namespace "gc-3659"
Sep  4 14:44:24.046: INFO: Deleting pod "simpletest.rc-ng7s4" in namespace "gc-3659"
Sep  4 14:44:24.062: INFO: Deleting pod "simpletest.rc-ngb8l" in namespace "gc-3659"
Sep  4 14:44:24.078: INFO: Deleting pod "simpletest.rc-nlkmp" in namespace "gc-3659"
Sep  4 14:44:24.099: INFO: Deleting pod "simpletest.rc-nvjw2" in namespace "gc-3659"
Sep  4 14:44:24.114: INFO: Deleting pod "simpletest.rc-nx9l8" in namespace "gc-3659"
Sep  4 14:44:24.129: INFO: Deleting pod "simpletest.rc-nz47w" in namespace "gc-3659"
Sep  4 14:44:24.144: INFO: Deleting pod "simpletest.rc-pbn2z" in namespace "gc-3659"
Sep  4 14:44:24.158: INFO: Deleting pod "simpletest.rc-pfpcx" in namespace "gc-3659"
Sep  4 14:44:24.173: INFO: Deleting pod "simpletest.rc-pmk5v" in namespace "gc-3659"
Sep  4 14:44:24.187: INFO: Deleting pod "simpletest.rc-pp226" in namespace "gc-3659"
Sep  4 14:44:24.201: INFO: Deleting pod "simpletest.rc-psp7n" in namespace "gc-3659"
Sep  4 14:44:24.216: INFO: Deleting pod "simpletest.rc-pzvt4" in namespace "gc-3659"
Sep  4 14:44:24.230: INFO: Deleting pod "simpletest.rc-q4rdl" in namespace "gc-3659"
Sep  4 14:44:24.245: INFO: Deleting pod "simpletest.rc-qjtj9" in namespace "gc-3659"
Sep  4 14:44:24.259: INFO: Deleting pod "simpletest.rc-qmhv2" in namespace "gc-3659"
Sep  4 14:44:24.281: INFO: Deleting pod "simpletest.rc-qs2mq" in namespace "gc-3659"
Sep  4 14:44:24.333: INFO: Deleting pod "simpletest.rc-qthcm" in namespace "gc-3659"
Sep  4 14:44:24.383: INFO: Deleting pod "simpletest.rc-r9zsn" in namespace "gc-3659"
Sep  4 14:44:24.434: INFO: Deleting pod "simpletest.rc-rdxsl" in namespace "gc-3659"
Sep  4 14:44:24.483: INFO: Deleting pod "simpletest.rc-rgcdk" in namespace "gc-3659"
Sep  4 14:44:24.531: INFO: Deleting pod "simpletest.rc-s4vnb" in namespace "gc-3659"
Sep  4 14:44:24.582: INFO: Deleting pod "simpletest.rc-s8nbs" in namespace "gc-3659"
Sep  4 14:44:24.633: INFO: Deleting pod "simpletest.rc-sf4k4" in namespace "gc-3659"
Sep  4 14:44:24.682: INFO: Deleting pod "simpletest.rc-sfjpm" in namespace "gc-3659"
Sep  4 14:44:24.731: INFO: Deleting pod "simpletest.rc-sqxtf" in namespace "gc-3659"
Sep  4 14:44:24.783: INFO: Deleting pod "simpletest.rc-ssr7q" in namespace "gc-3659"
Sep  4 14:44:24.833: INFO: Deleting pod "simpletest.rc-t9jmh" in namespace "gc-3659"
Sep  4 14:44:24.882: INFO: Deleting pod "simpletest.rc-tdzrb" in namespace "gc-3659"
Sep  4 14:44:24.932: INFO: Deleting pod "simpletest.rc-tfm54" in namespace "gc-3659"
Sep  4 14:44:24.983: INFO: Deleting pod "simpletest.rc-tlhsv" in namespace "gc-3659"
Sep  4 14:44:25.032: INFO: Deleting pod "simpletest.rc-tn8rf" in namespace "gc-3659"
Sep  4 14:44:25.083: INFO: Deleting pod "simpletest.rc-ttvkk" in namespace "gc-3659"
Sep  4 14:44:25.133: INFO: Deleting pod "simpletest.rc-txbk7" in namespace "gc-3659"
Sep  4 14:44:25.182: INFO: Deleting pod "simpletest.rc-tzf9s" in namespace "gc-3659"
Sep  4 14:44:25.232: INFO: Deleting pod "simpletest.rc-vl2ck" in namespace "gc-3659"
Sep  4 14:44:25.284: INFO: Deleting pod "simpletest.rc-vslhh" in namespace "gc-3659"
Sep  4 14:44:25.334: INFO: Deleting pod "simpletest.rc-vx7nx" in namespace "gc-3659"
Sep  4 14:44:25.382: INFO: Deleting pod "simpletest.rc-vz6gf" in namespace "gc-3659"
Sep  4 14:44:25.433: INFO: Deleting pod "simpletest.rc-wh46q" in namespace "gc-3659"
Sep  4 14:44:25.488: INFO: Deleting pod "simpletest.rc-wtlbp" in namespace "gc-3659"
Sep  4 14:44:25.533: INFO: Deleting pod "simpletest.rc-wznnp" in namespace "gc-3659"
Sep  4 14:44:25.581: INFO: Deleting pod "simpletest.rc-wzpf8" in namespace "gc-3659"
Sep  4 14:44:25.633: INFO: Deleting pod "simpletest.rc-xfnhc" in namespace "gc-3659"
Sep  4 14:44:25.682: INFO: Deleting pod "simpletest.rc-xjhj7" in namespace "gc-3659"
Sep  4 14:44:25.733: INFO: Deleting pod "simpletest.rc-zbc95" in namespace "gc-3659"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  4 14:44:25.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3659" for this suite. 09/04/23 14:44:25.829
------------------------------
• [42.863 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:43:43.018
    Sep  4 14:43:43.018: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 09/04/23 14:43:43.019
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:43:43.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:43:43.071
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 09/04/23 14:43:43.103
    STEP: delete the rc 09/04/23 14:43:48.127
    STEP: wait for the rc to be deleted 09/04/23 14:43:48.139
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/04/23 14:43:53.164
    STEP: Gathering metrics 09/04/23 14:44:23.195
    W0904 14:44:23.216176    7754 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep  4 14:44:23.216: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep  4 14:44:23.216: INFO: Deleting pod "simpletest.rc-28lpb" in namespace "gc-3659"
    Sep  4 14:44:23.232: INFO: Deleting pod "simpletest.rc-2k2zr" in namespace "gc-3659"
    Sep  4 14:44:23.248: INFO: Deleting pod "simpletest.rc-2kl7w" in namespace "gc-3659"
    Sep  4 14:44:23.266: INFO: Deleting pod "simpletest.rc-2p8pk" in namespace "gc-3659"
    Sep  4 14:44:23.281: INFO: Deleting pod "simpletest.rc-44rpc" in namespace "gc-3659"
    Sep  4 14:44:23.296: INFO: Deleting pod "simpletest.rc-49xnz" in namespace "gc-3659"
    Sep  4 14:44:23.311: INFO: Deleting pod "simpletest.rc-4ds6j" in namespace "gc-3659"
    Sep  4 14:44:23.327: INFO: Deleting pod "simpletest.rc-4tqtz" in namespace "gc-3659"
    Sep  4 14:44:23.341: INFO: Deleting pod "simpletest.rc-4vrbh" in namespace "gc-3659"
    Sep  4 14:44:23.357: INFO: Deleting pod "simpletest.rc-5r5ln" in namespace "gc-3659"
    Sep  4 14:44:23.372: INFO: Deleting pod "simpletest.rc-5rfhx" in namespace "gc-3659"
    Sep  4 14:44:23.387: INFO: Deleting pod "simpletest.rc-5wh2j" in namespace "gc-3659"
    Sep  4 14:44:23.402: INFO: Deleting pod "simpletest.rc-6995c" in namespace "gc-3659"
    Sep  4 14:44:23.417: INFO: Deleting pod "simpletest.rc-6gbkr" in namespace "gc-3659"
    Sep  4 14:44:23.433: INFO: Deleting pod "simpletest.rc-6hdnl" in namespace "gc-3659"
    Sep  4 14:44:23.448: INFO: Deleting pod "simpletest.rc-6mbdw" in namespace "gc-3659"
    Sep  4 14:44:23.462: INFO: Deleting pod "simpletest.rc-6t659" in namespace "gc-3659"
    Sep  4 14:44:23.478: INFO: Deleting pod "simpletest.rc-7n6v6" in namespace "gc-3659"
    Sep  4 14:44:23.492: INFO: Deleting pod "simpletest.rc-7nppm" in namespace "gc-3659"
    Sep  4 14:44:23.507: INFO: Deleting pod "simpletest.rc-9c8t5" in namespace "gc-3659"
    Sep  4 14:44:23.528: INFO: Deleting pod "simpletest.rc-9qrcc" in namespace "gc-3659"
    Sep  4 14:44:23.543: INFO: Deleting pod "simpletest.rc-9w58x" in namespace "gc-3659"
    Sep  4 14:44:23.558: INFO: Deleting pod "simpletest.rc-b7ng9" in namespace "gc-3659"
    Sep  4 14:44:23.572: INFO: Deleting pod "simpletest.rc-b8snk" in namespace "gc-3659"
    Sep  4 14:44:23.586: INFO: Deleting pod "simpletest.rc-bc75b" in namespace "gc-3659"
    Sep  4 14:44:23.600: INFO: Deleting pod "simpletest.rc-bxrg4" in namespace "gc-3659"
    Sep  4 14:44:23.613: INFO: Deleting pod "simpletest.rc-c9rdc" in namespace "gc-3659"
    Sep  4 14:44:23.627: INFO: Deleting pod "simpletest.rc-cd9n6" in namespace "gc-3659"
    Sep  4 14:44:23.641: INFO: Deleting pod "simpletest.rc-chbcg" in namespace "gc-3659"
    Sep  4 14:44:23.655: INFO: Deleting pod "simpletest.rc-chmkn" in namespace "gc-3659"
    Sep  4 14:44:23.669: INFO: Deleting pod "simpletest.rc-d2v4k" in namespace "gc-3659"
    Sep  4 14:44:23.683: INFO: Deleting pod "simpletest.rc-d4jrp" in namespace "gc-3659"
    Sep  4 14:44:23.698: INFO: Deleting pod "simpletest.rc-dftg5" in namespace "gc-3659"
    Sep  4 14:44:23.712: INFO: Deleting pod "simpletest.rc-dlgz4" in namespace "gc-3659"
    Sep  4 14:44:23.726: INFO: Deleting pod "simpletest.rc-dsq9q" in namespace "gc-3659"
    Sep  4 14:44:23.741: INFO: Deleting pod "simpletest.rc-dw82q" in namespace "gc-3659"
    Sep  4 14:44:23.755: INFO: Deleting pod "simpletest.rc-fjk6l" in namespace "gc-3659"
    Sep  4 14:44:23.770: INFO: Deleting pod "simpletest.rc-fpth5" in namespace "gc-3659"
    Sep  4 14:44:23.785: INFO: Deleting pod "simpletest.rc-frsh6" in namespace "gc-3659"
    Sep  4 14:44:23.801: INFO: Deleting pod "simpletest.rc-fs6dk" in namespace "gc-3659"
    Sep  4 14:44:23.817: INFO: Deleting pod "simpletest.rc-g2kmg" in namespace "gc-3659"
    Sep  4 14:44:23.833: INFO: Deleting pod "simpletest.rc-hcz4l" in namespace "gc-3659"
    Sep  4 14:44:23.848: INFO: Deleting pod "simpletest.rc-hrhn4" in namespace "gc-3659"
    Sep  4 14:44:23.863: INFO: Deleting pod "simpletest.rc-hwzhw" in namespace "gc-3659"
    Sep  4 14:44:23.878: INFO: Deleting pod "simpletest.rc-j4sfq" in namespace "gc-3659"
    Sep  4 14:44:23.893: INFO: Deleting pod "simpletest.rc-jcg7d" in namespace "gc-3659"
    Sep  4 14:44:23.908: INFO: Deleting pod "simpletest.rc-kdf2p" in namespace "gc-3659"
    Sep  4 14:44:23.924: INFO: Deleting pod "simpletest.rc-kdt2c" in namespace "gc-3659"
    Sep  4 14:44:23.940: INFO: Deleting pod "simpletest.rc-ksmt9" in namespace "gc-3659"
    Sep  4 14:44:23.956: INFO: Deleting pod "simpletest.rc-l7jp7" in namespace "gc-3659"
    Sep  4 14:44:23.971: INFO: Deleting pod "simpletest.rc-ld4nm" in namespace "gc-3659"
    Sep  4 14:44:23.986: INFO: Deleting pod "simpletest.rc-lw6zm" in namespace "gc-3659"
    Sep  4 14:44:24.001: INFO: Deleting pod "simpletest.rc-mfdts" in namespace "gc-3659"
    Sep  4 14:44:24.016: INFO: Deleting pod "simpletest.rc-mjbtc" in namespace "gc-3659"
    Sep  4 14:44:24.030: INFO: Deleting pod "simpletest.rc-mkbrz" in namespace "gc-3659"
    Sep  4 14:44:24.046: INFO: Deleting pod "simpletest.rc-ng7s4" in namespace "gc-3659"
    Sep  4 14:44:24.062: INFO: Deleting pod "simpletest.rc-ngb8l" in namespace "gc-3659"
    Sep  4 14:44:24.078: INFO: Deleting pod "simpletest.rc-nlkmp" in namespace "gc-3659"
    Sep  4 14:44:24.099: INFO: Deleting pod "simpletest.rc-nvjw2" in namespace "gc-3659"
    Sep  4 14:44:24.114: INFO: Deleting pod "simpletest.rc-nx9l8" in namespace "gc-3659"
    Sep  4 14:44:24.129: INFO: Deleting pod "simpletest.rc-nz47w" in namespace "gc-3659"
    Sep  4 14:44:24.144: INFO: Deleting pod "simpletest.rc-pbn2z" in namespace "gc-3659"
    Sep  4 14:44:24.158: INFO: Deleting pod "simpletest.rc-pfpcx" in namespace "gc-3659"
    Sep  4 14:44:24.173: INFO: Deleting pod "simpletest.rc-pmk5v" in namespace "gc-3659"
    Sep  4 14:44:24.187: INFO: Deleting pod "simpletest.rc-pp226" in namespace "gc-3659"
    Sep  4 14:44:24.201: INFO: Deleting pod "simpletest.rc-psp7n" in namespace "gc-3659"
    Sep  4 14:44:24.216: INFO: Deleting pod "simpletest.rc-pzvt4" in namespace "gc-3659"
    Sep  4 14:44:24.230: INFO: Deleting pod "simpletest.rc-q4rdl" in namespace "gc-3659"
    Sep  4 14:44:24.245: INFO: Deleting pod "simpletest.rc-qjtj9" in namespace "gc-3659"
    Sep  4 14:44:24.259: INFO: Deleting pod "simpletest.rc-qmhv2" in namespace "gc-3659"
    Sep  4 14:44:24.281: INFO: Deleting pod "simpletest.rc-qs2mq" in namespace "gc-3659"
    Sep  4 14:44:24.333: INFO: Deleting pod "simpletest.rc-qthcm" in namespace "gc-3659"
    Sep  4 14:44:24.383: INFO: Deleting pod "simpletest.rc-r9zsn" in namespace "gc-3659"
    Sep  4 14:44:24.434: INFO: Deleting pod "simpletest.rc-rdxsl" in namespace "gc-3659"
    Sep  4 14:44:24.483: INFO: Deleting pod "simpletest.rc-rgcdk" in namespace "gc-3659"
    Sep  4 14:44:24.531: INFO: Deleting pod "simpletest.rc-s4vnb" in namespace "gc-3659"
    Sep  4 14:44:24.582: INFO: Deleting pod "simpletest.rc-s8nbs" in namespace "gc-3659"
    Sep  4 14:44:24.633: INFO: Deleting pod "simpletest.rc-sf4k4" in namespace "gc-3659"
    Sep  4 14:44:24.682: INFO: Deleting pod "simpletest.rc-sfjpm" in namespace "gc-3659"
    Sep  4 14:44:24.731: INFO: Deleting pod "simpletest.rc-sqxtf" in namespace "gc-3659"
    Sep  4 14:44:24.783: INFO: Deleting pod "simpletest.rc-ssr7q" in namespace "gc-3659"
    Sep  4 14:44:24.833: INFO: Deleting pod "simpletest.rc-t9jmh" in namespace "gc-3659"
    Sep  4 14:44:24.882: INFO: Deleting pod "simpletest.rc-tdzrb" in namespace "gc-3659"
    Sep  4 14:44:24.932: INFO: Deleting pod "simpletest.rc-tfm54" in namespace "gc-3659"
    Sep  4 14:44:24.983: INFO: Deleting pod "simpletest.rc-tlhsv" in namespace "gc-3659"
    Sep  4 14:44:25.032: INFO: Deleting pod "simpletest.rc-tn8rf" in namespace "gc-3659"
    Sep  4 14:44:25.083: INFO: Deleting pod "simpletest.rc-ttvkk" in namespace "gc-3659"
    Sep  4 14:44:25.133: INFO: Deleting pod "simpletest.rc-txbk7" in namespace "gc-3659"
    Sep  4 14:44:25.182: INFO: Deleting pod "simpletest.rc-tzf9s" in namespace "gc-3659"
    Sep  4 14:44:25.232: INFO: Deleting pod "simpletest.rc-vl2ck" in namespace "gc-3659"
    Sep  4 14:44:25.284: INFO: Deleting pod "simpletest.rc-vslhh" in namespace "gc-3659"
    Sep  4 14:44:25.334: INFO: Deleting pod "simpletest.rc-vx7nx" in namespace "gc-3659"
    Sep  4 14:44:25.382: INFO: Deleting pod "simpletest.rc-vz6gf" in namespace "gc-3659"
    Sep  4 14:44:25.433: INFO: Deleting pod "simpletest.rc-wh46q" in namespace "gc-3659"
    Sep  4 14:44:25.488: INFO: Deleting pod "simpletest.rc-wtlbp" in namespace "gc-3659"
    Sep  4 14:44:25.533: INFO: Deleting pod "simpletest.rc-wznnp" in namespace "gc-3659"
    Sep  4 14:44:25.581: INFO: Deleting pod "simpletest.rc-wzpf8" in namespace "gc-3659"
    Sep  4 14:44:25.633: INFO: Deleting pod "simpletest.rc-xfnhc" in namespace "gc-3659"
    Sep  4 14:44:25.682: INFO: Deleting pod "simpletest.rc-xjhj7" in namespace "gc-3659"
    Sep  4 14:44:25.733: INFO: Deleting pod "simpletest.rc-zbc95" in namespace "gc-3659"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:44:25.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3659" for this suite. 09/04/23 14:44:25.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:44:25.881
Sep  4 14:44:25.881: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 09/04/23 14:44:25.882
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:44:25.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:44:25.94
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef 09/04/23 14:44:25.973
Sep  4 14:44:25.998: INFO: Pod name my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef: Found 1 pods out of 1
Sep  4 14:44:25.998: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef" are running
Sep  4 14:44:25.998: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x" in namespace "replication-controller-8186" to be "running"
Sep  4 14:44:26.009: INFO: Pod "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x": Phase="Pending", Reason="", readiness=false. Elapsed: 11.112485ms
Sep  4 14:44:28.022: INFO: Pod "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024837745s
Sep  4 14:44:30.023: INFO: Pod "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025577375s
Sep  4 14:44:32.022: INFO: Pod "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x": Phase="Running", Reason="", readiness=true. Elapsed: 6.024454912s
Sep  4 14:44:32.022: INFO: Pod "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x" satisfied condition "running"
Sep  4 14:44:32.022: INFO: Pod "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x" is running (conditions: [])
Sep  4 14:44:32.022: INFO: Trying to dial the pod
Sep  4 14:44:37.182: INFO: Controller my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef: Got expected result from replica 1 [my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x]: "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  4 14:44:37.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8186" for this suite. 09/04/23 14:44:37.205
------------------------------
• [11.338 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:44:25.881
    Sep  4 14:44:25.881: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 09/04/23 14:44:25.882
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:44:25.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:44:25.94
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef 09/04/23 14:44:25.973
    Sep  4 14:44:25.998: INFO: Pod name my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef: Found 1 pods out of 1
    Sep  4 14:44:25.998: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef" are running
    Sep  4 14:44:25.998: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x" in namespace "replication-controller-8186" to be "running"
    Sep  4 14:44:26.009: INFO: Pod "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x": Phase="Pending", Reason="", readiness=false. Elapsed: 11.112485ms
    Sep  4 14:44:28.022: INFO: Pod "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024837745s
    Sep  4 14:44:30.023: INFO: Pod "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025577375s
    Sep  4 14:44:32.022: INFO: Pod "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x": Phase="Running", Reason="", readiness=true. Elapsed: 6.024454912s
    Sep  4 14:44:32.022: INFO: Pod "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x" satisfied condition "running"
    Sep  4 14:44:32.022: INFO: Pod "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x" is running (conditions: [])
    Sep  4 14:44:32.022: INFO: Trying to dial the pod
    Sep  4 14:44:37.182: INFO: Controller my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef: Got expected result from replica 1 [my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x]: "my-hostname-basic-5e05fa7b-fc4f-4035-af2e-9e3cc212cfef-mp58x", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:44:37.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8186" for this suite. 09/04/23 14:44:37.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:44:37.219
Sep  4 14:44:37.219: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 09/04/23 14:44:37.22
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:44:37.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:44:37.279
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Sep  4 14:44:37.335: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9547 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  4 14:44:37.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9547" for this suite. 09/04/23 14:44:37.373
------------------------------
• [0.168 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:44:37.219
    Sep  4 14:44:37.219: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 09/04/23 14:44:37.22
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:44:37.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:44:37.279
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Sep  4 14:44:37.335: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9547 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:44:37.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9547" for this suite. 09/04/23 14:44:37.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:44:37.388
Sep  4 14:44:37.388: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 14:44:37.389
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:44:37.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:44:37.448
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Sep  4 14:44:37.470: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/04/23 14:44:39.086
Sep  4 14:44:39.087: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9439 --namespace=crd-publish-openapi-9439 create -f -'
Sep  4 14:44:39.795: INFO: stderr: ""
Sep  4 14:44:39.795: INFO: stdout: "e2e-test-crd-publish-openapi-8057-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  4 14:44:39.795: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9439 --namespace=crd-publish-openapi-9439 delete e2e-test-crd-publish-openapi-8057-crds test-cr'
Sep  4 14:44:39.940: INFO: stderr: ""
Sep  4 14:44:39.940: INFO: stdout: "e2e-test-crd-publish-openapi-8057-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep  4 14:44:39.940: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9439 --namespace=crd-publish-openapi-9439 apply -f -'
Sep  4 14:44:40.182: INFO: stderr: ""
Sep  4 14:44:40.182: INFO: stdout: "e2e-test-crd-publish-openapi-8057-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  4 14:44:40.182: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9439 --namespace=crd-publish-openapi-9439 delete e2e-test-crd-publish-openapi-8057-crds test-cr'
Sep  4 14:44:40.299: INFO: stderr: ""
Sep  4 14:44:40.299: INFO: stdout: "e2e-test-crd-publish-openapi-8057-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 09/04/23 14:44:40.299
Sep  4 14:44:40.299: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9439 explain e2e-test-crd-publish-openapi-8057-crds'
Sep  4 14:44:40.920: INFO: stderr: ""
Sep  4 14:44:40.920: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8057-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 14:44:43.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9439" for this suite. 09/04/23 14:44:43.14
------------------------------
• [5.764 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:44:37.388
    Sep  4 14:44:37.388: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 14:44:37.389
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:44:37.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:44:37.448
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Sep  4 14:44:37.470: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/04/23 14:44:39.086
    Sep  4 14:44:39.087: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9439 --namespace=crd-publish-openapi-9439 create -f -'
    Sep  4 14:44:39.795: INFO: stderr: ""
    Sep  4 14:44:39.795: INFO: stdout: "e2e-test-crd-publish-openapi-8057-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep  4 14:44:39.795: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9439 --namespace=crd-publish-openapi-9439 delete e2e-test-crd-publish-openapi-8057-crds test-cr'
    Sep  4 14:44:39.940: INFO: stderr: ""
    Sep  4 14:44:39.940: INFO: stdout: "e2e-test-crd-publish-openapi-8057-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Sep  4 14:44:39.940: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9439 --namespace=crd-publish-openapi-9439 apply -f -'
    Sep  4 14:44:40.182: INFO: stderr: ""
    Sep  4 14:44:40.182: INFO: stdout: "e2e-test-crd-publish-openapi-8057-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep  4 14:44:40.182: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9439 --namespace=crd-publish-openapi-9439 delete e2e-test-crd-publish-openapi-8057-crds test-cr'
    Sep  4 14:44:40.299: INFO: stderr: ""
    Sep  4 14:44:40.299: INFO: stdout: "e2e-test-crd-publish-openapi-8057-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 09/04/23 14:44:40.299
    Sep  4 14:44:40.299: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9439 explain e2e-test-crd-publish-openapi-8057-crds'
    Sep  4 14:44:40.920: INFO: stderr: ""
    Sep  4 14:44:40.920: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8057-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:44:43.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9439" for this suite. 09/04/23 14:44:43.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:44:43.154
Sep  4 14:44:43.154: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 09/04/23 14:44:43.154
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:44:43.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:44:43.21
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 09/04/23 14:44:43.231
Sep  4 14:44:43.249: INFO: Waiting up to 5m0s for pod "var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb" in namespace "var-expansion-7293" to be "Succeeded or Failed"
Sep  4 14:44:43.261: INFO: Pod "var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.120716ms
Sep  4 14:44:45.273: INFO: Pod "var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024029776s
Sep  4 14:44:47.273: INFO: Pod "var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023544834s
STEP: Saw pod success 09/04/23 14:44:47.273
Sep  4 14:44:47.273: INFO: Pod "var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb" satisfied condition "Succeeded or Failed"
Sep  4 14:44:47.285: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb container dapi-container: <nil>
STEP: delete the pod 09/04/23 14:44:47.474
Sep  4 14:44:47.491: INFO: Waiting for pod var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb to disappear
Sep  4 14:44:47.502: INFO: Pod var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 14:44:47.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7293" for this suite. 09/04/23 14:44:47.523
------------------------------
• [4.382 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:44:43.154
    Sep  4 14:44:43.154: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 09/04/23 14:44:43.154
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:44:43.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:44:43.21
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 09/04/23 14:44:43.231
    Sep  4 14:44:43.249: INFO: Waiting up to 5m0s for pod "var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb" in namespace "var-expansion-7293" to be "Succeeded or Failed"
    Sep  4 14:44:43.261: INFO: Pod "var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.120716ms
    Sep  4 14:44:45.273: INFO: Pod "var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024029776s
    Sep  4 14:44:47.273: INFO: Pod "var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023544834s
    STEP: Saw pod success 09/04/23 14:44:47.273
    Sep  4 14:44:47.273: INFO: Pod "var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb" satisfied condition "Succeeded or Failed"
    Sep  4 14:44:47.285: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb container dapi-container: <nil>
    STEP: delete the pod 09/04/23 14:44:47.474
    Sep  4 14:44:47.491: INFO: Waiting for pod var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb to disappear
    Sep  4 14:44:47.502: INFO: Pod var-expansion-94cdcf07-6b9d-4a89-8c70-86cee2f048cb no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:44:47.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7293" for this suite. 09/04/23 14:44:47.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:44:47.536
Sep  4 14:44:47.536: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 14:44:47.537
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:44:47.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:44:47.59
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 09/04/23 14:44:47.61
Sep  4 14:44:47.610: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep  4 14:44:47.610: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 create -f -'
Sep  4 14:44:48.248: INFO: stderr: ""
Sep  4 14:44:48.248: INFO: stdout: "service/agnhost-replica created\n"
Sep  4 14:44:48.248: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep  4 14:44:48.248: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 create -f -'
Sep  4 14:44:48.474: INFO: stderr: ""
Sep  4 14:44:48.474: INFO: stdout: "service/agnhost-primary created\n"
Sep  4 14:44:48.474: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  4 14:44:48.474: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 create -f -'
Sep  4 14:44:49.093: INFO: stderr: ""
Sep  4 14:44:49.093: INFO: stdout: "service/frontend created\n"
Sep  4 14:44:49.093: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep  4 14:44:49.093: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 create -f -'
Sep  4 14:44:49.320: INFO: stderr: ""
Sep  4 14:44:49.320: INFO: stdout: "deployment.apps/frontend created\n"
Sep  4 14:44:49.320: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  4 14:44:49.320: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 create -f -'
Sep  4 14:44:49.544: INFO: stderr: ""
Sep  4 14:44:49.544: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep  4 14:44:49.544: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  4 14:44:49.544: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 create -f -'
Sep  4 14:44:49.769: INFO: stderr: ""
Sep  4 14:44:49.769: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 09/04/23 14:44:49.769
Sep  4 14:44:49.769: INFO: Waiting for all frontend pods to be Running.
Sep  4 14:44:54.823: INFO: Waiting for frontend to serve content.
Sep  4 14:44:54.978: INFO: Trying to add a new entry to the guestbook.
Sep  4 14:44:55.142: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 09/04/23 14:44:55.298
Sep  4 14:44:55.298: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 delete --grace-period=0 --force -f -'
Sep  4 14:44:55.418: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 14:44:55.418: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 09/04/23 14:44:55.418
Sep  4 14:44:55.418: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 delete --grace-period=0 --force -f -'
Sep  4 14:44:55.526: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 14:44:55.526: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/04/23 14:44:55.526
Sep  4 14:44:55.526: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 delete --grace-period=0 --force -f -'
Sep  4 14:44:55.638: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 14:44:55.638: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/04/23 14:44:55.639
Sep  4 14:44:55.639: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 delete --grace-period=0 --force -f -'
Sep  4 14:44:55.752: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 14:44:55.752: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/04/23 14:44:55.752
Sep  4 14:44:55.753: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 delete --grace-period=0 --force -f -'
Sep  4 14:44:55.855: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 14:44:55.855: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/04/23 14:44:55.855
Sep  4 14:44:55.855: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 delete --grace-period=0 --force -f -'
Sep  4 14:44:55.973: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 14:44:55.973: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 14:44:55.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-61" for this suite. 09/04/23 14:44:55.995
------------------------------
• [8.472 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:44:47.536
    Sep  4 14:44:47.536: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 14:44:47.537
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:44:47.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:44:47.59
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 09/04/23 14:44:47.61
    Sep  4 14:44:47.610: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Sep  4 14:44:47.610: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 create -f -'
    Sep  4 14:44:48.248: INFO: stderr: ""
    Sep  4 14:44:48.248: INFO: stdout: "service/agnhost-replica created\n"
    Sep  4 14:44:48.248: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Sep  4 14:44:48.248: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 create -f -'
    Sep  4 14:44:48.474: INFO: stderr: ""
    Sep  4 14:44:48.474: INFO: stdout: "service/agnhost-primary created\n"
    Sep  4 14:44:48.474: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Sep  4 14:44:48.474: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 create -f -'
    Sep  4 14:44:49.093: INFO: stderr: ""
    Sep  4 14:44:49.093: INFO: stdout: "service/frontend created\n"
    Sep  4 14:44:49.093: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Sep  4 14:44:49.093: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 create -f -'
    Sep  4 14:44:49.320: INFO: stderr: ""
    Sep  4 14:44:49.320: INFO: stdout: "deployment.apps/frontend created\n"
    Sep  4 14:44:49.320: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep  4 14:44:49.320: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 create -f -'
    Sep  4 14:44:49.544: INFO: stderr: ""
    Sep  4 14:44:49.544: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Sep  4 14:44:49.544: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep  4 14:44:49.544: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 create -f -'
    Sep  4 14:44:49.769: INFO: stderr: ""
    Sep  4 14:44:49.769: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 09/04/23 14:44:49.769
    Sep  4 14:44:49.769: INFO: Waiting for all frontend pods to be Running.
    Sep  4 14:44:54.823: INFO: Waiting for frontend to serve content.
    Sep  4 14:44:54.978: INFO: Trying to add a new entry to the guestbook.
    Sep  4 14:44:55.142: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 09/04/23 14:44:55.298
    Sep  4 14:44:55.298: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 delete --grace-period=0 --force -f -'
    Sep  4 14:44:55.418: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 14:44:55.418: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 09/04/23 14:44:55.418
    Sep  4 14:44:55.418: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 delete --grace-period=0 --force -f -'
    Sep  4 14:44:55.526: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 14:44:55.526: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/04/23 14:44:55.526
    Sep  4 14:44:55.526: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 delete --grace-period=0 --force -f -'
    Sep  4 14:44:55.638: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 14:44:55.638: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/04/23 14:44:55.639
    Sep  4 14:44:55.639: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 delete --grace-period=0 --force -f -'
    Sep  4 14:44:55.752: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 14:44:55.752: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/04/23 14:44:55.752
    Sep  4 14:44:55.753: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 delete --grace-period=0 --force -f -'
    Sep  4 14:44:55.855: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 14:44:55.855: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/04/23 14:44:55.855
    Sep  4 14:44:55.855: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-61 delete --grace-period=0 --force -f -'
    Sep  4 14:44:55.973: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 14:44:55.973: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:44:55.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-61" for this suite. 09/04/23 14:44:55.995
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:44:56.008
Sep  4 14:44:56.008: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 14:44:56.009
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:44:56.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:44:56.063
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/04/23 14:44:56.083
Sep  4 14:44:56.084: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 14:44:58.238: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 14:45:06.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1913" for this suite. 09/04/23 14:45:06.872
------------------------------
• [10.876 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:44:56.008
    Sep  4 14:44:56.008: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 14:44:56.009
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:44:56.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:44:56.063
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/04/23 14:44:56.083
    Sep  4 14:44:56.084: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 14:44:58.238: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:45:06.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1913" for this suite. 09/04/23 14:45:06.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:45:06.884
Sep  4 14:45:06.884: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 14:45:06.885
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:06.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:06.942
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-539c47a3-cad5-4d7b-b5ac-97a73708f60f 09/04/23 14:45:06.97
STEP: Creating a pod to test consume configMaps 09/04/23 14:45:06.983
Sep  4 14:45:07.002: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d" in namespace "projected-7107" to be "Succeeded or Failed"
Sep  4 14:45:07.013: INFO: Pod "pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.49323ms
Sep  4 14:45:09.029: INFO: Pod "pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027084175s
Sep  4 14:45:11.028: INFO: Pod "pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026075979s
STEP: Saw pod success 09/04/23 14:45:11.028
Sep  4 14:45:11.028: INFO: Pod "pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d" satisfied condition "Succeeded or Failed"
Sep  4 14:45:11.040: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d container agnhost-container: <nil>
STEP: delete the pod 09/04/23 14:45:11.074
Sep  4 14:45:11.090: INFO: Waiting for pod pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d to disappear
Sep  4 14:45:11.103: INFO: Pod pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 14:45:11.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7107" for this suite. 09/04/23 14:45:11.125
------------------------------
• [4.255 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:45:06.884
    Sep  4 14:45:06.884: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 14:45:06.885
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:06.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:06.942
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-539c47a3-cad5-4d7b-b5ac-97a73708f60f 09/04/23 14:45:06.97
    STEP: Creating a pod to test consume configMaps 09/04/23 14:45:06.983
    Sep  4 14:45:07.002: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d" in namespace "projected-7107" to be "Succeeded or Failed"
    Sep  4 14:45:07.013: INFO: Pod "pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.49323ms
    Sep  4 14:45:09.029: INFO: Pod "pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027084175s
    Sep  4 14:45:11.028: INFO: Pod "pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026075979s
    STEP: Saw pod success 09/04/23 14:45:11.028
    Sep  4 14:45:11.028: INFO: Pod "pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d" satisfied condition "Succeeded or Failed"
    Sep  4 14:45:11.040: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 14:45:11.074
    Sep  4 14:45:11.090: INFO: Waiting for pod pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d to disappear
    Sep  4 14:45:11.103: INFO: Pod pod-projected-configmaps-322d887c-b273-4f28-af9a-f6ae248ad88d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:45:11.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7107" for this suite. 09/04/23 14:45:11.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:45:11.14
Sep  4 14:45:11.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 09/04/23 14:45:11.141
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:11.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:11.198
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 09/04/23 14:45:11.221
Sep  4 14:45:11.241: INFO: Waiting up to 5m0s for pod "var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e" in namespace "var-expansion-29" to be "Succeeded or Failed"
Sep  4 14:45:11.253: INFO: Pod "var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.96724ms
Sep  4 14:45:13.268: INFO: Pod "var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027553668s
Sep  4 14:45:15.267: INFO: Pod "var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025945455s
STEP: Saw pod success 09/04/23 14:45:15.267
Sep  4 14:45:15.267: INFO: Pod "var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e" satisfied condition "Succeeded or Failed"
Sep  4 14:45:15.280: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e container dapi-container: <nil>
STEP: delete the pod 09/04/23 14:45:15.355
Sep  4 14:45:15.372: INFO: Waiting for pod var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e to disappear
Sep  4 14:45:15.383: INFO: Pod var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 14:45:15.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-29" for this suite. 09/04/23 14:45:15.407
------------------------------
• [4.283 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:45:11.14
    Sep  4 14:45:11.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 09/04/23 14:45:11.141
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:11.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:11.198
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 09/04/23 14:45:11.221
    Sep  4 14:45:11.241: INFO: Waiting up to 5m0s for pod "var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e" in namespace "var-expansion-29" to be "Succeeded or Failed"
    Sep  4 14:45:11.253: INFO: Pod "var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.96724ms
    Sep  4 14:45:13.268: INFO: Pod "var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027553668s
    Sep  4 14:45:15.267: INFO: Pod "var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025945455s
    STEP: Saw pod success 09/04/23 14:45:15.267
    Sep  4 14:45:15.267: INFO: Pod "var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e" satisfied condition "Succeeded or Failed"
    Sep  4 14:45:15.280: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e container dapi-container: <nil>
    STEP: delete the pod 09/04/23 14:45:15.355
    Sep  4 14:45:15.372: INFO: Waiting for pod var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e to disappear
    Sep  4 14:45:15.383: INFO: Pod var-expansion-85fe131c-7e96-49fa-8cb5-2b501536cf6e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:45:15.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-29" for this suite. 09/04/23 14:45:15.407
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:45:15.424
Sep  4 14:45:15.424: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 09/04/23 14:45:15.425
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:15.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:15.482
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Sep  4 14:45:15.503: INFO: Creating ReplicaSet my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8
Sep  4 14:45:15.527: INFO: Pod name my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8: Found 1 pods out of 1
Sep  4 14:45:15.527: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8" is running
Sep  4 14:45:15.527: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8-l25kg" in namespace "replicaset-1809" to be "running"
Sep  4 14:45:15.538: INFO: Pod "my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8-l25kg": Phase="Pending", Reason="", readiness=false. Elapsed: 11.092709ms
Sep  4 14:45:17.551: INFO: Pod "my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8-l25kg": Phase="Running", Reason="", readiness=true. Elapsed: 2.023948638s
Sep  4 14:45:17.551: INFO: Pod "my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8-l25kg" satisfied condition "running"
Sep  4 14:45:17.551: INFO: Pod "my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8-l25kg" is running (conditions: [])
Sep  4 14:45:17.551: INFO: Trying to dial the pod
Sep  4 14:45:22.710: INFO: Controller my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8: Got expected result from replica 1 [my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8-l25kg]: "my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8-l25kg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  4 14:45:22.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1809" for this suite. 09/04/23 14:45:22.732
------------------------------
• [7.322 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:45:15.424
    Sep  4 14:45:15.424: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 09/04/23 14:45:15.425
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:15.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:15.482
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Sep  4 14:45:15.503: INFO: Creating ReplicaSet my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8
    Sep  4 14:45:15.527: INFO: Pod name my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8: Found 1 pods out of 1
    Sep  4 14:45:15.527: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8" is running
    Sep  4 14:45:15.527: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8-l25kg" in namespace "replicaset-1809" to be "running"
    Sep  4 14:45:15.538: INFO: Pod "my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8-l25kg": Phase="Pending", Reason="", readiness=false. Elapsed: 11.092709ms
    Sep  4 14:45:17.551: INFO: Pod "my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8-l25kg": Phase="Running", Reason="", readiness=true. Elapsed: 2.023948638s
    Sep  4 14:45:17.551: INFO: Pod "my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8-l25kg" satisfied condition "running"
    Sep  4 14:45:17.551: INFO: Pod "my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8-l25kg" is running (conditions: [])
    Sep  4 14:45:17.551: INFO: Trying to dial the pod
    Sep  4 14:45:22.710: INFO: Controller my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8: Got expected result from replica 1 [my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8-l25kg]: "my-hostname-basic-4e52d016-c6d7-4da1-86f5-dca6a18932b8-l25kg", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:45:22.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1809" for this suite. 09/04/23 14:45:22.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:45:22.746
Sep  4 14:45:22.746: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 09/04/23 14:45:22.747
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:22.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:22.801
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2982 09/04/23 14:45:22.822
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-2982 09/04/23 14:45:22.834
Sep  4 14:45:22.857: INFO: Found 0 stateful pods, waiting for 1
Sep  4 14:45:32.872: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 09/04/23 14:45:32.897
STEP: updating a scale subresource 09/04/23 14:45:32.909
STEP: verifying the statefulset Spec.Replicas was modified 09/04/23 14:45:32.923
STEP: Patch a scale subresource 09/04/23 14:45:32.934
STEP: verifying the statefulset Spec.Replicas was modified 09/04/23 14:45:32.948
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 14:45:32.961: INFO: Deleting all statefulset in ns statefulset-2982
Sep  4 14:45:32.975: INFO: Scaling statefulset ss to 0
Sep  4 14:45:43.030: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 14:45:43.043: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 14:45:43.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2982" for this suite. 09/04/23 14:45:43.101
------------------------------
• [20.369 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:45:22.746
    Sep  4 14:45:22.746: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 09/04/23 14:45:22.747
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:22.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:22.801
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2982 09/04/23 14:45:22.822
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-2982 09/04/23 14:45:22.834
    Sep  4 14:45:22.857: INFO: Found 0 stateful pods, waiting for 1
    Sep  4 14:45:32.872: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 09/04/23 14:45:32.897
    STEP: updating a scale subresource 09/04/23 14:45:32.909
    STEP: verifying the statefulset Spec.Replicas was modified 09/04/23 14:45:32.923
    STEP: Patch a scale subresource 09/04/23 14:45:32.934
    STEP: verifying the statefulset Spec.Replicas was modified 09/04/23 14:45:32.948
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 14:45:32.961: INFO: Deleting all statefulset in ns statefulset-2982
    Sep  4 14:45:32.975: INFO: Scaling statefulset ss to 0
    Sep  4 14:45:43.030: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 14:45:43.043: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:45:43.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2982" for this suite. 09/04/23 14:45:43.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:45:43.116
Sep  4 14:45:43.116: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 09/04/23 14:45:43.116
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:43.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:43.172
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 09/04/23 14:45:43.193
STEP: Ensuring active pods == parallelism 09/04/23 14:45:43.206
STEP: Orphaning one of the Job's Pods 09/04/23 14:45:45.22
Sep  4 14:45:45.761: INFO: Successfully updated pod "adopt-release-7xjxf"
STEP: Checking that the Job readopts the Pod 09/04/23 14:45:45.761
Sep  4 14:45:45.761: INFO: Waiting up to 15m0s for pod "adopt-release-7xjxf" in namespace "job-9905" to be "adopted"
Sep  4 14:45:45.773: INFO: Pod "adopt-release-7xjxf": Phase="Running", Reason="", readiness=true. Elapsed: 11.442048ms
Sep  4 14:45:47.786: INFO: Pod "adopt-release-7xjxf": Phase="Running", Reason="", readiness=true. Elapsed: 2.024841692s
Sep  4 14:45:47.786: INFO: Pod "adopt-release-7xjxf" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 09/04/23 14:45:47.786
Sep  4 14:45:48.312: INFO: Successfully updated pod "adopt-release-7xjxf"
STEP: Checking that the Job releases the Pod 09/04/23 14:45:48.312
Sep  4 14:45:48.313: INFO: Waiting up to 15m0s for pod "adopt-release-7xjxf" in namespace "job-9905" to be "released"
Sep  4 14:45:48.324: INFO: Pod "adopt-release-7xjxf": Phase="Running", Reason="", readiness=true. Elapsed: 11.232424ms
Sep  4 14:45:50.338: INFO: Pod "adopt-release-7xjxf": Phase="Running", Reason="", readiness=true. Elapsed: 2.025127653s
Sep  4 14:45:50.338: INFO: Pod "adopt-release-7xjxf" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  4 14:45:50.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9905" for this suite. 09/04/23 14:45:50.36
------------------------------
• [7.258 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:45:43.116
    Sep  4 14:45:43.116: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 09/04/23 14:45:43.116
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:43.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:43.172
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 09/04/23 14:45:43.193
    STEP: Ensuring active pods == parallelism 09/04/23 14:45:43.206
    STEP: Orphaning one of the Job's Pods 09/04/23 14:45:45.22
    Sep  4 14:45:45.761: INFO: Successfully updated pod "adopt-release-7xjxf"
    STEP: Checking that the Job readopts the Pod 09/04/23 14:45:45.761
    Sep  4 14:45:45.761: INFO: Waiting up to 15m0s for pod "adopt-release-7xjxf" in namespace "job-9905" to be "adopted"
    Sep  4 14:45:45.773: INFO: Pod "adopt-release-7xjxf": Phase="Running", Reason="", readiness=true. Elapsed: 11.442048ms
    Sep  4 14:45:47.786: INFO: Pod "adopt-release-7xjxf": Phase="Running", Reason="", readiness=true. Elapsed: 2.024841692s
    Sep  4 14:45:47.786: INFO: Pod "adopt-release-7xjxf" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 09/04/23 14:45:47.786
    Sep  4 14:45:48.312: INFO: Successfully updated pod "adopt-release-7xjxf"
    STEP: Checking that the Job releases the Pod 09/04/23 14:45:48.312
    Sep  4 14:45:48.313: INFO: Waiting up to 15m0s for pod "adopt-release-7xjxf" in namespace "job-9905" to be "released"
    Sep  4 14:45:48.324: INFO: Pod "adopt-release-7xjxf": Phase="Running", Reason="", readiness=true. Elapsed: 11.232424ms
    Sep  4 14:45:50.338: INFO: Pod "adopt-release-7xjxf": Phase="Running", Reason="", readiness=true. Elapsed: 2.025127653s
    Sep  4 14:45:50.338: INFO: Pod "adopt-release-7xjxf" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:45:50.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9905" for this suite. 09/04/23 14:45:50.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:45:50.374
Sep  4 14:45:50.374: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 09/04/23 14:45:50.374
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:50.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:50.43
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 09/04/23 14:45:50.451
STEP: modifying the configmap once 09/04/23 14:45:50.463
STEP: modifying the configmap a second time 09/04/23 14:45:50.488
STEP: deleting the configmap 09/04/23 14:45:50.513
STEP: creating a watch on configmaps from the resource version returned by the first update 09/04/23 14:45:50.526
STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/04/23 14:45:50.537
Sep  4 14:45:50.537: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6598  01dcbdf6-8d4c-4bc2-a676-919188aaa176 7075 0 2023-09-04 14:45:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-04 14:45:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 14:45:50.537: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6598  01dcbdf6-8d4c-4bc2-a676-919188aaa176 7076 0 2023-09-04 14:45:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-04 14:45:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  4 14:45:50.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6598" for this suite. 09/04/23 14:45:50.55
------------------------------
• [0.190 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:45:50.374
    Sep  4 14:45:50.374: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 09/04/23 14:45:50.374
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:50.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:50.43
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 09/04/23 14:45:50.451
    STEP: modifying the configmap once 09/04/23 14:45:50.463
    STEP: modifying the configmap a second time 09/04/23 14:45:50.488
    STEP: deleting the configmap 09/04/23 14:45:50.513
    STEP: creating a watch on configmaps from the resource version returned by the first update 09/04/23 14:45:50.526
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/04/23 14:45:50.537
    Sep  4 14:45:50.537: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6598  01dcbdf6-8d4c-4bc2-a676-919188aaa176 7075 0 2023-09-04 14:45:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-04 14:45:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 14:45:50.537: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6598  01dcbdf6-8d4c-4bc2-a676-919188aaa176 7076 0 2023-09-04 14:45:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-04 14:45:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:45:50.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6598" for this suite. 09/04/23 14:45:50.55
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:45:50.564
Sep  4 14:45:50.564: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 14:45:50.565
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:50.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:50.622
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 09/04/23 14:45:50.642
Sep  4 14:45:50.662: INFO: Waiting up to 5m0s for pod "downward-api-62799dd3-268c-476f-9909-d9e11d354d31" in namespace "downward-api-8097" to be "Succeeded or Failed"
Sep  4 14:45:50.673: INFO: Pod "downward-api-62799dd3-268c-476f-9909-d9e11d354d31": Phase="Pending", Reason="", readiness=false. Elapsed: 11.722692ms
Sep  4 14:45:52.686: INFO: Pod "downward-api-62799dd3-268c-476f-9909-d9e11d354d31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024938596s
Sep  4 14:45:54.686: INFO: Pod "downward-api-62799dd3-268c-476f-9909-d9e11d354d31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024723975s
Sep  4 14:45:56.687: INFO: Pod "downward-api-62799dd3-268c-476f-9909-d9e11d354d31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025037572s
STEP: Saw pod success 09/04/23 14:45:56.687
Sep  4 14:45:56.687: INFO: Pod "downward-api-62799dd3-268c-476f-9909-d9e11d354d31" satisfied condition "Succeeded or Failed"
Sep  4 14:45:56.699: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx pod downward-api-62799dd3-268c-476f-9909-d9e11d354d31 container dapi-container: <nil>
STEP: delete the pod 09/04/23 14:45:56.869
Sep  4 14:45:56.886: INFO: Waiting for pod downward-api-62799dd3-268c-476f-9909-d9e11d354d31 to disappear
Sep  4 14:45:56.898: INFO: Pod downward-api-62799dd3-268c-476f-9909-d9e11d354d31 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  4 14:45:56.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8097" for this suite. 09/04/23 14:45:56.921
------------------------------
• [6.371 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:45:50.564
    Sep  4 14:45:50.564: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 14:45:50.565
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:50.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:50.622
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 09/04/23 14:45:50.642
    Sep  4 14:45:50.662: INFO: Waiting up to 5m0s for pod "downward-api-62799dd3-268c-476f-9909-d9e11d354d31" in namespace "downward-api-8097" to be "Succeeded or Failed"
    Sep  4 14:45:50.673: INFO: Pod "downward-api-62799dd3-268c-476f-9909-d9e11d354d31": Phase="Pending", Reason="", readiness=false. Elapsed: 11.722692ms
    Sep  4 14:45:52.686: INFO: Pod "downward-api-62799dd3-268c-476f-9909-d9e11d354d31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024938596s
    Sep  4 14:45:54.686: INFO: Pod "downward-api-62799dd3-268c-476f-9909-d9e11d354d31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024723975s
    Sep  4 14:45:56.687: INFO: Pod "downward-api-62799dd3-268c-476f-9909-d9e11d354d31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025037572s
    STEP: Saw pod success 09/04/23 14:45:56.687
    Sep  4 14:45:56.687: INFO: Pod "downward-api-62799dd3-268c-476f-9909-d9e11d354d31" satisfied condition "Succeeded or Failed"
    Sep  4 14:45:56.699: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx pod downward-api-62799dd3-268c-476f-9909-d9e11d354d31 container dapi-container: <nil>
    STEP: delete the pod 09/04/23 14:45:56.869
    Sep  4 14:45:56.886: INFO: Waiting for pod downward-api-62799dd3-268c-476f-9909-d9e11d354d31 to disappear
    Sep  4 14:45:56.898: INFO: Pod downward-api-62799dd3-268c-476f-9909-d9e11d354d31 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:45:56.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8097" for this suite. 09/04/23 14:45:56.921
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:45:56.937
Sep  4 14:45:56.937: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 09/04/23 14:45:56.938
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:56.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:56.994
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  4 14:45:57.016: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  4 14:45:57.042: INFO: Waiting for terminating namespaces to be deleted...
Sep  4 14:45:57.054: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx before test
Sep  4 14:45:57.073: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-58685978db-gpxdq from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Sep  4 14:45:57.073: INFO: apiserver-proxy-bc5jh from kube-system started at 2023-09-04 14:34:44 +0000 UTC (2 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container proxy ready: true, restart count 0
Sep  4 14:45:57.073: INFO: 	Container sidecar ready: true, restart count 0
Sep  4 14:45:57.073: INFO: blackbox-exporter-585854d657-fvtbg from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container blackbox-exporter ready: true, restart count 0
Sep  4 14:45:57.073: INFO: blackbox-exporter-585854d657-vz8rw from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container blackbox-exporter ready: true, restart count 0
Sep  4 14:45:57.073: INFO: calico-kube-controllers-684b9f4889-f24wb from kube-system started at 2023-09-04 14:34:42 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  4 14:45:57.073: INFO: calico-node-959qs from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 14:45:57.073: INFO: calico-node-vertical-autoscaler-7bbd54698f-zb6st from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container autoscaler ready: true, restart count 0
Sep  4 14:45:57.073: INFO: calico-typha-deploy-6f4475c6d5-z7t2m from kube-system started at 2023-09-04 14:35:50 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container calico-typha ready: true, restart count 0
Sep  4 14:45:57.073: INFO: calico-typha-horizontal-autoscaler-7b89d5ff97-gdvkd from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container autoscaler ready: true, restart count 0
Sep  4 14:45:57.073: INFO: calico-typha-vertical-autoscaler-656479b7b5-khd8g from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container autoscaler ready: true, restart count 0
Sep  4 14:45:57.073: INFO: coredns-89679867b-2fr9s from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container coredns ready: true, restart count 0
Sep  4 14:45:57.073: INFO: coredns-89679867b-5qtl7 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container coredns ready: true, restart count 0
Sep  4 14:45:57.073: INFO: csi-driver-node-5kfkx from kube-system started at 2023-09-04 14:34:46 +0000 UTC (3 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container csi-driver ready: true, restart count 0
Sep  4 14:45:57.073: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Sep  4 14:45:57.073: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Sep  4 14:45:57.073: INFO: egress-filter-applier-n4kfc from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container egress-filter-applier ready: true, restart count 0
Sep  4 14:45:57.073: INFO: kube-proxy-worker-1-v1.26.8-j2qxz from kube-system started at 2023-09-04 14:37:10 +0000 UTC (2 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container conntrack-fix ready: true, restart count 0
Sep  4 14:45:57.073: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 14:45:57.073: INFO: metrics-server-78947f8d7c-9cdcs from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container metrics-server ready: true, restart count 0
Sep  4 14:45:57.073: INFO: network-problem-detector-host-xf9l5 from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Sep  4 14:45:57.073: INFO: network-problem-detector-pod-lbp4l from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Sep  4 14:45:57.073: INFO: node-exporter-ktjnf from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container node-exporter ready: true, restart count 0
Sep  4 14:45:57.073: INFO: node-local-dns-c8s84 from kube-system started at 2023-09-04 14:44:10 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container node-cache ready: true, restart count 0
Sep  4 14:45:57.073: INFO: node-problem-detector-rqmbk from kube-system started at 2023-09-04 14:41:10 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep  4 14:45:57.073: INFO: vpn-shoot-5d596dbb88-5vx52 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container vpn-shoot ready: true, restart count 0
Sep  4 14:45:57.073: INFO: dashboard-metrics-scraper-6c889fdd54-555v8 from kubernetes-dashboard started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.073: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Sep  4 14:45:57.073: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh before test
Sep  4 14:45:57.100: INFO: adopt-release-7xjxf from job-9905 started at 2023-09-04 14:45:43 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container c ready: true, restart count 0
Sep  4 14:45:57.100: INFO: adopt-release-m52kk from job-9905 started at 2023-09-04 14:45:49 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container c ready: true, restart count 0
Sep  4 14:45:57.100: INFO: adopt-release-rqzfs from job-9905 started at 2023-09-04 14:45:43 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container c ready: true, restart count 0
Sep  4 14:45:57.100: INFO: addons-nginx-ingress-controller-56b5dc8f6c-zwwh4 from kube-system started at 2023-09-04 14:43:10 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep  4 14:45:57.100: INFO: apiserver-proxy-4xjvl from kube-system started at 2023-09-04 14:35:03 +0000 UTC (2 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container proxy ready: true, restart count 0
Sep  4 14:45:57.100: INFO: 	Container sidecar ready: true, restart count 0
Sep  4 14:45:57.100: INFO: calico-node-dqrx8 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 14:45:57.100: INFO: calico-typha-deploy-6f4475c6d5-ws59v from kube-system started at 2023-09-04 14:35:42 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container calico-typha ready: true, restart count 0
Sep  4 14:45:57.100: INFO: csi-driver-node-fcbsh from kube-system started at 2023-09-04 14:35:03 +0000 UTC (3 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container csi-driver ready: true, restart count 0
Sep  4 14:45:57.100: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Sep  4 14:45:57.100: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Sep  4 14:45:57.100: INFO: egress-filter-applier-42dhz from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container egress-filter-applier ready: true, restart count 0
Sep  4 14:45:57.100: INFO: kube-proxy-worker-1-v1.26.8-zppm4 from kube-system started at 2023-09-04 14:36:10 +0000 UTC (2 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container conntrack-fix ready: true, restart count 0
Sep  4 14:45:57.100: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 14:45:57.100: INFO: metrics-server-78947f8d7c-rlkz6 from kube-system started at 2023-09-04 14:41:10 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container metrics-server ready: true, restart count 0
Sep  4 14:45:57.100: INFO: network-problem-detector-host-4k5k5 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Sep  4 14:45:57.100: INFO: network-problem-detector-pod-sv5cs from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Sep  4 14:45:57.100: INFO: node-exporter-mp64n from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container node-exporter ready: true, restart count 0
Sep  4 14:45:57.100: INFO: node-local-dns-fjhw2 from kube-system started at 2023-09-04 14:43:11 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container node-cache ready: true, restart count 0
Sep  4 14:45:57.100: INFO: node-problem-detector-gqcc9 from kube-system started at 2023-09-04 14:42:11 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep  4 14:45:57.100: INFO: kubernetes-dashboard-b9859c4d7-g9k72 from kubernetes-dashboard started at 2023-09-04 14:42:09 +0000 UTC (1 container statuses recorded)
Sep  4 14:45:57.100: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 09/04/23 14:45:57.1
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1781b9601340711f], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 09/04/23 14:45:57.167
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 14:45:58.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5523" for this suite. 09/04/23 14:45:58.198
------------------------------
• [1.277 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:45:56.937
    Sep  4 14:45:56.937: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 09/04/23 14:45:56.938
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:56.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:56.994
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  4 14:45:57.016: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  4 14:45:57.042: INFO: Waiting for terminating namespaces to be deleted...
    Sep  4 14:45:57.054: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx before test
    Sep  4 14:45:57.073: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-58685978db-gpxdq from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: apiserver-proxy-bc5jh from kube-system started at 2023-09-04 14:34:44 +0000 UTC (2 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container proxy ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: 	Container sidecar ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: blackbox-exporter-585854d657-fvtbg from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: blackbox-exporter-585854d657-vz8rw from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: calico-kube-controllers-684b9f4889-f24wb from kube-system started at 2023-09-04 14:34:42 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: calico-node-959qs from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: calico-node-vertical-autoscaler-7bbd54698f-zb6st from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: calico-typha-deploy-6f4475c6d5-z7t2m from kube-system started at 2023-09-04 14:35:50 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container calico-typha ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: calico-typha-horizontal-autoscaler-7b89d5ff97-gdvkd from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: calico-typha-vertical-autoscaler-656479b7b5-khd8g from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: coredns-89679867b-2fr9s from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: coredns-89679867b-5qtl7 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: csi-driver-node-5kfkx from kube-system started at 2023-09-04 14:34:46 +0000 UTC (3 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container csi-driver ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: egress-filter-applier-n4kfc from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: kube-proxy-worker-1-v1.26.8-j2qxz from kube-system started at 2023-09-04 14:37:10 +0000 UTC (2 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container conntrack-fix ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: metrics-server-78947f8d7c-9cdcs from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container metrics-server ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: network-problem-detector-host-xf9l5 from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: network-problem-detector-pod-lbp4l from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: node-exporter-ktjnf from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: node-local-dns-c8s84 from kube-system started at 2023-09-04 14:44:10 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container node-cache ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: node-problem-detector-rqmbk from kube-system started at 2023-09-04 14:41:10 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: vpn-shoot-5d596dbb88-5vx52 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container vpn-shoot ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: dashboard-metrics-scraper-6c889fdd54-555v8 from kubernetes-dashboard started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.073: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Sep  4 14:45:57.073: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh before test
    Sep  4 14:45:57.100: INFO: adopt-release-7xjxf from job-9905 started at 2023-09-04 14:45:43 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container c ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: adopt-release-m52kk from job-9905 started at 2023-09-04 14:45:49 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container c ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: adopt-release-rqzfs from job-9905 started at 2023-09-04 14:45:43 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container c ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: addons-nginx-ingress-controller-56b5dc8f6c-zwwh4 from kube-system started at 2023-09-04 14:43:10 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: apiserver-proxy-4xjvl from kube-system started at 2023-09-04 14:35:03 +0000 UTC (2 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container proxy ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: 	Container sidecar ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: calico-node-dqrx8 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: calico-typha-deploy-6f4475c6d5-ws59v from kube-system started at 2023-09-04 14:35:42 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container calico-typha ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: csi-driver-node-fcbsh from kube-system started at 2023-09-04 14:35:03 +0000 UTC (3 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container csi-driver ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: egress-filter-applier-42dhz from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: kube-proxy-worker-1-v1.26.8-zppm4 from kube-system started at 2023-09-04 14:36:10 +0000 UTC (2 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container conntrack-fix ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: metrics-server-78947f8d7c-rlkz6 from kube-system started at 2023-09-04 14:41:10 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container metrics-server ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: network-problem-detector-host-4k5k5 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: network-problem-detector-pod-sv5cs from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: node-exporter-mp64n from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: node-local-dns-fjhw2 from kube-system started at 2023-09-04 14:43:11 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container node-cache ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: node-problem-detector-gqcc9 from kube-system started at 2023-09-04 14:42:11 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep  4 14:45:57.100: INFO: kubernetes-dashboard-b9859c4d7-g9k72 from kubernetes-dashboard started at 2023-09-04 14:42:09 +0000 UTC (1 container statuses recorded)
    Sep  4 14:45:57.100: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 09/04/23 14:45:57.1
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1781b9601340711f], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 09/04/23 14:45:57.167
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:45:58.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5523" for this suite. 09/04/23 14:45:58.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:45:58.215
Sep  4 14:45:58.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 09/04/23 14:45:58.216
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:58.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:58.272
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Sep  4 14:45:58.313: INFO: Waiting up to 2m0s for pod "var-expansion-81da9fc2-6e8f-4e8f-8d1f-9385ca57844e" in namespace "var-expansion-5082" to be "container 0 failed with reason CreateContainerConfigError"
Sep  4 14:45:58.325: INFO: Pod "var-expansion-81da9fc2-6e8f-4e8f-8d1f-9385ca57844e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.11317ms
Sep  4 14:46:00.339: INFO: Pod "var-expansion-81da9fc2-6e8f-4e8f-8d1f-9385ca57844e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025344906s
Sep  4 14:46:00.339: INFO: Pod "var-expansion-81da9fc2-6e8f-4e8f-8d1f-9385ca57844e" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep  4 14:46:00.339: INFO: Deleting pod "var-expansion-81da9fc2-6e8f-4e8f-8d1f-9385ca57844e" in namespace "var-expansion-5082"
Sep  4 14:46:00.352: INFO: Wait up to 5m0s for pod "var-expansion-81da9fc2-6e8f-4e8f-8d1f-9385ca57844e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 14:46:02.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5082" for this suite. 09/04/23 14:46:02.401
------------------------------
• [4.200 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:45:58.215
    Sep  4 14:45:58.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 09/04/23 14:45:58.216
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:45:58.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:45:58.272
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Sep  4 14:45:58.313: INFO: Waiting up to 2m0s for pod "var-expansion-81da9fc2-6e8f-4e8f-8d1f-9385ca57844e" in namespace "var-expansion-5082" to be "container 0 failed with reason CreateContainerConfigError"
    Sep  4 14:45:58.325: INFO: Pod "var-expansion-81da9fc2-6e8f-4e8f-8d1f-9385ca57844e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.11317ms
    Sep  4 14:46:00.339: INFO: Pod "var-expansion-81da9fc2-6e8f-4e8f-8d1f-9385ca57844e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025344906s
    Sep  4 14:46:00.339: INFO: Pod "var-expansion-81da9fc2-6e8f-4e8f-8d1f-9385ca57844e" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep  4 14:46:00.339: INFO: Deleting pod "var-expansion-81da9fc2-6e8f-4e8f-8d1f-9385ca57844e" in namespace "var-expansion-5082"
    Sep  4 14:46:00.352: INFO: Wait up to 5m0s for pod "var-expansion-81da9fc2-6e8f-4e8f-8d1f-9385ca57844e" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:46:02.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5082" for this suite. 09/04/23 14:46:02.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:46:02.415
Sep  4 14:46:02.415: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 09/04/23 14:46:02.416
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:46:02.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:46:02.473
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 09/04/23 14:46:02.494
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5675.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local; sleep 1; done
 09/04/23 14:46:02.507
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5675.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local; sleep 1; done
 09/04/23 14:46:02.507
STEP: creating a pod to probe DNS 09/04/23 14:46:02.507
STEP: submitting the pod to kubernetes 09/04/23 14:46:02.507
Sep  4 14:46:02.527: INFO: Waiting up to 15m0s for pod "dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f" in namespace "dns-5675" to be "running"
Sep  4 14:46:02.538: INFO: Pod "dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.720013ms
Sep  4 14:46:04.552: INFO: Pod "dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024868257s
Sep  4 14:46:06.552: INFO: Pod "dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024783579s
Sep  4 14:46:08.552: INFO: Pod "dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025619139s
Sep  4 14:46:10.552: INFO: Pod "dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f": Phase="Running", Reason="", readiness=true. Elapsed: 8.025179159s
Sep  4 14:46:10.552: INFO: Pod "dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f" satisfied condition "running"
STEP: retrieving the pod 09/04/23 14:46:10.552
STEP: looking for the results for each expected name from probers 09/04/23 14:46:10.564
Sep  4 14:46:10.748: INFO: DNS probes using dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f succeeded

STEP: deleting the pod 09/04/23 14:46:10.748
STEP: changing the externalName to bar.example.com 09/04/23 14:46:10.764
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5675.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local; sleep 1; done
 09/04/23 14:46:10.787
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5675.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local; sleep 1; done
 09/04/23 14:46:10.787
STEP: creating a second pod to probe DNS 09/04/23 14:46:10.787
STEP: submitting the pod to kubernetes 09/04/23 14:46:10.787
Sep  4 14:46:10.806: INFO: Waiting up to 15m0s for pod "dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00" in namespace "dns-5675" to be "running"
Sep  4 14:46:10.817: INFO: Pod "dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00": Phase="Pending", Reason="", readiness=false. Elapsed: 11.328195ms
Sep  4 14:46:12.830: INFO: Pod "dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00": Phase="Running", Reason="", readiness=true. Elapsed: 2.023908891s
Sep  4 14:46:12.830: INFO: Pod "dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00" satisfied condition "running"
STEP: retrieving the pod 09/04/23 14:46:12.83
STEP: looking for the results for each expected name from probers 09/04/23 14:46:12.842
Sep  4 14:46:12.907: INFO: File wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  4 14:46:12.961: INFO: File jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  4 14:46:12.961: INFO: Lookups using dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 failed for: [wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local]

Sep  4 14:46:17.984: INFO: File wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  4 14:46:18.035: INFO: File jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  4 14:46:18.035: INFO: Lookups using dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 failed for: [wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local]

Sep  4 14:46:22.988: INFO: File wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  4 14:46:23.040: INFO: File jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  4 14:46:23.040: INFO: Lookups using dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 failed for: [wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local]

Sep  4 14:46:27.987: INFO: File wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  4 14:46:28.039: INFO: File jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  4 14:46:28.039: INFO: Lookups using dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 failed for: [wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local]

Sep  4 14:46:32.994: INFO: File wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  4 14:46:33.047: INFO: File jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  4 14:46:33.047: INFO: Lookups using dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 failed for: [wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local]

Sep  4 14:46:38.040: INFO: DNS probes using dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 succeeded

STEP: deleting the pod 09/04/23 14:46:38.04
STEP: changing the service to type=ClusterIP 09/04/23 14:46:38.057
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5675.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local; sleep 1; done
 09/04/23 14:46:38.086
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5675.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local; sleep 1; done
 09/04/23 14:46:38.086
STEP: creating a third pod to probe DNS 09/04/23 14:46:38.086
STEP: submitting the pod to kubernetes 09/04/23 14:46:38.097
Sep  4 14:46:38.117: INFO: Waiting up to 15m0s for pod "dns-test-8f6db18d-451a-4876-b345-ea6c69000950" in namespace "dns-5675" to be "running"
Sep  4 14:46:38.128: INFO: Pod "dns-test-8f6db18d-451a-4876-b345-ea6c69000950": Phase="Pending", Reason="", readiness=false. Elapsed: 11.249453ms
Sep  4 14:46:40.143: INFO: Pod "dns-test-8f6db18d-451a-4876-b345-ea6c69000950": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025597124s
Sep  4 14:46:42.142: INFO: Pod "dns-test-8f6db18d-451a-4876-b345-ea6c69000950": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024719153s
Sep  4 14:46:44.142: INFO: Pod "dns-test-8f6db18d-451a-4876-b345-ea6c69000950": Phase="Running", Reason="", readiness=true. Elapsed: 6.02475382s
Sep  4 14:46:44.142: INFO: Pod "dns-test-8f6db18d-451a-4876-b345-ea6c69000950" satisfied condition "running"
STEP: retrieving the pod 09/04/23 14:46:44.142
STEP: looking for the results for each expected name from probers 09/04/23 14:46:44.155
Sep  4 14:46:44.313: INFO: DNS probes using dns-test-8f6db18d-451a-4876-b345-ea6c69000950 succeeded

STEP: deleting the pod 09/04/23 14:46:44.313
STEP: deleting the test externalName service 09/04/23 14:46:44.329
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 14:46:44.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5675" for this suite. 09/04/23 14:46:44.368
------------------------------
• [41.965 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:46:02.415
    Sep  4 14:46:02.415: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 09/04/23 14:46:02.416
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:46:02.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:46:02.473
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 09/04/23 14:46:02.494
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5675.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local; sleep 1; done
     09/04/23 14:46:02.507
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5675.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local; sleep 1; done
     09/04/23 14:46:02.507
    STEP: creating a pod to probe DNS 09/04/23 14:46:02.507
    STEP: submitting the pod to kubernetes 09/04/23 14:46:02.507
    Sep  4 14:46:02.527: INFO: Waiting up to 15m0s for pod "dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f" in namespace "dns-5675" to be "running"
    Sep  4 14:46:02.538: INFO: Pod "dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.720013ms
    Sep  4 14:46:04.552: INFO: Pod "dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024868257s
    Sep  4 14:46:06.552: INFO: Pod "dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024783579s
    Sep  4 14:46:08.552: INFO: Pod "dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025619139s
    Sep  4 14:46:10.552: INFO: Pod "dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f": Phase="Running", Reason="", readiness=true. Elapsed: 8.025179159s
    Sep  4 14:46:10.552: INFO: Pod "dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 14:46:10.552
    STEP: looking for the results for each expected name from probers 09/04/23 14:46:10.564
    Sep  4 14:46:10.748: INFO: DNS probes using dns-test-6ff72296-8d2d-4d6f-8d2b-c0821ebb7d0f succeeded

    STEP: deleting the pod 09/04/23 14:46:10.748
    STEP: changing the externalName to bar.example.com 09/04/23 14:46:10.764
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5675.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local; sleep 1; done
     09/04/23 14:46:10.787
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5675.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local; sleep 1; done
     09/04/23 14:46:10.787
    STEP: creating a second pod to probe DNS 09/04/23 14:46:10.787
    STEP: submitting the pod to kubernetes 09/04/23 14:46:10.787
    Sep  4 14:46:10.806: INFO: Waiting up to 15m0s for pod "dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00" in namespace "dns-5675" to be "running"
    Sep  4 14:46:10.817: INFO: Pod "dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00": Phase="Pending", Reason="", readiness=false. Elapsed: 11.328195ms
    Sep  4 14:46:12.830: INFO: Pod "dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00": Phase="Running", Reason="", readiness=true. Elapsed: 2.023908891s
    Sep  4 14:46:12.830: INFO: Pod "dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 14:46:12.83
    STEP: looking for the results for each expected name from probers 09/04/23 14:46:12.842
    Sep  4 14:46:12.907: INFO: File wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  4 14:46:12.961: INFO: File jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  4 14:46:12.961: INFO: Lookups using dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 failed for: [wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local]

    Sep  4 14:46:17.984: INFO: File wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  4 14:46:18.035: INFO: File jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  4 14:46:18.035: INFO: Lookups using dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 failed for: [wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local]

    Sep  4 14:46:22.988: INFO: File wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  4 14:46:23.040: INFO: File jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  4 14:46:23.040: INFO: Lookups using dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 failed for: [wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local]

    Sep  4 14:46:27.987: INFO: File wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  4 14:46:28.039: INFO: File jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  4 14:46:28.039: INFO: Lookups using dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 failed for: [wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local]

    Sep  4 14:46:32.994: INFO: File wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  4 14:46:33.047: INFO: File jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local from pod  dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  4 14:46:33.047: INFO: Lookups using dns-5675/dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 failed for: [wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local]

    Sep  4 14:46:38.040: INFO: DNS probes using dns-test-a6ab0173-24f6-4f2a-8a5c-4394a03bda00 succeeded

    STEP: deleting the pod 09/04/23 14:46:38.04
    STEP: changing the service to type=ClusterIP 09/04/23 14:46:38.057
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5675.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5675.svc.cluster.local; sleep 1; done
     09/04/23 14:46:38.086
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5675.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5675.svc.cluster.local; sleep 1; done
     09/04/23 14:46:38.086
    STEP: creating a third pod to probe DNS 09/04/23 14:46:38.086
    STEP: submitting the pod to kubernetes 09/04/23 14:46:38.097
    Sep  4 14:46:38.117: INFO: Waiting up to 15m0s for pod "dns-test-8f6db18d-451a-4876-b345-ea6c69000950" in namespace "dns-5675" to be "running"
    Sep  4 14:46:38.128: INFO: Pod "dns-test-8f6db18d-451a-4876-b345-ea6c69000950": Phase="Pending", Reason="", readiness=false. Elapsed: 11.249453ms
    Sep  4 14:46:40.143: INFO: Pod "dns-test-8f6db18d-451a-4876-b345-ea6c69000950": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025597124s
    Sep  4 14:46:42.142: INFO: Pod "dns-test-8f6db18d-451a-4876-b345-ea6c69000950": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024719153s
    Sep  4 14:46:44.142: INFO: Pod "dns-test-8f6db18d-451a-4876-b345-ea6c69000950": Phase="Running", Reason="", readiness=true. Elapsed: 6.02475382s
    Sep  4 14:46:44.142: INFO: Pod "dns-test-8f6db18d-451a-4876-b345-ea6c69000950" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 14:46:44.142
    STEP: looking for the results for each expected name from probers 09/04/23 14:46:44.155
    Sep  4 14:46:44.313: INFO: DNS probes using dns-test-8f6db18d-451a-4876-b345-ea6c69000950 succeeded

    STEP: deleting the pod 09/04/23 14:46:44.313
    STEP: deleting the test externalName service 09/04/23 14:46:44.329
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:46:44.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5675" for this suite. 09/04/23 14:46:44.368
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:46:44.381
Sep  4 14:46:44.381: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 14:46:44.382
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:46:44.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:46:44.438
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 09/04/23 14:46:44.459
Sep  4 14:46:44.478: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61" in namespace "projected-9935" to be "Succeeded or Failed"
Sep  4 14:46:44.490: INFO: Pod "downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61": Phase="Pending", Reason="", readiness=false. Elapsed: 11.779564ms
Sep  4 14:46:46.504: INFO: Pod "downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02570161s
Sep  4 14:46:48.504: INFO: Pod "downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026223917s
STEP: Saw pod success 09/04/23 14:46:48.504
Sep  4 14:46:48.504: INFO: Pod "downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61" satisfied condition "Succeeded or Failed"
Sep  4 14:46:48.516: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61 container client-container: <nil>
STEP: delete the pod 09/04/23 14:46:48.697
Sep  4 14:46:48.713: INFO: Waiting for pod downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61 to disappear
Sep  4 14:46:48.724: INFO: Pod downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 14:46:48.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9935" for this suite. 09/04/23 14:46:48.746
------------------------------
• [4.378 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:46:44.381
    Sep  4 14:46:44.381: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 14:46:44.382
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:46:44.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:46:44.438
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 09/04/23 14:46:44.459
    Sep  4 14:46:44.478: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61" in namespace "projected-9935" to be "Succeeded or Failed"
    Sep  4 14:46:44.490: INFO: Pod "downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61": Phase="Pending", Reason="", readiness=false. Elapsed: 11.779564ms
    Sep  4 14:46:46.504: INFO: Pod "downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02570161s
    Sep  4 14:46:48.504: INFO: Pod "downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026223917s
    STEP: Saw pod success 09/04/23 14:46:48.504
    Sep  4 14:46:48.504: INFO: Pod "downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61" satisfied condition "Succeeded or Failed"
    Sep  4 14:46:48.516: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61 container client-container: <nil>
    STEP: delete the pod 09/04/23 14:46:48.697
    Sep  4 14:46:48.713: INFO: Waiting for pod downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61 to disappear
    Sep  4 14:46:48.724: INFO: Pod downwardapi-volume-84c10add-0563-4044-bb7c-1caf81e02e61 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:46:48.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9935" for this suite. 09/04/23 14:46:48.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:46:48.759
Sep  4 14:46:48.759: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 09/04/23 14:46:48.76
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:46:48.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:46:48.816
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-dd5bebe6-7678-416c-a79f-25ad2f58d8e2 09/04/23 14:46:48.851
STEP: Creating secret with name s-test-opt-upd-3149598e-6e1d-409e-9614-d319102beb90 09/04/23 14:46:48.863
STEP: Creating the pod 09/04/23 14:46:48.876
Sep  4 14:46:48.897: INFO: Waiting up to 5m0s for pod "pod-secrets-b7dadbf5-f76a-4a45-9547-33304c05f31a" in namespace "secrets-6845" to be "running and ready"
Sep  4 14:46:48.909: INFO: Pod "pod-secrets-b7dadbf5-f76a-4a45-9547-33304c05f31a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.232322ms
Sep  4 14:46:48.909: INFO: The phase of Pod pod-secrets-b7dadbf5-f76a-4a45-9547-33304c05f31a is Pending, waiting for it to be Running (with Ready = true)
Sep  4 14:46:50.922: INFO: Pod "pod-secrets-b7dadbf5-f76a-4a45-9547-33304c05f31a": Phase="Running", Reason="", readiness=true. Elapsed: 2.025189934s
Sep  4 14:46:50.922: INFO: The phase of Pod pod-secrets-b7dadbf5-f76a-4a45-9547-33304c05f31a is Running (Ready = true)
Sep  4 14:46:50.922: INFO: Pod "pod-secrets-b7dadbf5-f76a-4a45-9547-33304c05f31a" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-dd5bebe6-7678-416c-a79f-25ad2f58d8e2 09/04/23 14:46:51.227
STEP: Updating secret s-test-opt-upd-3149598e-6e1d-409e-9614-d319102beb90 09/04/23 14:46:51.24
STEP: Creating secret with name s-test-opt-create-d9e8374e-5872-454a-b01a-2fdfa8cec7e9 09/04/23 14:46:51.252
STEP: waiting to observe update in volume 09/04/23 14:46:51.265
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 14:48:02.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6845" for this suite. 09/04/23 14:48:02.713
------------------------------
• [73.968 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:46:48.759
    Sep  4 14:46:48.759: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 09/04/23 14:46:48.76
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:46:48.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:46:48.816
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-dd5bebe6-7678-416c-a79f-25ad2f58d8e2 09/04/23 14:46:48.851
    STEP: Creating secret with name s-test-opt-upd-3149598e-6e1d-409e-9614-d319102beb90 09/04/23 14:46:48.863
    STEP: Creating the pod 09/04/23 14:46:48.876
    Sep  4 14:46:48.897: INFO: Waiting up to 5m0s for pod "pod-secrets-b7dadbf5-f76a-4a45-9547-33304c05f31a" in namespace "secrets-6845" to be "running and ready"
    Sep  4 14:46:48.909: INFO: Pod "pod-secrets-b7dadbf5-f76a-4a45-9547-33304c05f31a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.232322ms
    Sep  4 14:46:48.909: INFO: The phase of Pod pod-secrets-b7dadbf5-f76a-4a45-9547-33304c05f31a is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 14:46:50.922: INFO: Pod "pod-secrets-b7dadbf5-f76a-4a45-9547-33304c05f31a": Phase="Running", Reason="", readiness=true. Elapsed: 2.025189934s
    Sep  4 14:46:50.922: INFO: The phase of Pod pod-secrets-b7dadbf5-f76a-4a45-9547-33304c05f31a is Running (Ready = true)
    Sep  4 14:46:50.922: INFO: Pod "pod-secrets-b7dadbf5-f76a-4a45-9547-33304c05f31a" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-dd5bebe6-7678-416c-a79f-25ad2f58d8e2 09/04/23 14:46:51.227
    STEP: Updating secret s-test-opt-upd-3149598e-6e1d-409e-9614-d319102beb90 09/04/23 14:46:51.24
    STEP: Creating secret with name s-test-opt-create-d9e8374e-5872-454a-b01a-2fdfa8cec7e9 09/04/23 14:46:51.252
    STEP: waiting to observe update in volume 09/04/23 14:46:51.265
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:48:02.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6845" for this suite. 09/04/23 14:48:02.713
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:48:02.728
Sep  4 14:48:02.728: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 09/04/23 14:48:02.729
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:48:02.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:48:02.786
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/04/23 14:48:02.807
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-v9qw 09/04/23 14:48:02.831
STEP: Creating a pod to test atomic-volume-subpath 09/04/23 14:48:02.831
Sep  4 14:48:02.850: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-v9qw" in namespace "subpath-5750" to be "Succeeded or Failed"
Sep  4 14:48:02.862: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Pending", Reason="", readiness=false. Elapsed: 11.956836ms
Sep  4 14:48:04.876: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 2.025447699s
Sep  4 14:48:06.875: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 4.024583082s
Sep  4 14:48:08.875: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 6.024390501s
Sep  4 14:48:10.876: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 8.025279991s
Sep  4 14:48:12.876: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 10.02543078s
Sep  4 14:48:14.876: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 12.025758959s
Sep  4 14:48:16.875: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 14.02433894s
Sep  4 14:48:18.875: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 16.024335601s
Sep  4 14:48:20.877: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 18.026249448s
Sep  4 14:48:22.876: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 20.026081417s
Sep  4 14:48:24.878: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=false. Elapsed: 22.027641807s
Sep  4 14:48:26.875: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.024354419s
STEP: Saw pod success 09/04/23 14:48:26.875
Sep  4 14:48:26.875: INFO: Pod "pod-subpath-test-secret-v9qw" satisfied condition "Succeeded or Failed"
Sep  4 14:48:26.886: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-subpath-test-secret-v9qw container test-container-subpath-secret-v9qw: <nil>
STEP: delete the pod 09/04/23 14:48:26.921
Sep  4 14:48:26.936: INFO: Waiting for pod pod-subpath-test-secret-v9qw to disappear
Sep  4 14:48:26.953: INFO: Pod pod-subpath-test-secret-v9qw no longer exists
STEP: Deleting pod pod-subpath-test-secret-v9qw 09/04/23 14:48:26.953
Sep  4 14:48:26.953: INFO: Deleting pod "pod-subpath-test-secret-v9qw" in namespace "subpath-5750"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  4 14:48:26.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5750" for this suite. 09/04/23 14:48:26.986
------------------------------
• [24.271 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:48:02.728
    Sep  4 14:48:02.728: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 09/04/23 14:48:02.729
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:48:02.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:48:02.786
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/04/23 14:48:02.807
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-v9qw 09/04/23 14:48:02.831
    STEP: Creating a pod to test atomic-volume-subpath 09/04/23 14:48:02.831
    Sep  4 14:48:02.850: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-v9qw" in namespace "subpath-5750" to be "Succeeded or Failed"
    Sep  4 14:48:02.862: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Pending", Reason="", readiness=false. Elapsed: 11.956836ms
    Sep  4 14:48:04.876: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 2.025447699s
    Sep  4 14:48:06.875: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 4.024583082s
    Sep  4 14:48:08.875: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 6.024390501s
    Sep  4 14:48:10.876: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 8.025279991s
    Sep  4 14:48:12.876: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 10.02543078s
    Sep  4 14:48:14.876: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 12.025758959s
    Sep  4 14:48:16.875: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 14.02433894s
    Sep  4 14:48:18.875: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 16.024335601s
    Sep  4 14:48:20.877: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 18.026249448s
    Sep  4 14:48:22.876: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=true. Elapsed: 20.026081417s
    Sep  4 14:48:24.878: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Running", Reason="", readiness=false. Elapsed: 22.027641807s
    Sep  4 14:48:26.875: INFO: Pod "pod-subpath-test-secret-v9qw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.024354419s
    STEP: Saw pod success 09/04/23 14:48:26.875
    Sep  4 14:48:26.875: INFO: Pod "pod-subpath-test-secret-v9qw" satisfied condition "Succeeded or Failed"
    Sep  4 14:48:26.886: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-subpath-test-secret-v9qw container test-container-subpath-secret-v9qw: <nil>
    STEP: delete the pod 09/04/23 14:48:26.921
    Sep  4 14:48:26.936: INFO: Waiting for pod pod-subpath-test-secret-v9qw to disappear
    Sep  4 14:48:26.953: INFO: Pod pod-subpath-test-secret-v9qw no longer exists
    STEP: Deleting pod pod-subpath-test-secret-v9qw 09/04/23 14:48:26.953
    Sep  4 14:48:26.953: INFO: Deleting pod "pod-subpath-test-secret-v9qw" in namespace "subpath-5750"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:48:26.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5750" for this suite. 09/04/23 14:48:26.986
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:48:26.999
Sep  4 14:48:26.999: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 09/04/23 14:48:27
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:48:27.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:48:27.056
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/04/23 14:48:27.078
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-j6lb 09/04/23 14:48:27.103
STEP: Creating a pod to test atomic-volume-subpath 09/04/23 14:48:27.103
Sep  4 14:48:27.121: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-j6lb" in namespace "subpath-6014" to be "Succeeded or Failed"
Sep  4 14:48:27.132: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.064184ms
Sep  4 14:48:29.146: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 2.024934415s
Sep  4 14:48:31.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 4.024310943s
Sep  4 14:48:33.146: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 6.025132983s
Sep  4 14:48:35.144: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 8.023120674s
Sep  4 14:48:37.144: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 10.023578203s
Sep  4 14:48:39.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 12.024085907s
Sep  4 14:48:41.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 14.023939938s
Sep  4 14:48:43.146: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 16.025518252s
Sep  4 14:48:45.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 18.024059316s
Sep  4 14:48:47.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 20.024447631s
Sep  4 14:48:49.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=false. Elapsed: 22.024322391s
Sep  4 14:48:51.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.024141603s
STEP: Saw pod success 09/04/23 14:48:51.145
Sep  4 14:48:51.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb" satisfied condition "Succeeded or Failed"
Sep  4 14:48:51.157: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-subpath-test-downwardapi-j6lb container test-container-subpath-downwardapi-j6lb: <nil>
STEP: delete the pod 09/04/23 14:48:51.189
Sep  4 14:48:51.205: INFO: Waiting for pod pod-subpath-test-downwardapi-j6lb to disappear
Sep  4 14:48:51.216: INFO: Pod pod-subpath-test-downwardapi-j6lb no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-j6lb 09/04/23 14:48:51.216
Sep  4 14:48:51.216: INFO: Deleting pod "pod-subpath-test-downwardapi-j6lb" in namespace "subpath-6014"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  4 14:48:51.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6014" for this suite. 09/04/23 14:48:51.252
------------------------------
• [24.268 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:48:26.999
    Sep  4 14:48:26.999: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 09/04/23 14:48:27
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:48:27.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:48:27.056
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/04/23 14:48:27.078
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-j6lb 09/04/23 14:48:27.103
    STEP: Creating a pod to test atomic-volume-subpath 09/04/23 14:48:27.103
    Sep  4 14:48:27.121: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-j6lb" in namespace "subpath-6014" to be "Succeeded or Failed"
    Sep  4 14:48:27.132: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.064184ms
    Sep  4 14:48:29.146: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 2.024934415s
    Sep  4 14:48:31.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 4.024310943s
    Sep  4 14:48:33.146: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 6.025132983s
    Sep  4 14:48:35.144: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 8.023120674s
    Sep  4 14:48:37.144: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 10.023578203s
    Sep  4 14:48:39.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 12.024085907s
    Sep  4 14:48:41.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 14.023939938s
    Sep  4 14:48:43.146: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 16.025518252s
    Sep  4 14:48:45.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 18.024059316s
    Sep  4 14:48:47.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=true. Elapsed: 20.024447631s
    Sep  4 14:48:49.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Running", Reason="", readiness=false. Elapsed: 22.024322391s
    Sep  4 14:48:51.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.024141603s
    STEP: Saw pod success 09/04/23 14:48:51.145
    Sep  4 14:48:51.145: INFO: Pod "pod-subpath-test-downwardapi-j6lb" satisfied condition "Succeeded or Failed"
    Sep  4 14:48:51.157: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-subpath-test-downwardapi-j6lb container test-container-subpath-downwardapi-j6lb: <nil>
    STEP: delete the pod 09/04/23 14:48:51.189
    Sep  4 14:48:51.205: INFO: Waiting for pod pod-subpath-test-downwardapi-j6lb to disappear
    Sep  4 14:48:51.216: INFO: Pod pod-subpath-test-downwardapi-j6lb no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-j6lb 09/04/23 14:48:51.216
    Sep  4 14:48:51.216: INFO: Deleting pod "pod-subpath-test-downwardapi-j6lb" in namespace "subpath-6014"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:48:51.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6014" for this suite. 09/04/23 14:48:51.252
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:48:51.267
Sep  4 14:48:51.267: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator 09/04/23 14:48:51.269
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:48:51.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:48:51.325
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Sep  4 14:48:51.346: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 09/04/23 14:48:51.348
Sep  4 14:48:51.767: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 14:48:53.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 14:48:55.781: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 14:48:57.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 14:48:59.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 14:49:01.781: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 14:49:03.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 14:49:05.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 14:49:07.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 14:49:09.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 14:49:11.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 14:49:14.115: INFO: Waited 322.347563ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 09/04/23 14:49:14.718
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/04/23 14:49:14.731
STEP: List APIServices 09/04/23 14:49:14.744
Sep  4 14:49:14.758: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Sep  4 14:49:15.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-9983" for this suite. 09/04/23 14:49:15.097
------------------------------
• [23.842 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:48:51.267
    Sep  4 14:48:51.267: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename aggregator 09/04/23 14:48:51.269
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:48:51.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:48:51.325
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Sep  4 14:48:51.346: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 09/04/23 14:48:51.348
    Sep  4 14:48:51.767: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 14:48:53.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 14:48:55.781: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 14:48:57.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 14:48:59.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 14:49:01.781: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 14:49:03.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 14:49:05.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 14:49:07.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 14:49:09.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 14:49:11.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 48, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 14:49:14.115: INFO: Waited 322.347563ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 09/04/23 14:49:14.718
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/04/23 14:49:14.731
    STEP: List APIServices 09/04/23 14:49:14.744
    Sep  4 14:49:14.758: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:49:15.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-9983" for this suite. 09/04/23 14:49:15.097
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:49:15.11
Sep  4 14:49:15.110: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 09/04/23 14:49:15.111
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:49:15.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:49:15.169
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Sep  4 14:49:15.210: INFO: Waiting up to 5m0s for pod "busybox-scheduling-4da70add-f65c-4932-9c7e-c02640e65862" in namespace "kubelet-test-4699" to be "running and ready"
Sep  4 14:49:15.222: INFO: Pod "busybox-scheduling-4da70add-f65c-4932-9c7e-c02640e65862": Phase="Pending", Reason="", readiness=false. Elapsed: 12.229556ms
Sep  4 14:49:15.222: INFO: The phase of Pod busybox-scheduling-4da70add-f65c-4932-9c7e-c02640e65862 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 14:49:17.236: INFO: Pod "busybox-scheduling-4da70add-f65c-4932-9c7e-c02640e65862": Phase="Running", Reason="", readiness=true. Elapsed: 2.025912625s
Sep  4 14:49:17.236: INFO: The phase of Pod busybox-scheduling-4da70add-f65c-4932-9c7e-c02640e65862 is Running (Ready = true)
Sep  4 14:49:17.236: INFO: Pod "busybox-scheduling-4da70add-f65c-4932-9c7e-c02640e65862" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  4 14:49:17.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4699" for this suite. 09/04/23 14:49:17.346
------------------------------
• [2.249 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:49:15.11
    Sep  4 14:49:15.110: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 09/04/23 14:49:15.111
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:49:15.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:49:15.169
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Sep  4 14:49:15.210: INFO: Waiting up to 5m0s for pod "busybox-scheduling-4da70add-f65c-4932-9c7e-c02640e65862" in namespace "kubelet-test-4699" to be "running and ready"
    Sep  4 14:49:15.222: INFO: Pod "busybox-scheduling-4da70add-f65c-4932-9c7e-c02640e65862": Phase="Pending", Reason="", readiness=false. Elapsed: 12.229556ms
    Sep  4 14:49:15.222: INFO: The phase of Pod busybox-scheduling-4da70add-f65c-4932-9c7e-c02640e65862 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 14:49:17.236: INFO: Pod "busybox-scheduling-4da70add-f65c-4932-9c7e-c02640e65862": Phase="Running", Reason="", readiness=true. Elapsed: 2.025912625s
    Sep  4 14:49:17.236: INFO: The phase of Pod busybox-scheduling-4da70add-f65c-4932-9c7e-c02640e65862 is Running (Ready = true)
    Sep  4 14:49:17.236: INFO: Pod "busybox-scheduling-4da70add-f65c-4932-9c7e-c02640e65862" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:49:17.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4699" for this suite. 09/04/23 14:49:17.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:49:17.36
Sep  4 14:49:17.360: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 14:49:17.361
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:49:17.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:49:17.419
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-9d753be0-1b0d-40eb-bc2f-e3dcb55e8dde 09/04/23 14:49:17.441
STEP: Creating a pod to test consume configMaps 09/04/23 14:49:17.453
Sep  4 14:49:17.472: INFO: Waiting up to 5m0s for pod "pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e" in namespace "configmap-3656" to be "Succeeded or Failed"
Sep  4 14:49:17.483: INFO: Pod "pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.733506ms
Sep  4 14:49:19.497: INFO: Pod "pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025808648s
Sep  4 14:49:21.497: INFO: Pod "pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025251176s
STEP: Saw pod success 09/04/23 14:49:21.497
Sep  4 14:49:21.497: INFO: Pod "pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e" satisfied condition "Succeeded or Failed"
Sep  4 14:49:21.509: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e container agnhost-container: <nil>
STEP: delete the pod 09/04/23 14:49:21.544
Sep  4 14:49:21.560: INFO: Waiting for pod pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e to disappear
Sep  4 14:49:21.574: INFO: Pod pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 14:49:21.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3656" for this suite. 09/04/23 14:49:21.596
------------------------------
• [4.249 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:49:17.36
    Sep  4 14:49:17.360: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 14:49:17.361
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:49:17.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:49:17.419
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-9d753be0-1b0d-40eb-bc2f-e3dcb55e8dde 09/04/23 14:49:17.441
    STEP: Creating a pod to test consume configMaps 09/04/23 14:49:17.453
    Sep  4 14:49:17.472: INFO: Waiting up to 5m0s for pod "pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e" in namespace "configmap-3656" to be "Succeeded or Failed"
    Sep  4 14:49:17.483: INFO: Pod "pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.733506ms
    Sep  4 14:49:19.497: INFO: Pod "pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025808648s
    Sep  4 14:49:21.497: INFO: Pod "pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025251176s
    STEP: Saw pod success 09/04/23 14:49:21.497
    Sep  4 14:49:21.497: INFO: Pod "pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e" satisfied condition "Succeeded or Failed"
    Sep  4 14:49:21.509: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 14:49:21.544
    Sep  4 14:49:21.560: INFO: Waiting for pod pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e to disappear
    Sep  4 14:49:21.574: INFO: Pod pod-configmaps-f0bf29d1-cbdf-4ec7-a037-ccf73bc75a2e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:49:21.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3656" for this suite. 09/04/23 14:49:21.596
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:49:21.61
Sep  4 14:49:21.610: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 09/04/23 14:49:21.611
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:49:21.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:49:21.669
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 09/04/23 14:49:21.703
STEP: Updating PodDisruptionBudget status 09/04/23 14:49:21.715
STEP: Waiting for all pods to be running 09/04/23 14:49:21.732
Sep  4 14:49:21.743: INFO: running pods: 0 < 1
STEP: locating a running pod 09/04/23 14:49:23.757
STEP: Waiting for the pdb to be processed 09/04/23 14:49:23.794
STEP: Patching PodDisruptionBudget status 09/04/23 14:49:23.817
STEP: Waiting for the pdb to be processed 09/04/23 14:49:23.842
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  4 14:49:23.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6597" for this suite. 09/04/23 14:49:23.875
------------------------------
• [2.279 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:49:21.61
    Sep  4 14:49:21.610: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 09/04/23 14:49:21.611
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:49:21.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:49:21.669
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 09/04/23 14:49:21.703
    STEP: Updating PodDisruptionBudget status 09/04/23 14:49:21.715
    STEP: Waiting for all pods to be running 09/04/23 14:49:21.732
    Sep  4 14:49:21.743: INFO: running pods: 0 < 1
    STEP: locating a running pod 09/04/23 14:49:23.757
    STEP: Waiting for the pdb to be processed 09/04/23 14:49:23.794
    STEP: Patching PodDisruptionBudget status 09/04/23 14:49:23.817
    STEP: Waiting for the pdb to be processed 09/04/23 14:49:23.842
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:49:23.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6597" for this suite. 09/04/23 14:49:23.875
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:49:23.889
Sep  4 14:49:23.889: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate 09/04/23 14:49:23.889
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:49:23.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:49:23.947
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 09/04/23 14:49:23.969
STEP: Replace a pod template 09/04/23 14:49:23.982
Sep  4 14:49:24.006: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  4 14:49:24.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8973" for this suite. 09/04/23 14:49:24.019
------------------------------
• [0.144 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:49:23.889
    Sep  4 14:49:23.889: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename podtemplate 09/04/23 14:49:23.889
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:49:23.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:49:23.947
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 09/04/23 14:49:23.969
    STEP: Replace a pod template 09/04/23 14:49:23.982
    Sep  4 14:49:24.006: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:49:24.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8973" for this suite. 09/04/23 14:49:24.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:49:24.033
Sep  4 14:49:24.033: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice 09/04/23 14:49:24.034
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:49:24.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:49:24.092
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 09/04/23 14:49:29.221
STEP: referencing matching pods with named port 09/04/23 14:49:34.247
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/04/23 14:49:39.272
STEP: recreating EndpointSlices after they've been deleted 09/04/23 14:49:44.297
Sep  4 14:49:44.347: INFO: EndpointSlice for Service endpointslice-2187/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  4 14:49:54.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2187" for this suite. 09/04/23 14:49:54.4
------------------------------
• [30.380 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:49:24.033
    Sep  4 14:49:24.033: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslice 09/04/23 14:49:24.034
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:49:24.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:49:24.092
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 09/04/23 14:49:29.221
    STEP: referencing matching pods with named port 09/04/23 14:49:34.247
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/04/23 14:49:39.272
    STEP: recreating EndpointSlices after they've been deleted 09/04/23 14:49:44.297
    Sep  4 14:49:44.347: INFO: EndpointSlice for Service endpointslice-2187/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:49:54.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2187" for this suite. 09/04/23 14:49:54.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:49:54.413
Sep  4 14:49:54.413: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sysctl 09/04/23 14:49:54.414
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:49:54.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:49:54.472
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/04/23 14:49:54.493
STEP: Watching for error events or started pod 09/04/23 14:49:54.511
STEP: Waiting for pod completion 09/04/23 14:49:56.525
Sep  4 14:49:56.525: INFO: Waiting up to 3m0s for pod "sysctl-33d4cda2-b5c7-408a-bed8-07fe257ddd8c" in namespace "sysctl-8319" to be "completed"
Sep  4 14:49:56.537: INFO: Pod "sysctl-33d4cda2-b5c7-408a-bed8-07fe257ddd8c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.483721ms
Sep  4 14:49:58.551: INFO: Pod "sysctl-33d4cda2-b5c7-408a-bed8-07fe257ddd8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026269789s
Sep  4 14:49:58.551: INFO: Pod "sysctl-33d4cda2-b5c7-408a-bed8-07fe257ddd8c" satisfied condition "completed"
STEP: Checking that the pod succeeded 09/04/23 14:49:58.563
STEP: Getting logs from the pod 09/04/23 14:49:58.564
STEP: Checking that the sysctl is actually updated 09/04/23 14:49:58.607
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  4 14:49:58.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-8319" for this suite. 09/04/23 14:49:58.63
------------------------------
• [4.229 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:49:54.413
    Sep  4 14:49:54.413: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sysctl 09/04/23 14:49:54.414
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:49:54.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:49:54.472
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/04/23 14:49:54.493
    STEP: Watching for error events or started pod 09/04/23 14:49:54.511
    STEP: Waiting for pod completion 09/04/23 14:49:56.525
    Sep  4 14:49:56.525: INFO: Waiting up to 3m0s for pod "sysctl-33d4cda2-b5c7-408a-bed8-07fe257ddd8c" in namespace "sysctl-8319" to be "completed"
    Sep  4 14:49:56.537: INFO: Pod "sysctl-33d4cda2-b5c7-408a-bed8-07fe257ddd8c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.483721ms
    Sep  4 14:49:58.551: INFO: Pod "sysctl-33d4cda2-b5c7-408a-bed8-07fe257ddd8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026269789s
    Sep  4 14:49:58.551: INFO: Pod "sysctl-33d4cda2-b5c7-408a-bed8-07fe257ddd8c" satisfied condition "completed"
    STEP: Checking that the pod succeeded 09/04/23 14:49:58.563
    STEP: Getting logs from the pod 09/04/23 14:49:58.564
    STEP: Checking that the sysctl is actually updated 09/04/23 14:49:58.607
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:49:58.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-8319" for this suite. 09/04/23 14:49:58.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:49:58.646
Sep  4 14:49:58.646: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 09/04/23 14:49:58.647
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:49:58.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:49:58.703
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 09/04/23 14:49:58.725
STEP: starting a background goroutine to produce watch events 09/04/23 14:49:58.737
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/04/23 14:49:58.737
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  4 14:50:01.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3436" for this suite. 09/04/23 14:50:01.511
------------------------------
• [2.915 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:49:58.646
    Sep  4 14:49:58.646: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 09/04/23 14:49:58.647
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:49:58.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:49:58.703
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 09/04/23 14:49:58.725
    STEP: starting a background goroutine to produce watch events 09/04/23 14:49:58.737
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/04/23 14:49:58.737
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:50:01.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3436" for this suite. 09/04/23 14:50:01.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:50:01.562
Sep  4 14:50:01.562: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test 09/04/23 14:50:01.562
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:50:01.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:50:01.621
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Sep  4 14:50:01.662: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-4fee4e4c-277a-4bca-96dc-97421bad81ec" in namespace "security-context-test-1313" to be "Succeeded or Failed"
Sep  4 14:50:01.674: INFO: Pod "busybox-privileged-false-4fee4e4c-277a-4bca-96dc-97421bad81ec": Phase="Pending", Reason="", readiness=false. Elapsed: 12.121288ms
Sep  4 14:50:03.687: INFO: Pod "busybox-privileged-false-4fee4e4c-277a-4bca-96dc-97421bad81ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025739669s
Sep  4 14:50:05.685: INFO: Pod "busybox-privileged-false-4fee4e4c-277a-4bca-96dc-97421bad81ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023764024s
Sep  4 14:50:05.685: INFO: Pod "busybox-privileged-false-4fee4e4c-277a-4bca-96dc-97421bad81ec" satisfied condition "Succeeded or Failed"
Sep  4 14:50:05.718: INFO: Got logs for pod "busybox-privileged-false-4fee4e4c-277a-4bca-96dc-97421bad81ec": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  4 14:50:05.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1313" for this suite. 09/04/23 14:50:05.74
------------------------------
• [4.191 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:50:01.562
    Sep  4 14:50:01.562: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context-test 09/04/23 14:50:01.562
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:50:01.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:50:01.621
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Sep  4 14:50:01.662: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-4fee4e4c-277a-4bca-96dc-97421bad81ec" in namespace "security-context-test-1313" to be "Succeeded or Failed"
    Sep  4 14:50:01.674: INFO: Pod "busybox-privileged-false-4fee4e4c-277a-4bca-96dc-97421bad81ec": Phase="Pending", Reason="", readiness=false. Elapsed: 12.121288ms
    Sep  4 14:50:03.687: INFO: Pod "busybox-privileged-false-4fee4e4c-277a-4bca-96dc-97421bad81ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025739669s
    Sep  4 14:50:05.685: INFO: Pod "busybox-privileged-false-4fee4e4c-277a-4bca-96dc-97421bad81ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023764024s
    Sep  4 14:50:05.685: INFO: Pod "busybox-privileged-false-4fee4e4c-277a-4bca-96dc-97421bad81ec" satisfied condition "Succeeded or Failed"
    Sep  4 14:50:05.718: INFO: Got logs for pod "busybox-privileged-false-4fee4e4c-277a-4bca-96dc-97421bad81ec": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:50:05.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1313" for this suite. 09/04/23 14:50:05.74
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:50:05.752
Sep  4 14:50:05.752: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate 09/04/23 14:50:05.753
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:50:05.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:50:05.812
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  4 14:50:05.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9793" for this suite. 09/04/23 14:50:05.93
------------------------------
• [0.190 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:50:05.752
    Sep  4 14:50:05.752: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename podtemplate 09/04/23 14:50:05.753
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:50:05.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:50:05.812
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:50:05.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9793" for this suite. 09/04/23 14:50:05.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:50:05.943
Sep  4 14:50:05.943: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 14:50:05.944
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:50:05.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:50:06.002
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/04/23 14:50:06.023
Sep  4 14:50:06.042: INFO: Waiting up to 5m0s for pod "pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5" in namespace "emptydir-6053" to be "Succeeded or Failed"
Sep  4 14:50:06.055: INFO: Pod "pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.022929ms
Sep  4 14:50:08.069: INFO: Pod "pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026907099s
Sep  4 14:50:10.069: INFO: Pod "pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027113273s
STEP: Saw pod success 09/04/23 14:50:10.069
Sep  4 14:50:10.070: INFO: Pod "pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5" satisfied condition "Succeeded or Failed"
Sep  4 14:50:10.082: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5 container test-container: <nil>
STEP: delete the pod 09/04/23 14:50:10.116
Sep  4 14:50:10.132: INFO: Waiting for pod pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5 to disappear
Sep  4 14:50:10.143: INFO: Pod pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 14:50:10.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6053" for this suite. 09/04/23 14:50:10.166
------------------------------
• [4.236 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:50:05.943
    Sep  4 14:50:05.943: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 14:50:05.944
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:50:05.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:50:06.002
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/04/23 14:50:06.023
    Sep  4 14:50:06.042: INFO: Waiting up to 5m0s for pod "pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5" in namespace "emptydir-6053" to be "Succeeded or Failed"
    Sep  4 14:50:06.055: INFO: Pod "pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.022929ms
    Sep  4 14:50:08.069: INFO: Pod "pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026907099s
    Sep  4 14:50:10.069: INFO: Pod "pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027113273s
    STEP: Saw pod success 09/04/23 14:50:10.069
    Sep  4 14:50:10.070: INFO: Pod "pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5" satisfied condition "Succeeded or Failed"
    Sep  4 14:50:10.082: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5 container test-container: <nil>
    STEP: delete the pod 09/04/23 14:50:10.116
    Sep  4 14:50:10.132: INFO: Waiting for pod pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5 to disappear
    Sep  4 14:50:10.143: INFO: Pod pod-712055f8-15a3-44ed-af2e-b9d796dc1ef5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:50:10.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6053" for this suite. 09/04/23 14:50:10.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:50:10.179
Sep  4 14:50:10.179: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 14:50:10.18
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:50:10.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:50:10.239
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 09/04/23 14:50:10.261
Sep  4 14:50:10.262: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2924 proxy --unix-socket=/tmp/kubectl-proxy-unix1950696164/test'
STEP: retrieving proxy /api/ output 09/04/23 14:50:10.309
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 14:50:10.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2924" for this suite. 09/04/23 14:50:10.324
------------------------------
• [0.163 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:50:10.179
    Sep  4 14:50:10.179: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 14:50:10.18
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:50:10.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:50:10.239
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 09/04/23 14:50:10.261
    Sep  4 14:50:10.262: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2924 proxy --unix-socket=/tmp/kubectl-proxy-unix1950696164/test'
    STEP: retrieving proxy /api/ output 09/04/23 14:50:10.309
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:50:10.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2924" for this suite. 09/04/23 14:50:10.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:50:10.343
Sep  4 14:50:10.343: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-multiple-pods 09/04/23 14:50:10.344
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:50:10.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:50:10.402
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Sep  4 14:50:10.423: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  4 14:51:10.524: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Sep  4 14:51:10.537: INFO: Starting informer...
STEP: Starting pods... 09/04/23 14:51:10.537
Sep  4 14:51:10.578: INFO: Pod1 is running on shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh. Tainting Node
Sep  4 14:51:10.605: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-4844" to be "running"
Sep  4 14:51:10.617: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.966001ms
Sep  4 14:51:12.630: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.025060351s
Sep  4 14:51:12.630: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Sep  4 14:51:12.630: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-4844" to be "running"
Sep  4 14:51:12.642: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 11.976381ms
Sep  4 14:51:12.642: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Sep  4 14:51:12.642: INFO: Pod2 is running on shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh. Tainting Node
STEP: Trying to apply a taint on the Node 09/04/23 14:51:12.642
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 14:51:12.685
STEP: Waiting for Pod1 and Pod2 to be deleted 09/04/23 14:51:12.697
Sep  4 14:51:18.786: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep  4 14:51:38.824: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 14:51:38.856
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 14:51:38.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-4844" for this suite. 09/04/23 14:51:38.882
------------------------------
• [88.553 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:50:10.343
    Sep  4 14:50:10.343: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename taint-multiple-pods 09/04/23 14:50:10.344
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:50:10.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:50:10.402
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Sep  4 14:50:10.423: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  4 14:51:10.524: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Sep  4 14:51:10.537: INFO: Starting informer...
    STEP: Starting pods... 09/04/23 14:51:10.537
    Sep  4 14:51:10.578: INFO: Pod1 is running on shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh. Tainting Node
    Sep  4 14:51:10.605: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-4844" to be "running"
    Sep  4 14:51:10.617: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.966001ms
    Sep  4 14:51:12.630: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.025060351s
    Sep  4 14:51:12.630: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Sep  4 14:51:12.630: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-4844" to be "running"
    Sep  4 14:51:12.642: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 11.976381ms
    Sep  4 14:51:12.642: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Sep  4 14:51:12.642: INFO: Pod2 is running on shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh. Tainting Node
    STEP: Trying to apply a taint on the Node 09/04/23 14:51:12.642
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 14:51:12.685
    STEP: Waiting for Pod1 and Pod2 to be deleted 09/04/23 14:51:12.697
    Sep  4 14:51:18.786: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Sep  4 14:51:38.824: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 14:51:38.856
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:51:38.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-4844" for this suite. 09/04/23 14:51:38.882
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:51:38.896
Sep  4 14:51:38.896: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 14:51:38.897
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:51:38.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:51:38.954
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-6bc052fa-7513-490a-812a-dd66d32fe32d 09/04/23 14:51:38.989
STEP: Creating configMap with name cm-test-opt-upd-6b81f27d-843e-4133-a137-3fcdfacd39e6 09/04/23 14:51:39.002
STEP: Creating the pod 09/04/23 14:51:39.018
Sep  4 14:51:39.039: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-62cdee0d-a0c5-4c68-a836-ed144ef47a47" in namespace "projected-3323" to be "running and ready"
Sep  4 14:51:39.051: INFO: Pod "pod-projected-configmaps-62cdee0d-a0c5-4c68-a836-ed144ef47a47": Phase="Pending", Reason="", readiness=false. Elapsed: 11.875624ms
Sep  4 14:51:39.051: INFO: The phase of Pod pod-projected-configmaps-62cdee0d-a0c5-4c68-a836-ed144ef47a47 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 14:51:41.065: INFO: Pod "pod-projected-configmaps-62cdee0d-a0c5-4c68-a836-ed144ef47a47": Phase="Running", Reason="", readiness=true. Elapsed: 2.025416522s
Sep  4 14:51:41.065: INFO: The phase of Pod pod-projected-configmaps-62cdee0d-a0c5-4c68-a836-ed144ef47a47 is Running (Ready = true)
Sep  4 14:51:41.065: INFO: Pod "pod-projected-configmaps-62cdee0d-a0c5-4c68-a836-ed144ef47a47" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-6bc052fa-7513-490a-812a-dd66d32fe32d 09/04/23 14:51:41.399
STEP: Updating configmap cm-test-opt-upd-6b81f27d-843e-4133-a137-3fcdfacd39e6 09/04/23 14:51:41.413
STEP: Creating configMap with name cm-test-opt-create-bb20dd07-e84c-48ed-98c8-2eccb9e1c8a2 09/04/23 14:51:41.426
STEP: waiting to observe update in volume 09/04/23 14:51:41.438
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 14:51:45.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3323" for this suite. 09/04/23 14:51:45.782
------------------------------
• [6.901 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:51:38.896
    Sep  4 14:51:38.896: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 14:51:38.897
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:51:38.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:51:38.954
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-6bc052fa-7513-490a-812a-dd66d32fe32d 09/04/23 14:51:38.989
    STEP: Creating configMap with name cm-test-opt-upd-6b81f27d-843e-4133-a137-3fcdfacd39e6 09/04/23 14:51:39.002
    STEP: Creating the pod 09/04/23 14:51:39.018
    Sep  4 14:51:39.039: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-62cdee0d-a0c5-4c68-a836-ed144ef47a47" in namespace "projected-3323" to be "running and ready"
    Sep  4 14:51:39.051: INFO: Pod "pod-projected-configmaps-62cdee0d-a0c5-4c68-a836-ed144ef47a47": Phase="Pending", Reason="", readiness=false. Elapsed: 11.875624ms
    Sep  4 14:51:39.051: INFO: The phase of Pod pod-projected-configmaps-62cdee0d-a0c5-4c68-a836-ed144ef47a47 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 14:51:41.065: INFO: Pod "pod-projected-configmaps-62cdee0d-a0c5-4c68-a836-ed144ef47a47": Phase="Running", Reason="", readiness=true. Elapsed: 2.025416522s
    Sep  4 14:51:41.065: INFO: The phase of Pod pod-projected-configmaps-62cdee0d-a0c5-4c68-a836-ed144ef47a47 is Running (Ready = true)
    Sep  4 14:51:41.065: INFO: Pod "pod-projected-configmaps-62cdee0d-a0c5-4c68-a836-ed144ef47a47" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-6bc052fa-7513-490a-812a-dd66d32fe32d 09/04/23 14:51:41.399
    STEP: Updating configmap cm-test-opt-upd-6b81f27d-843e-4133-a137-3fcdfacd39e6 09/04/23 14:51:41.413
    STEP: Creating configMap with name cm-test-opt-create-bb20dd07-e84c-48ed-98c8-2eccb9e1c8a2 09/04/23 14:51:41.426
    STEP: waiting to observe update in volume 09/04/23 14:51:41.438
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:51:45.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3323" for this suite. 09/04/23 14:51:45.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:51:45.797
Sep  4 14:51:45.797: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 09/04/23 14:51:45.798
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:51:45.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:51:45.858
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/04/23 14:51:45.881
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/04/23 14:51:45.881
STEP: creating a pod to probe DNS 09/04/23 14:51:45.881
STEP: submitting the pod to kubernetes 09/04/23 14:51:45.881
Sep  4 14:51:45.900: INFO: Waiting up to 15m0s for pod "dns-test-2a602e7c-9587-4650-90a8-89c33cfd85b0" in namespace "dns-6320" to be "running"
Sep  4 14:51:45.912: INFO: Pod "dns-test-2a602e7c-9587-4650-90a8-89c33cfd85b0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.886879ms
Sep  4 14:51:47.926: INFO: Pod "dns-test-2a602e7c-9587-4650-90a8-89c33cfd85b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.026037111s
Sep  4 14:51:47.926: INFO: Pod "dns-test-2a602e7c-9587-4650-90a8-89c33cfd85b0" satisfied condition "running"
STEP: retrieving the pod 09/04/23 14:51:47.926
STEP: looking for the results for each expected name from probers 09/04/23 14:51:47.939
Sep  4 14:51:48.172: INFO: DNS probes using dns-6320/dns-test-2a602e7c-9587-4650-90a8-89c33cfd85b0 succeeded

STEP: deleting the pod 09/04/23 14:51:48.172
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 14:51:48.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6320" for this suite. 09/04/23 14:51:48.212
------------------------------
• [2.429 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:51:45.797
    Sep  4 14:51:45.797: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 09/04/23 14:51:45.798
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:51:45.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:51:45.858
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/04/23 14:51:45.881
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/04/23 14:51:45.881
    STEP: creating a pod to probe DNS 09/04/23 14:51:45.881
    STEP: submitting the pod to kubernetes 09/04/23 14:51:45.881
    Sep  4 14:51:45.900: INFO: Waiting up to 15m0s for pod "dns-test-2a602e7c-9587-4650-90a8-89c33cfd85b0" in namespace "dns-6320" to be "running"
    Sep  4 14:51:45.912: INFO: Pod "dns-test-2a602e7c-9587-4650-90a8-89c33cfd85b0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.886879ms
    Sep  4 14:51:47.926: INFO: Pod "dns-test-2a602e7c-9587-4650-90a8-89c33cfd85b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.026037111s
    Sep  4 14:51:47.926: INFO: Pod "dns-test-2a602e7c-9587-4650-90a8-89c33cfd85b0" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 14:51:47.926
    STEP: looking for the results for each expected name from probers 09/04/23 14:51:47.939
    Sep  4 14:51:48.172: INFO: DNS probes using dns-6320/dns-test-2a602e7c-9587-4650-90a8-89c33cfd85b0 succeeded

    STEP: deleting the pod 09/04/23 14:51:48.172
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:51:48.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6320" for this suite. 09/04/23 14:51:48.212
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:51:48.226
Sep  4 14:51:48.226: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 14:51:48.227
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:51:48.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:51:48.285
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-897ba7a2-6f94-432a-b732-1662a6480db7 09/04/23 14:51:48.307
STEP: Creating a pod to test consume secrets 09/04/23 14:51:48.321
Sep  4 14:51:48.341: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c" in namespace "projected-8662" to be "Succeeded or Failed"
Sep  4 14:51:48.353: INFO: Pod "pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.150841ms
Sep  4 14:51:50.367: INFO: Pod "pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026089991s
Sep  4 14:51:52.368: INFO: Pod "pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026666695s
STEP: Saw pod success 09/04/23 14:51:52.368
Sep  4 14:51:52.368: INFO: Pod "pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c" satisfied condition "Succeeded or Failed"
Sep  4 14:51:52.380: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c container projected-secret-volume-test: <nil>
STEP: delete the pod 09/04/23 14:51:52.414
Sep  4 14:51:52.436: INFO: Waiting for pod pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c to disappear
Sep  4 14:51:52.449: INFO: Pod pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  4 14:51:52.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8662" for this suite. 09/04/23 14:51:52.472
------------------------------
• [4.259 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:51:48.226
    Sep  4 14:51:48.226: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 14:51:48.227
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:51:48.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:51:48.285
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-897ba7a2-6f94-432a-b732-1662a6480db7 09/04/23 14:51:48.307
    STEP: Creating a pod to test consume secrets 09/04/23 14:51:48.321
    Sep  4 14:51:48.341: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c" in namespace "projected-8662" to be "Succeeded or Failed"
    Sep  4 14:51:48.353: INFO: Pod "pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.150841ms
    Sep  4 14:51:50.367: INFO: Pod "pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026089991s
    Sep  4 14:51:52.368: INFO: Pod "pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026666695s
    STEP: Saw pod success 09/04/23 14:51:52.368
    Sep  4 14:51:52.368: INFO: Pod "pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c" satisfied condition "Succeeded or Failed"
    Sep  4 14:51:52.380: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 14:51:52.414
    Sep  4 14:51:52.436: INFO: Waiting for pod pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c to disappear
    Sep  4 14:51:52.449: INFO: Pod pod-projected-secrets-ff00a3ec-1efc-4ec2-ad23-77c39bda652c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:51:52.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8662" for this suite. 09/04/23 14:51:52.472
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:51:52.486
Sep  4 14:51:52.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 14:51:52.487
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:51:52.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:51:52.545
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 09/04/23 14:51:52.567
Sep  4 14:51:52.586: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c" in namespace "projected-7465" to be "Succeeded or Failed"
Sep  4 14:51:52.598: INFO: Pod "downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.032743ms
Sep  4 14:51:54.611: INFO: Pod "downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025238471s
Sep  4 14:51:56.612: INFO: Pod "downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025797178s
STEP: Saw pod success 09/04/23 14:51:56.612
Sep  4 14:51:56.612: INFO: Pod "downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c" satisfied condition "Succeeded or Failed"
Sep  4 14:51:56.625: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c container client-container: <nil>
STEP: delete the pod 09/04/23 14:51:56.659
Sep  4 14:51:56.679: INFO: Waiting for pod downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c to disappear
Sep  4 14:51:56.690: INFO: Pod downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 14:51:56.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7465" for this suite. 09/04/23 14:51:56.712
------------------------------
• [4.240 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:51:52.486
    Sep  4 14:51:52.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 14:51:52.487
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:51:52.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:51:52.545
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 09/04/23 14:51:52.567
    Sep  4 14:51:52.586: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c" in namespace "projected-7465" to be "Succeeded or Failed"
    Sep  4 14:51:52.598: INFO: Pod "downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.032743ms
    Sep  4 14:51:54.611: INFO: Pod "downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025238471s
    Sep  4 14:51:56.612: INFO: Pod "downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025797178s
    STEP: Saw pod success 09/04/23 14:51:56.612
    Sep  4 14:51:56.612: INFO: Pod "downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c" satisfied condition "Succeeded or Failed"
    Sep  4 14:51:56.625: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c container client-container: <nil>
    STEP: delete the pod 09/04/23 14:51:56.659
    Sep  4 14:51:56.679: INFO: Waiting for pod downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c to disappear
    Sep  4 14:51:56.690: INFO: Pod downwardapi-volume-3fabfe4b-290f-4553-a7b2-c8c864ca9c9c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:51:56.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7465" for this suite. 09/04/23 14:51:56.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:51:56.728
Sep  4 14:51:56.728: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 09/04/23 14:51:56.729
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:51:56.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:51:56.789
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Sep  4 14:51:56.837: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/04/23 14:51:56.837
Sep  4 14:51:56.837: INFO: Waiting up to 5m0s for pod "test-cleanup-controller-snkvt" in namespace "deployment-8450" to be "running"
Sep  4 14:51:56.849: INFO: Pod "test-cleanup-controller-snkvt": Phase="Pending", Reason="", readiness=false. Elapsed: 11.970286ms
Sep  4 14:51:58.862: INFO: Pod "test-cleanup-controller-snkvt": Phase="Running", Reason="", readiness=true. Elapsed: 2.025658219s
Sep  4 14:51:58.863: INFO: Pod "test-cleanup-controller-snkvt" satisfied condition "running"
Sep  4 14:51:58.863: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/04/23 14:51:58.897
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 14:52:00.962: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8450  516998e3-d315-4304-8c9b-c52ec0170457 9907 1 2023-09-04 14:51:58 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-04 14:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 14:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e813d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-04 14:51:58 +0000 UTC,LastTransitionTime:2023-09-04 14:51:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-09-04 14:51:59 +0000 UTC,LastTransitionTime:2023-09-04 14:51:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  4 14:52:00.975: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-8450  2c1b3867-961b-40a1-a831-634fc45ec0f2 9900 1 2023-09-04 14:51:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 516998e3-d315-4304-8c9b-c52ec0170457 0xc00366f287 0xc00366f288}] [] [{kube-controller-manager Update apps/v1 2023-09-04 14:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"516998e3-d315-4304-8c9b-c52ec0170457\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 14:51:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00366f338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  4 14:52:00.987: INFO: Pod "test-cleanup-deployment-7698ff6f6b-ksh5c" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-ksh5c test-cleanup-deployment-7698ff6f6b- deployment-8450  33acb9bd-ea66-40e7-ae8f-53c91d8ae6e7 9899 0 2023-09-04 14:51:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[cni.projectcalico.org/containerID:04ee34982e1f901739b779b13067064f4290ae8089d8f72659aa3c4c717e791a cni.projectcalico.org/podIP:100.64.1.97/32 cni.projectcalico.org/podIPs:100.64.1.97/32] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 2c1b3867-961b-40a1-a831-634fc45ec0f2 0xc00366f6c7 0xc00366f6c8}] [] [{kube-controller-manager Update v1 2023-09-04 14:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c1b3867-961b-40a1-a831-634fc45ec0f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 14:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 14:51:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kgtdc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kgtdc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 14:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 14:51:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 14:51:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 14:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.97,StartTime:2023-09-04 14:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 14:51:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://a3aff15900e169d13b092628a9d4cdf604fb2a34d6c262d834a569c2017edda3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 14:52:00.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8450" for this suite. 09/04/23 14:52:01.01
------------------------------
• [4.296 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:51:56.728
    Sep  4 14:51:56.728: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 09/04/23 14:51:56.729
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:51:56.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:51:56.789
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Sep  4 14:51:56.837: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/04/23 14:51:56.837
    Sep  4 14:51:56.837: INFO: Waiting up to 5m0s for pod "test-cleanup-controller-snkvt" in namespace "deployment-8450" to be "running"
    Sep  4 14:51:56.849: INFO: Pod "test-cleanup-controller-snkvt": Phase="Pending", Reason="", readiness=false. Elapsed: 11.970286ms
    Sep  4 14:51:58.862: INFO: Pod "test-cleanup-controller-snkvt": Phase="Running", Reason="", readiness=true. Elapsed: 2.025658219s
    Sep  4 14:51:58.863: INFO: Pod "test-cleanup-controller-snkvt" satisfied condition "running"
    Sep  4 14:51:58.863: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/04/23 14:51:58.897
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 14:52:00.962: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8450  516998e3-d315-4304-8c9b-c52ec0170457 9907 1 2023-09-04 14:51:58 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-04 14:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 14:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e813d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-04 14:51:58 +0000 UTC,LastTransitionTime:2023-09-04 14:51:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-09-04 14:51:59 +0000 UTC,LastTransitionTime:2023-09-04 14:51:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  4 14:52:00.975: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-8450  2c1b3867-961b-40a1-a831-634fc45ec0f2 9900 1 2023-09-04 14:51:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 516998e3-d315-4304-8c9b-c52ec0170457 0xc00366f287 0xc00366f288}] [] [{kube-controller-manager Update apps/v1 2023-09-04 14:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"516998e3-d315-4304-8c9b-c52ec0170457\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 14:51:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00366f338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 14:52:00.987: INFO: Pod "test-cleanup-deployment-7698ff6f6b-ksh5c" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-ksh5c test-cleanup-deployment-7698ff6f6b- deployment-8450  33acb9bd-ea66-40e7-ae8f-53c91d8ae6e7 9899 0 2023-09-04 14:51:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[cni.projectcalico.org/containerID:04ee34982e1f901739b779b13067064f4290ae8089d8f72659aa3c4c717e791a cni.projectcalico.org/podIP:100.64.1.97/32 cni.projectcalico.org/podIPs:100.64.1.97/32] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 2c1b3867-961b-40a1-a831-634fc45ec0f2 0xc00366f6c7 0xc00366f6c8}] [] [{kube-controller-manager Update v1 2023-09-04 14:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c1b3867-961b-40a1-a831-634fc45ec0f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 14:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 14:51:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kgtdc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kgtdc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 14:51:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 14:51:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 14:51:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 14:51:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.97,StartTime:2023-09-04 14:51:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 14:51:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://a3aff15900e169d13b092628a9d4cdf604fb2a34d6c262d834a569c2017edda3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:52:00.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8450" for this suite. 09/04/23 14:52:01.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:52:01.025
Sep  4 14:52:01.025: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 14:52:01.026
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:52:01.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:52:01.086
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 09/04/23 14:52:01.108
Sep  4 14:52:01.108: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8755 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 09/04/23 14:52:01.157
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 14:52:01.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8755" for this suite. 09/04/23 14:52:01.218
------------------------------
• [0.208 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:52:01.025
    Sep  4 14:52:01.025: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 14:52:01.026
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:52:01.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:52:01.086
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 09/04/23 14:52:01.108
    Sep  4 14:52:01.108: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8755 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 09/04/23 14:52:01.157
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:52:01.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8755" for this suite. 09/04/23 14:52:01.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:52:01.233
Sep  4 14:52:01.233: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container 09/04/23 14:52:01.234
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:52:01.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:52:01.293
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 09/04/23 14:52:01.316
Sep  4 14:52:01.316: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  4 14:52:06.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6958" for this suite. 09/04/23 14:52:06.947
------------------------------
• [5.729 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:52:01.233
    Sep  4 14:52:01.233: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename init-container 09/04/23 14:52:01.234
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:52:01.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:52:01.293
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 09/04/23 14:52:01.316
    Sep  4 14:52:01.316: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:52:06.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6958" for this suite. 09/04/23 14:52:06.947
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:52:06.962
Sep  4 14:52:06.962: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 09/04/23 14:52:06.963
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:52:06.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:52:07.02
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  4 14:52:07.082: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  4 14:53:07.194: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 09/04/23 14:53:07.206
Sep  4 14:53:07.244: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep  4 14:53:07.261: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep  4 14:53:07.297: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep  4 14:53:07.314: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/04/23 14:53:07.314
Sep  4 14:53:07.314: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3744" to be "running"
Sep  4 14:53:07.326: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 11.791272ms
Sep  4 14:53:09.339: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.02553316s
Sep  4 14:53:09.339: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep  4 14:53:09.339: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3744" to be "running"
Sep  4 14:53:09.352: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.529834ms
Sep  4 14:53:09.352: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  4 14:53:09.352: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3744" to be "running"
Sep  4 14:53:09.365: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.726427ms
Sep  4 14:53:09.365: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  4 14:53:09.365: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3744" to be "running"
Sep  4 14:53:09.377: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.663245ms
Sep  4 14:53:09.377: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 09/04/23 14:53:09.377
Sep  4 14:53:09.405: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Sep  4 14:53:09.416: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.152077ms
Sep  4 14:53:11.430: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025417092s
Sep  4 14:53:13.430: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025334659s
Sep  4 14:53:15.433: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.028552586s
Sep  4 14:53:15.433: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 14:53:15.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3744" for this suite. 09/04/23 14:53:15.632
------------------------------
• [68.684 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:52:06.962
    Sep  4 14:52:06.962: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 09/04/23 14:52:06.963
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:52:06.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:52:07.02
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  4 14:52:07.082: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  4 14:53:07.194: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 09/04/23 14:53:07.206
    Sep  4 14:53:07.244: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep  4 14:53:07.261: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep  4 14:53:07.297: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep  4 14:53:07.314: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/04/23 14:53:07.314
    Sep  4 14:53:07.314: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3744" to be "running"
    Sep  4 14:53:07.326: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 11.791272ms
    Sep  4 14:53:09.339: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.02553316s
    Sep  4 14:53:09.339: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep  4 14:53:09.339: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3744" to be "running"
    Sep  4 14:53:09.352: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.529834ms
    Sep  4 14:53:09.352: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  4 14:53:09.352: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3744" to be "running"
    Sep  4 14:53:09.365: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.726427ms
    Sep  4 14:53:09.365: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  4 14:53:09.365: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3744" to be "running"
    Sep  4 14:53:09.377: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.663245ms
    Sep  4 14:53:09.377: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 09/04/23 14:53:09.377
    Sep  4 14:53:09.405: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Sep  4 14:53:09.416: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.152077ms
    Sep  4 14:53:11.430: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025417092s
    Sep  4 14:53:13.430: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025334659s
    Sep  4 14:53:15.433: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.028552586s
    Sep  4 14:53:15.433: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:53:15.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3744" for this suite. 09/04/23 14:53:15.632
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:53:15.646
Sep  4 14:53:15.646: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 14:53:15.647
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:53:15.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:53:15.705
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 14:53:15.754
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 14:53:16.103
STEP: Deploying the webhook pod 09/04/23 14:53:16.116
STEP: Wait for the deployment to be ready 09/04/23 14:53:16.142
Sep  4 14:53:16.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 53, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 53, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 53, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 53, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 14:53:18.191
STEP: Verifying the service has paired with the endpoint 09/04/23 14:53:18.208
Sep  4 14:53:19.208: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 09/04/23 14:53:19.221
STEP: Creating a custom resource definition that should be denied by the webhook 09/04/23 14:53:19.349
Sep  4 14:53:19.349: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 14:53:19.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9567" for this suite. 09/04/23 14:53:19.566
STEP: Destroying namespace "webhook-9567-markers" for this suite. 09/04/23 14:53:19.58
------------------------------
• [3.947 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:53:15.646
    Sep  4 14:53:15.646: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 14:53:15.647
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:53:15.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:53:15.705
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 14:53:15.754
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 14:53:16.103
    STEP: Deploying the webhook pod 09/04/23 14:53:16.116
    STEP: Wait for the deployment to be ready 09/04/23 14:53:16.142
    Sep  4 14:53:16.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 53, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 53, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 53, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 53, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 14:53:18.191
    STEP: Verifying the service has paired with the endpoint 09/04/23 14:53:18.208
    Sep  4 14:53:19.208: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 09/04/23 14:53:19.221
    STEP: Creating a custom resource definition that should be denied by the webhook 09/04/23 14:53:19.349
    Sep  4 14:53:19.349: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:53:19.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9567" for this suite. 09/04/23 14:53:19.566
    STEP: Destroying namespace "webhook-9567-markers" for this suite. 09/04/23 14:53:19.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:53:19.594
Sep  4 14:53:19.594: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 14:53:19.594
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:53:19.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:53:19.652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 14:53:19.674
Sep  4 14:53:19.674: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9485 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep  4 14:53:19.789: INFO: stderr: ""
Sep  4 14:53:19.789: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 09/04/23 14:53:19.789
Sep  4 14:53:19.789: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9485 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Sep  4 14:53:20.650: INFO: stderr: ""
Sep  4 14:53:20.650: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 14:53:20.65
Sep  4 14:53:20.662: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9485 delete pods e2e-test-httpd-pod'
Sep  4 14:53:23.076: INFO: stderr: ""
Sep  4 14:53:23.076: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 14:53:23.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9485" for this suite. 09/04/23 14:53:23.099
------------------------------
• [3.519 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:53:19.594
    Sep  4 14:53:19.594: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 14:53:19.594
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:53:19.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:53:19.652
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 14:53:19.674
    Sep  4 14:53:19.674: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9485 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep  4 14:53:19.789: INFO: stderr: ""
    Sep  4 14:53:19.789: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 09/04/23 14:53:19.789
    Sep  4 14:53:19.789: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9485 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Sep  4 14:53:20.650: INFO: stderr: ""
    Sep  4 14:53:20.650: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 14:53:20.65
    Sep  4 14:53:20.662: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9485 delete pods e2e-test-httpd-pod'
    Sep  4 14:53:23.076: INFO: stderr: ""
    Sep  4 14:53:23.076: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:53:23.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9485" for this suite. 09/04/23 14:53:23.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:53:23.113
Sep  4 14:53:23.113: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 14:53:23.114
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:53:23.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:53:23.171
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 14:53:23.219
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 14:53:23.587
STEP: Deploying the webhook pod 09/04/23 14:53:23.601
STEP: Wait for the deployment to be ready 09/04/23 14:53:23.629
Sep  4 14:53:23.664: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 53, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 53, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 53, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 53, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 14:53:25.677
STEP: Verifying the service has paired with the endpoint 09/04/23 14:53:25.694
Sep  4 14:53:26.695: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 09/04/23 14:53:26.708
STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/04/23 14:53:26.935
STEP: Creating a configMap that should not be mutated 09/04/23 14:53:26.951
STEP: Patching a mutating webhook configuration's rules to include the create operation 09/04/23 14:53:26.977
STEP: Creating a configMap that should be mutated 09/04/23 14:53:26.992
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 14:53:27.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7073" for this suite. 09/04/23 14:53:27.217
STEP: Destroying namespace "webhook-7073-markers" for this suite. 09/04/23 14:53:27.231
------------------------------
• [4.132 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:53:23.113
    Sep  4 14:53:23.113: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 14:53:23.114
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:53:23.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:53:23.171
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 14:53:23.219
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 14:53:23.587
    STEP: Deploying the webhook pod 09/04/23 14:53:23.601
    STEP: Wait for the deployment to be ready 09/04/23 14:53:23.629
    Sep  4 14:53:23.664: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 14, 53, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 53, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 14, 53, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 14, 53, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 14:53:25.677
    STEP: Verifying the service has paired with the endpoint 09/04/23 14:53:25.694
    Sep  4 14:53:26.695: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 09/04/23 14:53:26.708
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/04/23 14:53:26.935
    STEP: Creating a configMap that should not be mutated 09/04/23 14:53:26.951
    STEP: Patching a mutating webhook configuration's rules to include the create operation 09/04/23 14:53:26.977
    STEP: Creating a configMap that should be mutated 09/04/23 14:53:26.992
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:53:27.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7073" for this suite. 09/04/23 14:53:27.217
    STEP: Destroying namespace "webhook-7073-markers" for this suite. 09/04/23 14:53:27.231
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:53:27.245
Sep  4 14:53:27.245: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 09/04/23 14:53:27.246
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:53:27.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:53:27.304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5606 09/04/23 14:53:27.326
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 09/04/23 14:53:27.339
STEP: Creating stateful set ss in namespace statefulset-5606 09/04/23 14:53:27.351
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5606 09/04/23 14:53:27.364
Sep  4 14:53:27.376: INFO: Found 0 stateful pods, waiting for 1
Sep  4 14:53:37.390: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/04/23 14:53:37.39
Sep  4 14:53:37.403: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 14:53:37.896: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 14:53:37.896: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 14:53:37.896: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 14:53:37.910: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  4 14:53:47.924: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 14:53:47.924: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 14:53:47.975: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999845s
Sep  4 14:53:48.989: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.987431645s
Sep  4 14:53:50.003: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.973733508s
Sep  4 14:53:51.017: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.959654137s
Sep  4 14:53:52.031: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.944527383s
Sep  4 14:53:53.045: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.931590429s
Sep  4 14:53:54.058: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.918142379s
Sep  4 14:53:55.072: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.904698277s
Sep  4 14:53:56.086: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.890511693s
Sep  4 14:53:57.099: INFO: Verifying statefulset ss doesn't scale past 1 for another 876.810522ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5606 09/04/23 14:53:58.1
Sep  4 14:53:58.113: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:53:58.698: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  4 14:53:58.698: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 14:53:58.698: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  4 14:53:58.711: INFO: Found 1 stateful pods, waiting for 3
Sep  4 14:54:08.724: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 14:54:08.724: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 14:54:08.724: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 09/04/23 14:54:08.724
STEP: Scale down will halt with unhealthy stateful pod 09/04/23 14:54:08.724
Sep  4 14:54:08.749: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 14:54:09.164: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 14:54:09.164: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 14:54:09.164: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 14:54:09.164: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 14:54:09.767: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 14:54:09.767: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 14:54:09.767: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 14:54:09.767: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 14:54:10.259: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 14:54:10.259: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 14:54:10.260: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 14:54:10.260: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 14:54:10.272: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Sep  4 14:54:20.302: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 14:54:20.302: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 14:54:20.302: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 14:54:20.341: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999832s
Sep  4 14:54:21.355: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986780248s
Sep  4 14:54:22.367: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973630964s
Sep  4 14:54:23.381: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.96019743s
Sep  4 14:54:24.394: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.946976108s
Sep  4 14:54:25.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.934053744s
Sep  4 14:54:26.422: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.919720133s
Sep  4 14:54:27.435: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.906329574s
Sep  4 14:54:28.449: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.892842764s
Sep  4 14:54:29.463: INFO: Verifying statefulset ss doesn't scale past 3 for another 879.295091ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5606 09/04/23 14:54:30.464
Sep  4 14:54:30.477: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:54:30.984: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  4 14:54:30.984: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 14:54:30.984: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  4 14:54:30.984: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:54:31.549: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  4 14:54:31.549: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 14:54:31.549: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  4 14:54:31.549: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:54:32.213: INFO: rc: 1
Sep  4 14:54:32.213: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec "a271ecbf118f93874ea6709c5196d0c2f38a044bc02b779bd2d2daefd9c5245a": OCI runtime exec failed: exec failed: unable to start container process: error writing config to pipe: write init-p: broken pipe: unknown

error:
exit status 1
Sep  4 14:54:42.214: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:54:42.325: INFO: rc: 1
Sep  4 14:54:42.325: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:54:52.326: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:54:52.428: INFO: rc: 1
Sep  4 14:54:52.428: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:55:02.428: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:55:02.540: INFO: rc: 1
Sep  4 14:55:02.540: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:55:12.540: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:55:12.657: INFO: rc: 1
Sep  4 14:55:12.657: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:55:22.658: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:55:22.771: INFO: rc: 1
Sep  4 14:55:22.771: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:55:32.771: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:55:32.910: INFO: rc: 1
Sep  4 14:55:32.910: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:55:42.911: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:55:43.026: INFO: rc: 1
Sep  4 14:55:43.026: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:55:53.027: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:55:53.137: INFO: rc: 1
Sep  4 14:55:53.138: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:56:03.139: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:56:03.271: INFO: rc: 1
Sep  4 14:56:03.271: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:56:13.272: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:56:13.384: INFO: rc: 1
Sep  4 14:56:13.384: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:56:23.386: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:56:23.488: INFO: rc: 1
Sep  4 14:56:23.488: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:56:33.489: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:56:33.604: INFO: rc: 1
Sep  4 14:56:33.604: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:56:43.606: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:56:43.710: INFO: rc: 1
Sep  4 14:56:43.710: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:56:53.712: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:56:53.818: INFO: rc: 1
Sep  4 14:56:53.818: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:57:03.818: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:57:03.930: INFO: rc: 1
Sep  4 14:57:03.930: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:57:13.931: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:57:14.051: INFO: rc: 1
Sep  4 14:57:14.051: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:57:24.051: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:57:24.181: INFO: rc: 1
Sep  4 14:57:24.181: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:57:34.183: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:57:34.289: INFO: rc: 1
Sep  4 14:57:34.289: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:57:44.289: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:57:44.421: INFO: rc: 1
Sep  4 14:57:44.421: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:57:54.423: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:57:54.556: INFO: rc: 1
Sep  4 14:57:54.556: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:58:04.559: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:58:04.682: INFO: rc: 1
Sep  4 14:58:04.682: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:58:14.683: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:58:14.805: INFO: rc: 1
Sep  4 14:58:14.805: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:58:24.808: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:58:24.914: INFO: rc: 1
Sep  4 14:58:24.914: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
------------------------------
Automatically polling progress:
  [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance] (Spec Runtime: 5m0.095s)
    test/e2e/apps/statefulset.go:587
    In [It] (Node Runtime: 5m0.001s)
      test/e2e/apps/statefulset.go:587
      At [By Step] Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5606 (Step Runtime: 3m56.876s)
        test/e2e/apps/statefulset.go:683

      Spec Goroutine
      goroutine 5261 [sleep]
        time.Sleep(0x2540be400)
          /usr/local/go/src/runtime/time.go:195
        k8s.io/kubernetes/test/e2e/framework/pod/output.RunHostCmdWithRetries({0xc00557ed10, 0x10}, {0xc00557ecdc, 0x4}, {0xc003c8ff00, 0x38}, 0x3?, 0x45d964b800)
          test/e2e/framework/pod/output/output.go:113
        k8s.io/kubernetes/test/e2e/framework/statefulset.ExecInStatefulPods({0x81ba5a8?, 0xc004524340?}, 0xc003b61e20?, {0xc003c8ff00, 0x38})
          test/e2e/framework/statefulset/rest.go:240
      > k8s.io/kubernetes/test/e2e/apps.restoreHTTPProbe({0x81ba5a8, 0xc004524340}, 0x0?)
          test/e2e/apps/statefulset.go:1728
      > k8s.io/kubernetes/test/e2e/apps.glob..func11.2.10()
          test/e2e/apps/statefulset.go:684
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0x2dc5bee, 0xc004396c00})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
Sep  4 14:58:34.916: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:58:35.030: INFO: rc: 1
Sep  4 14:58:35.030: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:58:45.032: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:58:45.150: INFO: rc: 1
Sep  4 14:58:45.150: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Automatically polling progress:
  [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance] (Spec Runtime: 5m20.096s)
    test/e2e/apps/statefulset.go:587
    In [It] (Node Runtime: 5m20.003s)
      test/e2e/apps/statefulset.go:587
      At [By Step] Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5606 (Step Runtime: 4m16.878s)
        test/e2e/apps/statefulset.go:683

      Spec Goroutine
      goroutine 5261 [sleep]
        time.Sleep(0x2540be400)
          /usr/local/go/src/runtime/time.go:195
        k8s.io/kubernetes/test/e2e/framework/pod/output.RunHostCmdWithRetries({0xc00557ed10, 0x10}, {0xc00557ecdc, 0x4}, {0xc003c8ff00, 0x38}, 0x3?, 0x45d964b800)
          test/e2e/framework/pod/output/output.go:113
        k8s.io/kubernetes/test/e2e/framework/statefulset.ExecInStatefulPods({0x81ba5a8?, 0xc004524340?}, 0xc003b61e20?, {0xc003c8ff00, 0x38})
          test/e2e/framework/statefulset/rest.go:240
      > k8s.io/kubernetes/test/e2e/apps.restoreHTTPProbe({0x81ba5a8, 0xc004524340}, 0x0?)
          test/e2e/apps/statefulset.go:1728
      > k8s.io/kubernetes/test/e2e/apps.glob..func11.2.10()
          test/e2e/apps/statefulset.go:684
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0x2dc5bee, 0xc004396c00})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
Sep  4 14:58:55.154: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:58:55.258: INFO: rc: 1
Sep  4 14:58:55.258: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:59:05.258: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:59:05.370: INFO: rc: 1
Sep  4 14:59:05.370: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Automatically polling progress:
  [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance] (Spec Runtime: 5m40.098s)
    test/e2e/apps/statefulset.go:587
    In [It] (Node Runtime: 5m40.005s)
      test/e2e/apps/statefulset.go:587
      At [By Step] Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5606 (Step Runtime: 4m36.88s)
        test/e2e/apps/statefulset.go:683

      Spec Goroutine
      goroutine 5261 [sleep]
        time.Sleep(0x2540be400)
          /usr/local/go/src/runtime/time.go:195
        k8s.io/kubernetes/test/e2e/framework/pod/output.RunHostCmdWithRetries({0xc00557ed10, 0x10}, {0xc00557ecdc, 0x4}, {0xc003c8ff00, 0x38}, 0x3?, 0x45d964b800)
          test/e2e/framework/pod/output/output.go:113
        k8s.io/kubernetes/test/e2e/framework/statefulset.ExecInStatefulPods({0x81ba5a8?, 0xc004524340?}, 0xc003b61e20?, {0xc003c8ff00, 0x38})
          test/e2e/framework/statefulset/rest.go:240
      > k8s.io/kubernetes/test/e2e/apps.restoreHTTPProbe({0x81ba5a8, 0xc004524340}, 0x0?)
          test/e2e/apps/statefulset.go:1728
      > k8s.io/kubernetes/test/e2e/apps.glob..func11.2.10()
          test/e2e/apps/statefulset.go:684
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0x2dc5bee, 0xc004396c00})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
Sep  4 14:59:15.371: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:59:15.489: INFO: rc: 1
Sep  4 14:59:15.489: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  4 14:59:25.490: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:59:25.603: INFO: rc: 1
Sep  4 14:59:25.603: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Automatically polling progress:
  [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance] (Spec Runtime: 6m0.099s)
    test/e2e/apps/statefulset.go:587
    In [It] (Node Runtime: 6m0.006s)
      test/e2e/apps/statefulset.go:587
      At [By Step] Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5606 (Step Runtime: 4m56.881s)
        test/e2e/apps/statefulset.go:683

      Spec Goroutine
      goroutine 5261 [sleep]
        time.Sleep(0x2540be400)
          /usr/local/go/src/runtime/time.go:195
        k8s.io/kubernetes/test/e2e/framework/pod/output.RunHostCmdWithRetries({0xc00557ed10, 0x10}, {0xc00557ecdc, 0x4}, {0xc003c8ff00, 0x38}, 0x3?, 0x45d964b800)
          test/e2e/framework/pod/output/output.go:113
        k8s.io/kubernetes/test/e2e/framework/statefulset.ExecInStatefulPods({0x81ba5a8?, 0xc004524340?}, 0xc003b61e20?, {0xc003c8ff00, 0x38})
          test/e2e/framework/statefulset/rest.go:240
      > k8s.io/kubernetes/test/e2e/apps.restoreHTTPProbe({0x81ba5a8, 0xc004524340}, 0x0?)
          test/e2e/apps/statefulset.go:1728
      > k8s.io/kubernetes/test/e2e/apps.glob..func11.2.10()
          test/e2e/apps/statefulset.go:684
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0x2dc5bee, 0xc004396c00})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
Sep  4 14:59:35.606: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 14:59:35.717: INFO: rc: 1
Sep  4 14:59:35.717: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Sep  4 14:59:35.717: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 09/04/23 14:59:35.755
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 14:59:35.755: INFO: Deleting all statefulset in ns statefulset-5606
Sep  4 14:59:35.767: INFO: Scaling statefulset ss to 0
Sep  4 14:59:35.804: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 14:59:35.817: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 14:59:35.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5606" for this suite. 09/04/23 14:59:35.877
• [SLOW TEST] [368.646 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:53:27.245
    Sep  4 14:53:27.245: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 09/04/23 14:53:27.246
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:53:27.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:53:27.304
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5606 09/04/23 14:53:27.326
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 09/04/23 14:53:27.339
    STEP: Creating stateful set ss in namespace statefulset-5606 09/04/23 14:53:27.351
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5606 09/04/23 14:53:27.364
    Sep  4 14:53:27.376: INFO: Found 0 stateful pods, waiting for 1
    Sep  4 14:53:37.390: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/04/23 14:53:37.39
    Sep  4 14:53:37.403: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 14:53:37.896: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 14:53:37.896: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 14:53:37.896: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 14:53:37.910: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep  4 14:53:47.924: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 14:53:47.924: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 14:53:47.975: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999845s
    Sep  4 14:53:48.989: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.987431645s
    Sep  4 14:53:50.003: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.973733508s
    Sep  4 14:53:51.017: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.959654137s
    Sep  4 14:53:52.031: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.944527383s
    Sep  4 14:53:53.045: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.931590429s
    Sep  4 14:53:54.058: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.918142379s
    Sep  4 14:53:55.072: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.904698277s
    Sep  4 14:53:56.086: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.890511693s
    Sep  4 14:53:57.099: INFO: Verifying statefulset ss doesn't scale past 1 for another 876.810522ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5606 09/04/23 14:53:58.1
    Sep  4 14:53:58.113: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:53:58.698: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  4 14:53:58.698: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 14:53:58.698: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  4 14:53:58.711: INFO: Found 1 stateful pods, waiting for 3
    Sep  4 14:54:08.724: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 14:54:08.724: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 14:54:08.724: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 09/04/23 14:54:08.724
    STEP: Scale down will halt with unhealthy stateful pod 09/04/23 14:54:08.724
    Sep  4 14:54:08.749: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 14:54:09.164: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 14:54:09.164: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 14:54:09.164: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 14:54:09.164: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 14:54:09.767: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 14:54:09.767: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 14:54:09.767: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 14:54:09.767: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 14:54:10.259: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 14:54:10.259: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 14:54:10.260: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 14:54:10.260: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 14:54:10.272: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Sep  4 14:54:20.302: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 14:54:20.302: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 14:54:20.302: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 14:54:20.341: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999832s
    Sep  4 14:54:21.355: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986780248s
    Sep  4 14:54:22.367: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973630964s
    Sep  4 14:54:23.381: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.96019743s
    Sep  4 14:54:24.394: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.946976108s
    Sep  4 14:54:25.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.934053744s
    Sep  4 14:54:26.422: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.919720133s
    Sep  4 14:54:27.435: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.906329574s
    Sep  4 14:54:28.449: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.892842764s
    Sep  4 14:54:29.463: INFO: Verifying statefulset ss doesn't scale past 3 for another 879.295091ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5606 09/04/23 14:54:30.464
    Sep  4 14:54:30.477: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:54:30.984: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  4 14:54:30.984: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 14:54:30.984: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  4 14:54:30.984: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:54:31.549: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  4 14:54:31.549: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 14:54:31.549: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  4 14:54:31.549: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:54:32.213: INFO: rc: 1
    Sep  4 14:54:32.213: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    error: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec "a271ecbf118f93874ea6709c5196d0c2f38a044bc02b779bd2d2daefd9c5245a": OCI runtime exec failed: exec failed: unable to start container process: error writing config to pipe: write init-p: broken pipe: unknown

    error:
    exit status 1
    Sep  4 14:54:42.214: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:54:42.325: INFO: rc: 1
    Sep  4 14:54:42.325: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:54:52.326: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:54:52.428: INFO: rc: 1
    Sep  4 14:54:52.428: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:55:02.428: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:55:02.540: INFO: rc: 1
    Sep  4 14:55:02.540: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:55:12.540: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:55:12.657: INFO: rc: 1
    Sep  4 14:55:12.657: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:55:22.658: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:55:22.771: INFO: rc: 1
    Sep  4 14:55:22.771: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:55:32.771: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:55:32.910: INFO: rc: 1
    Sep  4 14:55:32.910: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:55:42.911: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:55:43.026: INFO: rc: 1
    Sep  4 14:55:43.026: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:55:53.027: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:55:53.137: INFO: rc: 1
    Sep  4 14:55:53.138: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:56:03.139: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:56:03.271: INFO: rc: 1
    Sep  4 14:56:03.271: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:56:13.272: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:56:13.384: INFO: rc: 1
    Sep  4 14:56:13.384: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:56:23.386: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:56:23.488: INFO: rc: 1
    Sep  4 14:56:23.488: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:56:33.489: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:56:33.604: INFO: rc: 1
    Sep  4 14:56:33.604: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:56:43.606: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:56:43.710: INFO: rc: 1
    Sep  4 14:56:43.710: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:56:53.712: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:56:53.818: INFO: rc: 1
    Sep  4 14:56:53.818: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:57:03.818: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:57:03.930: INFO: rc: 1
    Sep  4 14:57:03.930: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:57:13.931: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:57:14.051: INFO: rc: 1
    Sep  4 14:57:14.051: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:57:24.051: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:57:24.181: INFO: rc: 1
    Sep  4 14:57:24.181: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:57:34.183: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:57:34.289: INFO: rc: 1
    Sep  4 14:57:34.289: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:57:44.289: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:57:44.421: INFO: rc: 1
    Sep  4 14:57:44.421: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:57:54.423: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:57:54.556: INFO: rc: 1
    Sep  4 14:57:54.556: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:58:04.559: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:58:04.682: INFO: rc: 1
    Sep  4 14:58:04.682: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:58:14.683: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:58:14.805: INFO: rc: 1
    Sep  4 14:58:14.805: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:58:24.808: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:58:24.914: INFO: rc: 1
    Sep  4 14:58:24.914: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:58:34.916: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:58:35.030: INFO: rc: 1
    Sep  4 14:58:35.030: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:58:45.032: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:58:45.150: INFO: rc: 1
    Sep  4 14:58:45.150: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:58:55.154: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:58:55.258: INFO: rc: 1
    Sep  4 14:58:55.258: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:59:05.258: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:59:05.370: INFO: rc: 1
    Sep  4 14:59:05.370: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:59:15.371: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:59:15.489: INFO: rc: 1
    Sep  4 14:59:15.489: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:59:25.490: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:59:25.603: INFO: rc: 1
    Sep  4 14:59:25.603: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Sep  4 14:59:35.606: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-5606 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 14:59:35.717: INFO: rc: 1
    Sep  4 14:59:35.717: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
    Sep  4 14:59:35.717: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 09/04/23 14:59:35.755
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 14:59:35.755: INFO: Deleting all statefulset in ns statefulset-5606
    Sep  4 14:59:35.767: INFO: Scaling statefulset ss to 0
    Sep  4 14:59:35.804: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 14:59:35.817: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:59:35.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5606" for this suite. 09/04/23 14:59:35.877
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:59:35.892
Sep  4 14:59:35.892: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 09/04/23 14:59:35.893
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:59:35.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:59:35.954
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 09/04/23 14:59:35.977
STEP: delete the rc 09/04/23 14:59:41.008
STEP: wait for all pods to be garbage collected 09/04/23 14:59:41.021
STEP: Gathering metrics 09/04/23 14:59:46.046
W0904 14:59:46.075259    7754 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep  4 14:59:46.075: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  4 14:59:46.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3843" for this suite. 09/04/23 14:59:46.088
------------------------------
• [10.210 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:59:35.892
    Sep  4 14:59:35.892: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 09/04/23 14:59:35.893
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:59:35.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:59:35.954
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 09/04/23 14:59:35.977
    STEP: delete the rc 09/04/23 14:59:41.008
    STEP: wait for all pods to be garbage collected 09/04/23 14:59:41.021
    STEP: Gathering metrics 09/04/23 14:59:46.046
    W0904 14:59:46.075259    7754 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep  4 14:59:46.075: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:59:46.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3843" for this suite. 09/04/23 14:59:46.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:59:46.102
Sep  4 14:59:46.102: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 14:59:46.103
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:59:46.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:59:46.161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 09/04/23 14:59:46.183
Sep  4 14:59:46.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5" in namespace "downward-api-3426" to be "Succeeded or Failed"
Sep  4 14:59:46.214: INFO: Pod "downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.01852ms
Sep  4 14:59:48.228: INFO: Pod "downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026511891s
Sep  4 14:59:50.228: INFO: Pod "downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026923596s
Sep  4 14:59:52.228: INFO: Pod "downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026840077s
STEP: Saw pod success 09/04/23 14:59:52.228
Sep  4 14:59:52.229: INFO: Pod "downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5" satisfied condition "Succeeded or Failed"
Sep  4 14:59:52.241: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5 container client-container: <nil>
STEP: delete the pod 09/04/23 14:59:52.429
Sep  4 14:59:52.445: INFO: Waiting for pod downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5 to disappear
Sep  4 14:59:52.457: INFO: Pod downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 14:59:52.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3426" for this suite. 09/04/23 14:59:52.48
------------------------------
• [6.391 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:59:46.102
    Sep  4 14:59:46.102: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 14:59:46.103
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:59:46.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:59:46.161
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 09/04/23 14:59:46.183
    Sep  4 14:59:46.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5" in namespace "downward-api-3426" to be "Succeeded or Failed"
    Sep  4 14:59:46.214: INFO: Pod "downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.01852ms
    Sep  4 14:59:48.228: INFO: Pod "downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026511891s
    Sep  4 14:59:50.228: INFO: Pod "downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026923596s
    Sep  4 14:59:52.228: INFO: Pod "downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026840077s
    STEP: Saw pod success 09/04/23 14:59:52.228
    Sep  4 14:59:52.229: INFO: Pod "downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5" satisfied condition "Succeeded or Failed"
    Sep  4 14:59:52.241: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5 container client-container: <nil>
    STEP: delete the pod 09/04/23 14:59:52.429
    Sep  4 14:59:52.445: INFO: Waiting for pod downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5 to disappear
    Sep  4 14:59:52.457: INFO: Pod downwardapi-volume-90d64fc1-3615-4495-bd4c-148ccfb36de5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:59:52.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3426" for this suite. 09/04/23 14:59:52.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:59:52.494
Sep  4 14:59:52.494: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 14:59:52.495
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:59:52.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:59:52.556
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-23d0fa53-9964-47a5-9d29-9072bc539d66 09/04/23 14:59:52.591
STEP: Creating the pod 09/04/23 14:59:52.604
Sep  4 14:59:52.624: INFO: Waiting up to 5m0s for pod "pod-configmaps-09b28a38-0abb-4ba6-ae4e-486a34668bfb" in namespace "configmap-2653" to be "running"
Sep  4 14:59:52.636: INFO: Pod "pod-configmaps-09b28a38-0abb-4ba6-ae4e-486a34668bfb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.451096ms
Sep  4 14:59:54.649: INFO: Pod "pod-configmaps-09b28a38-0abb-4ba6-ae4e-486a34668bfb": Phase="Running", Reason="", readiness=false. Elapsed: 2.024428025s
Sep  4 14:59:54.649: INFO: Pod "pod-configmaps-09b28a38-0abb-4ba6-ae4e-486a34668bfb" satisfied condition "running"
STEP: Waiting for pod with text data 09/04/23 14:59:54.649
STEP: Waiting for pod with binary data 09/04/23 14:59:54.695
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 14:59:54.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2653" for this suite. 09/04/23 14:59:54.757
------------------------------
• [2.276 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:59:52.494
    Sep  4 14:59:52.494: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 14:59:52.495
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:59:52.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:59:52.556
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-23d0fa53-9964-47a5-9d29-9072bc539d66 09/04/23 14:59:52.591
    STEP: Creating the pod 09/04/23 14:59:52.604
    Sep  4 14:59:52.624: INFO: Waiting up to 5m0s for pod "pod-configmaps-09b28a38-0abb-4ba6-ae4e-486a34668bfb" in namespace "configmap-2653" to be "running"
    Sep  4 14:59:52.636: INFO: Pod "pod-configmaps-09b28a38-0abb-4ba6-ae4e-486a34668bfb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.451096ms
    Sep  4 14:59:54.649: INFO: Pod "pod-configmaps-09b28a38-0abb-4ba6-ae4e-486a34668bfb": Phase="Running", Reason="", readiness=false. Elapsed: 2.024428025s
    Sep  4 14:59:54.649: INFO: Pod "pod-configmaps-09b28a38-0abb-4ba6-ae4e-486a34668bfb" satisfied condition "running"
    STEP: Waiting for pod with text data 09/04/23 14:59:54.649
    STEP: Waiting for pod with binary data 09/04/23 14:59:54.695
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 14:59:54.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2653" for this suite. 09/04/23 14:59:54.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 14:59:54.771
Sep  4 14:59:54.771: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 09/04/23 14:59:54.772
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:59:54.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:59:54.83
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 09/04/23 14:59:54.853
STEP: Ensuring active pods == parallelism 09/04/23 14:59:54.866
STEP: delete a job 09/04/23 14:59:56.881
STEP: deleting Job.batch foo in namespace job-8002, will wait for the garbage collector to delete the pods 09/04/23 14:59:56.881
Sep  4 14:59:56.958: INFO: Deleting Job.batch foo took: 14.321207ms
Sep  4 14:59:57.059: INFO: Terminating Job.batch foo pods took: 101.053575ms
STEP: Ensuring job was deleted 09/04/23 15:00:28.96
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  4 15:00:28.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8002" for this suite. 09/04/23 15:00:28.997
------------------------------
• [34.240 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 14:59:54.771
    Sep  4 14:59:54.771: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 09/04/23 14:59:54.772
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 14:59:54.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 14:59:54.83
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 09/04/23 14:59:54.853
    STEP: Ensuring active pods == parallelism 09/04/23 14:59:54.866
    STEP: delete a job 09/04/23 14:59:56.881
    STEP: deleting Job.batch foo in namespace job-8002, will wait for the garbage collector to delete the pods 09/04/23 14:59:56.881
    Sep  4 14:59:56.958: INFO: Deleting Job.batch foo took: 14.321207ms
    Sep  4 14:59:57.059: INFO: Terminating Job.batch foo pods took: 101.053575ms
    STEP: Ensuring job was deleted 09/04/23 15:00:28.96
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:00:28.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8002" for this suite. 09/04/23 15:00:28.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:00:29.011
Sep  4 15:00:29.011: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename csiinlinevolumes 09/04/23 15:00:29.012
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:00:29.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:00:29.07
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 09/04/23 15:00:29.092
STEP: getting 09/04/23 15:00:29.133
STEP: listing 09/04/23 15:00:29.163
STEP: deleting 09/04/23 15:00:29.175
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Sep  4 15:00:29.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-7610" for this suite. 09/04/23 15:00:29.242
------------------------------
• [0.245 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:00:29.011
    Sep  4 15:00:29.011: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename csiinlinevolumes 09/04/23 15:00:29.012
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:00:29.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:00:29.07
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 09/04/23 15:00:29.092
    STEP: getting 09/04/23 15:00:29.133
    STEP: listing 09/04/23 15:00:29.163
    STEP: deleting 09/04/23 15:00:29.175
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:00:29.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-7610" for this suite. 09/04/23 15:00:29.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:00:29.259
Sep  4 15:00:29.259: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 15:00:29.26
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:00:29.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:00:29.319
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/04/23 15:00:29.342
Sep  4 15:00:29.362: INFO: Waiting up to 5m0s for pod "pod-43af14f5-7039-4c3a-8f38-608b670be41e" in namespace "emptydir-1332" to be "Succeeded or Failed"
Sep  4 15:00:29.374: INFO: Pod "pod-43af14f5-7039-4c3a-8f38-608b670be41e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.283982ms
Sep  4 15:00:31.389: INFO: Pod "pod-43af14f5-7039-4c3a-8f38-608b670be41e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026475707s
Sep  4 15:00:33.388: INFO: Pod "pod-43af14f5-7039-4c3a-8f38-608b670be41e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026203398s
STEP: Saw pod success 09/04/23 15:00:33.388
Sep  4 15:00:33.389: INFO: Pod "pod-43af14f5-7039-4c3a-8f38-608b670be41e" satisfied condition "Succeeded or Failed"
Sep  4 15:00:33.401: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-43af14f5-7039-4c3a-8f38-608b670be41e container test-container: <nil>
STEP: delete the pod 09/04/23 15:00:33.435
Sep  4 15:00:33.452: INFO: Waiting for pod pod-43af14f5-7039-4c3a-8f38-608b670be41e to disappear
Sep  4 15:00:33.464: INFO: Pod pod-43af14f5-7039-4c3a-8f38-608b670be41e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 15:00:33.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1332" for this suite. 09/04/23 15:00:33.487
------------------------------
• [4.242 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:00:29.259
    Sep  4 15:00:29.259: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 15:00:29.26
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:00:29.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:00:29.319
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/04/23 15:00:29.342
    Sep  4 15:00:29.362: INFO: Waiting up to 5m0s for pod "pod-43af14f5-7039-4c3a-8f38-608b670be41e" in namespace "emptydir-1332" to be "Succeeded or Failed"
    Sep  4 15:00:29.374: INFO: Pod "pod-43af14f5-7039-4c3a-8f38-608b670be41e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.283982ms
    Sep  4 15:00:31.389: INFO: Pod "pod-43af14f5-7039-4c3a-8f38-608b670be41e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026475707s
    Sep  4 15:00:33.388: INFO: Pod "pod-43af14f5-7039-4c3a-8f38-608b670be41e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026203398s
    STEP: Saw pod success 09/04/23 15:00:33.388
    Sep  4 15:00:33.389: INFO: Pod "pod-43af14f5-7039-4c3a-8f38-608b670be41e" satisfied condition "Succeeded or Failed"
    Sep  4 15:00:33.401: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-43af14f5-7039-4c3a-8f38-608b670be41e container test-container: <nil>
    STEP: delete the pod 09/04/23 15:00:33.435
    Sep  4 15:00:33.452: INFO: Waiting for pod pod-43af14f5-7039-4c3a-8f38-608b670be41e to disappear
    Sep  4 15:00:33.464: INFO: Pod pod-43af14f5-7039-4c3a-8f38-608b670be41e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:00:33.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1332" for this suite. 09/04/23 15:00:33.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:00:33.501
Sep  4 15:00:33.501: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 15:00:33.502
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:00:33.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:00:33.559
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-1859 09/04/23 15:00:33.581
STEP: creating service affinity-clusterip in namespace services-1859 09/04/23 15:00:33.581
STEP: creating replication controller affinity-clusterip in namespace services-1859 09/04/23 15:00:33.598
I0904 15:00:33.611898    7754 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1859, replica count: 3
I0904 15:00:36.663255    7754 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 15:00:36.688: INFO: Creating new exec pod
Sep  4 15:00:36.704: INFO: Waiting up to 5m0s for pod "execpod-affinityfbnm9" in namespace "services-1859" to be "running"
Sep  4 15:00:36.717: INFO: Pod "execpod-affinityfbnm9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.457787ms
Sep  4 15:00:38.730: INFO: Pod "execpod-affinityfbnm9": Phase="Running", Reason="", readiness=true. Elapsed: 2.025985796s
Sep  4 15:00:38.730: INFO: Pod "execpod-affinityfbnm9" satisfied condition "running"
Sep  4 15:00:39.731: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1859 exec execpod-affinityfbnm9 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Sep  4 15:00:40.172: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep  4 15:00:40.172: INFO: stdout: ""
Sep  4 15:00:40.172: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1859 exec execpod-affinityfbnm9 -- /bin/sh -x -c nc -v -z -w 2 100.104.163.130 80'
Sep  4 15:00:40.819: INFO: stderr: "+ nc -v -z -w 2 100.104.163.130 80\nConnection to 100.104.163.130 80 port [tcp/http] succeeded!\n"
Sep  4 15:00:40.820: INFO: stdout: ""
Sep  4 15:00:40.820: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1859 exec execpod-affinityfbnm9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.104.163.130:80/ ; done'
Sep  4 15:00:41.377: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n"
Sep  4 15:00:41.377: INFO: stdout: "\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j"
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
Sep  4 15:00:41.377: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1859, will wait for the garbage collector to delete the pods 09/04/23 15:00:41.393
Sep  4 15:00:41.471: INFO: Deleting ReplicationController affinity-clusterip took: 15.080181ms
Sep  4 15:00:41.572: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.807626ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 15:00:42.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1859" for this suite. 09/04/23 15:00:43.016
------------------------------
• [9.535 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:00:33.501
    Sep  4 15:00:33.501: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 15:00:33.502
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:00:33.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:00:33.559
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-1859 09/04/23 15:00:33.581
    STEP: creating service affinity-clusterip in namespace services-1859 09/04/23 15:00:33.581
    STEP: creating replication controller affinity-clusterip in namespace services-1859 09/04/23 15:00:33.598
    I0904 15:00:33.611898    7754 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1859, replica count: 3
    I0904 15:00:36.663255    7754 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 15:00:36.688: INFO: Creating new exec pod
    Sep  4 15:00:36.704: INFO: Waiting up to 5m0s for pod "execpod-affinityfbnm9" in namespace "services-1859" to be "running"
    Sep  4 15:00:36.717: INFO: Pod "execpod-affinityfbnm9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.457787ms
    Sep  4 15:00:38.730: INFO: Pod "execpod-affinityfbnm9": Phase="Running", Reason="", readiness=true. Elapsed: 2.025985796s
    Sep  4 15:00:38.730: INFO: Pod "execpod-affinityfbnm9" satisfied condition "running"
    Sep  4 15:00:39.731: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1859 exec execpod-affinityfbnm9 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Sep  4 15:00:40.172: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Sep  4 15:00:40.172: INFO: stdout: ""
    Sep  4 15:00:40.172: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1859 exec execpod-affinityfbnm9 -- /bin/sh -x -c nc -v -z -w 2 100.104.163.130 80'
    Sep  4 15:00:40.819: INFO: stderr: "+ nc -v -z -w 2 100.104.163.130 80\nConnection to 100.104.163.130 80 port [tcp/http] succeeded!\n"
    Sep  4 15:00:40.820: INFO: stdout: ""
    Sep  4 15:00:40.820: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1859 exec execpod-affinityfbnm9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.104.163.130:80/ ; done'
    Sep  4 15:00:41.377: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.104.163.130:80/\n"
    Sep  4 15:00:41.377: INFO: stdout: "\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j\naffinity-clusterip-jqk5j"
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Received response from host: affinity-clusterip-jqk5j
    Sep  4 15:00:41.377: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-1859, will wait for the garbage collector to delete the pods 09/04/23 15:00:41.393
    Sep  4 15:00:41.471: INFO: Deleting ReplicationController affinity-clusterip took: 15.080181ms
    Sep  4 15:00:41.572: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.807626ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:00:42.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1859" for this suite. 09/04/23 15:00:43.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:00:43.037
Sep  4 15:00:43.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 09/04/23 15:00:43.038
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:00:43.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:00:43.097
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  4 15:00:43.158: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  4 15:01:43.281: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:01:43.293
Sep  4 15:01:43.293: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path 09/04/23 15:01:43.294
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:01:43.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:01:43.352
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Sep  4 15:01:43.412: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Sep  4 15:01:43.424: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Sep  4 15:01:43.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:01:43.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-615" for this suite. 09/04/23 15:01:43.612
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-819" for this suite. 09/04/23 15:01:43.624
------------------------------
• [60.600 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:00:43.037
    Sep  4 15:00:43.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 09/04/23 15:00:43.038
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:00:43.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:00:43.097
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  4 15:00:43.158: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  4 15:01:43.281: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:01:43.293
    Sep  4 15:01:43.293: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption-path 09/04/23 15:01:43.294
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:01:43.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:01:43.352
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Sep  4 15:01:43.412: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Sep  4 15:01:43.424: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:01:43.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:01:43.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-615" for this suite. 09/04/23 15:01:43.612
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-819" for this suite. 09/04/23 15:01:43.624
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:01:43.637
Sep  4 15:01:43.637: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 15:01:43.638
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:01:43.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:01:43.696
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 15:01:43.745
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:01:44.078
STEP: Deploying the webhook pod 09/04/23 15:01:44.092
STEP: Wait for the deployment to be ready 09/04/23 15:01:44.118
Sep  4 15:01:44.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 1, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 1, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 1, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 1, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 15:01:46.166
STEP: Verifying the service has paired with the endpoint 09/04/23 15:01:46.183
Sep  4 15:01:47.183: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/04/23 15:01:47.196
STEP: create a configmap that should be updated by the webhook 09/04/23 15:01:47.325
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:01:47.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-596" for this suite. 09/04/23 15:01:47.594
STEP: Destroying namespace "webhook-596-markers" for this suite. 09/04/23 15:01:47.607
------------------------------
• [3.983 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:01:43.637
    Sep  4 15:01:43.637: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 15:01:43.638
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:01:43.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:01:43.696
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 15:01:43.745
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:01:44.078
    STEP: Deploying the webhook pod 09/04/23 15:01:44.092
    STEP: Wait for the deployment to be ready 09/04/23 15:01:44.118
    Sep  4 15:01:44.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 1, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 1, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 1, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 1, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 15:01:46.166
    STEP: Verifying the service has paired with the endpoint 09/04/23 15:01:46.183
    Sep  4 15:01:47.183: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/04/23 15:01:47.196
    STEP: create a configmap that should be updated by the webhook 09/04/23 15:01:47.325
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:01:47.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-596" for this suite. 09/04/23 15:01:47.594
    STEP: Destroying namespace "webhook-596-markers" for this suite. 09/04/23 15:01:47.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:01:47.621
Sep  4 15:01:47.621: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 15:01:47.622
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:01:47.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:01:47.68
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-9609/configmap-test-87291eaf-4766-47d9-acfc-b5a4bdf0fe56 09/04/23 15:01:47.702
STEP: Creating a pod to test consume configMaps 09/04/23 15:01:47.714
Sep  4 15:01:47.732: INFO: Waiting up to 5m0s for pod "pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236" in namespace "configmap-9609" to be "Succeeded or Failed"
Sep  4 15:01:47.744: INFO: Pod "pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236": Phase="Pending", Reason="", readiness=false. Elapsed: 11.81857ms
Sep  4 15:01:49.758: INFO: Pod "pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026026734s
Sep  4 15:01:51.758: INFO: Pod "pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025718852s
Sep  4 15:01:53.757: INFO: Pod "pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024709637s
STEP: Saw pod success 09/04/23 15:01:53.757
Sep  4 15:01:53.757: INFO: Pod "pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236" satisfied condition "Succeeded or Failed"
Sep  4 15:01:53.769: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236 container env-test: <nil>
STEP: delete the pod 09/04/23 15:01:53.803
Sep  4 15:01:53.820: INFO: Waiting for pod pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236 to disappear
Sep  4 15:01:53.831: INFO: Pod pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:01:53.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9609" for this suite. 09/04/23 15:01:53.853
------------------------------
• [6.244 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:01:47.621
    Sep  4 15:01:47.621: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 15:01:47.622
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:01:47.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:01:47.68
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-9609/configmap-test-87291eaf-4766-47d9-acfc-b5a4bdf0fe56 09/04/23 15:01:47.702
    STEP: Creating a pod to test consume configMaps 09/04/23 15:01:47.714
    Sep  4 15:01:47.732: INFO: Waiting up to 5m0s for pod "pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236" in namespace "configmap-9609" to be "Succeeded or Failed"
    Sep  4 15:01:47.744: INFO: Pod "pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236": Phase="Pending", Reason="", readiness=false. Elapsed: 11.81857ms
    Sep  4 15:01:49.758: INFO: Pod "pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026026734s
    Sep  4 15:01:51.758: INFO: Pod "pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025718852s
    Sep  4 15:01:53.757: INFO: Pod "pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024709637s
    STEP: Saw pod success 09/04/23 15:01:53.757
    Sep  4 15:01:53.757: INFO: Pod "pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236" satisfied condition "Succeeded or Failed"
    Sep  4 15:01:53.769: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236 container env-test: <nil>
    STEP: delete the pod 09/04/23 15:01:53.803
    Sep  4 15:01:53.820: INFO: Waiting for pod pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236 to disappear
    Sep  4 15:01:53.831: INFO: Pod pod-configmaps-39036ca0-b83f-44d0-92a9-46a390ce2236 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:01:53.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9609" for this suite. 09/04/23 15:01:53.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:01:53.865
Sep  4 15:01:53.865: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 09/04/23 15:01:53.866
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:01:53.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:01:53.924
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 09/04/23 15:01:53.946
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:01:53.982
STEP: Creating a pod in the namespace 09/04/23 15:01:54.004
STEP: Waiting for the pod to have running status 09/04/23 15:01:54.021
Sep  4 15:01:54.022: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5729" to be "running"
Sep  4 15:01:54.033: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.249997ms
Sep  4 15:01:56.047: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025331696s
Sep  4 15:01:56.047: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 09/04/23 15:01:56.047
STEP: Waiting for the namespace to be removed. 09/04/23 15:01:56.06
STEP: Recreating the namespace 09/04/23 15:02:07.073
STEP: Verifying there are no pods in the namespace 09/04/23 15:02:07.111
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:02:07.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7553" for this suite. 09/04/23 15:02:07.146
STEP: Destroying namespace "nsdeletetest-5729" for this suite. 09/04/23 15:02:07.16
Sep  4 15:02:07.172: INFO: Namespace nsdeletetest-5729 was already deleted
STEP: Destroying namespace "nsdeletetest-3977" for this suite. 09/04/23 15:02:07.172
------------------------------
• [13.320 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:01:53.865
    Sep  4 15:01:53.865: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 09/04/23 15:01:53.866
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:01:53.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:01:53.924
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 09/04/23 15:01:53.946
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:01:53.982
    STEP: Creating a pod in the namespace 09/04/23 15:01:54.004
    STEP: Waiting for the pod to have running status 09/04/23 15:01:54.021
    Sep  4 15:01:54.022: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5729" to be "running"
    Sep  4 15:01:54.033: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.249997ms
    Sep  4 15:01:56.047: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025331696s
    Sep  4 15:01:56.047: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 09/04/23 15:01:56.047
    STEP: Waiting for the namespace to be removed. 09/04/23 15:01:56.06
    STEP: Recreating the namespace 09/04/23 15:02:07.073
    STEP: Verifying there are no pods in the namespace 09/04/23 15:02:07.111
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:02:07.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7553" for this suite. 09/04/23 15:02:07.146
    STEP: Destroying namespace "nsdeletetest-5729" for this suite. 09/04/23 15:02:07.16
    Sep  4 15:02:07.172: INFO: Namespace nsdeletetest-5729 was already deleted
    STEP: Destroying namespace "nsdeletetest-3977" for this suite. 09/04/23 15:02:07.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:02:07.186
Sep  4 15:02:07.186: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 09/04/23 15:02:07.186
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:02:07.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:02:07.245
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 09/04/23 15:02:07.266
STEP: waiting for pod running 09/04/23 15:02:07.285
Sep  4 15:02:07.285: INFO: Waiting up to 2m0s for pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785" in namespace "var-expansion-3484" to be "running"
Sep  4 15:02:07.297: INFO: Pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785": Phase="Pending", Reason="", readiness=false. Elapsed: 12.110823ms
Sep  4 15:02:09.313: INFO: Pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785": Phase="Running", Reason="", readiness=true. Elapsed: 2.027771791s
Sep  4 15:02:09.313: INFO: Pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785" satisfied condition "running"
STEP: creating a file in subpath 09/04/23 15:02:09.313
Sep  4 15:02:09.326: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3484 PodName:var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:02:09.326: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:02:09.326: INFO: ExecWithOptions: Clientset creation
Sep  4 15:02:09.326: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/var-expansion-3484/pods/var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 09/04/23 15:02:09.795
Sep  4 15:02:09.809: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3484 PodName:var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:02:09.809: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:02:09.809: INFO: ExecWithOptions: Clientset creation
Sep  4 15:02:09.810: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/var-expansion-3484/pods/var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 09/04/23 15:02:10.087
Sep  4 15:02:10.616: INFO: Successfully updated pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785"
STEP: waiting for annotated pod running 09/04/23 15:02:10.616
Sep  4 15:02:10.616: INFO: Waiting up to 2m0s for pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785" in namespace "var-expansion-3484" to be "running"
Sep  4 15:02:10.629: INFO: Pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785": Phase="Running", Reason="", readiness=true. Elapsed: 12.556467ms
Sep  4 15:02:10.629: INFO: Pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785" satisfied condition "running"
STEP: deleting the pod gracefully 09/04/23 15:02:10.629
Sep  4 15:02:10.629: INFO: Deleting pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785" in namespace "var-expansion-3484"
Sep  4 15:02:10.644: INFO: Wait up to 5m0s for pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 15:02:42.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3484" for this suite. 09/04/23 15:02:42.699
------------------------------
• [35.528 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:02:07.186
    Sep  4 15:02:07.186: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 09/04/23 15:02:07.186
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:02:07.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:02:07.245
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 09/04/23 15:02:07.266
    STEP: waiting for pod running 09/04/23 15:02:07.285
    Sep  4 15:02:07.285: INFO: Waiting up to 2m0s for pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785" in namespace "var-expansion-3484" to be "running"
    Sep  4 15:02:07.297: INFO: Pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785": Phase="Pending", Reason="", readiness=false. Elapsed: 12.110823ms
    Sep  4 15:02:09.313: INFO: Pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785": Phase="Running", Reason="", readiness=true. Elapsed: 2.027771791s
    Sep  4 15:02:09.313: INFO: Pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785" satisfied condition "running"
    STEP: creating a file in subpath 09/04/23 15:02:09.313
    Sep  4 15:02:09.326: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3484 PodName:var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:02:09.326: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:02:09.326: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:02:09.326: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/var-expansion-3484/pods/var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 09/04/23 15:02:09.795
    Sep  4 15:02:09.809: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3484 PodName:var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:02:09.809: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:02:09.809: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:02:09.810: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/var-expansion-3484/pods/var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 09/04/23 15:02:10.087
    Sep  4 15:02:10.616: INFO: Successfully updated pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785"
    STEP: waiting for annotated pod running 09/04/23 15:02:10.616
    Sep  4 15:02:10.616: INFO: Waiting up to 2m0s for pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785" in namespace "var-expansion-3484" to be "running"
    Sep  4 15:02:10.629: INFO: Pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785": Phase="Running", Reason="", readiness=true. Elapsed: 12.556467ms
    Sep  4 15:02:10.629: INFO: Pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785" satisfied condition "running"
    STEP: deleting the pod gracefully 09/04/23 15:02:10.629
    Sep  4 15:02:10.629: INFO: Deleting pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785" in namespace "var-expansion-3484"
    Sep  4 15:02:10.644: INFO: Wait up to 5m0s for pod "var-expansion-46e6fa50-f3b0-4328-96c0-b6a2a04f0785" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:02:42.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3484" for this suite. 09/04/23 15:02:42.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:02:42.715
Sep  4 15:02:42.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 15:02:42.716
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:02:42.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:02:42.778
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 09/04/23 15:02:42.801
Sep  4 15:02:42.823: INFO: Waiting up to 5m0s for pod "downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45" in namespace "downward-api-4127" to be "Succeeded or Failed"
Sep  4 15:02:42.835: INFO: Pod "downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45": Phase="Pending", Reason="", readiness=false. Elapsed: 12.114853ms
Sep  4 15:02:44.850: INFO: Pod "downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027026553s
Sep  4 15:02:46.850: INFO: Pod "downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027102535s
STEP: Saw pod success 09/04/23 15:02:46.85
Sep  4 15:02:46.850: INFO: Pod "downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45" satisfied condition "Succeeded or Failed"
Sep  4 15:02:46.863: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45 container dapi-container: <nil>
STEP: delete the pod 09/04/23 15:02:46.899
Sep  4 15:02:46.916: INFO: Waiting for pod downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45 to disappear
Sep  4 15:02:46.928: INFO: Pod downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  4 15:02:46.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4127" for this suite. 09/04/23 15:02:46.951
------------------------------
• [4.250 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:02:42.715
    Sep  4 15:02:42.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 15:02:42.716
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:02:42.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:02:42.778
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 09/04/23 15:02:42.801
    Sep  4 15:02:42.823: INFO: Waiting up to 5m0s for pod "downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45" in namespace "downward-api-4127" to be "Succeeded or Failed"
    Sep  4 15:02:42.835: INFO: Pod "downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45": Phase="Pending", Reason="", readiness=false. Elapsed: 12.114853ms
    Sep  4 15:02:44.850: INFO: Pod "downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027026553s
    Sep  4 15:02:46.850: INFO: Pod "downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027102535s
    STEP: Saw pod success 09/04/23 15:02:46.85
    Sep  4 15:02:46.850: INFO: Pod "downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45" satisfied condition "Succeeded or Failed"
    Sep  4 15:02:46.863: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45 container dapi-container: <nil>
    STEP: delete the pod 09/04/23 15:02:46.899
    Sep  4 15:02:46.916: INFO: Waiting for pod downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45 to disappear
    Sep  4 15:02:46.928: INFO: Pod downward-api-d94f56f6-00e6-4ef2-a751-0a8368b7be45 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:02:46.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4127" for this suite. 09/04/23 15:02:46.951
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:02:46.965
Sep  4 15:02:46.966: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 09/04/23 15:02:46.966
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:02:47.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:02:47.029
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-7656-delete-me 09/04/23 15:02:47.066
STEP: Waiting for the RuntimeClass to disappear 09/04/23 15:02:47.081
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  4 15:02:47.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7656" for this suite. 09/04/23 15:02:47.123
------------------------------
• [0.171 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:02:46.965
    Sep  4 15:02:46.966: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 09/04/23 15:02:46.966
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:02:47.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:02:47.029
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-7656-delete-me 09/04/23 15:02:47.066
    STEP: Waiting for the RuntimeClass to disappear 09/04/23 15:02:47.081
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:02:47.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7656" for this suite. 09/04/23 15:02:47.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:02:47.139
Sep  4 15:02:47.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 09/04/23 15:02:47.14
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:02:47.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:02:47.202
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6369 09/04/23 15:02:47.225
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 09/04/23 15:02:47.238
Sep  4 15:02:47.264: INFO: Found 0 stateful pods, waiting for 3
Sep  4 15:02:57.279: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 15:02:57.279: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 15:02:57.279: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/04/23 15:02:57.32
Sep  4 15:02:57.358: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/04/23 15:02:57.358
STEP: Not applying an update when the partition is greater than the number of replicas 09/04/23 15:02:57.384
STEP: Performing a canary update 09/04/23 15:02:57.384
Sep  4 15:02:57.422: INFO: Updating stateful set ss2
Sep  4 15:02:57.455: INFO: Waiting for Pod statefulset-6369/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 09/04/23 15:03:07.483
Sep  4 15:03:07.530: INFO: Found 2 stateful pods, waiting for 3
Sep  4 15:03:17.545: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 15:03:17.545: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 15:03:17.545: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 09/04/23 15:03:17.569
Sep  4 15:03:17.608: INFO: Updating stateful set ss2
Sep  4 15:03:17.633: INFO: Waiting for Pod statefulset-6369/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Sep  4 15:03:27.697: INFO: Updating stateful set ss2
Sep  4 15:03:27.721: INFO: Waiting for StatefulSet statefulset-6369/ss2 to complete update
Sep  4 15:03:27.721: INFO: Waiting for Pod statefulset-6369/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 15:03:37.748: INFO: Deleting all statefulset in ns statefulset-6369
Sep  4 15:03:37.761: INFO: Scaling statefulset ss2 to 0
Sep  4 15:03:47.815: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 15:03:47.828: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 15:03:47.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6369" for this suite. 09/04/23 15:03:47.893
------------------------------
• [60.767 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:02:47.139
    Sep  4 15:02:47.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 09/04/23 15:02:47.14
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:02:47.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:02:47.202
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6369 09/04/23 15:02:47.225
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 09/04/23 15:02:47.238
    Sep  4 15:02:47.264: INFO: Found 0 stateful pods, waiting for 3
    Sep  4 15:02:57.279: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 15:02:57.279: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 15:02:57.279: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/04/23 15:02:57.32
    Sep  4 15:02:57.358: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/04/23 15:02:57.358
    STEP: Not applying an update when the partition is greater than the number of replicas 09/04/23 15:02:57.384
    STEP: Performing a canary update 09/04/23 15:02:57.384
    Sep  4 15:02:57.422: INFO: Updating stateful set ss2
    Sep  4 15:02:57.455: INFO: Waiting for Pod statefulset-6369/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 09/04/23 15:03:07.483
    Sep  4 15:03:07.530: INFO: Found 2 stateful pods, waiting for 3
    Sep  4 15:03:17.545: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 15:03:17.545: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 15:03:17.545: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 09/04/23 15:03:17.569
    Sep  4 15:03:17.608: INFO: Updating stateful set ss2
    Sep  4 15:03:17.633: INFO: Waiting for Pod statefulset-6369/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Sep  4 15:03:27.697: INFO: Updating stateful set ss2
    Sep  4 15:03:27.721: INFO: Waiting for StatefulSet statefulset-6369/ss2 to complete update
    Sep  4 15:03:27.721: INFO: Waiting for Pod statefulset-6369/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 15:03:37.748: INFO: Deleting all statefulset in ns statefulset-6369
    Sep  4 15:03:37.761: INFO: Scaling statefulset ss2 to 0
    Sep  4 15:03:47.815: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 15:03:47.828: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:03:47.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6369" for this suite. 09/04/23 15:03:47.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:03:47.908
Sep  4 15:03:47.908: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:03:47.908
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:03:47.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:03:47.971
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-6686fc38-d76c-412a-abcc-ec73a06b49fc 09/04/23 15:03:47.994
STEP: Creating a pod to test consume configMaps 09/04/23 15:03:48.012
Sep  4 15:03:48.035: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e" in namespace "projected-9472" to be "Succeeded or Failed"
Sep  4 15:03:48.047: INFO: Pod "pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.990261ms
Sep  4 15:03:50.061: INFO: Pod "pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026479145s
Sep  4 15:03:52.060: INFO: Pod "pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025802573s
STEP: Saw pod success 09/04/23 15:03:52.06
Sep  4 15:03:52.061: INFO: Pod "pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e" satisfied condition "Succeeded or Failed"
Sep  4 15:03:52.073: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e container agnhost-container: <nil>
STEP: delete the pod 09/04/23 15:03:52.152
Sep  4 15:03:52.172: INFO: Waiting for pod pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e to disappear
Sep  4 15:03:52.186: INFO: Pod pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:03:52.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9472" for this suite. 09/04/23 15:03:52.21
------------------------------
• [4.321 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:03:47.908
    Sep  4 15:03:47.908: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:03:47.908
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:03:47.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:03:47.971
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-6686fc38-d76c-412a-abcc-ec73a06b49fc 09/04/23 15:03:47.994
    STEP: Creating a pod to test consume configMaps 09/04/23 15:03:48.012
    Sep  4 15:03:48.035: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e" in namespace "projected-9472" to be "Succeeded or Failed"
    Sep  4 15:03:48.047: INFO: Pod "pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.990261ms
    Sep  4 15:03:50.061: INFO: Pod "pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026479145s
    Sep  4 15:03:52.060: INFO: Pod "pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025802573s
    STEP: Saw pod success 09/04/23 15:03:52.06
    Sep  4 15:03:52.061: INFO: Pod "pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e" satisfied condition "Succeeded or Failed"
    Sep  4 15:03:52.073: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 15:03:52.152
    Sep  4 15:03:52.172: INFO: Waiting for pod pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e to disappear
    Sep  4 15:03:52.186: INFO: Pod pod-projected-configmaps-7eb93510-7291-4e54-9f22-bbe959059b3e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:03:52.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9472" for this suite. 09/04/23 15:03:52.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:03:52.229
Sep  4 15:03:52.229: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 09/04/23 15:03:52.23
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:03:52.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:03:52.293
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 09/04/23 15:03:52.33
STEP: Waiting for all pods to be running 09/04/23 15:03:52.402
Sep  4 15:03:52.415: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  4 15:03:54.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2923" for this suite. 09/04/23 15:03:54.464
------------------------------
• [2.249 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:03:52.229
    Sep  4 15:03:52.229: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 09/04/23 15:03:52.23
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:03:52.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:03:52.293
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 09/04/23 15:03:52.33
    STEP: Waiting for all pods to be running 09/04/23 15:03:52.402
    Sep  4 15:03:52.415: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:03:54.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2923" for this suite. 09/04/23 15:03:54.464
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:03:54.478
Sep  4 15:03:54.478: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename limitrange 09/04/23 15:03:54.479
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:03:54.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:03:54.541
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 09/04/23 15:03:54.564
STEP: Setting up watch 09/04/23 15:03:54.564
STEP: Submitting a LimitRange 09/04/23 15:03:54.677
STEP: Verifying LimitRange creation was observed 09/04/23 15:03:54.692
STEP: Fetching the LimitRange to ensure it has proper values 09/04/23 15:03:54.692
Sep  4 15:03:54.705: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  4 15:03:54.705: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 09/04/23 15:03:54.705
STEP: Ensuring Pod has resource requirements applied from LimitRange 09/04/23 15:03:54.725
Sep  4 15:03:54.738: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  4 15:03:54.738: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 09/04/23 15:03:54.738
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/04/23 15:03:54.758
Sep  4 15:03:54.770: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep  4 15:03:54.770: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 09/04/23 15:03:54.77
STEP: Failing to create a Pod with more than max resources 09/04/23 15:03:54.788
STEP: Updating a LimitRange 09/04/23 15:03:54.805
STEP: Verifying LimitRange updating is effective 09/04/23 15:03:54.82
STEP: Creating a Pod with less than former min resources 09/04/23 15:03:56.834
STEP: Failing to create a Pod with more than max resources 09/04/23 15:03:56.854
STEP: Deleting a LimitRange 09/04/23 15:03:56.87
STEP: Verifying the LimitRange was deleted 09/04/23 15:03:56.886
Sep  4 15:04:01.900: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 09/04/23 15:04:01.9
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Sep  4 15:04:01.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-2038" for this suite. 09/04/23 15:04:01.946
------------------------------
• [7.482 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:03:54.478
    Sep  4 15:03:54.478: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename limitrange 09/04/23 15:03:54.479
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:03:54.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:03:54.541
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 09/04/23 15:03:54.564
    STEP: Setting up watch 09/04/23 15:03:54.564
    STEP: Submitting a LimitRange 09/04/23 15:03:54.677
    STEP: Verifying LimitRange creation was observed 09/04/23 15:03:54.692
    STEP: Fetching the LimitRange to ensure it has proper values 09/04/23 15:03:54.692
    Sep  4 15:03:54.705: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep  4 15:03:54.705: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 09/04/23 15:03:54.705
    STEP: Ensuring Pod has resource requirements applied from LimitRange 09/04/23 15:03:54.725
    Sep  4 15:03:54.738: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep  4 15:03:54.738: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 09/04/23 15:03:54.738
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/04/23 15:03:54.758
    Sep  4 15:03:54.770: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Sep  4 15:03:54.770: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 09/04/23 15:03:54.77
    STEP: Failing to create a Pod with more than max resources 09/04/23 15:03:54.788
    STEP: Updating a LimitRange 09/04/23 15:03:54.805
    STEP: Verifying LimitRange updating is effective 09/04/23 15:03:54.82
    STEP: Creating a Pod with less than former min resources 09/04/23 15:03:56.834
    STEP: Failing to create a Pod with more than max resources 09/04/23 15:03:56.854
    STEP: Deleting a LimitRange 09/04/23 15:03:56.87
    STEP: Verifying the LimitRange was deleted 09/04/23 15:03:56.886
    Sep  4 15:04:01.900: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 09/04/23 15:04:01.9
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:04:01.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-2038" for this suite. 09/04/23 15:04:01.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:04:01.961
Sep  4 15:04:01.961: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context 09/04/23 15:04:01.962
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:04:02.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:04:02.024
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/04/23 15:04:02.048
Sep  4 15:04:02.072: INFO: Waiting up to 5m0s for pod "security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7" in namespace "security-context-6586" to be "Succeeded or Failed"
Sep  4 15:04:02.086: INFO: Pod "security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.397706ms
Sep  4 15:04:04.098: INFO: Pod "security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026141933s
Sep  4 15:04:06.101: INFO: Pod "security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028764938s
STEP: Saw pod success 09/04/23 15:04:06.101
Sep  4 15:04:06.101: INFO: Pod "security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7" satisfied condition "Succeeded or Failed"
Sep  4 15:04:06.113: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7 container test-container: <nil>
STEP: delete the pod 09/04/23 15:04:06.192
Sep  4 15:04:06.211: INFO: Waiting for pod security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7 to disappear
Sep  4 15:04:06.224: INFO: Pod security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  4 15:04:06.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-6586" for this suite. 09/04/23 15:04:06.247
------------------------------
• [4.301 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:04:01.961
    Sep  4 15:04:01.961: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context 09/04/23 15:04:01.962
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:04:02.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:04:02.024
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/04/23 15:04:02.048
    Sep  4 15:04:02.072: INFO: Waiting up to 5m0s for pod "security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7" in namespace "security-context-6586" to be "Succeeded or Failed"
    Sep  4 15:04:02.086: INFO: Pod "security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.397706ms
    Sep  4 15:04:04.098: INFO: Pod "security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026141933s
    Sep  4 15:04:06.101: INFO: Pod "security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028764938s
    STEP: Saw pod success 09/04/23 15:04:06.101
    Sep  4 15:04:06.101: INFO: Pod "security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7" satisfied condition "Succeeded or Failed"
    Sep  4 15:04:06.113: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7 container test-container: <nil>
    STEP: delete the pod 09/04/23 15:04:06.192
    Sep  4 15:04:06.211: INFO: Waiting for pod security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7 to disappear
    Sep  4 15:04:06.224: INFO: Pod security-context-22e86cc8-e6a8-40e5-b71c-d2ab22c790b7 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:04:06.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-6586" for this suite. 09/04/23 15:04:06.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:04:06.262
Sep  4 15:04:06.262: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 09/04/23 15:04:06.263
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:04:06.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:04:06.328
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 09/04/23 15:04:06.352
STEP: patching the Namespace 09/04/23 15:04:06.393
STEP: get the Namespace and ensuring it has the label 09/04/23 15:04:06.408
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:04:06.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2012" for this suite. 09/04/23 15:04:06.436
STEP: Destroying namespace "nspatchtest-54d61442-0225-4adb-aba8-ae7b1535d55d-6650" for this suite. 09/04/23 15:04:06.451
------------------------------
• [0.204 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:04:06.262
    Sep  4 15:04:06.262: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 09/04/23 15:04:06.263
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:04:06.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:04:06.328
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 09/04/23 15:04:06.352
    STEP: patching the Namespace 09/04/23 15:04:06.393
    STEP: get the Namespace and ensuring it has the label 09/04/23 15:04:06.408
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:04:06.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2012" for this suite. 09/04/23 15:04:06.436
    STEP: Destroying namespace "nspatchtest-54d61442-0225-4adb-aba8-ae7b1535d55d-6650" for this suite. 09/04/23 15:04:06.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:04:06.467
Sep  4 15:04:06.467: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 15:04:06.468
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:04:06.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:04:06.531
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 09/04/23 15:04:06.553
Sep  4 15:04:06.554: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 create -f -'
Sep  4 15:04:07.564: INFO: stderr: ""
Sep  4 15:04:07.564: INFO: stdout: "pod/pause created\n"
Sep  4 15:04:07.564: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  4 15:04:07.564: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4062" to be "running and ready"
Sep  4 15:04:07.577: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.676707ms
Sep  4 15:04:07.577: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh' to be 'Running' but was 'Pending'
Sep  4 15:04:09.591: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.027077419s
Sep  4 15:04:09.591: INFO: Pod "pause" satisfied condition "running and ready"
Sep  4 15:04:09.591: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 09/04/23 15:04:09.591
Sep  4 15:04:09.591: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 label pods pause testing-label=testing-label-value'
Sep  4 15:04:09.699: INFO: stderr: ""
Sep  4 15:04:09.699: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 09/04/23 15:04:09.699
Sep  4 15:04:09.699: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 get pod pause -L testing-label'
Sep  4 15:04:09.800: INFO: stderr: ""
Sep  4 15:04:09.800: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 09/04/23 15:04:09.8
Sep  4 15:04:09.800: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 label pods pause testing-label-'
Sep  4 15:04:09.919: INFO: stderr: ""
Sep  4 15:04:09.919: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 09/04/23 15:04:09.919
Sep  4 15:04:09.919: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 get pod pause -L testing-label'
Sep  4 15:04:10.023: INFO: stderr: ""
Sep  4 15:04:10.023: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 09/04/23 15:04:10.024
Sep  4 15:04:10.024: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 delete --grace-period=0 --force -f -'
Sep  4 15:04:10.138: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 15:04:10.138: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  4 15:04:10.138: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 get rc,svc -l name=pause --no-headers'
Sep  4 15:04:10.245: INFO: stderr: "No resources found in kubectl-4062 namespace.\n"
Sep  4 15:04:10.245: INFO: stdout: ""
Sep  4 15:04:10.245: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  4 15:04:10.344: INFO: stderr: ""
Sep  4 15:04:10.344: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 15:04:10.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4062" for this suite. 09/04/23 15:04:10.367
------------------------------
• [3.914 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:04:06.467
    Sep  4 15:04:06.467: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 15:04:06.468
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:04:06.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:04:06.531
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 09/04/23 15:04:06.553
    Sep  4 15:04:06.554: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 create -f -'
    Sep  4 15:04:07.564: INFO: stderr: ""
    Sep  4 15:04:07.564: INFO: stdout: "pod/pause created\n"
    Sep  4 15:04:07.564: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Sep  4 15:04:07.564: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4062" to be "running and ready"
    Sep  4 15:04:07.577: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.676707ms
    Sep  4 15:04:07.577: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh' to be 'Running' but was 'Pending'
    Sep  4 15:04:09.591: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.027077419s
    Sep  4 15:04:09.591: INFO: Pod "pause" satisfied condition "running and ready"
    Sep  4 15:04:09.591: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 09/04/23 15:04:09.591
    Sep  4 15:04:09.591: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 label pods pause testing-label=testing-label-value'
    Sep  4 15:04:09.699: INFO: stderr: ""
    Sep  4 15:04:09.699: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 09/04/23 15:04:09.699
    Sep  4 15:04:09.699: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 get pod pause -L testing-label'
    Sep  4 15:04:09.800: INFO: stderr: ""
    Sep  4 15:04:09.800: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 09/04/23 15:04:09.8
    Sep  4 15:04:09.800: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 label pods pause testing-label-'
    Sep  4 15:04:09.919: INFO: stderr: ""
    Sep  4 15:04:09.919: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 09/04/23 15:04:09.919
    Sep  4 15:04:09.919: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 get pod pause -L testing-label'
    Sep  4 15:04:10.023: INFO: stderr: ""
    Sep  4 15:04:10.023: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 09/04/23 15:04:10.024
    Sep  4 15:04:10.024: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 delete --grace-period=0 --force -f -'
    Sep  4 15:04:10.138: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 15:04:10.138: INFO: stdout: "pod \"pause\" force deleted\n"
    Sep  4 15:04:10.138: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 get rc,svc -l name=pause --no-headers'
    Sep  4 15:04:10.245: INFO: stderr: "No resources found in kubectl-4062 namespace.\n"
    Sep  4 15:04:10.245: INFO: stdout: ""
    Sep  4 15:04:10.245: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4062 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  4 15:04:10.344: INFO: stderr: ""
    Sep  4 15:04:10.344: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:04:10.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4062" for this suite. 09/04/23 15:04:10.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:04:10.382
Sep  4 15:04:10.382: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 15:04:10.383
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:04:10.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:04:10.445
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 09/04/23 15:04:10.468
Sep  4 15:04:10.489: INFO: Waiting up to 5m0s for pod "pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa" in namespace "emptydir-4411" to be "Succeeded or Failed"
Sep  4 15:04:10.502: INFO: Pod "pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa": Phase="Pending", Reason="", readiness=false. Elapsed: 12.429159ms
Sep  4 15:04:12.515: INFO: Pod "pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026001447s
Sep  4 15:04:14.517: INFO: Pod "pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02757289s
STEP: Saw pod success 09/04/23 15:04:14.517
Sep  4 15:04:14.517: INFO: Pod "pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa" satisfied condition "Succeeded or Failed"
Sep  4 15:04:14.530: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa container test-container: <nil>
STEP: delete the pod 09/04/23 15:04:14.561
Sep  4 15:04:14.579: INFO: Waiting for pod pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa to disappear
Sep  4 15:04:14.591: INFO: Pod pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 15:04:14.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4411" for this suite. 09/04/23 15:04:14.614
------------------------------
• [4.246 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:04:10.382
    Sep  4 15:04:10.382: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 15:04:10.383
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:04:10.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:04:10.445
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/04/23 15:04:10.468
    Sep  4 15:04:10.489: INFO: Waiting up to 5m0s for pod "pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa" in namespace "emptydir-4411" to be "Succeeded or Failed"
    Sep  4 15:04:10.502: INFO: Pod "pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa": Phase="Pending", Reason="", readiness=false. Elapsed: 12.429159ms
    Sep  4 15:04:12.515: INFO: Pod "pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026001447s
    Sep  4 15:04:14.517: INFO: Pod "pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02757289s
    STEP: Saw pod success 09/04/23 15:04:14.517
    Sep  4 15:04:14.517: INFO: Pod "pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa" satisfied condition "Succeeded or Failed"
    Sep  4 15:04:14.530: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa container test-container: <nil>
    STEP: delete the pod 09/04/23 15:04:14.561
    Sep  4 15:04:14.579: INFO: Waiting for pod pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa to disappear
    Sep  4 15:04:14.591: INFO: Pod pod-8bc3d0cd-ef89-43d3-b034-d85fc2fa18aa no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:04:14.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4411" for this suite. 09/04/23 15:04:14.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:04:14.628
Sep  4 15:04:14.628: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 09/04/23 15:04:14.629
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:04:14.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:04:14.69
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 15:05:14.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6631" for this suite. 09/04/23 15:05:14.769
------------------------------
• [60.154 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:04:14.628
    Sep  4 15:04:14.628: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 09/04/23 15:04:14.629
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:04:14.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:04:14.69
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:05:14.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6631" for this suite. 09/04/23 15:05:14.769
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:05:14.782
Sep  4 15:05:14.782: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 15:05:14.783
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:05:14.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:05:14.841
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 09/04/23 15:05:14.863
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/04/23 15:05:14.874
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/04/23 15:05:14.874
STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/04/23 15:05:14.874
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/04/23 15:05:14.885
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/04/23 15:05:14.885
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/04/23 15:05:14.896
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:05:14.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4011" for this suite. 09/04/23 15:05:14.909
------------------------------
• [0.140 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:05:14.782
    Sep  4 15:05:14.782: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 15:05:14.783
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:05:14.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:05:14.841
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 09/04/23 15:05:14.863
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/04/23 15:05:14.874
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/04/23 15:05:14.874
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/04/23 15:05:14.874
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/04/23 15:05:14.885
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/04/23 15:05:14.885
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/04/23 15:05:14.896
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:05:14.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4011" for this suite. 09/04/23 15:05:14.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:05:14.922
Sep  4 15:05:14.922: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 09/04/23 15:05:14.923
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:05:14.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:05:14.98
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Sep  4 15:05:15.031: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-831 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  4 15:05:15.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-831" for this suite. 09/04/23 15:05:15.068
------------------------------
• [0.159 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:05:14.922
    Sep  4 15:05:14.922: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 09/04/23 15:05:14.923
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:05:14.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:05:14.98
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Sep  4 15:05:15.031: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-831 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:05:15.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-831" for this suite. 09/04/23 15:05:15.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:05:15.081
Sep  4 15:05:15.082: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 09/04/23 15:05:15.082
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:05:15.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:05:15.141
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/04/23 15:05:15.164
Sep  4 15:05:15.184: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5743  1d03649d-eecb-4b32-b1d6-00efb4b08010 15425 0 2023-09-04 15:05:15 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-09-04 15:05:15 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tkx4l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tkx4l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:05:15.184: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5743" to be "running and ready"
Sep  4 15:05:15.196: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 11.892706ms
Sep  4 15:05:15.196: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:05:17.209: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.024852025s
Sep  4 15:05:17.209: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Sep  4 15:05:17.209: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 09/04/23 15:05:17.209
Sep  4 15:05:17.209: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5743 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:05:17.209: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:05:17.209: INFO: ExecWithOptions: Clientset creation
Sep  4 15:05:17.209: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/dns-5743/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 09/04/23 15:05:17.687
Sep  4 15:05:17.687: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5743 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:05:17.687: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:05:17.688: INFO: ExecWithOptions: Clientset creation
Sep  4 15:05:17.688: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/dns-5743/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  4 15:05:18.124: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 15:05:18.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5743" for this suite. 09/04/23 15:05:18.163
------------------------------
• [3.095 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:05:15.081
    Sep  4 15:05:15.082: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 09/04/23 15:05:15.082
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:05:15.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:05:15.141
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/04/23 15:05:15.164
    Sep  4 15:05:15.184: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5743  1d03649d-eecb-4b32-b1d6-00efb4b08010 15425 0 2023-09-04 15:05:15 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-09-04 15:05:15 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tkx4l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tkx4l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:05:15.184: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5743" to be "running and ready"
    Sep  4 15:05:15.196: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 11.892706ms
    Sep  4 15:05:15.196: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:05:17.209: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.024852025s
    Sep  4 15:05:17.209: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Sep  4 15:05:17.209: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 09/04/23 15:05:17.209
    Sep  4 15:05:17.209: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5743 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:05:17.209: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:05:17.209: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:05:17.209: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/dns-5743/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 09/04/23 15:05:17.687
    Sep  4 15:05:17.687: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5743 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:05:17.687: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:05:17.688: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:05:17.688: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/dns-5743/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  4 15:05:18.124: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:05:18.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5743" for this suite. 09/04/23 15:05:18.163
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:05:18.177
Sep  4 15:05:18.177: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 09/04/23 15:05:18.177
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:05:18.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:05:18.236
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-704 09/04/23 15:05:18.258
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-704 09/04/23 15:05:18.27
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-704 09/04/23 15:05:18.283
Sep  4 15:05:18.295: INFO: Found 0 stateful pods, waiting for 1
Sep  4 15:05:28.309: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/04/23 15:05:28.309
Sep  4 15:05:28.321: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-704 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 15:05:28.849: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 15:05:28.849: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 15:05:28.849: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 15:05:28.862: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  4 15:05:38.874: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 15:05:38.874: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 15:05:38.932: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999789s
Sep  4 15:05:39.946: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.979313393s
Sep  4 15:05:40.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.965218675s
Sep  4 15:05:41.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.952378783s
Sep  4 15:05:42.986: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.939075598s
Sep  4 15:05:43.999: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.926398116s
Sep  4 15:05:45.012: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.912626265s
Sep  4 15:05:46.025: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.899896009s
Sep  4 15:05:47.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.886533805s
Sep  4 15:05:48.052: INFO: Verifying statefulset ss doesn't scale past 3 for another 873.362506ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-704 09/04/23 15:05:49.052
Sep  4 15:05:49.065: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-704 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 15:05:49.723: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  4 15:05:49.723: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 15:05:49.723: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  4 15:05:49.723: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-704 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 15:05:50.311: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  4 15:05:50.311: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 15:05:50.311: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  4 15:05:50.311: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-704 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 15:05:50.869: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  4 15:05:50.869: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 15:05:50.869: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  4 15:05:50.882: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 15:05:50.882: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 15:05:50.882: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 09/04/23 15:05:50.882
Sep  4 15:05:50.894: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-704 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 15:05:51.374: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 15:05:51.374: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 15:05:51.374: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 15:05:51.374: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-704 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 15:05:51.791: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 15:05:51.791: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 15:05:51.791: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 15:05:51.791: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-704 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 15:05:52.448: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 15:05:52.448: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 15:05:52.448: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 15:05:52.448: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 15:05:52.460: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Sep  4 15:06:02.485: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 15:06:02.485: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 15:06:02.485: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  4 15:06:02.524: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Sep  4 15:06:02.524: INFO: ss-0  shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:18 +0000 UTC  }]
Sep  4 15:06:02.524: INFO: ss-1  shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  }]
Sep  4 15:06:02.524: INFO: ss-2  shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  }]
Sep  4 15:06:02.524: INFO: 
Sep  4 15:06:02.524: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  4 15:06:03.536: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Sep  4 15:06:03.536: INFO: ss-0  shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:18 +0000 UTC  }]
Sep  4 15:06:03.536: INFO: ss-1  shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  }]
Sep  4 15:06:03.536: INFO: ss-2  shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  }]
Sep  4 15:06:03.536: INFO: 
Sep  4 15:06:03.536: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  4 15:06:04.548: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.974465258s
Sep  4 15:06:05.561: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.962556267s
Sep  4 15:06:06.574: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.949218558s
Sep  4 15:06:07.588: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.936203737s
Sep  4 15:06:08.601: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.922933361s
Sep  4 15:06:09.614: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.910249443s
Sep  4 15:06:10.627: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.896145512s
Sep  4 15:06:11.641: INFO: Verifying statefulset ss doesn't scale past 0 for another 882.91505ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-704 09/04/23 15:06:12.641
Sep  4 15:06:12.654: INFO: Scaling statefulset ss to 0
Sep  4 15:06:12.691: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 15:06:12.703: INFO: Deleting all statefulset in ns statefulset-704
Sep  4 15:06:12.715: INFO: Scaling statefulset ss to 0
Sep  4 15:06:12.752: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 15:06:12.763: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 15:06:12.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-704" for this suite. 09/04/23 15:06:12.822
------------------------------
• [54.658 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:05:18.177
    Sep  4 15:05:18.177: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 09/04/23 15:05:18.177
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:05:18.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:05:18.236
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-704 09/04/23 15:05:18.258
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-704 09/04/23 15:05:18.27
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-704 09/04/23 15:05:18.283
    Sep  4 15:05:18.295: INFO: Found 0 stateful pods, waiting for 1
    Sep  4 15:05:28.309: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/04/23 15:05:28.309
    Sep  4 15:05:28.321: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-704 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 15:05:28.849: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 15:05:28.849: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 15:05:28.849: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 15:05:28.862: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep  4 15:05:38.874: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 15:05:38.874: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 15:05:38.932: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999789s
    Sep  4 15:05:39.946: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.979313393s
    Sep  4 15:05:40.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.965218675s
    Sep  4 15:05:41.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.952378783s
    Sep  4 15:05:42.986: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.939075598s
    Sep  4 15:05:43.999: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.926398116s
    Sep  4 15:05:45.012: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.912626265s
    Sep  4 15:05:46.025: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.899896009s
    Sep  4 15:05:47.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.886533805s
    Sep  4 15:05:48.052: INFO: Verifying statefulset ss doesn't scale past 3 for another 873.362506ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-704 09/04/23 15:05:49.052
    Sep  4 15:05:49.065: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-704 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 15:05:49.723: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  4 15:05:49.723: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 15:05:49.723: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  4 15:05:49.723: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-704 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 15:05:50.311: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep  4 15:05:50.311: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 15:05:50.311: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  4 15:05:50.311: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-704 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 15:05:50.869: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep  4 15:05:50.869: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 15:05:50.869: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  4 15:05:50.882: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 15:05:50.882: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 15:05:50.882: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 09/04/23 15:05:50.882
    Sep  4 15:05:50.894: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-704 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 15:05:51.374: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 15:05:51.374: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 15:05:51.374: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 15:05:51.374: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-704 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 15:05:51.791: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 15:05:51.791: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 15:05:51.791: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 15:05:51.791: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-704 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 15:05:52.448: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 15:05:52.448: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 15:05:52.448: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 15:05:52.448: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 15:05:52.460: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Sep  4 15:06:02.485: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 15:06:02.485: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 15:06:02.485: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep  4 15:06:02.524: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
    Sep  4 15:06:02.524: INFO: ss-0  shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:18 +0000 UTC  }]
    Sep  4 15:06:02.524: INFO: ss-1  shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  }]
    Sep  4 15:06:02.524: INFO: ss-2  shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  }]
    Sep  4 15:06:02.524: INFO: 
    Sep  4 15:06:02.524: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep  4 15:06:03.536: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
    Sep  4 15:06:03.536: INFO: ss-0  shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:18 +0000 UTC  }]
    Sep  4 15:06:03.536: INFO: ss-1  shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  }]
    Sep  4 15:06:03.536: INFO: ss-2  shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:05:38 +0000 UTC  }]
    Sep  4 15:06:03.536: INFO: 
    Sep  4 15:06:03.536: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep  4 15:06:04.548: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.974465258s
    Sep  4 15:06:05.561: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.962556267s
    Sep  4 15:06:06.574: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.949218558s
    Sep  4 15:06:07.588: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.936203737s
    Sep  4 15:06:08.601: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.922933361s
    Sep  4 15:06:09.614: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.910249443s
    Sep  4 15:06:10.627: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.896145512s
    Sep  4 15:06:11.641: INFO: Verifying statefulset ss doesn't scale past 0 for another 882.91505ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-704 09/04/23 15:06:12.641
    Sep  4 15:06:12.654: INFO: Scaling statefulset ss to 0
    Sep  4 15:06:12.691: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 15:06:12.703: INFO: Deleting all statefulset in ns statefulset-704
    Sep  4 15:06:12.715: INFO: Scaling statefulset ss to 0
    Sep  4 15:06:12.752: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 15:06:12.763: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:06:12.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-704" for this suite. 09/04/23 15:06:12.822
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:06:12.835
Sep  4 15:06:12.835: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 09/04/23 15:06:12.836
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:06:12.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:06:12.894
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 09/04/23 15:06:12.917
STEP: Verify that the required pods have come up 09/04/23 15:06:12.93
Sep  4 15:06:12.944: INFO: Pod name sample-pod: Found 1 pods out of 3
Sep  4 15:06:17.957: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 09/04/23 15:06:17.957
Sep  4 15:06:17.969: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 09/04/23 15:06:17.969
STEP: DeleteCollection of the ReplicaSets 09/04/23 15:06:17.981
STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/04/23 15:06:17.995
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  4 15:06:18.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-551" for this suite. 09/04/23 15:06:18.028
------------------------------
• [5.206 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:06:12.835
    Sep  4 15:06:12.835: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 09/04/23 15:06:12.836
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:06:12.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:06:12.894
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 09/04/23 15:06:12.917
    STEP: Verify that the required pods have come up 09/04/23 15:06:12.93
    Sep  4 15:06:12.944: INFO: Pod name sample-pod: Found 1 pods out of 3
    Sep  4 15:06:17.957: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 09/04/23 15:06:17.957
    Sep  4 15:06:17.969: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 09/04/23 15:06:17.969
    STEP: DeleteCollection of the ReplicaSets 09/04/23 15:06:17.981
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/04/23 15:06:17.995
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:06:18.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-551" for this suite. 09/04/23 15:06:18.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:06:18.042
Sep  4 15:06:18.042: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 09/04/23 15:06:18.043
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:06:18.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:06:18.101
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
Sep  4 15:06:18.186: INFO: Create a RollingUpdate DaemonSet
Sep  4 15:06:18.204: INFO: Check that daemon pods launch on every node of the cluster
Sep  4 15:06:18.229: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:06:18.229: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:06:19.264: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:06:19.264: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:06:20.264: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 15:06:20.264: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Sep  4 15:06:20.264: INFO: Update the DaemonSet to trigger a rollout
Sep  4 15:06:20.289: INFO: Updating DaemonSet daemon-set
Sep  4 15:06:23.354: INFO: Roll back the DaemonSet before rollout is complete
Sep  4 15:06:23.379: INFO: Updating DaemonSet daemon-set
Sep  4 15:06:23.379: INFO: Make sure DaemonSet rollback is complete
Sep  4 15:06:26.417: INFO: Pod daemon-set-mmtnp is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/04/23 15:06:26.463
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3501, will wait for the garbage collector to delete the pods 09/04/23 15:06:26.463
Sep  4 15:06:26.538: INFO: Deleting DaemonSet.extensions daemon-set took: 13.124959ms
Sep  4 15:06:26.639: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.678267ms
Sep  4 15:06:28.752: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:06:28.752: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  4 15:06:28.764: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16051"},"items":null}

Sep  4 15:06:28.776: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16051"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:06:28.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3501" for this suite. 09/04/23 15:06:28.835
------------------------------
• [10.806 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:06:18.042
    Sep  4 15:06:18.042: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 09/04/23 15:06:18.043
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:06:18.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:06:18.101
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:443
    Sep  4 15:06:18.186: INFO: Create a RollingUpdate DaemonSet
    Sep  4 15:06:18.204: INFO: Check that daemon pods launch on every node of the cluster
    Sep  4 15:06:18.229: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:06:18.229: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:06:19.264: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:06:19.264: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:06:20.264: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 15:06:20.264: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Sep  4 15:06:20.264: INFO: Update the DaemonSet to trigger a rollout
    Sep  4 15:06:20.289: INFO: Updating DaemonSet daemon-set
    Sep  4 15:06:23.354: INFO: Roll back the DaemonSet before rollout is complete
    Sep  4 15:06:23.379: INFO: Updating DaemonSet daemon-set
    Sep  4 15:06:23.379: INFO: Make sure DaemonSet rollback is complete
    Sep  4 15:06:26.417: INFO: Pod daemon-set-mmtnp is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/04/23 15:06:26.463
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3501, will wait for the garbage collector to delete the pods 09/04/23 15:06:26.463
    Sep  4 15:06:26.538: INFO: Deleting DaemonSet.extensions daemon-set took: 13.124959ms
    Sep  4 15:06:26.639: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.678267ms
    Sep  4 15:06:28.752: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:06:28.752: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  4 15:06:28.764: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16051"},"items":null}

    Sep  4 15:06:28.776: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16051"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:06:28.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3501" for this suite. 09/04/23 15:06:28.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:06:28.848
Sep  4 15:06:28.848: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 15:06:28.849
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:06:28.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:06:28.907
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 09/04/23 15:06:28.929
Sep  4 15:06:28.948: INFO: Waiting up to 5m0s for pod "pod-50ebfba6-d178-4668-b47b-1131a5344831" in namespace "emptydir-3640" to be "Succeeded or Failed"
Sep  4 15:06:28.959: INFO: Pod "pod-50ebfba6-d178-4668-b47b-1131a5344831": Phase="Pending", Reason="", readiness=false. Elapsed: 11.699524ms
Sep  4 15:06:30.973: INFO: Pod "pod-50ebfba6-d178-4668-b47b-1131a5344831": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025345012s
Sep  4 15:06:32.974: INFO: Pod "pod-50ebfba6-d178-4668-b47b-1131a5344831": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026499036s
STEP: Saw pod success 09/04/23 15:06:32.974
Sep  4 15:06:32.974: INFO: Pod "pod-50ebfba6-d178-4668-b47b-1131a5344831" satisfied condition "Succeeded or Failed"
Sep  4 15:06:32.987: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-50ebfba6-d178-4668-b47b-1131a5344831 container test-container: <nil>
STEP: delete the pod 09/04/23 15:06:33.076
Sep  4 15:06:33.090: INFO: Waiting for pod pod-50ebfba6-d178-4668-b47b-1131a5344831 to disappear
Sep  4 15:06:33.102: INFO: Pod pod-50ebfba6-d178-4668-b47b-1131a5344831 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 15:06:33.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3640" for this suite. 09/04/23 15:06:33.124
------------------------------
• [4.289 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:06:28.848
    Sep  4 15:06:28.848: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 15:06:28.849
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:06:28.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:06:28.907
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/04/23 15:06:28.929
    Sep  4 15:06:28.948: INFO: Waiting up to 5m0s for pod "pod-50ebfba6-d178-4668-b47b-1131a5344831" in namespace "emptydir-3640" to be "Succeeded or Failed"
    Sep  4 15:06:28.959: INFO: Pod "pod-50ebfba6-d178-4668-b47b-1131a5344831": Phase="Pending", Reason="", readiness=false. Elapsed: 11.699524ms
    Sep  4 15:06:30.973: INFO: Pod "pod-50ebfba6-d178-4668-b47b-1131a5344831": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025345012s
    Sep  4 15:06:32.974: INFO: Pod "pod-50ebfba6-d178-4668-b47b-1131a5344831": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026499036s
    STEP: Saw pod success 09/04/23 15:06:32.974
    Sep  4 15:06:32.974: INFO: Pod "pod-50ebfba6-d178-4668-b47b-1131a5344831" satisfied condition "Succeeded or Failed"
    Sep  4 15:06:32.987: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-50ebfba6-d178-4668-b47b-1131a5344831 container test-container: <nil>
    STEP: delete the pod 09/04/23 15:06:33.076
    Sep  4 15:06:33.090: INFO: Waiting for pod pod-50ebfba6-d178-4668-b47b-1131a5344831 to disappear
    Sep  4 15:06:33.102: INFO: Pod pod-50ebfba6-d178-4668-b47b-1131a5344831 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:06:33.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3640" for this suite. 09/04/23 15:06:33.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:06:33.137
Sep  4 15:06:33.137: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 15:06:33.138
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:06:33.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:06:33.195
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:06:33.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2687" for this suite. 09/04/23 15:06:33.341
------------------------------
• [0.217 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:06:33.137
    Sep  4 15:06:33.137: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 15:06:33.138
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:06:33.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:06:33.195
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:06:33.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2687" for this suite. 09/04/23 15:06:33.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:06:33.355
Sep  4 15:06:33.355: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 09/04/23 15:06:33.356
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:06:33.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:06:33.413
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 09/04/23 15:06:33.436
STEP: Waiting for the pdb to be processed 09/04/23 15:06:33.448
STEP: updating the pdb 09/04/23 15:06:33.459
STEP: Waiting for the pdb to be processed 09/04/23 15:06:33.484
STEP: patching the pdb 09/04/23 15:06:33.495
STEP: Waiting for the pdb to be processed 09/04/23 15:06:33.521
STEP: Waiting for the pdb to be deleted 09/04/23 15:06:33.546
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  4 15:06:33.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7296" for this suite. 09/04/23 15:06:33.57
------------------------------
• [0.228 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:06:33.355
    Sep  4 15:06:33.355: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 09/04/23 15:06:33.356
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:06:33.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:06:33.413
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 09/04/23 15:06:33.436
    STEP: Waiting for the pdb to be processed 09/04/23 15:06:33.448
    STEP: updating the pdb 09/04/23 15:06:33.459
    STEP: Waiting for the pdb to be processed 09/04/23 15:06:33.484
    STEP: patching the pdb 09/04/23 15:06:33.495
    STEP: Waiting for the pdb to be processed 09/04/23 15:06:33.521
    STEP: Waiting for the pdb to be deleted 09/04/23 15:06:33.546
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:06:33.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7296" for this suite. 09/04/23 15:06:33.57
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:06:33.583
Sep  4 15:06:33.583: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 15:06:33.583
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:06:33.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:06:33.64
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4644 09/04/23 15:06:33.662
STEP: changing the ExternalName service to type=ClusterIP 09/04/23 15:06:33.675
STEP: creating replication controller externalname-service in namespace services-4644 09/04/23 15:06:33.704
I0904 15:06:33.716644    7754 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4644, replica count: 2
I0904 15:06:36.767187    7754 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 15:06:36.767: INFO: Creating new exec pod
Sep  4 15:06:36.783: INFO: Waiting up to 5m0s for pod "execpodrmc76" in namespace "services-4644" to be "running"
Sep  4 15:06:36.795: INFO: Pod "execpodrmc76": Phase="Pending", Reason="", readiness=false. Elapsed: 12.15191ms
Sep  4 15:06:38.808: INFO: Pod "execpodrmc76": Phase="Running", Reason="", readiness=true. Elapsed: 2.025495625s
Sep  4 15:06:38.808: INFO: Pod "execpodrmc76" satisfied condition "running"
Sep  4 15:06:39.809: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4644 exec execpodrmc76 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Sep  4 15:06:40.307: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  4 15:06:40.307: INFO: stdout: ""
Sep  4 15:06:40.307: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4644 exec execpodrmc76 -- /bin/sh -x -c nc -v -z -w 2 100.105.239.254 80'
Sep  4 15:06:40.862: INFO: stderr: "+ nc -v -z -w 2 100.105.239.254 80\nConnection to 100.105.239.254 80 port [tcp/http] succeeded!\n"
Sep  4 15:06:40.862: INFO: stdout: ""
Sep  4 15:06:40.862: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 15:06:40.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4644" for this suite. 09/04/23 15:06:40.903
------------------------------
• [7.333 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:06:33.583
    Sep  4 15:06:33.583: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 15:06:33.583
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:06:33.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:06:33.64
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4644 09/04/23 15:06:33.662
    STEP: changing the ExternalName service to type=ClusterIP 09/04/23 15:06:33.675
    STEP: creating replication controller externalname-service in namespace services-4644 09/04/23 15:06:33.704
    I0904 15:06:33.716644    7754 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4644, replica count: 2
    I0904 15:06:36.767187    7754 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 15:06:36.767: INFO: Creating new exec pod
    Sep  4 15:06:36.783: INFO: Waiting up to 5m0s for pod "execpodrmc76" in namespace "services-4644" to be "running"
    Sep  4 15:06:36.795: INFO: Pod "execpodrmc76": Phase="Pending", Reason="", readiness=false. Elapsed: 12.15191ms
    Sep  4 15:06:38.808: INFO: Pod "execpodrmc76": Phase="Running", Reason="", readiness=true. Elapsed: 2.025495625s
    Sep  4 15:06:38.808: INFO: Pod "execpodrmc76" satisfied condition "running"
    Sep  4 15:06:39.809: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4644 exec execpodrmc76 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Sep  4 15:06:40.307: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  4 15:06:40.307: INFO: stdout: ""
    Sep  4 15:06:40.307: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4644 exec execpodrmc76 -- /bin/sh -x -c nc -v -z -w 2 100.105.239.254 80'
    Sep  4 15:06:40.862: INFO: stderr: "+ nc -v -z -w 2 100.105.239.254 80\nConnection to 100.105.239.254 80 port [tcp/http] succeeded!\n"
    Sep  4 15:06:40.862: INFO: stdout: ""
    Sep  4 15:06:40.862: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:06:40.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4644" for this suite. 09/04/23 15:06:40.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:06:40.916
Sep  4 15:06:40.916: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper 09/04/23 15:06:40.917
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:06:40.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:06:40.974
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 09/04/23 15:06:40.997
STEP: Creating RC which spawns configmap-volume pods 09/04/23 15:06:41.647
Sep  4 15:06:41.675: INFO: Pod name wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291: Found 0 pods out of 5
Sep  4 15:06:46.718: INFO: Pod name wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/04/23 15:06:46.718
Sep  4 15:06:46.718: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-bdfsm" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:06:46.731: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-bdfsm": Phase="Running", Reason="", readiness=true. Elapsed: 12.45764ms
Sep  4 15:06:46.731: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-bdfsm" satisfied condition "running"
Sep  4 15:06:46.731: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-c5hc9" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:06:46.743: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-c5hc9": Phase="Running", Reason="", readiness=true. Elapsed: 12.333127ms
Sep  4 15:06:46.743: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-c5hc9" satisfied condition "running"
Sep  4 15:06:46.743: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-fx4ng" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:06:46.756: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-fx4ng": Phase="Running", Reason="", readiness=true. Elapsed: 12.310121ms
Sep  4 15:06:46.756: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-fx4ng" satisfied condition "running"
Sep  4 15:06:46.756: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-s8wbb" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:06:46.769: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-s8wbb": Phase="Running", Reason="", readiness=true. Elapsed: 12.867647ms
Sep  4 15:06:46.769: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-s8wbb" satisfied condition "running"
Sep  4 15:06:46.769: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-x6kcn" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:06:46.781: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-x6kcn": Phase="Running", Reason="", readiness=true. Elapsed: 12.21576ms
Sep  4 15:06:46.781: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-x6kcn" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291 in namespace emptydir-wrapper-605, will wait for the garbage collector to delete the pods 09/04/23 15:06:46.781
Sep  4 15:06:46.857: INFO: Deleting ReplicationController wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291 took: 13.016876ms
Sep  4 15:06:46.957: INFO: Terminating ReplicationController wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291 pods took: 100.429912ms
STEP: Creating RC which spawns configmap-volume pods 09/04/23 15:06:47.97
Sep  4 15:06:48.006: INFO: Pod name wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196: Found 0 pods out of 5
Sep  4 15:06:53.051: INFO: Pod name wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/04/23 15:06:53.051
Sep  4 15:06:53.051: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-6f7gz" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:06:53.064: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-6f7gz": Phase="Running", Reason="", readiness=true. Elapsed: 12.629911ms
Sep  4 15:06:53.064: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-6f7gz" satisfied condition "running"
Sep  4 15:06:53.064: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-78v2b" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:06:53.076: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-78v2b": Phase="Running", Reason="", readiness=true. Elapsed: 12.404477ms
Sep  4 15:06:53.076: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-78v2b" satisfied condition "running"
Sep  4 15:06:53.076: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-fjfvx" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:06:53.088: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-fjfvx": Phase="Running", Reason="", readiness=true. Elapsed: 12.138681ms
Sep  4 15:06:53.088: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-fjfvx" satisfied condition "running"
Sep  4 15:06:53.088: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-hnp89" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:06:53.100: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-hnp89": Phase="Running", Reason="", readiness=true. Elapsed: 12.058694ms
Sep  4 15:06:53.100: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-hnp89" satisfied condition "running"
Sep  4 15:06:53.100: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-jctkh" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:06:53.113: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-jctkh": Phase="Running", Reason="", readiness=true. Elapsed: 12.545868ms
Sep  4 15:06:53.113: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-jctkh" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196 in namespace emptydir-wrapper-605, will wait for the garbage collector to delete the pods 09/04/23 15:06:53.113
Sep  4 15:06:53.191: INFO: Deleting ReplicationController wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196 took: 13.786053ms
Sep  4 15:06:53.292: INFO: Terminating ReplicationController wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196 pods took: 101.065249ms
STEP: Creating RC which spawns configmap-volume pods 09/04/23 15:06:55.106
Sep  4 15:06:55.142: INFO: Pod name wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022: Found 0 pods out of 5
Sep  4 15:07:00.190: INFO: Pod name wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/04/23 15:07:00.19
Sep  4 15:07:00.190: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-74gh2" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:07:00.202: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-74gh2": Phase="Running", Reason="", readiness=true. Elapsed: 11.955538ms
Sep  4 15:07:00.202: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-74gh2" satisfied condition "running"
Sep  4 15:07:00.202: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-q4lpl" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:07:00.214: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-q4lpl": Phase="Running", Reason="", readiness=true. Elapsed: 12.241221ms
Sep  4 15:07:00.214: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-q4lpl" satisfied condition "running"
Sep  4 15:07:00.214: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-qfqg7" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:07:00.226: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-qfqg7": Phase="Running", Reason="", readiness=true. Elapsed: 12.020895ms
Sep  4 15:07:00.226: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-qfqg7" satisfied condition "running"
Sep  4 15:07:00.226: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-rct7z" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:07:00.239: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-rct7z": Phase="Running", Reason="", readiness=true. Elapsed: 12.62803ms
Sep  4 15:07:00.239: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-rct7z" satisfied condition "running"
Sep  4 15:07:00.239: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-xgshh" in namespace "emptydir-wrapper-605" to be "running"
Sep  4 15:07:00.251: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-xgshh": Phase="Running", Reason="", readiness=true. Elapsed: 12.439917ms
Sep  4 15:07:00.251: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-xgshh" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022 in namespace emptydir-wrapper-605, will wait for the garbage collector to delete the pods 09/04/23 15:07:00.251
Sep  4 15:07:00.329: INFO: Deleting ReplicationController wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022 took: 14.505769ms
Sep  4 15:07:00.430: INFO: Terminating ReplicationController wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022 pods took: 100.842958ms
STEP: Cleaning up the configMaps 09/04/23 15:07:02.03
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:02.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-605" for this suite. 09/04/23 15:07:02.686
------------------------------
• [21.782 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:06:40.916
    Sep  4 15:06:40.916: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir-wrapper 09/04/23 15:06:40.917
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:06:40.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:06:40.974
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 09/04/23 15:06:40.997
    STEP: Creating RC which spawns configmap-volume pods 09/04/23 15:06:41.647
    Sep  4 15:06:41.675: INFO: Pod name wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291: Found 0 pods out of 5
    Sep  4 15:06:46.718: INFO: Pod name wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/04/23 15:06:46.718
    Sep  4 15:06:46.718: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-bdfsm" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:06:46.731: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-bdfsm": Phase="Running", Reason="", readiness=true. Elapsed: 12.45764ms
    Sep  4 15:06:46.731: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-bdfsm" satisfied condition "running"
    Sep  4 15:06:46.731: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-c5hc9" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:06:46.743: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-c5hc9": Phase="Running", Reason="", readiness=true. Elapsed: 12.333127ms
    Sep  4 15:06:46.743: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-c5hc9" satisfied condition "running"
    Sep  4 15:06:46.743: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-fx4ng" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:06:46.756: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-fx4ng": Phase="Running", Reason="", readiness=true. Elapsed: 12.310121ms
    Sep  4 15:06:46.756: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-fx4ng" satisfied condition "running"
    Sep  4 15:06:46.756: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-s8wbb" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:06:46.769: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-s8wbb": Phase="Running", Reason="", readiness=true. Elapsed: 12.867647ms
    Sep  4 15:06:46.769: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-s8wbb" satisfied condition "running"
    Sep  4 15:06:46.769: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-x6kcn" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:06:46.781: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-x6kcn": Phase="Running", Reason="", readiness=true. Elapsed: 12.21576ms
    Sep  4 15:06:46.781: INFO: Pod "wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291-x6kcn" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291 in namespace emptydir-wrapper-605, will wait for the garbage collector to delete the pods 09/04/23 15:06:46.781
    Sep  4 15:06:46.857: INFO: Deleting ReplicationController wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291 took: 13.016876ms
    Sep  4 15:06:46.957: INFO: Terminating ReplicationController wrapped-volume-race-94076115-369c-489b-9e5b-45ab273ff291 pods took: 100.429912ms
    STEP: Creating RC which spawns configmap-volume pods 09/04/23 15:06:47.97
    Sep  4 15:06:48.006: INFO: Pod name wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196: Found 0 pods out of 5
    Sep  4 15:06:53.051: INFO: Pod name wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/04/23 15:06:53.051
    Sep  4 15:06:53.051: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-6f7gz" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:06:53.064: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-6f7gz": Phase="Running", Reason="", readiness=true. Elapsed: 12.629911ms
    Sep  4 15:06:53.064: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-6f7gz" satisfied condition "running"
    Sep  4 15:06:53.064: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-78v2b" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:06:53.076: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-78v2b": Phase="Running", Reason="", readiness=true. Elapsed: 12.404477ms
    Sep  4 15:06:53.076: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-78v2b" satisfied condition "running"
    Sep  4 15:06:53.076: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-fjfvx" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:06:53.088: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-fjfvx": Phase="Running", Reason="", readiness=true. Elapsed: 12.138681ms
    Sep  4 15:06:53.088: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-fjfvx" satisfied condition "running"
    Sep  4 15:06:53.088: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-hnp89" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:06:53.100: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-hnp89": Phase="Running", Reason="", readiness=true. Elapsed: 12.058694ms
    Sep  4 15:06:53.100: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-hnp89" satisfied condition "running"
    Sep  4 15:06:53.100: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-jctkh" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:06:53.113: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-jctkh": Phase="Running", Reason="", readiness=true. Elapsed: 12.545868ms
    Sep  4 15:06:53.113: INFO: Pod "wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196-jctkh" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196 in namespace emptydir-wrapper-605, will wait for the garbage collector to delete the pods 09/04/23 15:06:53.113
    Sep  4 15:06:53.191: INFO: Deleting ReplicationController wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196 took: 13.786053ms
    Sep  4 15:06:53.292: INFO: Terminating ReplicationController wrapped-volume-race-1b88d3b1-7524-4818-8216-7a95a1b24196 pods took: 101.065249ms
    STEP: Creating RC which spawns configmap-volume pods 09/04/23 15:06:55.106
    Sep  4 15:06:55.142: INFO: Pod name wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022: Found 0 pods out of 5
    Sep  4 15:07:00.190: INFO: Pod name wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/04/23 15:07:00.19
    Sep  4 15:07:00.190: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-74gh2" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:07:00.202: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-74gh2": Phase="Running", Reason="", readiness=true. Elapsed: 11.955538ms
    Sep  4 15:07:00.202: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-74gh2" satisfied condition "running"
    Sep  4 15:07:00.202: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-q4lpl" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:07:00.214: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-q4lpl": Phase="Running", Reason="", readiness=true. Elapsed: 12.241221ms
    Sep  4 15:07:00.214: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-q4lpl" satisfied condition "running"
    Sep  4 15:07:00.214: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-qfqg7" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:07:00.226: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-qfqg7": Phase="Running", Reason="", readiness=true. Elapsed: 12.020895ms
    Sep  4 15:07:00.226: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-qfqg7" satisfied condition "running"
    Sep  4 15:07:00.226: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-rct7z" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:07:00.239: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-rct7z": Phase="Running", Reason="", readiness=true. Elapsed: 12.62803ms
    Sep  4 15:07:00.239: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-rct7z" satisfied condition "running"
    Sep  4 15:07:00.239: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-xgshh" in namespace "emptydir-wrapper-605" to be "running"
    Sep  4 15:07:00.251: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-xgshh": Phase="Running", Reason="", readiness=true. Elapsed: 12.439917ms
    Sep  4 15:07:00.251: INFO: Pod "wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022-xgshh" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022 in namespace emptydir-wrapper-605, will wait for the garbage collector to delete the pods 09/04/23 15:07:00.251
    Sep  4 15:07:00.329: INFO: Deleting ReplicationController wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022 took: 14.505769ms
    Sep  4 15:07:00.430: INFO: Terminating ReplicationController wrapped-volume-race-f2df2c1b-71a3-4464-b28a-8924e9bef022 pods took: 100.842958ms
    STEP: Cleaning up the configMaps 09/04/23 15:07:02.03
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:02.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-605" for this suite. 09/04/23 15:07:02.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:02.698
Sep  4 15:07:02.699: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 09/04/23 15:07:02.699
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:02.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:02.757
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 09/04/23 15:07:02.78
STEP: submitting the pod to kubernetes 09/04/23 15:07:02.78
Sep  4 15:07:02.798: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986" in namespace "pods-8570" to be "running and ready"
Sep  4 15:07:02.809: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986": Phase="Pending", Reason="", readiness=false. Elapsed: 11.273089ms
Sep  4 15:07:02.810: INFO: The phase of Pod pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:07:04.822: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986": Phase="Running", Reason="", readiness=true. Elapsed: 2.023711494s
Sep  4 15:07:04.822: INFO: The phase of Pod pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986 is Running (Ready = true)
Sep  4 15:07:04.822: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/04/23 15:07:04.833
STEP: updating the pod 09/04/23 15:07:04.845
Sep  4 15:07:05.372: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986"
Sep  4 15:07:05.372: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986" in namespace "pods-8570" to be "terminated with reason DeadlineExceeded"
Sep  4 15:07:05.383: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986": Phase="Running", Reason="", readiness=true. Elapsed: 11.212495ms
Sep  4 15:07:07.396: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986": Phase="Running", Reason="", readiness=true. Elapsed: 2.023856222s
Sep  4 15:07:09.397: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986": Phase="Running", Reason="", readiness=false. Elapsed: 4.024854917s
Sep  4 15:07:11.397: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.02477878s
Sep  4 15:07:11.397: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:11.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8570" for this suite. 09/04/23 15:07:11.42
------------------------------
• [8.734 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:02.698
    Sep  4 15:07:02.699: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 09/04/23 15:07:02.699
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:02.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:02.757
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 09/04/23 15:07:02.78
    STEP: submitting the pod to kubernetes 09/04/23 15:07:02.78
    Sep  4 15:07:02.798: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986" in namespace "pods-8570" to be "running and ready"
    Sep  4 15:07:02.809: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986": Phase="Pending", Reason="", readiness=false. Elapsed: 11.273089ms
    Sep  4 15:07:02.810: INFO: The phase of Pod pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:07:04.822: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986": Phase="Running", Reason="", readiness=true. Elapsed: 2.023711494s
    Sep  4 15:07:04.822: INFO: The phase of Pod pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986 is Running (Ready = true)
    Sep  4 15:07:04.822: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/04/23 15:07:04.833
    STEP: updating the pod 09/04/23 15:07:04.845
    Sep  4 15:07:05.372: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986"
    Sep  4 15:07:05.372: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986" in namespace "pods-8570" to be "terminated with reason DeadlineExceeded"
    Sep  4 15:07:05.383: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986": Phase="Running", Reason="", readiness=true. Elapsed: 11.212495ms
    Sep  4 15:07:07.396: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986": Phase="Running", Reason="", readiness=true. Elapsed: 2.023856222s
    Sep  4 15:07:09.397: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986": Phase="Running", Reason="", readiness=false. Elapsed: 4.024854917s
    Sep  4 15:07:11.397: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.02477878s
    Sep  4 15:07:11.397: INFO: Pod "pod-update-activedeadlineseconds-7511645b-5bc3-4443-999b-990b4ebbd986" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:11.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8570" for this suite. 09/04/23 15:07:11.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:11.433
Sep  4 15:07:11.433: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test 09/04/23 15:07:11.434
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:11.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:11.491
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Sep  4 15:07:11.532: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ec9e05b6-3291-4a5c-83fa-ff6682167bea" in namespace "security-context-test-3378" to be "Succeeded or Failed"
Sep  4 15:07:11.544: INFO: Pod "busybox-user-65534-ec9e05b6-3291-4a5c-83fa-ff6682167bea": Phase="Pending", Reason="", readiness=false. Elapsed: 11.539384ms
Sep  4 15:07:13.557: INFO: Pod "busybox-user-65534-ec9e05b6-3291-4a5c-83fa-ff6682167bea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024766216s
Sep  4 15:07:15.556: INFO: Pod "busybox-user-65534-ec9e05b6-3291-4a5c-83fa-ff6682167bea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023642424s
Sep  4 15:07:15.556: INFO: Pod "busybox-user-65534-ec9e05b6-3291-4a5c-83fa-ff6682167bea" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:15.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3378" for this suite. 09/04/23 15:07:15.577
------------------------------
• [4.156 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:11.433
    Sep  4 15:07:11.433: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context-test 09/04/23 15:07:11.434
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:11.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:11.491
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Sep  4 15:07:11.532: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ec9e05b6-3291-4a5c-83fa-ff6682167bea" in namespace "security-context-test-3378" to be "Succeeded or Failed"
    Sep  4 15:07:11.544: INFO: Pod "busybox-user-65534-ec9e05b6-3291-4a5c-83fa-ff6682167bea": Phase="Pending", Reason="", readiness=false. Elapsed: 11.539384ms
    Sep  4 15:07:13.557: INFO: Pod "busybox-user-65534-ec9e05b6-3291-4a5c-83fa-ff6682167bea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024766216s
    Sep  4 15:07:15.556: INFO: Pod "busybox-user-65534-ec9e05b6-3291-4a5c-83fa-ff6682167bea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023642424s
    Sep  4 15:07:15.556: INFO: Pod "busybox-user-65534-ec9e05b6-3291-4a5c-83fa-ff6682167bea" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:15.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3378" for this suite. 09/04/23 15:07:15.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:15.59
Sep  4 15:07:15.590: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 09/04/23 15:07:15.591
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:15.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:15.649
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 09/04/23 15:07:15.671
STEP: Getting a ResourceQuota 09/04/23 15:07:15.683
STEP: Listing all ResourceQuotas with LabelSelector 09/04/23 15:07:15.695
STEP: Patching the ResourceQuota 09/04/23 15:07:15.706
STEP: Deleting a Collection of ResourceQuotas 09/04/23 15:07:15.72
STEP: Verifying the deleted ResourceQuota 09/04/23 15:07:15.733
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:15.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3696" for this suite. 09/04/23 15:07:15.758
------------------------------
• [0.180 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:15.59
    Sep  4 15:07:15.590: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 09/04/23 15:07:15.591
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:15.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:15.649
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 09/04/23 15:07:15.671
    STEP: Getting a ResourceQuota 09/04/23 15:07:15.683
    STEP: Listing all ResourceQuotas with LabelSelector 09/04/23 15:07:15.695
    STEP: Patching the ResourceQuota 09/04/23 15:07:15.706
    STEP: Deleting a Collection of ResourceQuotas 09/04/23 15:07:15.72
    STEP: Verifying the deleted ResourceQuota 09/04/23 15:07:15.733
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:15.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3696" for this suite. 09/04/23 15:07:15.758
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:15.77
Sep  4 15:07:15.770: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 09/04/23 15:07:15.771
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:15.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:15.827
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 09/04/23 15:07:15.866
Sep  4 15:07:15.866: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases0a857d24-82e9-4621-ae49-13b6686826fc" in namespace "kubelet-test-999" to be "completed"
Sep  4 15:07:15.877: INFO: Pod "agnhost-host-aliases0a857d24-82e9-4621-ae49-13b6686826fc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.318007ms
Sep  4 15:07:17.890: INFO: Pod "agnhost-host-aliases0a857d24-82e9-4621-ae49-13b6686826fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024263682s
Sep  4 15:07:19.891: INFO: Pod "agnhost-host-aliases0a857d24-82e9-4621-ae49-13b6686826fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025625236s
Sep  4 15:07:19.891: INFO: Pod "agnhost-host-aliases0a857d24-82e9-4621-ae49-13b6686826fc" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:19.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-999" for this suite. 09/04/23 15:07:19.946
------------------------------
• [4.189 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:15.77
    Sep  4 15:07:15.770: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 09/04/23 15:07:15.771
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:15.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:15.827
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 09/04/23 15:07:15.866
    Sep  4 15:07:15.866: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases0a857d24-82e9-4621-ae49-13b6686826fc" in namespace "kubelet-test-999" to be "completed"
    Sep  4 15:07:15.877: INFO: Pod "agnhost-host-aliases0a857d24-82e9-4621-ae49-13b6686826fc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.318007ms
    Sep  4 15:07:17.890: INFO: Pod "agnhost-host-aliases0a857d24-82e9-4621-ae49-13b6686826fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024263682s
    Sep  4 15:07:19.891: INFO: Pod "agnhost-host-aliases0a857d24-82e9-4621-ae49-13b6686826fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025625236s
    Sep  4 15:07:19.891: INFO: Pod "agnhost-host-aliases0a857d24-82e9-4621-ae49-13b6686826fc" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:19.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-999" for this suite. 09/04/23 15:07:19.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:19.96
Sep  4 15:07:19.960: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 09/04/23 15:07:19.961
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:19.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:20.017
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 09/04/23 15:07:20.039
STEP: listing secrets in all namespaces to ensure that there are more than zero 09/04/23 15:07:20.051
STEP: patching the secret 09/04/23 15:07:20.065
STEP: deleting the secret using a LabelSelector 09/04/23 15:07:20.089
STEP: listing secrets in all namespaces, searching for label name and value in patch 09/04/23 15:07:20.102
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:20.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7255" for this suite. 09/04/23 15:07:20.129
------------------------------
• [0.181 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:19.96
    Sep  4 15:07:19.960: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 09/04/23 15:07:19.961
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:19.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:20.017
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 09/04/23 15:07:20.039
    STEP: listing secrets in all namespaces to ensure that there are more than zero 09/04/23 15:07:20.051
    STEP: patching the secret 09/04/23 15:07:20.065
    STEP: deleting the secret using a LabelSelector 09/04/23 15:07:20.089
    STEP: listing secrets in all namespaces, searching for label name and value in patch 09/04/23 15:07:20.102
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:20.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7255" for this suite. 09/04/23 15:07:20.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:20.141
Sep  4 15:07:20.142: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename discovery 09/04/23 15:07:20.142
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:20.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:20.199
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 09/04/23 15:07:20.231
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Sep  4 15:07:20.557: INFO: Checking APIGroup: apiregistration.k8s.io
Sep  4 15:07:20.568: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep  4 15:07:20.568: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Sep  4 15:07:20.568: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep  4 15:07:20.568: INFO: Checking APIGroup: apps
Sep  4 15:07:20.578: INFO: PreferredVersion.GroupVersion: apps/v1
Sep  4 15:07:20.578: INFO: Versions found [{apps/v1 v1}]
Sep  4 15:07:20.578: INFO: apps/v1 matches apps/v1
Sep  4 15:07:20.578: INFO: Checking APIGroup: events.k8s.io
Sep  4 15:07:20.589: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep  4 15:07:20.589: INFO: Versions found [{events.k8s.io/v1 v1}]
Sep  4 15:07:20.589: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep  4 15:07:20.589: INFO: Checking APIGroup: authentication.k8s.io
Sep  4 15:07:20.600: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep  4 15:07:20.600: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Sep  4 15:07:20.600: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep  4 15:07:20.600: INFO: Checking APIGroup: authorization.k8s.io
Sep  4 15:07:20.610: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep  4 15:07:20.610: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Sep  4 15:07:20.610: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep  4 15:07:20.610: INFO: Checking APIGroup: autoscaling
Sep  4 15:07:20.621: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Sep  4 15:07:20.621: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Sep  4 15:07:20.621: INFO: autoscaling/v2 matches autoscaling/v2
Sep  4 15:07:20.621: INFO: Checking APIGroup: batch
Sep  4 15:07:20.631: INFO: PreferredVersion.GroupVersion: batch/v1
Sep  4 15:07:20.631: INFO: Versions found [{batch/v1 v1}]
Sep  4 15:07:20.631: INFO: batch/v1 matches batch/v1
Sep  4 15:07:20.631: INFO: Checking APIGroup: certificates.k8s.io
Sep  4 15:07:20.642: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep  4 15:07:20.642: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Sep  4 15:07:20.642: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep  4 15:07:20.642: INFO: Checking APIGroup: networking.k8s.io
Sep  4 15:07:20.652: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep  4 15:07:20.652: INFO: Versions found [{networking.k8s.io/v1 v1}]
Sep  4 15:07:20.652: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep  4 15:07:20.652: INFO: Checking APIGroup: policy
Sep  4 15:07:20.663: INFO: PreferredVersion.GroupVersion: policy/v1
Sep  4 15:07:20.663: INFO: Versions found [{policy/v1 v1}]
Sep  4 15:07:20.663: INFO: policy/v1 matches policy/v1
Sep  4 15:07:20.663: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep  4 15:07:20.673: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep  4 15:07:20.673: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Sep  4 15:07:20.673: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep  4 15:07:20.673: INFO: Checking APIGroup: storage.k8s.io
Sep  4 15:07:20.684: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep  4 15:07:20.684: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep  4 15:07:20.684: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep  4 15:07:20.684: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep  4 15:07:20.695: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep  4 15:07:20.695: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Sep  4 15:07:20.695: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep  4 15:07:20.695: INFO: Checking APIGroup: apiextensions.k8s.io
Sep  4 15:07:20.705: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep  4 15:07:20.705: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Sep  4 15:07:20.705: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep  4 15:07:20.705: INFO: Checking APIGroup: scheduling.k8s.io
Sep  4 15:07:20.716: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep  4 15:07:20.716: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Sep  4 15:07:20.716: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep  4 15:07:20.716: INFO: Checking APIGroup: coordination.k8s.io
Sep  4 15:07:20.727: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep  4 15:07:20.727: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Sep  4 15:07:20.727: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep  4 15:07:20.727: INFO: Checking APIGroup: node.k8s.io
Sep  4 15:07:20.737: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Sep  4 15:07:20.737: INFO: Versions found [{node.k8s.io/v1 v1}]
Sep  4 15:07:20.737: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Sep  4 15:07:20.737: INFO: Checking APIGroup: discovery.k8s.io
Sep  4 15:07:20.748: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Sep  4 15:07:20.748: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Sep  4 15:07:20.748: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Sep  4 15:07:20.748: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep  4 15:07:20.758: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Sep  4 15:07:20.758: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Sep  4 15:07:20.758: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Sep  4 15:07:20.758: INFO: Checking APIGroup: autoscaling.k8s.io
Sep  4 15:07:20.769: INFO: PreferredVersion.GroupVersion: autoscaling.k8s.io/v1
Sep  4 15:07:20.769: INFO: Versions found [{autoscaling.k8s.io/v1 v1} {autoscaling.k8s.io/v1beta2 v1beta2}]
Sep  4 15:07:20.769: INFO: autoscaling.k8s.io/v1 matches autoscaling.k8s.io/v1
Sep  4 15:07:20.769: INFO: Checking APIGroup: crd.projectcalico.org
Sep  4 15:07:20.780: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Sep  4 15:07:20.780: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Sep  4 15:07:20.780: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Sep  4 15:07:20.780: INFO: Checking APIGroup: snapshot.storage.k8s.io
Sep  4 15:07:20.790: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Sep  4 15:07:20.790: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Sep  4 15:07:20.790: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Sep  4 15:07:20.790: INFO: Checking APIGroup: cert.gardener.cloud
Sep  4 15:07:20.801: INFO: PreferredVersion.GroupVersion: cert.gardener.cloud/v1alpha1
Sep  4 15:07:20.801: INFO: Versions found [{cert.gardener.cloud/v1alpha1 v1alpha1}]
Sep  4 15:07:20.801: INFO: cert.gardener.cloud/v1alpha1 matches cert.gardener.cloud/v1alpha1
Sep  4 15:07:20.801: INFO: Checking APIGroup: dns.gardener.cloud
Sep  4 15:07:20.811: INFO: PreferredVersion.GroupVersion: dns.gardener.cloud/v1alpha1
Sep  4 15:07:20.811: INFO: Versions found [{dns.gardener.cloud/v1alpha1 v1alpha1}]
Sep  4 15:07:20.811: INFO: dns.gardener.cloud/v1alpha1 matches dns.gardener.cloud/v1alpha1
Sep  4 15:07:20.811: INFO: Checking APIGroup: metrics.k8s.io
Sep  4 15:07:20.822: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Sep  4 15:07:20.822: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Sep  4 15:07:20.822: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:20.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-6780" for this suite. 09/04/23 15:07:20.844
------------------------------
• [0.715 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:20.141
    Sep  4 15:07:20.142: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename discovery 09/04/23 15:07:20.142
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:20.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:20.199
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 09/04/23 15:07:20.231
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Sep  4 15:07:20.557: INFO: Checking APIGroup: apiregistration.k8s.io
    Sep  4 15:07:20.568: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Sep  4 15:07:20.568: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Sep  4 15:07:20.568: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Sep  4 15:07:20.568: INFO: Checking APIGroup: apps
    Sep  4 15:07:20.578: INFO: PreferredVersion.GroupVersion: apps/v1
    Sep  4 15:07:20.578: INFO: Versions found [{apps/v1 v1}]
    Sep  4 15:07:20.578: INFO: apps/v1 matches apps/v1
    Sep  4 15:07:20.578: INFO: Checking APIGroup: events.k8s.io
    Sep  4 15:07:20.589: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Sep  4 15:07:20.589: INFO: Versions found [{events.k8s.io/v1 v1}]
    Sep  4 15:07:20.589: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Sep  4 15:07:20.589: INFO: Checking APIGroup: authentication.k8s.io
    Sep  4 15:07:20.600: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Sep  4 15:07:20.600: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Sep  4 15:07:20.600: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Sep  4 15:07:20.600: INFO: Checking APIGroup: authorization.k8s.io
    Sep  4 15:07:20.610: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Sep  4 15:07:20.610: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Sep  4 15:07:20.610: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Sep  4 15:07:20.610: INFO: Checking APIGroup: autoscaling
    Sep  4 15:07:20.621: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Sep  4 15:07:20.621: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Sep  4 15:07:20.621: INFO: autoscaling/v2 matches autoscaling/v2
    Sep  4 15:07:20.621: INFO: Checking APIGroup: batch
    Sep  4 15:07:20.631: INFO: PreferredVersion.GroupVersion: batch/v1
    Sep  4 15:07:20.631: INFO: Versions found [{batch/v1 v1}]
    Sep  4 15:07:20.631: INFO: batch/v1 matches batch/v1
    Sep  4 15:07:20.631: INFO: Checking APIGroup: certificates.k8s.io
    Sep  4 15:07:20.642: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Sep  4 15:07:20.642: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Sep  4 15:07:20.642: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Sep  4 15:07:20.642: INFO: Checking APIGroup: networking.k8s.io
    Sep  4 15:07:20.652: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Sep  4 15:07:20.652: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Sep  4 15:07:20.652: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Sep  4 15:07:20.652: INFO: Checking APIGroup: policy
    Sep  4 15:07:20.663: INFO: PreferredVersion.GroupVersion: policy/v1
    Sep  4 15:07:20.663: INFO: Versions found [{policy/v1 v1}]
    Sep  4 15:07:20.663: INFO: policy/v1 matches policy/v1
    Sep  4 15:07:20.663: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Sep  4 15:07:20.673: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Sep  4 15:07:20.673: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Sep  4 15:07:20.673: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Sep  4 15:07:20.673: INFO: Checking APIGroup: storage.k8s.io
    Sep  4 15:07:20.684: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Sep  4 15:07:20.684: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Sep  4 15:07:20.684: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Sep  4 15:07:20.684: INFO: Checking APIGroup: admissionregistration.k8s.io
    Sep  4 15:07:20.695: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Sep  4 15:07:20.695: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Sep  4 15:07:20.695: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Sep  4 15:07:20.695: INFO: Checking APIGroup: apiextensions.k8s.io
    Sep  4 15:07:20.705: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Sep  4 15:07:20.705: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Sep  4 15:07:20.705: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Sep  4 15:07:20.705: INFO: Checking APIGroup: scheduling.k8s.io
    Sep  4 15:07:20.716: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Sep  4 15:07:20.716: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Sep  4 15:07:20.716: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Sep  4 15:07:20.716: INFO: Checking APIGroup: coordination.k8s.io
    Sep  4 15:07:20.727: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Sep  4 15:07:20.727: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Sep  4 15:07:20.727: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Sep  4 15:07:20.727: INFO: Checking APIGroup: node.k8s.io
    Sep  4 15:07:20.737: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Sep  4 15:07:20.737: INFO: Versions found [{node.k8s.io/v1 v1}]
    Sep  4 15:07:20.737: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Sep  4 15:07:20.737: INFO: Checking APIGroup: discovery.k8s.io
    Sep  4 15:07:20.748: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Sep  4 15:07:20.748: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Sep  4 15:07:20.748: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Sep  4 15:07:20.748: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Sep  4 15:07:20.758: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Sep  4 15:07:20.758: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Sep  4 15:07:20.758: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Sep  4 15:07:20.758: INFO: Checking APIGroup: autoscaling.k8s.io
    Sep  4 15:07:20.769: INFO: PreferredVersion.GroupVersion: autoscaling.k8s.io/v1
    Sep  4 15:07:20.769: INFO: Versions found [{autoscaling.k8s.io/v1 v1} {autoscaling.k8s.io/v1beta2 v1beta2}]
    Sep  4 15:07:20.769: INFO: autoscaling.k8s.io/v1 matches autoscaling.k8s.io/v1
    Sep  4 15:07:20.769: INFO: Checking APIGroup: crd.projectcalico.org
    Sep  4 15:07:20.780: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Sep  4 15:07:20.780: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Sep  4 15:07:20.780: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Sep  4 15:07:20.780: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Sep  4 15:07:20.790: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Sep  4 15:07:20.790: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Sep  4 15:07:20.790: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Sep  4 15:07:20.790: INFO: Checking APIGroup: cert.gardener.cloud
    Sep  4 15:07:20.801: INFO: PreferredVersion.GroupVersion: cert.gardener.cloud/v1alpha1
    Sep  4 15:07:20.801: INFO: Versions found [{cert.gardener.cloud/v1alpha1 v1alpha1}]
    Sep  4 15:07:20.801: INFO: cert.gardener.cloud/v1alpha1 matches cert.gardener.cloud/v1alpha1
    Sep  4 15:07:20.801: INFO: Checking APIGroup: dns.gardener.cloud
    Sep  4 15:07:20.811: INFO: PreferredVersion.GroupVersion: dns.gardener.cloud/v1alpha1
    Sep  4 15:07:20.811: INFO: Versions found [{dns.gardener.cloud/v1alpha1 v1alpha1}]
    Sep  4 15:07:20.811: INFO: dns.gardener.cloud/v1alpha1 matches dns.gardener.cloud/v1alpha1
    Sep  4 15:07:20.811: INFO: Checking APIGroup: metrics.k8s.io
    Sep  4 15:07:20.822: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Sep  4 15:07:20.822: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Sep  4 15:07:20.822: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:20.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-6780" for this suite. 09/04/23 15:07:20.844
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:20.856
Sep  4 15:07:20.857: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 15:07:20.857
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:20.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:20.914
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 09/04/23 15:07:20.936
Sep  4 15:07:20.955: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96" in namespace "downward-api-223" to be "Succeeded or Failed"
Sep  4 15:07:20.967: INFO: Pod "downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96": Phase="Pending", Reason="", readiness=false. Elapsed: 11.718971ms
Sep  4 15:07:22.982: INFO: Pod "downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026483323s
Sep  4 15:07:24.980: INFO: Pod "downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02435308s
STEP: Saw pod success 09/04/23 15:07:24.98
Sep  4 15:07:24.980: INFO: Pod "downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96" satisfied condition "Succeeded or Failed"
Sep  4 15:07:24.991: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96 container client-container: <nil>
STEP: delete the pod 09/04/23 15:07:25.018
Sep  4 15:07:25.032: INFO: Waiting for pod downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96 to disappear
Sep  4 15:07:25.043: INFO: Pod downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:25.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-223" for this suite. 09/04/23 15:07:25.065
------------------------------
• [4.221 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:20.856
    Sep  4 15:07:20.857: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 15:07:20.857
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:20.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:20.914
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 09/04/23 15:07:20.936
    Sep  4 15:07:20.955: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96" in namespace "downward-api-223" to be "Succeeded or Failed"
    Sep  4 15:07:20.967: INFO: Pod "downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96": Phase="Pending", Reason="", readiness=false. Elapsed: 11.718971ms
    Sep  4 15:07:22.982: INFO: Pod "downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026483323s
    Sep  4 15:07:24.980: INFO: Pod "downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02435308s
    STEP: Saw pod success 09/04/23 15:07:24.98
    Sep  4 15:07:24.980: INFO: Pod "downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96" satisfied condition "Succeeded or Failed"
    Sep  4 15:07:24.991: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96 container client-container: <nil>
    STEP: delete the pod 09/04/23 15:07:25.018
    Sep  4 15:07:25.032: INFO: Waiting for pod downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96 to disappear
    Sep  4 15:07:25.043: INFO: Pod downwardapi-volume-1b35bda2-c590-4797-b206-28d05650ba96 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:25.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-223" for this suite. 09/04/23 15:07:25.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:25.078
Sep  4 15:07:25.078: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 15:07:25.079
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:25.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:25.136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 09/04/23 15:07:25.158
Sep  4 15:07:25.176: INFO: Waiting up to 5m0s for pod "pod-9ddb466d-5e71-4536-969e-13685c2edf3c" in namespace "emptydir-8110" to be "Succeeded or Failed"
Sep  4 15:07:25.187: INFO: Pod "pod-9ddb466d-5e71-4536-969e-13685c2edf3c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.274559ms
Sep  4 15:07:27.200: INFO: Pod "pod-9ddb466d-5e71-4536-969e-13685c2edf3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024079226s
Sep  4 15:07:29.200: INFO: Pod "pod-9ddb466d-5e71-4536-969e-13685c2edf3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024145765s
STEP: Saw pod success 09/04/23 15:07:29.2
Sep  4 15:07:29.200: INFO: Pod "pod-9ddb466d-5e71-4536-969e-13685c2edf3c" satisfied condition "Succeeded or Failed"
Sep  4 15:07:29.212: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-9ddb466d-5e71-4536-969e-13685c2edf3c container test-container: <nil>
STEP: delete the pod 09/04/23 15:07:29.244
Sep  4 15:07:29.259: INFO: Waiting for pod pod-9ddb466d-5e71-4536-969e-13685c2edf3c to disappear
Sep  4 15:07:29.271: INFO: Pod pod-9ddb466d-5e71-4536-969e-13685c2edf3c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:29.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8110" for this suite. 09/04/23 15:07:29.294
------------------------------
• [4.228 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:25.078
    Sep  4 15:07:25.078: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 15:07:25.079
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:25.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:25.136
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/04/23 15:07:25.158
    Sep  4 15:07:25.176: INFO: Waiting up to 5m0s for pod "pod-9ddb466d-5e71-4536-969e-13685c2edf3c" in namespace "emptydir-8110" to be "Succeeded or Failed"
    Sep  4 15:07:25.187: INFO: Pod "pod-9ddb466d-5e71-4536-969e-13685c2edf3c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.274559ms
    Sep  4 15:07:27.200: INFO: Pod "pod-9ddb466d-5e71-4536-969e-13685c2edf3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024079226s
    Sep  4 15:07:29.200: INFO: Pod "pod-9ddb466d-5e71-4536-969e-13685c2edf3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024145765s
    STEP: Saw pod success 09/04/23 15:07:29.2
    Sep  4 15:07:29.200: INFO: Pod "pod-9ddb466d-5e71-4536-969e-13685c2edf3c" satisfied condition "Succeeded or Failed"
    Sep  4 15:07:29.212: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-9ddb466d-5e71-4536-969e-13685c2edf3c container test-container: <nil>
    STEP: delete the pod 09/04/23 15:07:29.244
    Sep  4 15:07:29.259: INFO: Waiting for pod pod-9ddb466d-5e71-4536-969e-13685c2edf3c to disappear
    Sep  4 15:07:29.271: INFO: Pod pod-9ddb466d-5e71-4536-969e-13685c2edf3c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:29.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8110" for this suite. 09/04/23 15:07:29.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:29.307
Sep  4 15:07:29.307: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 09/04/23 15:07:29.309
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:29.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:29.368
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Sep  4 15:07:29.391: INFO: Creating simple deployment test-new-deployment
Sep  4 15:07:29.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 7, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 7, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 7, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 7, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 09/04/23 15:07:31.463
STEP: updating a scale subresource 09/04/23 15:07:31.475
STEP: verifying the deployment Spec.Replicas was modified 09/04/23 15:07:31.488
STEP: Patch a scale subresource 09/04/23 15:07:31.5
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 15:07:31.536: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-8919  cf301220-0676-43d3-9672-e852d89e03fe 16932 3 2023-09-04 15:07:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-09-04 15:07:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051b6348 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:3,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-09-04 15:07:30 +0000 UTC,LastTransitionTime:2023-09-04 15:07:29 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-04 15:07:31 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  4 15:07:31.548: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-8919  23fbb84e-26a7-4244-ab02-be2b7830551c 16938 3 2023-09-04 15:07:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment cf301220-0676-43d3-9672-e852d89e03fe 0xc0044f3bc7 0xc0044f3bc8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf301220-0676-43d3-9672-e852d89e03fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044f3c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:4,FullyLabeledReplicas:4,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  4 15:07:31.559: INFO: Pod "test-new-deployment-7f5969cbc7-8fczq" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-8fczq test-new-deployment-7f5969cbc7- deployment-8919  25bfdf35-817c-4d6b-8e66-3da9a56e321d 16908 0 2023-09-04 15:07:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7c363deb8540de8c3343ed4c90412266ab21c8fc402d5bee59c880ec3a7c2b0d cni.projectcalico.org/podIP:100.64.1.159/32 cni.projectcalico.org/podIPs:100.64.1.159/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 23fbb84e-26a7-4244-ab02-be2b7830551c 0xc0050b2057 0xc0050b2058}] [] [{calico Update v1 2023-09-04 15:07:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-04 15:07:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23fbb84e-26a7-4244-ab02-be2b7830551c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:07:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4864q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4864q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.159,StartTime:2023-09-04 15:07:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:07:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c721671f8c7602526f79143827e0004cc11dc8667ebc6b8ca4b7b467cda5b9f9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:07:31.559: INFO: Pod "test-new-deployment-7f5969cbc7-9bv6r" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-9bv6r test-new-deployment-7f5969cbc7- deployment-8919  0d663a77-f51a-4c2b-bc74-2c28ae7a8eb5 16937 0 2023-09-04 15:07:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 23fbb84e-26a7-4244-ab02-be2b7830551c 0xc0050b2247 0xc0050b2248}] [] [{kube-controller-manager Update v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23fbb84e-26a7-4244-ab02-be2b7830551c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bb6f6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bb6f6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:07:31.560: INFO: Pod "test-new-deployment-7f5969cbc7-m8hsp" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-m8hsp test-new-deployment-7f5969cbc7- deployment-8919  bd694c35-16d8-4115-99ab-366473f91e46 16939 0 2023-09-04 15:07:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 23fbb84e-26a7-4244-ab02-be2b7830551c 0xc0050b23a0 0xc0050b23a1}] [] [{kube-controller-manager Update v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23fbb84e-26a7-4244-ab02-be2b7830551c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dz7p8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dz7p8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:07:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:07:31.560: INFO: Pod "test-new-deployment-7f5969cbc7-xf4sx" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-xf4sx test-new-deployment-7f5969cbc7- deployment-8919  14cb4586-f197-459b-a301-d8252703f2e1 16941 0 2023-09-04 15:07:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 23fbb84e-26a7-4244-ab02-be2b7830551c 0xc0050b2557 0xc0050b2558}] [] [{kube-controller-manager Update v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23fbb84e-26a7-4244-ab02-be2b7830551c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vfkdt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vfkdt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:,StartTime:2023-09-04 15:07:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:31.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8919" for this suite. 09/04/23 15:07:31.573
------------------------------
• [2.279 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:29.307
    Sep  4 15:07:29.307: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 09/04/23 15:07:29.309
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:29.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:29.368
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Sep  4 15:07:29.391: INFO: Creating simple deployment test-new-deployment
    Sep  4 15:07:29.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 7, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 7, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 7, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 7, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 09/04/23 15:07:31.463
    STEP: updating a scale subresource 09/04/23 15:07:31.475
    STEP: verifying the deployment Spec.Replicas was modified 09/04/23 15:07:31.488
    STEP: Patch a scale subresource 09/04/23 15:07:31.5
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 15:07:31.536: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-8919  cf301220-0676-43d3-9672-e852d89e03fe 16932 3 2023-09-04 15:07:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-09-04 15:07:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051b6348 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:3,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-09-04 15:07:30 +0000 UTC,LastTransitionTime:2023-09-04 15:07:29 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-04 15:07:31 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  4 15:07:31.548: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-8919  23fbb84e-26a7-4244-ab02-be2b7830551c 16938 3 2023-09-04 15:07:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment cf301220-0676-43d3-9672-e852d89e03fe 0xc0044f3bc7 0xc0044f3bc8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf301220-0676-43d3-9672-e852d89e03fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044f3c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:4,FullyLabeledReplicas:4,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 15:07:31.559: INFO: Pod "test-new-deployment-7f5969cbc7-8fczq" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-8fczq test-new-deployment-7f5969cbc7- deployment-8919  25bfdf35-817c-4d6b-8e66-3da9a56e321d 16908 0 2023-09-04 15:07:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7c363deb8540de8c3343ed4c90412266ab21c8fc402d5bee59c880ec3a7c2b0d cni.projectcalico.org/podIP:100.64.1.159/32 cni.projectcalico.org/podIPs:100.64.1.159/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 23fbb84e-26a7-4244-ab02-be2b7830551c 0xc0050b2057 0xc0050b2058}] [] [{calico Update v1 2023-09-04 15:07:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-04 15:07:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23fbb84e-26a7-4244-ab02-be2b7830551c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:07:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4864q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4864q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.159,StartTime:2023-09-04 15:07:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:07:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c721671f8c7602526f79143827e0004cc11dc8667ebc6b8ca4b7b467cda5b9f9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:07:31.559: INFO: Pod "test-new-deployment-7f5969cbc7-9bv6r" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-9bv6r test-new-deployment-7f5969cbc7- deployment-8919  0d663a77-f51a-4c2b-bc74-2c28ae7a8eb5 16937 0 2023-09-04 15:07:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 23fbb84e-26a7-4244-ab02-be2b7830551c 0xc0050b2247 0xc0050b2248}] [] [{kube-controller-manager Update v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23fbb84e-26a7-4244-ab02-be2b7830551c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bb6f6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bb6f6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:07:31.560: INFO: Pod "test-new-deployment-7f5969cbc7-m8hsp" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-m8hsp test-new-deployment-7f5969cbc7- deployment-8919  bd694c35-16d8-4115-99ab-366473f91e46 16939 0 2023-09-04 15:07:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 23fbb84e-26a7-4244-ab02-be2b7830551c 0xc0050b23a0 0xc0050b23a1}] [] [{kube-controller-manager Update v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23fbb84e-26a7-4244-ab02-be2b7830551c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dz7p8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dz7p8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:07:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:07:31.560: INFO: Pod "test-new-deployment-7f5969cbc7-xf4sx" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-xf4sx test-new-deployment-7f5969cbc7- deployment-8919  14cb4586-f197-459b-a301-d8252703f2e1 16941 0 2023-09-04 15:07:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 23fbb84e-26a7-4244-ab02-be2b7830551c 0xc0050b2557 0xc0050b2558}] [] [{kube-controller-manager Update v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23fbb84e-26a7-4244-ab02-be2b7830551c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:07:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vfkdt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vfkdt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:07:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:,StartTime:2023-09-04 15:07:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:31.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8919" for this suite. 09/04/23 15:07:31.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:31.589
Sep  4 15:07:31.589: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 09/04/23 15:07:31.59
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:31.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:31.649
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 09/04/23 15:07:31.672
Sep  4 15:07:31.685: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 09/04/23 15:07:31.685
Sep  4 15:07:31.703: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 09/04/23 15:07:31.703
Sep  4 15:07:31.728: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:31.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-714" for this suite. 09/04/23 15:07:31.741
------------------------------
• [0.165 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:31.589
    Sep  4 15:07:31.589: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 09/04/23 15:07:31.59
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:31.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:31.649
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 09/04/23 15:07:31.672
    Sep  4 15:07:31.685: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 09/04/23 15:07:31.685
    Sep  4 15:07:31.703: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 09/04/23 15:07:31.703
    Sep  4 15:07:31.728: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:31.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-714" for this suite. 09/04/23 15:07:31.741
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:31.754
Sep  4 15:07:31.754: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy 09/04/23 15:07:31.755
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:31.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:31.812
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Sep  4 15:07:31.835: INFO: Creating pod...
Sep  4 15:07:31.851: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5758" to be "running"
Sep  4 15:07:31.863: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 11.674775ms
Sep  4 15:07:33.876: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.024455323s
Sep  4 15:07:33.876: INFO: Pod "agnhost" satisfied condition "running"
Sep  4 15:07:33.876: INFO: Creating service...
Sep  4 15:07:33.892: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/pods/agnhost/proxy?method=DELETE
Sep  4 15:07:34.028: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  4 15:07:34.028: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/pods/agnhost/proxy?method=OPTIONS
Sep  4 15:07:34.081: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  4 15:07:34.081: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/pods/agnhost/proxy?method=PATCH
Sep  4 15:07:34.105: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  4 15:07:34.105: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/pods/agnhost/proxy?method=POST
Sep  4 15:07:34.128: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  4 15:07:34.128: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/pods/agnhost/proxy?method=PUT
Sep  4 15:07:34.151: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  4 15:07:34.151: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/services/e2e-proxy-test-service/proxy?method=DELETE
Sep  4 15:07:34.174: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  4 15:07:34.174: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/services/e2e-proxy-test-service/proxy?method=OPTIONS
Sep  4 15:07:34.197: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  4 15:07:34.197: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/services/e2e-proxy-test-service/proxy?method=PATCH
Sep  4 15:07:34.220: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  4 15:07:34.220: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/services/e2e-proxy-test-service/proxy?method=POST
Sep  4 15:07:34.242: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  4 15:07:34.242: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/services/e2e-proxy-test-service/proxy?method=PUT
Sep  4 15:07:34.265: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  4 15:07:34.265: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/pods/agnhost/proxy?method=GET
Sep  4 15:07:34.277: INFO: http.Client request:GET StatusCode:301
Sep  4 15:07:34.277: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/services/e2e-proxy-test-service/proxy?method=GET
Sep  4 15:07:34.289: INFO: http.Client request:GET StatusCode:301
Sep  4 15:07:34.289: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/pods/agnhost/proxy?method=HEAD
Sep  4 15:07:34.301: INFO: http.Client request:HEAD StatusCode:301
Sep  4 15:07:34.301: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/services/e2e-proxy-test-service/proxy?method=HEAD
Sep  4 15:07:34.313: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:34.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5758" for this suite. 09/04/23 15:07:34.335
------------------------------
• [2.594 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:31.754
    Sep  4 15:07:31.754: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename proxy 09/04/23 15:07:31.755
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:31.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:31.812
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Sep  4 15:07:31.835: INFO: Creating pod...
    Sep  4 15:07:31.851: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5758" to be "running"
    Sep  4 15:07:31.863: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 11.674775ms
    Sep  4 15:07:33.876: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.024455323s
    Sep  4 15:07:33.876: INFO: Pod "agnhost" satisfied condition "running"
    Sep  4 15:07:33.876: INFO: Creating service...
    Sep  4 15:07:33.892: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/pods/agnhost/proxy?method=DELETE
    Sep  4 15:07:34.028: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  4 15:07:34.028: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/pods/agnhost/proxy?method=OPTIONS
    Sep  4 15:07:34.081: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  4 15:07:34.081: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/pods/agnhost/proxy?method=PATCH
    Sep  4 15:07:34.105: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  4 15:07:34.105: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/pods/agnhost/proxy?method=POST
    Sep  4 15:07:34.128: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  4 15:07:34.128: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/pods/agnhost/proxy?method=PUT
    Sep  4 15:07:34.151: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  4 15:07:34.151: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/services/e2e-proxy-test-service/proxy?method=DELETE
    Sep  4 15:07:34.174: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  4 15:07:34.174: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Sep  4 15:07:34.197: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  4 15:07:34.197: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/services/e2e-proxy-test-service/proxy?method=PATCH
    Sep  4 15:07:34.220: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  4 15:07:34.220: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/services/e2e-proxy-test-service/proxy?method=POST
    Sep  4 15:07:34.242: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  4 15:07:34.242: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/services/e2e-proxy-test-service/proxy?method=PUT
    Sep  4 15:07:34.265: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  4 15:07:34.265: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/pods/agnhost/proxy?method=GET
    Sep  4 15:07:34.277: INFO: http.Client request:GET StatusCode:301
    Sep  4 15:07:34.277: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/services/e2e-proxy-test-service/proxy?method=GET
    Sep  4 15:07:34.289: INFO: http.Client request:GET StatusCode:301
    Sep  4 15:07:34.289: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/pods/agnhost/proxy?method=HEAD
    Sep  4 15:07:34.301: INFO: http.Client request:HEAD StatusCode:301
    Sep  4 15:07:34.301: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5758/services/e2e-proxy-test-service/proxy?method=HEAD
    Sep  4 15:07:34.313: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:34.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5758" for this suite. 09/04/23 15:07:34.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:34.349
Sep  4 15:07:34.349: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 09/04/23 15:07:34.35
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:34.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:34.407
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 09/04/23 15:07:34.443
STEP: watching for Pod to be ready 09/04/23 15:07:34.461
Sep  4 15:07:34.472: INFO: observed Pod pod-test in namespace pods-3904 in phase Pending with labels: map[test-pod-static:true] & conditions []
Sep  4 15:07:34.472: INFO: observed Pod pod-test in namespace pods-3904 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC  }]
Sep  4 15:07:34.491: INFO: observed Pod pod-test in namespace pods-3904 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC  }]
Sep  4 15:07:34.963: INFO: observed Pod pod-test in namespace pods-3904 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC  }]
Sep  4 15:07:35.807: INFO: Found Pod pod-test in namespace pods-3904 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 09/04/23 15:07:35.82
STEP: getting the Pod and ensuring that it's patched 09/04/23 15:07:35.846
STEP: replacing the Pod's status Ready condition to False 09/04/23 15:07:35.859
STEP: check the Pod again to ensure its Ready conditions are False 09/04/23 15:07:35.886
STEP: deleting the Pod via a Collection with a LabelSelector 09/04/23 15:07:35.887
STEP: watching for the Pod to be deleted 09/04/23 15:07:35.901
Sep  4 15:07:35.912: INFO: observed event type MODIFIED
Sep  4 15:07:37.028: INFO: observed event type MODIFIED
Sep  4 15:07:37.977: INFO: observed event type MODIFIED
Sep  4 15:07:38.824: INFO: observed event type MODIFIED
Sep  4 15:07:38.837: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:38.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3904" for this suite. 09/04/23 15:07:38.873
------------------------------
• [4.538 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:34.349
    Sep  4 15:07:34.349: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 09/04/23 15:07:34.35
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:34.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:34.407
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 09/04/23 15:07:34.443
    STEP: watching for Pod to be ready 09/04/23 15:07:34.461
    Sep  4 15:07:34.472: INFO: observed Pod pod-test in namespace pods-3904 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Sep  4 15:07:34.472: INFO: observed Pod pod-test in namespace pods-3904 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC  }]
    Sep  4 15:07:34.491: INFO: observed Pod pod-test in namespace pods-3904 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC  }]
    Sep  4 15:07:34.963: INFO: observed Pod pod-test in namespace pods-3904 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC  }]
    Sep  4 15:07:35.807: INFO: Found Pod pod-test in namespace pods-3904 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:07:34 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 09/04/23 15:07:35.82
    STEP: getting the Pod and ensuring that it's patched 09/04/23 15:07:35.846
    STEP: replacing the Pod's status Ready condition to False 09/04/23 15:07:35.859
    STEP: check the Pod again to ensure its Ready conditions are False 09/04/23 15:07:35.886
    STEP: deleting the Pod via a Collection with a LabelSelector 09/04/23 15:07:35.887
    STEP: watching for the Pod to be deleted 09/04/23 15:07:35.901
    Sep  4 15:07:35.912: INFO: observed event type MODIFIED
    Sep  4 15:07:37.028: INFO: observed event type MODIFIED
    Sep  4 15:07:37.977: INFO: observed event type MODIFIED
    Sep  4 15:07:38.824: INFO: observed event type MODIFIED
    Sep  4 15:07:38.837: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:38.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3904" for this suite. 09/04/23 15:07:38.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:38.887
Sep  4 15:07:38.887: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 15:07:38.888
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:38.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:38.946
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 09/04/23 15:07:38.968
Sep  4 15:07:38.992: INFO: Waiting up to 5m0s for pod "downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714" in namespace "downward-api-5755" to be "Succeeded or Failed"
Sep  4 15:07:39.003: INFO: Pod "downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714": Phase="Pending", Reason="", readiness=false. Elapsed: 11.392826ms
Sep  4 15:07:41.018: INFO: Pod "downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026124916s
Sep  4 15:07:43.017: INFO: Pod "downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025175825s
STEP: Saw pod success 09/04/23 15:07:43.017
Sep  4 15:07:43.017: INFO: Pod "downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714" satisfied condition "Succeeded or Failed"
Sep  4 15:07:43.029: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714 container client-container: <nil>
STEP: delete the pod 09/04/23 15:07:43.104
Sep  4 15:07:43.124: INFO: Waiting for pod downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714 to disappear
Sep  4 15:07:43.136: INFO: Pod downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:43.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5755" for this suite. 09/04/23 15:07:43.159
------------------------------
• [4.286 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:38.887
    Sep  4 15:07:38.887: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 15:07:38.888
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:38.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:38.946
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 09/04/23 15:07:38.968
    Sep  4 15:07:38.992: INFO: Waiting up to 5m0s for pod "downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714" in namespace "downward-api-5755" to be "Succeeded or Failed"
    Sep  4 15:07:39.003: INFO: Pod "downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714": Phase="Pending", Reason="", readiness=false. Elapsed: 11.392826ms
    Sep  4 15:07:41.018: INFO: Pod "downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026124916s
    Sep  4 15:07:43.017: INFO: Pod "downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025175825s
    STEP: Saw pod success 09/04/23 15:07:43.017
    Sep  4 15:07:43.017: INFO: Pod "downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714" satisfied condition "Succeeded or Failed"
    Sep  4 15:07:43.029: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714 container client-container: <nil>
    STEP: delete the pod 09/04/23 15:07:43.104
    Sep  4 15:07:43.124: INFO: Waiting for pod downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714 to disappear
    Sep  4 15:07:43.136: INFO: Pod downwardapi-volume-48f66d01-f422-4b7b-8159-419911bf0714 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:43.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5755" for this suite. 09/04/23 15:07:43.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:43.176
Sep  4 15:07:43.176: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 09/04/23 15:07:43.177
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:43.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:43.235
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 09/04/23 15:07:43.257
STEP: creating a new configmap 09/04/23 15:07:43.268
STEP: modifying the configmap once 09/04/23 15:07:43.28
STEP: closing the watch once it receives two notifications 09/04/23 15:07:43.306
Sep  4 15:07:43.306: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5654  c130692d-57a8-49d9-b823-463138016777 17097 0 2023-09-04 15:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 15:07:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 15:07:43.306: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5654  c130692d-57a8-49d9-b823-463138016777 17098 0 2023-09-04 15:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 15:07:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 09/04/23 15:07:43.306
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/04/23 15:07:43.33
STEP: deleting the configmap 09/04/23 15:07:43.341
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/04/23 15:07:43.354
Sep  4 15:07:43.354: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5654  c130692d-57a8-49d9-b823-463138016777 17099 0 2023-09-04 15:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 15:07:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 15:07:43.354: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5654  c130692d-57a8-49d9-b823-463138016777 17100 0 2023-09-04 15:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 15:07:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:43.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5654" for this suite. 09/04/23 15:07:43.367
------------------------------
• [0.205 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:43.176
    Sep  4 15:07:43.176: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 09/04/23 15:07:43.177
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:43.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:43.235
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 09/04/23 15:07:43.257
    STEP: creating a new configmap 09/04/23 15:07:43.268
    STEP: modifying the configmap once 09/04/23 15:07:43.28
    STEP: closing the watch once it receives two notifications 09/04/23 15:07:43.306
    Sep  4 15:07:43.306: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5654  c130692d-57a8-49d9-b823-463138016777 17097 0 2023-09-04 15:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 15:07:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 15:07:43.306: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5654  c130692d-57a8-49d9-b823-463138016777 17098 0 2023-09-04 15:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 15:07:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 09/04/23 15:07:43.306
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/04/23 15:07:43.33
    STEP: deleting the configmap 09/04/23 15:07:43.341
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/04/23 15:07:43.354
    Sep  4 15:07:43.354: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5654  c130692d-57a8-49d9-b823-463138016777 17099 0 2023-09-04 15:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 15:07:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 15:07:43.354: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5654  c130692d-57a8-49d9-b823-463138016777 17100 0 2023-09-04 15:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-04 15:07:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:43.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5654" for this suite. 09/04/23 15:07:43.367
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:43.381
Sep  4 15:07:43.381: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 15:07:43.381
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:43.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:43.439
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Sep  4 15:07:43.461: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:45.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6003" for this suite. 09/04/23 15:07:45.221
------------------------------
• [1.854 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:43.381
    Sep  4 15:07:43.381: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 15:07:43.381
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:43.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:43.439
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Sep  4 15:07:43.461: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:45.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6003" for this suite. 09/04/23 15:07:45.221
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:45.235
Sep  4 15:07:45.235: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice 09/04/23 15:07:45.236
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:45.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:45.295
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 09/04/23 15:07:45.317
STEP: getting /apis/discovery.k8s.io 09/04/23 15:07:45.34
STEP: getting /apis/discovery.k8s.iov1 09/04/23 15:07:45.351
STEP: creating 09/04/23 15:07:45.362
STEP: getting 09/04/23 15:07:45.4
STEP: listing 09/04/23 15:07:45.412
STEP: watching 09/04/23 15:07:45.424
Sep  4 15:07:45.424: INFO: starting watch
STEP: cluster-wide listing 09/04/23 15:07:45.435
STEP: cluster-wide watching 09/04/23 15:07:45.449
Sep  4 15:07:45.450: INFO: starting watch
STEP: patching 09/04/23 15:07:45.461
STEP: updating 09/04/23 15:07:45.475
Sep  4 15:07:45.500: INFO: waiting for watch events with expected annotations
Sep  4 15:07:45.500: INFO: saw patched and updated annotations
STEP: deleting 09/04/23 15:07:45.5
STEP: deleting a collection 09/04/23 15:07:45.537
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:45.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2088" for this suite. 09/04/23 15:07:45.581
------------------------------
• [0.359 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:45.235
    Sep  4 15:07:45.235: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslice 09/04/23 15:07:45.236
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:45.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:45.295
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 09/04/23 15:07:45.317
    STEP: getting /apis/discovery.k8s.io 09/04/23 15:07:45.34
    STEP: getting /apis/discovery.k8s.iov1 09/04/23 15:07:45.351
    STEP: creating 09/04/23 15:07:45.362
    STEP: getting 09/04/23 15:07:45.4
    STEP: listing 09/04/23 15:07:45.412
    STEP: watching 09/04/23 15:07:45.424
    Sep  4 15:07:45.424: INFO: starting watch
    STEP: cluster-wide listing 09/04/23 15:07:45.435
    STEP: cluster-wide watching 09/04/23 15:07:45.449
    Sep  4 15:07:45.450: INFO: starting watch
    STEP: patching 09/04/23 15:07:45.461
    STEP: updating 09/04/23 15:07:45.475
    Sep  4 15:07:45.500: INFO: waiting for watch events with expected annotations
    Sep  4 15:07:45.500: INFO: saw patched and updated annotations
    STEP: deleting 09/04/23 15:07:45.5
    STEP: deleting a collection 09/04/23 15:07:45.537
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:45.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2088" for this suite. 09/04/23 15:07:45.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:45.595
Sep  4 15:07:45.595: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events 09/04/23 15:07:45.596
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:45.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:45.653
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 09/04/23 15:07:45.675
STEP: get a list of Events with a label in the current namespace 09/04/23 15:07:45.712
STEP: delete a list of events 09/04/23 15:07:45.724
Sep  4 15:07:45.724: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/04/23 15:07:45.741
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Sep  4 15:07:45.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7229" for this suite. 09/04/23 15:07:45.765
------------------------------
• [0.183 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:45.595
    Sep  4 15:07:45.595: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename events 09/04/23 15:07:45.596
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:45.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:45.653
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 09/04/23 15:07:45.675
    STEP: get a list of Events with a label in the current namespace 09/04/23 15:07:45.712
    STEP: delete a list of events 09/04/23 15:07:45.724
    Sep  4 15:07:45.724: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/04/23 15:07:45.741
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:07:45.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7229" for this suite. 09/04/23 15:07:45.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:07:45.779
Sep  4 15:07:45.779: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 09/04/23 15:07:45.78
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:45.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:45.838
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/04/23 15:07:45.879
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/04/23 15:08:02.103
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/04/23 15:08:02.115
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/04/23 15:08:02.141
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/04/23 15:08:02.141
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/04/23 15:08:02.187
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/04/23 15:08:05.238
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/04/23 15:08:07.277
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/04/23 15:08:07.302
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/04/23 15:08:07.302
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/04/23 15:08:07.347
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/04/23 15:08:08.372
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/04/23 15:08:11.425
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/04/23 15:08:11.45
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/04/23 15:08:11.45
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  4 15:08:11.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5612" for this suite. 09/04/23 15:08:11.539
------------------------------
• [25.776 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:07:45.779
    Sep  4 15:07:45.779: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 09/04/23 15:07:45.78
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:07:45.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:07:45.838
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/04/23 15:07:45.879
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/04/23 15:08:02.103
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/04/23 15:08:02.115
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/04/23 15:08:02.141
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/04/23 15:08:02.141
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/04/23 15:08:02.187
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/04/23 15:08:05.238
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/04/23 15:08:07.277
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/04/23 15:08:07.302
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/04/23 15:08:07.302
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/04/23 15:08:07.347
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/04/23 15:08:08.372
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/04/23 15:08:11.425
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/04/23 15:08:11.45
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/04/23 15:08:11.45
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:08:11.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5612" for this suite. 09/04/23 15:08:11.539
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:08:11.555
Sep  4 15:08:11.555: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 09/04/23 15:08:11.555
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:08:11.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:08:11.615
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Sep  4 15:08:11.651: INFO: Got root ca configmap in namespace "svcaccounts-4799"
Sep  4 15:08:11.665: INFO: Deleted root ca configmap in namespace "svcaccounts-4799"
STEP: waiting for a new root ca configmap created 09/04/23 15:08:12.165
Sep  4 15:08:12.179: INFO: Recreated root ca configmap in namespace "svcaccounts-4799"
Sep  4 15:08:12.192: INFO: Updated root ca configmap in namespace "svcaccounts-4799"
STEP: waiting for the root ca configmap reconciled 09/04/23 15:08:12.692
Sep  4 15:08:12.705: INFO: Reconciled root ca configmap in namespace "svcaccounts-4799"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  4 15:08:12.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4799" for this suite. 09/04/23 15:08:12.728
------------------------------
• [1.187 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:08:11.555
    Sep  4 15:08:11.555: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 09/04/23 15:08:11.555
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:08:11.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:08:11.615
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Sep  4 15:08:11.651: INFO: Got root ca configmap in namespace "svcaccounts-4799"
    Sep  4 15:08:11.665: INFO: Deleted root ca configmap in namespace "svcaccounts-4799"
    STEP: waiting for a new root ca configmap created 09/04/23 15:08:12.165
    Sep  4 15:08:12.179: INFO: Recreated root ca configmap in namespace "svcaccounts-4799"
    Sep  4 15:08:12.192: INFO: Updated root ca configmap in namespace "svcaccounts-4799"
    STEP: waiting for the root ca configmap reconciled 09/04/23 15:08:12.692
    Sep  4 15:08:12.705: INFO: Reconciled root ca configmap in namespace "svcaccounts-4799"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:08:12.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4799" for this suite. 09/04/23 15:08:12.728
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:08:12.742
Sep  4 15:08:12.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 15:08:12.743
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:08:12.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:08:12.801
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-d4d32db5-c48a-4799-aec5-68726e4b3201 09/04/23 15:08:12.823
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:08:12.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7605" for this suite. 09/04/23 15:08:12.848
------------------------------
• [0.119 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:08:12.742
    Sep  4 15:08:12.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 15:08:12.743
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:08:12.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:08:12.801
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-d4d32db5-c48a-4799-aec5-68726e4b3201 09/04/23 15:08:12.823
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:08:12.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7605" for this suite. 09/04/23 15:08:12.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:08:12.862
Sep  4 15:08:12.862: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events 09/04/23 15:08:12.863
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:08:12.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:08:12.922
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 09/04/23 15:08:12.944
Sep  4 15:08:12.958: INFO: created test-event-1
Sep  4 15:08:12.974: INFO: created test-event-2
Sep  4 15:08:12.986: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 09/04/23 15:08:12.986
STEP: delete collection of events 09/04/23 15:08:12.998
Sep  4 15:08:12.998: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/04/23 15:08:13.015
Sep  4 15:08:13.015: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Sep  4 15:08:13.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9496" for this suite. 09/04/23 15:08:13.041
------------------------------
• [0.193 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:08:12.862
    Sep  4 15:08:12.862: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename events 09/04/23 15:08:12.863
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:08:12.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:08:12.922
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 09/04/23 15:08:12.944
    Sep  4 15:08:12.958: INFO: created test-event-1
    Sep  4 15:08:12.974: INFO: created test-event-2
    Sep  4 15:08:12.986: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 09/04/23 15:08:12.986
    STEP: delete collection of events 09/04/23 15:08:12.998
    Sep  4 15:08:12.998: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/04/23 15:08:13.015
    Sep  4 15:08:13.015: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:08:13.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9496" for this suite. 09/04/23 15:08:13.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:08:13.055
Sep  4 15:08:13.056: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 09/04/23 15:08:13.056
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:08:13.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:08:13.114
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Sep  4 15:08:13.155: INFO: Waiting up to 5m0s for pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919" in namespace "container-probe-5946" to be "running and ready"
Sep  4 15:08:13.166: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Pending", Reason="", readiness=false. Elapsed: 11.730935ms
Sep  4 15:08:13.166: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:08:15.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 2.025015052s
Sep  4 15:08:15.180: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
Sep  4 15:08:17.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 4.025036058s
Sep  4 15:08:17.180: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
Sep  4 15:08:19.179: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 6.024277791s
Sep  4 15:08:19.179: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
Sep  4 15:08:21.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 8.025148497s
Sep  4 15:08:21.180: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
Sep  4 15:08:23.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 10.025225527s
Sep  4 15:08:23.180: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
Sep  4 15:08:25.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 12.025073066s
Sep  4 15:08:25.180: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
Sep  4 15:08:27.179: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 14.024427487s
Sep  4 15:08:27.179: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
Sep  4 15:08:29.182: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 16.026979699s
Sep  4 15:08:29.182: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
Sep  4 15:08:31.181: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 18.026333517s
Sep  4 15:08:31.181: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
Sep  4 15:08:33.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 20.025028026s
Sep  4 15:08:33.180: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
Sep  4 15:08:35.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=true. Elapsed: 22.024959072s
Sep  4 15:08:35.180: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = true)
Sep  4 15:08:35.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919" satisfied condition "running and ready"
Sep  4 15:08:35.192: INFO: Container started at 2023-09-04 15:08:13 +0000 UTC, pod became ready at 2023-09-04 15:08:33 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 15:08:35.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5946" for this suite. 09/04/23 15:08:35.214
------------------------------
• [22.172 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:08:13.055
    Sep  4 15:08:13.056: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 09/04/23 15:08:13.056
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:08:13.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:08:13.114
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Sep  4 15:08:13.155: INFO: Waiting up to 5m0s for pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919" in namespace "container-probe-5946" to be "running and ready"
    Sep  4 15:08:13.166: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Pending", Reason="", readiness=false. Elapsed: 11.730935ms
    Sep  4 15:08:13.166: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:08:15.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 2.025015052s
    Sep  4 15:08:15.180: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
    Sep  4 15:08:17.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 4.025036058s
    Sep  4 15:08:17.180: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
    Sep  4 15:08:19.179: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 6.024277791s
    Sep  4 15:08:19.179: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
    Sep  4 15:08:21.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 8.025148497s
    Sep  4 15:08:21.180: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
    Sep  4 15:08:23.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 10.025225527s
    Sep  4 15:08:23.180: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
    Sep  4 15:08:25.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 12.025073066s
    Sep  4 15:08:25.180: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
    Sep  4 15:08:27.179: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 14.024427487s
    Sep  4 15:08:27.179: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
    Sep  4 15:08:29.182: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 16.026979699s
    Sep  4 15:08:29.182: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
    Sep  4 15:08:31.181: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 18.026333517s
    Sep  4 15:08:31.181: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
    Sep  4 15:08:33.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=false. Elapsed: 20.025028026s
    Sep  4 15:08:33.180: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = false)
    Sep  4 15:08:35.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919": Phase="Running", Reason="", readiness=true. Elapsed: 22.024959072s
    Sep  4 15:08:35.180: INFO: The phase of Pod test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919 is Running (Ready = true)
    Sep  4 15:08:35.180: INFO: Pod "test-webserver-ece49ee4-9586-4743-ad7b-13b56ec8e919" satisfied condition "running and ready"
    Sep  4 15:08:35.192: INFO: Container started at 2023-09-04 15:08:13 +0000 UTC, pod became ready at 2023-09-04 15:08:33 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:08:35.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5946" for this suite. 09/04/23 15:08:35.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:08:35.228
Sep  4 15:08:35.228: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 09/04/23 15:08:35.228
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:08:35.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:08:35.287
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 09/04/23 15:08:35.309
STEP: creating a watch on configmaps with label B 09/04/23 15:08:35.319
STEP: creating a watch on configmaps with label A or B 09/04/23 15:08:35.33
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/04/23 15:08:35.341
Sep  4 15:08:35.354: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17562 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 15:08:35.354: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17562 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/04/23 15:08:35.354
Sep  4 15:08:35.379: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17563 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 15:08:35.379: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17563 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/04/23 15:08:35.379
Sep  4 15:08:35.404: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17564 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 15:08:35.404: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17564 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/04/23 15:08:35.404
Sep  4 15:08:35.418: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17565 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 15:08:35.418: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17565 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/04/23 15:08:35.418
Sep  4 15:08:35.430: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8891  b165af28-f235-49b4-aa5d-6385a2dbe932 17566 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 15:08:35.430: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8891  b165af28-f235-49b4-aa5d-6385a2dbe932 17566 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/04/23 15:08:45.431
Sep  4 15:08:45.445: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8891  b165af28-f235-49b4-aa5d-6385a2dbe932 17625 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 15:08:45.445: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8891  b165af28-f235-49b4-aa5d-6385a2dbe932 17625 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  4 15:08:55.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8891" for this suite. 09/04/23 15:08:55.471
------------------------------
• [20.258 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:08:35.228
    Sep  4 15:08:35.228: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 09/04/23 15:08:35.228
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:08:35.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:08:35.287
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 09/04/23 15:08:35.309
    STEP: creating a watch on configmaps with label B 09/04/23 15:08:35.319
    STEP: creating a watch on configmaps with label A or B 09/04/23 15:08:35.33
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/04/23 15:08:35.341
    Sep  4 15:08:35.354: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17562 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 15:08:35.354: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17562 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/04/23 15:08:35.354
    Sep  4 15:08:35.379: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17563 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 15:08:35.379: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17563 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/04/23 15:08:35.379
    Sep  4 15:08:35.404: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17564 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 15:08:35.404: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17564 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/04/23 15:08:35.404
    Sep  4 15:08:35.418: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17565 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 15:08:35.418: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8891  f83521fd-f464-4a55-9be6-30c96e8ccb9d 17565 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/04/23 15:08:35.418
    Sep  4 15:08:35.430: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8891  b165af28-f235-49b4-aa5d-6385a2dbe932 17566 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 15:08:35.430: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8891  b165af28-f235-49b4-aa5d-6385a2dbe932 17566 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/04/23 15:08:45.431
    Sep  4 15:08:45.445: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8891  b165af28-f235-49b4-aa5d-6385a2dbe932 17625 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 15:08:45.445: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8891  b165af28-f235-49b4-aa5d-6385a2dbe932 17625 0 2023-09-04 15:08:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-04 15:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:08:55.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8891" for this suite. 09/04/23 15:08:55.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:08:55.486
Sep  4 15:08:55.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 09/04/23 15:08:55.487
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:08:55.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:08:55.546
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 09/04/23 15:08:55.568
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local;sleep 1; done
 09/04/23 15:08:55.581
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local;sleep 1; done
 09/04/23 15:08:55.581
STEP: creating a pod to probe DNS 09/04/23 15:08:55.581
STEP: submitting the pod to kubernetes 09/04/23 15:08:55.581
Sep  4 15:08:55.602: INFO: Waiting up to 15m0s for pod "dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973" in namespace "dns-7066" to be "running"
Sep  4 15:08:55.614: INFO: Pod "dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973": Phase="Pending", Reason="", readiness=false. Elapsed: 11.987446ms
Sep  4 15:08:57.626: INFO: Pod "dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973": Phase="Running", Reason="", readiness=true. Elapsed: 2.024282929s
Sep  4 15:08:57.626: INFO: Pod "dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973" satisfied condition "running"
STEP: retrieving the pod 09/04/23 15:08:57.626
STEP: looking for the results for each expected name from probers 09/04/23 15:08:57.639
Sep  4 15:08:57.772: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:08:57.825: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:08:57.849: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:08:57.873: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:08:57.897: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:08:57.922: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:08:57.955: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:08:57.979: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:08:57.979: INFO: Lookups using dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local]

Sep  4 15:09:03.004: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:03.056: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:03.080: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:03.105: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:03.128: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:03.151: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:03.175: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:03.199: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:03.199: INFO: Lookups using dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local]

Sep  4 15:09:08.004: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:08.056: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:08.080: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:08.103: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:08.126: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:08.150: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:08.173: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:08.206: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:08.206: INFO: Lookups using dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local]

Sep  4 15:09:13.005: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:13.061: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:13.086: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:13.109: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:13.133: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:13.157: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:13.180: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:13.203: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:13.203: INFO: Lookups using dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local]

Sep  4 15:09:18.004: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:18.056: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:18.080: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:18.104: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:18.127: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:18.150: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:18.173: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:18.196: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:18.196: INFO: Lookups using dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local]

Sep  4 15:09:23.006: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:23.045: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:23.101: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:23.125: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:23.149: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:23.175: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:23.198: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:23.223: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
Sep  4 15:09:23.223: INFO: Lookups using dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local]

Sep  4 15:09:28.203: INFO: DNS probes using dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973 succeeded

STEP: deleting the pod 09/04/23 15:09:28.203
STEP: deleting the test headless service 09/04/23 15:09:28.221
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 15:09:28.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7066" for this suite. 09/04/23 15:09:28.259
------------------------------
• [32.787 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:08:55.486
    Sep  4 15:08:55.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 09/04/23 15:08:55.487
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:08:55.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:08:55.546
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 09/04/23 15:08:55.568
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local;sleep 1; done
     09/04/23 15:08:55.581
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7066.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local;sleep 1; done
     09/04/23 15:08:55.581
    STEP: creating a pod to probe DNS 09/04/23 15:08:55.581
    STEP: submitting the pod to kubernetes 09/04/23 15:08:55.581
    Sep  4 15:08:55.602: INFO: Waiting up to 15m0s for pod "dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973" in namespace "dns-7066" to be "running"
    Sep  4 15:08:55.614: INFO: Pod "dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973": Phase="Pending", Reason="", readiness=false. Elapsed: 11.987446ms
    Sep  4 15:08:57.626: INFO: Pod "dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973": Phase="Running", Reason="", readiness=true. Elapsed: 2.024282929s
    Sep  4 15:08:57.626: INFO: Pod "dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 15:08:57.626
    STEP: looking for the results for each expected name from probers 09/04/23 15:08:57.639
    Sep  4 15:08:57.772: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:08:57.825: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:08:57.849: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:08:57.873: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:08:57.897: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:08:57.922: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:08:57.955: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:08:57.979: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:08:57.979: INFO: Lookups using dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local]

    Sep  4 15:09:03.004: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:03.056: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:03.080: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:03.105: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:03.128: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:03.151: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:03.175: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:03.199: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:03.199: INFO: Lookups using dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local]

    Sep  4 15:09:08.004: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:08.056: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:08.080: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:08.103: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:08.126: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:08.150: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:08.173: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:08.206: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:08.206: INFO: Lookups using dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local]

    Sep  4 15:09:13.005: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:13.061: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:13.086: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:13.109: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:13.133: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:13.157: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:13.180: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:13.203: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:13.203: INFO: Lookups using dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local]

    Sep  4 15:09:18.004: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:18.056: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:18.080: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:18.104: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:18.127: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:18.150: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:18.173: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:18.196: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:18.196: INFO: Lookups using dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local]

    Sep  4 15:09:23.006: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:23.045: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:23.101: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:23.125: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:23.149: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:23.175: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:23.198: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:23.223: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local from pod dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973: the server could not find the requested resource (get pods dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973)
    Sep  4 15:09:23.223: INFO: Lookups using dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7066.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7066.svc.cluster.local jessie_udp@dns-test-service-2.dns-7066.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7066.svc.cluster.local]

    Sep  4 15:09:28.203: INFO: DNS probes using dns-7066/dns-test-e37a367b-7fb3-408b-8aa6-5f2cd4114973 succeeded

    STEP: deleting the pod 09/04/23 15:09:28.203
    STEP: deleting the test headless service 09/04/23 15:09:28.221
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:09:28.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7066" for this suite. 09/04/23 15:09:28.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:09:28.273
Sep  4 15:09:28.273: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 15:09:28.274
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:09:28.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:09:28.334
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 09/04/23 15:09:28.357
Sep  4 15:09:28.376: INFO: Waiting up to 5m0s for pod "pod-7952a801-0a86-49f0-8471-b5e459e2e946" in namespace "emptydir-9923" to be "Succeeded or Failed"
Sep  4 15:09:28.389: INFO: Pod "pod-7952a801-0a86-49f0-8471-b5e459e2e946": Phase="Pending", Reason="", readiness=false. Elapsed: 12.674776ms
Sep  4 15:09:30.402: INFO: Pod "pod-7952a801-0a86-49f0-8471-b5e459e2e946": Phase="Running", Reason="", readiness=false. Elapsed: 2.025756561s
Sep  4 15:09:32.402: INFO: Pod "pod-7952a801-0a86-49f0-8471-b5e459e2e946": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02572188s
STEP: Saw pod success 09/04/23 15:09:32.402
Sep  4 15:09:32.402: INFO: Pod "pod-7952a801-0a86-49f0-8471-b5e459e2e946" satisfied condition "Succeeded or Failed"
Sep  4 15:09:32.415: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-7952a801-0a86-49f0-8471-b5e459e2e946 container test-container: <nil>
STEP: delete the pod 09/04/23 15:09:32.448
Sep  4 15:09:32.472: INFO: Waiting for pod pod-7952a801-0a86-49f0-8471-b5e459e2e946 to disappear
Sep  4 15:09:32.484: INFO: Pod pod-7952a801-0a86-49f0-8471-b5e459e2e946 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 15:09:32.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9923" for this suite. 09/04/23 15:09:32.506
------------------------------
• [4.246 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:09:28.273
    Sep  4 15:09:28.273: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 15:09:28.274
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:09:28.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:09:28.334
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/04/23 15:09:28.357
    Sep  4 15:09:28.376: INFO: Waiting up to 5m0s for pod "pod-7952a801-0a86-49f0-8471-b5e459e2e946" in namespace "emptydir-9923" to be "Succeeded or Failed"
    Sep  4 15:09:28.389: INFO: Pod "pod-7952a801-0a86-49f0-8471-b5e459e2e946": Phase="Pending", Reason="", readiness=false. Elapsed: 12.674776ms
    Sep  4 15:09:30.402: INFO: Pod "pod-7952a801-0a86-49f0-8471-b5e459e2e946": Phase="Running", Reason="", readiness=false. Elapsed: 2.025756561s
    Sep  4 15:09:32.402: INFO: Pod "pod-7952a801-0a86-49f0-8471-b5e459e2e946": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02572188s
    STEP: Saw pod success 09/04/23 15:09:32.402
    Sep  4 15:09:32.402: INFO: Pod "pod-7952a801-0a86-49f0-8471-b5e459e2e946" satisfied condition "Succeeded or Failed"
    Sep  4 15:09:32.415: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-7952a801-0a86-49f0-8471-b5e459e2e946 container test-container: <nil>
    STEP: delete the pod 09/04/23 15:09:32.448
    Sep  4 15:09:32.472: INFO: Waiting for pod pod-7952a801-0a86-49f0-8471-b5e459e2e946 to disappear
    Sep  4 15:09:32.484: INFO: Pod pod-7952a801-0a86-49f0-8471-b5e459e2e946 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:09:32.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9923" for this suite. 09/04/23 15:09:32.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:09:32.52
Sep  4 15:09:32.520: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:09:32.521
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:09:32.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:09:32.578
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 09/04/23 15:09:32.599
Sep  4 15:09:32.620: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8" in namespace "projected-7544" to be "Succeeded or Failed"
Sep  4 15:09:32.631: INFO: Pod "downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.423535ms
Sep  4 15:09:34.646: INFO: Pod "downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025919918s
Sep  4 15:09:36.648: INFO: Pod "downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027586708s
STEP: Saw pod success 09/04/23 15:09:36.648
Sep  4 15:09:36.648: INFO: Pod "downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8" satisfied condition "Succeeded or Failed"
Sep  4 15:09:36.661: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8 container client-container: <nil>
STEP: delete the pod 09/04/23 15:09:36.74
Sep  4 15:09:36.758: INFO: Waiting for pod downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8 to disappear
Sep  4 15:09:36.770: INFO: Pod downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 15:09:36.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7544" for this suite. 09/04/23 15:09:36.793
------------------------------
• [4.288 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:09:32.52
    Sep  4 15:09:32.520: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:09:32.521
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:09:32.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:09:32.578
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 09/04/23 15:09:32.599
    Sep  4 15:09:32.620: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8" in namespace "projected-7544" to be "Succeeded or Failed"
    Sep  4 15:09:32.631: INFO: Pod "downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.423535ms
    Sep  4 15:09:34.646: INFO: Pod "downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025919918s
    Sep  4 15:09:36.648: INFO: Pod "downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027586708s
    STEP: Saw pod success 09/04/23 15:09:36.648
    Sep  4 15:09:36.648: INFO: Pod "downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8" satisfied condition "Succeeded or Failed"
    Sep  4 15:09:36.661: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8 container client-container: <nil>
    STEP: delete the pod 09/04/23 15:09:36.74
    Sep  4 15:09:36.758: INFO: Waiting for pod downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8 to disappear
    Sep  4 15:09:36.770: INFO: Pod downwardapi-volume-5b043743-0891-47e3-82ca-16d75cff99d8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:09:36.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7544" for this suite. 09/04/23 15:09:36.793
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:09:36.808
Sep  4 15:09:36.809: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 09/04/23 15:09:36.809
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:09:36.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:09:36.87
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 09/04/23 15:09:36.894
Sep  4 15:09:36.916: INFO: Waiting up to 2m0s for pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32" in namespace "var-expansion-7814" to be "running"
Sep  4 15:09:36.929: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 12.595779ms
Sep  4 15:09:38.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026423062s
Sep  4 15:09:40.945: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028545471s
Sep  4 15:09:42.944: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02827929s
Sep  4 15:09:44.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025975682s
Sep  4 15:09:46.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026143306s
Sep  4 15:09:48.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 12.026699699s
Sep  4 15:09:50.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026731631s
Sep  4 15:09:52.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 16.026340897s
Sep  4 15:09:54.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 18.025986344s
Sep  4 15:09:56.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 20.026573383s
Sep  4 15:09:58.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 22.025861318s
Sep  4 15:10:00.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 24.026071927s
Sep  4 15:10:02.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 26.026798949s
Sep  4 15:10:04.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 28.025991741s
Sep  4 15:10:06.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 30.026727967s
Sep  4 15:10:08.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 32.025592464s
Sep  4 15:10:10.944: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 34.027414203s
Sep  4 15:10:12.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 36.026202042s
Sep  4 15:10:14.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 38.026861572s
Sep  4 15:10:16.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 40.026147435s
Sep  4 15:10:18.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 42.026861609s
Sep  4 15:10:20.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 44.025897471s
Sep  4 15:10:22.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 46.025459672s
Sep  4 15:10:24.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 48.025517107s
Sep  4 15:10:26.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 50.026474616s
Sep  4 15:10:28.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 52.026033244s
Sep  4 15:10:30.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 54.026829217s
Sep  4 15:10:32.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 56.027044215s
Sep  4 15:10:34.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 58.027344162s
Sep  4 15:10:36.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.025894822s
Sep  4 15:10:38.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.026377065s
Sep  4 15:10:40.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.027140539s
Sep  4 15:10:42.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.026930687s
Sep  4 15:10:44.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025969865s
Sep  4 15:10:46.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.02665761s
Sep  4 15:10:48.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.027300196s
Sep  4 15:10:50.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.025655101s
Sep  4 15:10:52.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.026389822s
Sep  4 15:10:54.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.025931923s
Sep  4 15:10:56.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.026841558s
Sep  4 15:10:58.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.02559634s
Sep  4 15:11:00.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.026222978s
Sep  4 15:11:02.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.025540258s
Sep  4 15:11:04.944: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.02799921s
Sep  4 15:11:06.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.026484382s
Sep  4 15:11:08.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.026129958s
Sep  4 15:11:10.944: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.027767704s
Sep  4 15:11:12.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.02643804s
Sep  4 15:11:14.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.025702381s
Sep  4 15:11:16.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.026717991s
Sep  4 15:11:18.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.025972709s
Sep  4 15:11:20.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.025655578s
Sep  4 15:11:22.944: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.028121334s
Sep  4 15:11:24.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.026989674s
Sep  4 15:11:26.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.025922896s
Sep  4 15:11:28.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.027003289s
Sep  4 15:11:30.944: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.027618152s
Sep  4 15:11:32.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.026204313s
Sep  4 15:11:34.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.02661556s
Sep  4 15:11:36.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.026234871s
Sep  4 15:11:36.954: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.038005405s
STEP: updating the pod 09/04/23 15:11:36.954
Sep  4 15:11:37.481: INFO: Successfully updated pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32"
STEP: waiting for pod running 09/04/23 15:11:37.481
Sep  4 15:11:37.481: INFO: Waiting up to 2m0s for pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32" in namespace "var-expansion-7814" to be "running"
Sep  4 15:11:37.493: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 11.893786ms
Sep  4 15:11:39.506: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Running", Reason="", readiness=true. Elapsed: 2.02524722s
Sep  4 15:11:39.506: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32" satisfied condition "running"
STEP: deleting the pod gracefully 09/04/23 15:11:39.506
Sep  4 15:11:39.507: INFO: Deleting pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32" in namespace "var-expansion-7814"
Sep  4 15:11:39.521: INFO: Wait up to 5m0s for pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 15:12:11.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7814" for this suite. 09/04/23 15:12:11.568
------------------------------
• [154.772 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:09:36.808
    Sep  4 15:09:36.809: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 09/04/23 15:09:36.809
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:09:36.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:09:36.87
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 09/04/23 15:09:36.894
    Sep  4 15:09:36.916: INFO: Waiting up to 2m0s for pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32" in namespace "var-expansion-7814" to be "running"
    Sep  4 15:09:36.929: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 12.595779ms
    Sep  4 15:09:38.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026423062s
    Sep  4 15:09:40.945: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028545471s
    Sep  4 15:09:42.944: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02827929s
    Sep  4 15:09:44.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025975682s
    Sep  4 15:09:46.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026143306s
    Sep  4 15:09:48.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 12.026699699s
    Sep  4 15:09:50.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026731631s
    Sep  4 15:09:52.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 16.026340897s
    Sep  4 15:09:54.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 18.025986344s
    Sep  4 15:09:56.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 20.026573383s
    Sep  4 15:09:58.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 22.025861318s
    Sep  4 15:10:00.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 24.026071927s
    Sep  4 15:10:02.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 26.026798949s
    Sep  4 15:10:04.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 28.025991741s
    Sep  4 15:10:06.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 30.026727967s
    Sep  4 15:10:08.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 32.025592464s
    Sep  4 15:10:10.944: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 34.027414203s
    Sep  4 15:10:12.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 36.026202042s
    Sep  4 15:10:14.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 38.026861572s
    Sep  4 15:10:16.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 40.026147435s
    Sep  4 15:10:18.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 42.026861609s
    Sep  4 15:10:20.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 44.025897471s
    Sep  4 15:10:22.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 46.025459672s
    Sep  4 15:10:24.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 48.025517107s
    Sep  4 15:10:26.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 50.026474616s
    Sep  4 15:10:28.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 52.026033244s
    Sep  4 15:10:30.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 54.026829217s
    Sep  4 15:10:32.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 56.027044215s
    Sep  4 15:10:34.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 58.027344162s
    Sep  4 15:10:36.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.025894822s
    Sep  4 15:10:38.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.026377065s
    Sep  4 15:10:40.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.027140539s
    Sep  4 15:10:42.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.026930687s
    Sep  4 15:10:44.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025969865s
    Sep  4 15:10:46.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.02665761s
    Sep  4 15:10:48.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.027300196s
    Sep  4 15:10:50.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.025655101s
    Sep  4 15:10:52.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.026389822s
    Sep  4 15:10:54.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.025931923s
    Sep  4 15:10:56.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.026841558s
    Sep  4 15:10:58.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.02559634s
    Sep  4 15:11:00.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.026222978s
    Sep  4 15:11:02.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.025540258s
    Sep  4 15:11:04.944: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.02799921s
    Sep  4 15:11:06.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.026484382s
    Sep  4 15:11:08.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.026129958s
    Sep  4 15:11:10.944: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.027767704s
    Sep  4 15:11:12.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.02643804s
    Sep  4 15:11:14.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.025702381s
    Sep  4 15:11:16.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.026717991s
    Sep  4 15:11:18.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.025972709s
    Sep  4 15:11:20.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.025655578s
    Sep  4 15:11:22.944: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.028121334s
    Sep  4 15:11:24.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.026989674s
    Sep  4 15:11:26.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.025922896s
    Sep  4 15:11:28.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.027003289s
    Sep  4 15:11:30.944: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.027618152s
    Sep  4 15:11:32.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.026204313s
    Sep  4 15:11:34.943: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.02661556s
    Sep  4 15:11:36.942: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.026234871s
    Sep  4 15:11:36.954: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.038005405s
    STEP: updating the pod 09/04/23 15:11:36.954
    Sep  4 15:11:37.481: INFO: Successfully updated pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32"
    STEP: waiting for pod running 09/04/23 15:11:37.481
    Sep  4 15:11:37.481: INFO: Waiting up to 2m0s for pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32" in namespace "var-expansion-7814" to be "running"
    Sep  4 15:11:37.493: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Pending", Reason="", readiness=false. Elapsed: 11.893786ms
    Sep  4 15:11:39.506: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32": Phase="Running", Reason="", readiness=true. Elapsed: 2.02524722s
    Sep  4 15:11:39.506: INFO: Pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32" satisfied condition "running"
    STEP: deleting the pod gracefully 09/04/23 15:11:39.506
    Sep  4 15:11:39.507: INFO: Deleting pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32" in namespace "var-expansion-7814"
    Sep  4 15:11:39.521: INFO: Wait up to 5m0s for pod "var-expansion-05097e9a-088c-4ed6-92b2-00de99944d32" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:12:11.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7814" for this suite. 09/04/23 15:12:11.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:12:11.581
Sep  4 15:12:11.581: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:12:11.582
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:11.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:11.639
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-6e554f71-f6bc-47dc-9de0-3cd74784279b 09/04/23 15:12:11.661
STEP: Creating a pod to test consume configMaps 09/04/23 15:12:11.673
Sep  4 15:12:11.697: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d" in namespace "projected-6439" to be "Succeeded or Failed"
Sep  4 15:12:11.708: INFO: Pod "pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.061379ms
Sep  4 15:12:13.721: INFO: Pod "pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d": Phase="Running", Reason="", readiness=false. Elapsed: 2.023733051s
Sep  4 15:12:15.721: INFO: Pod "pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024058648s
STEP: Saw pod success 09/04/23 15:12:15.721
Sep  4 15:12:15.721: INFO: Pod "pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d" satisfied condition "Succeeded or Failed"
Sep  4 15:12:15.733: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d container projected-configmap-volume-test: <nil>
STEP: delete the pod 09/04/23 15:12:15.82
Sep  4 15:12:15.835: INFO: Waiting for pod pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d to disappear
Sep  4 15:12:15.846: INFO: Pod pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:12:15.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6439" for this suite. 09/04/23 15:12:15.867
------------------------------
• [4.299 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:12:11.581
    Sep  4 15:12:11.581: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:12:11.582
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:11.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:11.639
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-6e554f71-f6bc-47dc-9de0-3cd74784279b 09/04/23 15:12:11.661
    STEP: Creating a pod to test consume configMaps 09/04/23 15:12:11.673
    Sep  4 15:12:11.697: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d" in namespace "projected-6439" to be "Succeeded or Failed"
    Sep  4 15:12:11.708: INFO: Pod "pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.061379ms
    Sep  4 15:12:13.721: INFO: Pod "pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d": Phase="Running", Reason="", readiness=false. Elapsed: 2.023733051s
    Sep  4 15:12:15.721: INFO: Pod "pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024058648s
    STEP: Saw pod success 09/04/23 15:12:15.721
    Sep  4 15:12:15.721: INFO: Pod "pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d" satisfied condition "Succeeded or Failed"
    Sep  4 15:12:15.733: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d container projected-configmap-volume-test: <nil>
    STEP: delete the pod 09/04/23 15:12:15.82
    Sep  4 15:12:15.835: INFO: Waiting for pod pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d to disappear
    Sep  4 15:12:15.846: INFO: Pod pod-projected-configmaps-026416ce-7452-471d-bbf3-1f3c59687b6d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:12:15.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6439" for this suite. 09/04/23 15:12:15.867
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:12:15.881
Sep  4 15:12:15.881: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 09/04/23 15:12:15.881
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:15.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:15.938
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-64eb7521-90f9-401c-b775-f8fdc3ab3cde 09/04/23 15:12:15.959
STEP: Creating a pod to test consume secrets 09/04/23 15:12:15.971
Sep  4 15:12:15.988: INFO: Waiting up to 5m0s for pod "pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7" in namespace "secrets-3375" to be "Succeeded or Failed"
Sep  4 15:12:15.999: INFO: Pod "pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.483472ms
Sep  4 15:12:18.012: INFO: Pod "pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024245588s
Sep  4 15:12:20.012: INFO: Pod "pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024106078s
STEP: Saw pod success 09/04/23 15:12:20.012
Sep  4 15:12:20.012: INFO: Pod "pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7" satisfied condition "Succeeded or Failed"
Sep  4 15:12:20.024: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7 container secret-env-test: <nil>
STEP: delete the pod 09/04/23 15:12:20.056
Sep  4 15:12:20.071: INFO: Waiting for pod pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7 to disappear
Sep  4 15:12:20.082: INFO: Pod pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 15:12:20.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3375" for this suite. 09/04/23 15:12:20.105
------------------------------
• [4.238 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:12:15.881
    Sep  4 15:12:15.881: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 09/04/23 15:12:15.881
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:15.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:15.938
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-64eb7521-90f9-401c-b775-f8fdc3ab3cde 09/04/23 15:12:15.959
    STEP: Creating a pod to test consume secrets 09/04/23 15:12:15.971
    Sep  4 15:12:15.988: INFO: Waiting up to 5m0s for pod "pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7" in namespace "secrets-3375" to be "Succeeded or Failed"
    Sep  4 15:12:15.999: INFO: Pod "pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.483472ms
    Sep  4 15:12:18.012: INFO: Pod "pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024245588s
    Sep  4 15:12:20.012: INFO: Pod "pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024106078s
    STEP: Saw pod success 09/04/23 15:12:20.012
    Sep  4 15:12:20.012: INFO: Pod "pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7" satisfied condition "Succeeded or Failed"
    Sep  4 15:12:20.024: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7 container secret-env-test: <nil>
    STEP: delete the pod 09/04/23 15:12:20.056
    Sep  4 15:12:20.071: INFO: Waiting for pod pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7 to disappear
    Sep  4 15:12:20.082: INFO: Pod pod-secrets-d0479180-bfeb-4b49-aa68-aec0d8e917a7 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:12:20.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3375" for this suite. 09/04/23 15:12:20.105
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:12:20.118
Sep  4 15:12:20.118: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 15:12:20.119
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:20.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:20.176
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/04/23 15:12:20.212
Sep  4 15:12:20.230: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9689" to be "running and ready"
Sep  4 15:12:20.241: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.699703ms
Sep  4 15:12:20.241: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:12:22.254: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.024117684s
Sep  4 15:12:22.254: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  4 15:12:22.254: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 09/04/23 15:12:22.266
Sep  4 15:12:22.282: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9689" to be "running and ready"
Sep  4 15:12:22.294: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.500262ms
Sep  4 15:12:22.294: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:12:24.309: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.026325818s
Sep  4 15:12:24.309: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Sep  4 15:12:24.309: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/04/23 15:12:24.321
Sep  4 15:12:24.335: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  4 15:12:24.347: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  4 15:12:26.347: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  4 15:12:26.360: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  4 15:12:28.347: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  4 15:12:28.360: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 09/04/23 15:12:28.36
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  4 15:12:28.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9689" for this suite. 09/04/23 15:12:28.462
------------------------------
• [8.357 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:12:20.118
    Sep  4 15:12:20.118: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 15:12:20.119
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:20.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:20.176
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/04/23 15:12:20.212
    Sep  4 15:12:20.230: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9689" to be "running and ready"
    Sep  4 15:12:20.241: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.699703ms
    Sep  4 15:12:20.241: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:12:22.254: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.024117684s
    Sep  4 15:12:22.254: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  4 15:12:22.254: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 09/04/23 15:12:22.266
    Sep  4 15:12:22.282: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9689" to be "running and ready"
    Sep  4 15:12:22.294: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.500262ms
    Sep  4 15:12:22.294: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:12:24.309: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.026325818s
    Sep  4 15:12:24.309: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Sep  4 15:12:24.309: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/04/23 15:12:24.321
    Sep  4 15:12:24.335: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  4 15:12:24.347: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep  4 15:12:26.347: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  4 15:12:26.360: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep  4 15:12:28.347: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  4 15:12:28.360: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 09/04/23 15:12:28.36
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:12:28.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9689" for this suite. 09/04/23 15:12:28.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:12:28.476
Sep  4 15:12:28.476: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 15:12:28.477
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:28.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:28.536
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 09/04/23 15:12:28.557
Sep  4 15:12:28.576: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-65b7c658-616d-4a3f-aad7-058a0969762c" in namespace "emptydir-4035" to be "running"
Sep  4 15:12:28.588: INFO: Pod "pod-sharedvolume-65b7c658-616d-4a3f-aad7-058a0969762c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.140527ms
Sep  4 15:12:30.601: INFO: Pod "pod-sharedvolume-65b7c658-616d-4a3f-aad7-058a0969762c": Phase="Running", Reason="", readiness=false. Elapsed: 2.02532307s
Sep  4 15:12:30.601: INFO: Pod "pod-sharedvolume-65b7c658-616d-4a3f-aad7-058a0969762c" satisfied condition "running"
STEP: Reading file content from the nginx-container 09/04/23 15:12:30.601
Sep  4 15:12:30.601: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4035 PodName:pod-sharedvolume-65b7c658-616d-4a3f-aad7-058a0969762c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:12:30.601: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:12:30.602: INFO: ExecWithOptions: Clientset creation
Sep  4 15:12:30.602: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/emptydir-4035/pods/pod-sharedvolume-65b7c658-616d-4a3f-aad7-058a0969762c/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Sep  4 15:12:30.968: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 15:12:30.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4035" for this suite. 09/04/23 15:12:30.991
------------------------------
• [2.528 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:12:28.476
    Sep  4 15:12:28.476: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 15:12:28.477
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:28.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:28.536
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 09/04/23 15:12:28.557
    Sep  4 15:12:28.576: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-65b7c658-616d-4a3f-aad7-058a0969762c" in namespace "emptydir-4035" to be "running"
    Sep  4 15:12:28.588: INFO: Pod "pod-sharedvolume-65b7c658-616d-4a3f-aad7-058a0969762c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.140527ms
    Sep  4 15:12:30.601: INFO: Pod "pod-sharedvolume-65b7c658-616d-4a3f-aad7-058a0969762c": Phase="Running", Reason="", readiness=false. Elapsed: 2.02532307s
    Sep  4 15:12:30.601: INFO: Pod "pod-sharedvolume-65b7c658-616d-4a3f-aad7-058a0969762c" satisfied condition "running"
    STEP: Reading file content from the nginx-container 09/04/23 15:12:30.601
    Sep  4 15:12:30.601: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4035 PodName:pod-sharedvolume-65b7c658-616d-4a3f-aad7-058a0969762c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:12:30.601: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:12:30.602: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:12:30.602: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/emptydir-4035/pods/pod-sharedvolume-65b7c658-616d-4a3f-aad7-058a0969762c/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Sep  4 15:12:30.968: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:12:30.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4035" for this suite. 09/04/23 15:12:30.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:12:31.004
Sep  4 15:12:31.005: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 09/04/23 15:12:31.005
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:31.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:31.065
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  4 15:12:31.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5120" for this suite. 09/04/23 15:12:31.115
------------------------------
• [0.124 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:12:31.004
    Sep  4 15:12:31.005: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 09/04/23 15:12:31.005
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:31.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:31.065
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:12:31.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5120" for this suite. 09/04/23 15:12:31.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:12:31.129
Sep  4 15:12:31.129: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 09/04/23 15:12:31.129
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:31.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:31.187
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 09/04/23 15:12:31.222
STEP: delete the rc 09/04/23 15:12:36.248
STEP: wait for the rc to be deleted 09/04/23 15:12:36.261
Sep  4 15:12:37.309: INFO: 93 pods remaining
Sep  4 15:12:37.309: INFO: 93 pods has nil DeletionTimestamp
Sep  4 15:12:37.309: INFO: 
Sep  4 15:12:38.303: INFO: 70 pods remaining
Sep  4 15:12:38.303: INFO: 70 pods has nil DeletionTimestamp
Sep  4 15:12:38.303: INFO: 
Sep  4 15:12:39.301: INFO: 70 pods remaining
Sep  4 15:12:39.301: INFO: 70 pods has nil DeletionTimestamp
Sep  4 15:12:39.301: INFO: 
Sep  4 15:12:40.318: INFO: 40 pods remaining
Sep  4 15:12:40.318: INFO: 40 pods has nil DeletionTimestamp
Sep  4 15:12:40.318: INFO: 
Sep  4 15:12:41.319: INFO: 40 pods remaining
Sep  4 15:12:41.320: INFO: 40 pods has nil DeletionTimestamp
Sep  4 15:12:41.320: INFO: 
Sep  4 15:12:42.305: INFO: 13 pods remaining
Sep  4 15:12:42.305: INFO: 13 pods has nil DeletionTimestamp
Sep  4 15:12:42.305: INFO: 
STEP: Gathering metrics 09/04/23 15:12:43.287
W0904 15:12:43.317225    7754 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep  4 15:12:43.317: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  4 15:12:43.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2314" for this suite. 09/04/23 15:12:43.332
------------------------------
• [12.218 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:12:31.129
    Sep  4 15:12:31.129: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 09/04/23 15:12:31.129
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:31.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:31.187
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 09/04/23 15:12:31.222
    STEP: delete the rc 09/04/23 15:12:36.248
    STEP: wait for the rc to be deleted 09/04/23 15:12:36.261
    Sep  4 15:12:37.309: INFO: 93 pods remaining
    Sep  4 15:12:37.309: INFO: 93 pods has nil DeletionTimestamp
    Sep  4 15:12:37.309: INFO: 
    Sep  4 15:12:38.303: INFO: 70 pods remaining
    Sep  4 15:12:38.303: INFO: 70 pods has nil DeletionTimestamp
    Sep  4 15:12:38.303: INFO: 
    Sep  4 15:12:39.301: INFO: 70 pods remaining
    Sep  4 15:12:39.301: INFO: 70 pods has nil DeletionTimestamp
    Sep  4 15:12:39.301: INFO: 
    Sep  4 15:12:40.318: INFO: 40 pods remaining
    Sep  4 15:12:40.318: INFO: 40 pods has nil DeletionTimestamp
    Sep  4 15:12:40.318: INFO: 
    Sep  4 15:12:41.319: INFO: 40 pods remaining
    Sep  4 15:12:41.320: INFO: 40 pods has nil DeletionTimestamp
    Sep  4 15:12:41.320: INFO: 
    Sep  4 15:12:42.305: INFO: 13 pods remaining
    Sep  4 15:12:42.305: INFO: 13 pods has nil DeletionTimestamp
    Sep  4 15:12:42.305: INFO: 
    STEP: Gathering metrics 09/04/23 15:12:43.287
    W0904 15:12:43.317225    7754 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep  4 15:12:43.317: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:12:43.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2314" for this suite. 09/04/23 15:12:43.332
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:12:43.346
Sep  4 15:12:43.346: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 15:12:43.347
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:43.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:43.405
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1617 09/04/23 15:12:43.427
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/04/23 15:12:43.447
STEP: creating service externalsvc in namespace services-1617 09/04/23 15:12:43.447
STEP: creating replication controller externalsvc in namespace services-1617 09/04/23 15:12:43.464
I0904 15:12:43.477321    7754 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1617, replica count: 2
I0904 15:12:46.528632    7754 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0904 15:12:49.531454    7754 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 09/04/23 15:12:49.543
Sep  4 15:12:49.573: INFO: Creating new exec pod
Sep  4 15:12:49.590: INFO: Waiting up to 5m0s for pod "execpodhxjxg" in namespace "services-1617" to be "running"
Sep  4 15:12:49.602: INFO: Pod "execpodhxjxg": Phase="Pending", Reason="", readiness=false. Elapsed: 12.045073ms
Sep  4 15:12:51.616: INFO: Pod "execpodhxjxg": Phase="Running", Reason="", readiness=true. Elapsed: 2.025584044s
Sep  4 15:12:51.616: INFO: Pod "execpodhxjxg" satisfied condition "running"
Sep  4 15:12:51.616: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1617 exec execpodhxjxg -- /bin/sh -x -c nslookup clusterip-service.services-1617.svc.cluster.local'
Sep  4 15:12:52.207: INFO: stderr: "+ nslookup clusterip-service.services-1617.svc.cluster.local\n"
Sep  4 15:12:52.207: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nclusterip-service.services-1617.svc.cluster.local.svc.cluster.local\tcanonical name = externalsvc.services-1617.svc.cluster.local.svc.cluster.local.\nName:\texternalsvc.services-1617.svc.cluster.local.svc.cluster.local\nAddress: 100.107.74.219\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1617, will wait for the garbage collector to delete the pods 09/04/23 15:12:52.207
Sep  4 15:12:52.285: INFO: Deleting ReplicationController externalsvc took: 14.363686ms
Sep  4 15:12:52.386: INFO: Terminating ReplicationController externalsvc pods took: 100.662221ms
Sep  4 15:12:54.410: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 15:12:54.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1617" for this suite. 09/04/23 15:12:54.447
------------------------------
• [11.114 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:12:43.346
    Sep  4 15:12:43.346: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 15:12:43.347
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:43.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:43.405
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1617 09/04/23 15:12:43.427
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/04/23 15:12:43.447
    STEP: creating service externalsvc in namespace services-1617 09/04/23 15:12:43.447
    STEP: creating replication controller externalsvc in namespace services-1617 09/04/23 15:12:43.464
    I0904 15:12:43.477321    7754 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1617, replica count: 2
    I0904 15:12:46.528632    7754 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0904 15:12:49.531454    7754 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 09/04/23 15:12:49.543
    Sep  4 15:12:49.573: INFO: Creating new exec pod
    Sep  4 15:12:49.590: INFO: Waiting up to 5m0s for pod "execpodhxjxg" in namespace "services-1617" to be "running"
    Sep  4 15:12:49.602: INFO: Pod "execpodhxjxg": Phase="Pending", Reason="", readiness=false. Elapsed: 12.045073ms
    Sep  4 15:12:51.616: INFO: Pod "execpodhxjxg": Phase="Running", Reason="", readiness=true. Elapsed: 2.025584044s
    Sep  4 15:12:51.616: INFO: Pod "execpodhxjxg" satisfied condition "running"
    Sep  4 15:12:51.616: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1617 exec execpodhxjxg -- /bin/sh -x -c nslookup clusterip-service.services-1617.svc.cluster.local'
    Sep  4 15:12:52.207: INFO: stderr: "+ nslookup clusterip-service.services-1617.svc.cluster.local\n"
    Sep  4 15:12:52.207: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nclusterip-service.services-1617.svc.cluster.local.svc.cluster.local\tcanonical name = externalsvc.services-1617.svc.cluster.local.svc.cluster.local.\nName:\texternalsvc.services-1617.svc.cluster.local.svc.cluster.local\nAddress: 100.107.74.219\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1617, will wait for the garbage collector to delete the pods 09/04/23 15:12:52.207
    Sep  4 15:12:52.285: INFO: Deleting ReplicationController externalsvc took: 14.363686ms
    Sep  4 15:12:52.386: INFO: Terminating ReplicationController externalsvc pods took: 100.662221ms
    Sep  4 15:12:54.410: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:12:54.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1617" for this suite. 09/04/23 15:12:54.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:12:54.461
Sep  4 15:12:54.461: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 15:12:54.462
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:54.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:54.521
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 09/04/23 15:12:54.543
Sep  4 15:12:54.543: INFO: Creating e2e-svc-a-j88m8
Sep  4 15:12:54.559: INFO: Creating e2e-svc-b-6b8k5
Sep  4 15:12:54.576: INFO: Creating e2e-svc-c-j5www
STEP: deleting service collection 09/04/23 15:12:54.604
Sep  4 15:12:54.642: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 15:12:54.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1732" for this suite. 09/04/23 15:12:54.655
------------------------------
• [0.207 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:12:54.461
    Sep  4 15:12:54.461: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 15:12:54.462
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:54.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:54.521
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 09/04/23 15:12:54.543
    Sep  4 15:12:54.543: INFO: Creating e2e-svc-a-j88m8
    Sep  4 15:12:54.559: INFO: Creating e2e-svc-b-6b8k5
    Sep  4 15:12:54.576: INFO: Creating e2e-svc-c-j5www
    STEP: deleting service collection 09/04/23 15:12:54.604
    Sep  4 15:12:54.642: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:12:54.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1732" for this suite. 09/04/23 15:12:54.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:12:54.669
Sep  4 15:12:54.669: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 15:12:54.669
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:54.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:54.727
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/04/23 15:12:54.762
Sep  4 15:12:54.782: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5948" to be "running and ready"
Sep  4 15:12:54.794: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.814517ms
Sep  4 15:12:54.794: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:12:56.807: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.024593593s
Sep  4 15:12:56.807: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  4 15:12:56.807: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 09/04/23 15:12:56.819
Sep  4 15:12:56.836: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5948" to be "running and ready"
Sep  4 15:12:56.848: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.753666ms
Sep  4 15:12:56.848: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:12:58.860: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.024652794s
Sep  4 15:12:58.861: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Sep  4 15:12:58.861: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/04/23 15:12:58.873
Sep  4 15:12:58.887: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  4 15:12:58.899: INFO: Pod pod-with-prestop-http-hook still exists
Sep  4 15:13:00.899: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  4 15:13:00.911: INFO: Pod pod-with-prestop-http-hook still exists
Sep  4 15:13:02.900: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  4 15:13:02.913: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 09/04/23 15:13:02.913
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  4 15:13:02.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5948" for this suite. 09/04/23 15:13:02.969
------------------------------
• [8.314 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:12:54.669
    Sep  4 15:12:54.669: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 15:12:54.669
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:12:54.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:12:54.727
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/04/23 15:12:54.762
    Sep  4 15:12:54.782: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5948" to be "running and ready"
    Sep  4 15:12:54.794: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.814517ms
    Sep  4 15:12:54.794: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:12:56.807: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.024593593s
    Sep  4 15:12:56.807: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  4 15:12:56.807: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 09/04/23 15:12:56.819
    Sep  4 15:12:56.836: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5948" to be "running and ready"
    Sep  4 15:12:56.848: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.753666ms
    Sep  4 15:12:56.848: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:12:58.860: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.024652794s
    Sep  4 15:12:58.861: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Sep  4 15:12:58.861: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/04/23 15:12:58.873
    Sep  4 15:12:58.887: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  4 15:12:58.899: INFO: Pod pod-with-prestop-http-hook still exists
    Sep  4 15:13:00.899: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  4 15:13:00.911: INFO: Pod pod-with-prestop-http-hook still exists
    Sep  4 15:13:02.900: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  4 15:13:02.913: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 09/04/23 15:13:02.913
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:13:02.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5948" for this suite. 09/04/23 15:13:02.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:13:02.984
Sep  4 15:13:02.985: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename controllerrevisions 09/04/23 15:13:02.986
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:13:03.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:13:03.048
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-fmhgt-daemon-set" 09/04/23 15:13:03.13
STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 15:13:03.144
Sep  4 15:13:03.169: INFO: Number of nodes with available pods controlled by daemonset e2e-fmhgt-daemon-set: 0
Sep  4 15:13:03.169: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:13:04.206: INFO: Number of nodes with available pods controlled by daemonset e2e-fmhgt-daemon-set: 0
Sep  4 15:13:04.206: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:13:05.204: INFO: Number of nodes with available pods controlled by daemonset e2e-fmhgt-daemon-set: 2
Sep  4 15:13:05.204: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-fmhgt-daemon-set
STEP: Confirm DaemonSet "e2e-fmhgt-daemon-set" successfully created with "daemonset-name=e2e-fmhgt-daemon-set" label 09/04/23 15:13:05.216
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-fmhgt-daemon-set" 09/04/23 15:13:05.241
Sep  4 15:13:05.253: INFO: Located ControllerRevision: "e2e-fmhgt-daemon-set-8567754f"
STEP: Patching ControllerRevision "e2e-fmhgt-daemon-set-8567754f" 09/04/23 15:13:05.266
Sep  4 15:13:05.279: INFO: e2e-fmhgt-daemon-set-8567754f has been patched
STEP: Create a new ControllerRevision 09/04/23 15:13:05.279
Sep  4 15:13:05.292: INFO: Created ControllerRevision: e2e-fmhgt-daemon-set-7c7b9dccdf
STEP: Confirm that there are two ControllerRevisions 09/04/23 15:13:05.292
Sep  4 15:13:05.292: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  4 15:13:05.304: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-fmhgt-daemon-set-8567754f" 09/04/23 15:13:05.304
STEP: Confirm that there is only one ControllerRevision 09/04/23 15:13:05.317
Sep  4 15:13:05.317: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  4 15:13:05.329: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-fmhgt-daemon-set-7c7b9dccdf" 09/04/23 15:13:05.341
Sep  4 15:13:05.367: INFO: e2e-fmhgt-daemon-set-7c7b9dccdf has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 09/04/23 15:13:05.367
W0904 15:13:05.381514    7754 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 09/04/23 15:13:05.381
Sep  4 15:13:05.381: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  4 15:13:06.394: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  4 15:13:06.408: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-fmhgt-daemon-set-7c7b9dccdf=updated" 09/04/23 15:13:06.408
STEP: Confirm that there is only one ControllerRevision 09/04/23 15:13:06.423
Sep  4 15:13:06.423: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  4 15:13:06.435: INFO: Found 1 ControllerRevisions
Sep  4 15:13:06.447: INFO: ControllerRevision "e2e-fmhgt-daemon-set-5948fbfb56" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-fmhgt-daemon-set" 09/04/23 15:13:06.46
STEP: deleting DaemonSet.extensions e2e-fmhgt-daemon-set in namespace controllerrevisions-645, will wait for the garbage collector to delete the pods 09/04/23 15:13:06.46
Sep  4 15:13:06.535: INFO: Deleting DaemonSet.extensions e2e-fmhgt-daemon-set took: 13.604056ms
Sep  4 15:13:06.636: INFO: Terminating DaemonSet.extensions e2e-fmhgt-daemon-set pods took: 100.628703ms
Sep  4 15:13:07.848: INFO: Number of nodes with available pods controlled by daemonset e2e-fmhgt-daemon-set: 0
Sep  4 15:13:07.848: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-fmhgt-daemon-set
Sep  4 15:13:07.860: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20156"},"items":null}

Sep  4 15:13:07.872: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20156"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:13:07.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-645" for this suite. 09/04/23 15:13:07.932
------------------------------
• [4.960 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:13:02.984
    Sep  4 15:13:02.985: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename controllerrevisions 09/04/23 15:13:02.986
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:13:03.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:13:03.048
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-fmhgt-daemon-set" 09/04/23 15:13:03.13
    STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 15:13:03.144
    Sep  4 15:13:03.169: INFO: Number of nodes with available pods controlled by daemonset e2e-fmhgt-daemon-set: 0
    Sep  4 15:13:03.169: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:13:04.206: INFO: Number of nodes with available pods controlled by daemonset e2e-fmhgt-daemon-set: 0
    Sep  4 15:13:04.206: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:13:05.204: INFO: Number of nodes with available pods controlled by daemonset e2e-fmhgt-daemon-set: 2
    Sep  4 15:13:05.204: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-fmhgt-daemon-set
    STEP: Confirm DaemonSet "e2e-fmhgt-daemon-set" successfully created with "daemonset-name=e2e-fmhgt-daemon-set" label 09/04/23 15:13:05.216
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-fmhgt-daemon-set" 09/04/23 15:13:05.241
    Sep  4 15:13:05.253: INFO: Located ControllerRevision: "e2e-fmhgt-daemon-set-8567754f"
    STEP: Patching ControllerRevision "e2e-fmhgt-daemon-set-8567754f" 09/04/23 15:13:05.266
    Sep  4 15:13:05.279: INFO: e2e-fmhgt-daemon-set-8567754f has been patched
    STEP: Create a new ControllerRevision 09/04/23 15:13:05.279
    Sep  4 15:13:05.292: INFO: Created ControllerRevision: e2e-fmhgt-daemon-set-7c7b9dccdf
    STEP: Confirm that there are two ControllerRevisions 09/04/23 15:13:05.292
    Sep  4 15:13:05.292: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  4 15:13:05.304: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-fmhgt-daemon-set-8567754f" 09/04/23 15:13:05.304
    STEP: Confirm that there is only one ControllerRevision 09/04/23 15:13:05.317
    Sep  4 15:13:05.317: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  4 15:13:05.329: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-fmhgt-daemon-set-7c7b9dccdf" 09/04/23 15:13:05.341
    Sep  4 15:13:05.367: INFO: e2e-fmhgt-daemon-set-7c7b9dccdf has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 09/04/23 15:13:05.367
    W0904 15:13:05.381514    7754 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 09/04/23 15:13:05.381
    Sep  4 15:13:05.381: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  4 15:13:06.394: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  4 15:13:06.408: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-fmhgt-daemon-set-7c7b9dccdf=updated" 09/04/23 15:13:06.408
    STEP: Confirm that there is only one ControllerRevision 09/04/23 15:13:06.423
    Sep  4 15:13:06.423: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  4 15:13:06.435: INFO: Found 1 ControllerRevisions
    Sep  4 15:13:06.447: INFO: ControllerRevision "e2e-fmhgt-daemon-set-5948fbfb56" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-fmhgt-daemon-set" 09/04/23 15:13:06.46
    STEP: deleting DaemonSet.extensions e2e-fmhgt-daemon-set in namespace controllerrevisions-645, will wait for the garbage collector to delete the pods 09/04/23 15:13:06.46
    Sep  4 15:13:06.535: INFO: Deleting DaemonSet.extensions e2e-fmhgt-daemon-set took: 13.604056ms
    Sep  4 15:13:06.636: INFO: Terminating DaemonSet.extensions e2e-fmhgt-daemon-set pods took: 100.628703ms
    Sep  4 15:13:07.848: INFO: Number of nodes with available pods controlled by daemonset e2e-fmhgt-daemon-set: 0
    Sep  4 15:13:07.848: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-fmhgt-daemon-set
    Sep  4 15:13:07.860: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20156"},"items":null}

    Sep  4 15:13:07.872: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20156"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:13:07.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-645" for this suite. 09/04/23 15:13:07.932
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:13:07.945
Sep  4 15:13:07.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test 09/04/23 15:13:07.946
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:13:07.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:13:08.004
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Sep  4 15:13:08.044: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b3a2fbbf-700f-47af-bee3-cf9febc6a559" in namespace "security-context-test-7060" to be "Succeeded or Failed"
Sep  4 15:13:08.055: INFO: Pod "alpine-nnp-false-b3a2fbbf-700f-47af-bee3-cf9febc6a559": Phase="Pending", Reason="", readiness=false. Elapsed: 11.45455ms
Sep  4 15:13:10.068: INFO: Pod "alpine-nnp-false-b3a2fbbf-700f-47af-bee3-cf9febc6a559": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024676011s
Sep  4 15:13:12.068: INFO: Pod "alpine-nnp-false-b3a2fbbf-700f-47af-bee3-cf9febc6a559": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024028683s
Sep  4 15:13:14.068: INFO: Pod "alpine-nnp-false-b3a2fbbf-700f-47af-bee3-cf9febc6a559": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024609401s
Sep  4 15:13:14.068: INFO: Pod "alpine-nnp-false-b3a2fbbf-700f-47af-bee3-cf9febc6a559" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  4 15:13:14.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7060" for this suite. 09/04/23 15:13:14.125
------------------------------
• [6.193 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:13:07.945
    Sep  4 15:13:07.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context-test 09/04/23 15:13:07.946
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:13:07.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:13:08.004
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Sep  4 15:13:08.044: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b3a2fbbf-700f-47af-bee3-cf9febc6a559" in namespace "security-context-test-7060" to be "Succeeded or Failed"
    Sep  4 15:13:08.055: INFO: Pod "alpine-nnp-false-b3a2fbbf-700f-47af-bee3-cf9febc6a559": Phase="Pending", Reason="", readiness=false. Elapsed: 11.45455ms
    Sep  4 15:13:10.068: INFO: Pod "alpine-nnp-false-b3a2fbbf-700f-47af-bee3-cf9febc6a559": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024676011s
    Sep  4 15:13:12.068: INFO: Pod "alpine-nnp-false-b3a2fbbf-700f-47af-bee3-cf9febc6a559": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024028683s
    Sep  4 15:13:14.068: INFO: Pod "alpine-nnp-false-b3a2fbbf-700f-47af-bee3-cf9febc6a559": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024609401s
    Sep  4 15:13:14.068: INFO: Pod "alpine-nnp-false-b3a2fbbf-700f-47af-bee3-cf9febc6a559" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:13:14.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7060" for this suite. 09/04/23 15:13:14.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:13:14.138
Sep  4 15:13:14.138: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 09/04/23 15:13:14.139
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:13:14.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:13:14.197
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-htkwx" 09/04/23 15:13:14.232
Sep  4 15:13:14.257: INFO: Resource quota "e2e-rq-status-htkwx" reports spec: hard cpu limit of 500m
Sep  4 15:13:14.257: INFO: Resource quota "e2e-rq-status-htkwx" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-htkwx" /status 09/04/23 15:13:14.257
STEP: Confirm /status for "e2e-rq-status-htkwx" resourceQuota via watch 09/04/23 15:13:14.284
Sep  4 15:13:14.295: INFO: observed resourceQuota "e2e-rq-status-htkwx" in namespace "resourcequota-7761" with hard status: v1.ResourceList(nil)
Sep  4 15:13:14.295: INFO: Found resourceQuota "e2e-rq-status-htkwx" in namespace "resourcequota-7761" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Sep  4 15:13:14.295: INFO: ResourceQuota "e2e-rq-status-htkwx" /status was updated
STEP: Patching hard spec values for cpu & memory 09/04/23 15:13:14.307
Sep  4 15:13:14.321: INFO: Resource quota "e2e-rq-status-htkwx" reports spec: hard cpu limit of 1
Sep  4 15:13:14.321: INFO: Resource quota "e2e-rq-status-htkwx" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-htkwx" /status 09/04/23 15:13:14.321
STEP: Confirm /status for "e2e-rq-status-htkwx" resourceQuota via watch 09/04/23 15:13:14.333
Sep  4 15:13:14.344: INFO: observed resourceQuota "e2e-rq-status-htkwx" in namespace "resourcequota-7761" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Sep  4 15:13:14.344: INFO: Found resourceQuota "e2e-rq-status-htkwx" in namespace "resourcequota-7761" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Sep  4 15:13:14.344: INFO: ResourceQuota "e2e-rq-status-htkwx" /status was patched
STEP: Get "e2e-rq-status-htkwx" /status 09/04/23 15:13:14.344
Sep  4 15:13:14.356: INFO: Resourcequota "e2e-rq-status-htkwx" reports status: hard cpu of 1
Sep  4 15:13:14.356: INFO: Resourcequota "e2e-rq-status-htkwx" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-htkwx" /status before checking Spec is unchanged 09/04/23 15:13:14.369
Sep  4 15:13:14.382: INFO: Resourcequota "e2e-rq-status-htkwx" reports status: hard cpu of 2
Sep  4 15:13:14.382: INFO: Resourcequota "e2e-rq-status-htkwx" reports status: hard memory of 2Gi
Sep  4 15:13:14.393: INFO: Found resourceQuota "e2e-rq-status-htkwx" in namespace "resourcequota-7761" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
------------------------------
Automatically polling progress:
  [sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance] (Spec Runtime: 5m0.081s)
    test/e2e/apimachinery/resource_quota.go:1010
    In [It] (Node Runtime: 5m0s)
      test/e2e/apimachinery/resource_quota.go:1010
      At [By Step] Repatching "e2e-rq-status-htkwx" /status before checking Spec is unchanged (Step Runtime: 4m59.851s)
        test/e2e/apimachinery/resource_quota.go:1148

      Spec Goroutine
      goroutine 11254 [select]
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.WaitForWithContext({0x817c768, 0xc0001a6000}, 0xc004eb4a20, 0x306620a?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:660
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.poll({0x817c768, 0xc0001a6000}, 0x58?, 0x3064da5?, 0x8153e60?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:596
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.PollImmediateWithContext({0x817c768, 0xc0001a6000}, 0x0?, 0xc00502cba8?, 0x26724e7?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:528
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.PollImmediate(0xc004f01e60?, 0x0?, 0x0?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:514
      > k8s.io/kubernetes/test/e2e/apimachinery.glob..func20.15()
          test/e2e/apimachinery/resource_quota.go:1184
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0xc004dccab0, 0xc004dd6360})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
Sep  4 15:18:14.418: INFO: ResourceQuota "e2e-rq-status-htkwx" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 15:18:14.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7761" for this suite. 09/04/23 15:18:14.441
• [SLOW TEST] [300.322 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:13:14.138
    Sep  4 15:13:14.138: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 09/04/23 15:13:14.139
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:13:14.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:13:14.197
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-htkwx" 09/04/23 15:13:14.232
    Sep  4 15:13:14.257: INFO: Resource quota "e2e-rq-status-htkwx" reports spec: hard cpu limit of 500m
    Sep  4 15:13:14.257: INFO: Resource quota "e2e-rq-status-htkwx" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-htkwx" /status 09/04/23 15:13:14.257
    STEP: Confirm /status for "e2e-rq-status-htkwx" resourceQuota via watch 09/04/23 15:13:14.284
    Sep  4 15:13:14.295: INFO: observed resourceQuota "e2e-rq-status-htkwx" in namespace "resourcequota-7761" with hard status: v1.ResourceList(nil)
    Sep  4 15:13:14.295: INFO: Found resourceQuota "e2e-rq-status-htkwx" in namespace "resourcequota-7761" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Sep  4 15:13:14.295: INFO: ResourceQuota "e2e-rq-status-htkwx" /status was updated
    STEP: Patching hard spec values for cpu & memory 09/04/23 15:13:14.307
    Sep  4 15:13:14.321: INFO: Resource quota "e2e-rq-status-htkwx" reports spec: hard cpu limit of 1
    Sep  4 15:13:14.321: INFO: Resource quota "e2e-rq-status-htkwx" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-htkwx" /status 09/04/23 15:13:14.321
    STEP: Confirm /status for "e2e-rq-status-htkwx" resourceQuota via watch 09/04/23 15:13:14.333
    Sep  4 15:13:14.344: INFO: observed resourceQuota "e2e-rq-status-htkwx" in namespace "resourcequota-7761" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Sep  4 15:13:14.344: INFO: Found resourceQuota "e2e-rq-status-htkwx" in namespace "resourcequota-7761" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Sep  4 15:13:14.344: INFO: ResourceQuota "e2e-rq-status-htkwx" /status was patched
    STEP: Get "e2e-rq-status-htkwx" /status 09/04/23 15:13:14.344
    Sep  4 15:13:14.356: INFO: Resourcequota "e2e-rq-status-htkwx" reports status: hard cpu of 1
    Sep  4 15:13:14.356: INFO: Resourcequota "e2e-rq-status-htkwx" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-htkwx" /status before checking Spec is unchanged 09/04/23 15:13:14.369
    Sep  4 15:13:14.382: INFO: Resourcequota "e2e-rq-status-htkwx" reports status: hard cpu of 2
    Sep  4 15:13:14.382: INFO: Resourcequota "e2e-rq-status-htkwx" reports status: hard memory of 2Gi
    Sep  4 15:13:14.393: INFO: Found resourceQuota "e2e-rq-status-htkwx" in namespace "resourcequota-7761" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Sep  4 15:18:14.418: INFO: ResourceQuota "e2e-rq-status-htkwx" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:18:14.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7761" for this suite. 09/04/23 15:18:14.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:18:14.461
Sep  4 15:18:14.461: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 09/04/23 15:18:14.462
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:18:14.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:18:14.52
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 09/04/23 15:18:14.555
STEP: waiting for Deployment to be created 09/04/23 15:18:14.569
STEP: waiting for all Replicas to be Ready 09/04/23 15:18:14.581
Sep  4 15:18:14.592: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 15:18:14.592: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 15:18:14.592: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 15:18:14.592: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 15:18:14.592: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 15:18:14.592: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 15:18:14.606: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 15:18:14.606: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  4 15:18:15.446: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  4 15:18:15.446: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  4 15:18:15.941: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 09/04/23 15:18:15.941
W0904 15:18:15.960107    7754 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  4 15:18:15.970: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 09/04/23 15:18:15.97
Sep  4 15:18:15.981: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
Sep  4 15:18:15.981: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:15.989: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:15.989: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:15.989: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
Sep  4 15:18:15.989: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
Sep  4 15:18:15.989: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
Sep  4 15:18:15.989: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
Sep  4 15:18:16.950: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:16.950: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:16.957: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
STEP: listing Deployments 09/04/23 15:18:16.957
Sep  4 15:18:16.970: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 09/04/23 15:18:16.97
Sep  4 15:18:16.998: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 09/04/23 15:18:16.998
Sep  4 15:18:17.022: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 15:18:17.022: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 15:18:17.022: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 15:18:17.022: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 15:18:17.022: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 15:18:17.022: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 15:18:17.949: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 15:18:17.958: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 15:18:17.963: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 15:18:17.971: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 15:18:17.977: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  4 15:18:19.482: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 09/04/23 15:18:19.491
STEP: fetching the DeploymentStatus 09/04/23 15:18:19.516
Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:19.540: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:19.540: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
Sep  4 15:18:19.540: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 3
STEP: deleting the Deployment 09/04/23 15:18:19.54
Sep  4 15:18:19.564: INFO: observed event type MODIFIED
Sep  4 15:18:19.564: INFO: observed event type MODIFIED
Sep  4 15:18:19.564: INFO: observed event type MODIFIED
Sep  4 15:18:19.564: INFO: observed event type MODIFIED
Sep  4 15:18:19.564: INFO: observed event type MODIFIED
Sep  4 15:18:19.564: INFO: observed event type MODIFIED
Sep  4 15:18:19.564: INFO: observed event type MODIFIED
Sep  4 15:18:19.564: INFO: observed event type MODIFIED
Sep  4 15:18:19.564: INFO: observed event type MODIFIED
Sep  4 15:18:19.564: INFO: observed event type MODIFIED
Sep  4 15:18:19.564: INFO: observed event type MODIFIED
Sep  4 15:18:19.564: INFO: observed event type MODIFIED
Sep  4 15:18:19.564: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 15:18:19.577: INFO: Log out all the ReplicaSets if there is no deployment created
Sep  4 15:18:19.589: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-5894  0a465396-c80c-4c43-b56a-c6954bb67542 22012 2 2023-09-04 15:18:16 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 7920e068-289c-4d2d-9747-d8f1ab5b9807 0xc002e80be7 0xc002e80be8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:18:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7920e068-289c-4d2d-9747-d8f1ab5b9807\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:18:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e80c70 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Sep  4 15:18:19.601: INFO: pod: "test-deployment-7b7876f9d6-dxpzd":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-dxpzd test-deployment-7b7876f9d6- deployment-5894  03274848-3591-4e4b-90fa-1f2113caccc4 21979 0 2023-09-04 15:18:16 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:2563e469926581786446f0036d0163e19af3524930cd0d3538d0873265e3d6a4 cni.projectcalico.org/podIP:100.64.1.232/32 cni.projectcalico.org/podIPs:100.64.1.232/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 0a465396-c80c-4c43-b56a-c6954bb67542 0xc002e81107 0xc002e81108}] [] [{kube-controller-manager Update v1 2023-09-04 15:18:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a465396-c80c-4c43-b56a-c6954bb67542\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:18:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:18:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vgm84,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vgm84,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.232,StartTime:2023-09-04 15:18:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:18:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2e62a431c2ea52f3cb2dfa8f087d10762eac4abac6ec80b9abf9dcbe85b3efee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  4 15:18:19.601: INFO: pod: "test-deployment-7b7876f9d6-qf7wg":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-qf7wg test-deployment-7b7876f9d6- deployment-5894  185f2a94-d61c-43ed-901a-e1ce9cf61970 22011 0 2023-09-04 15:18:17 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:286825e12948853acdaece7a102579380904157f451b1a4d33b99f38d3965d09 cni.projectcalico.org/podIP:100.64.0.154/32 cni.projectcalico.org/podIPs:100.64.0.154/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 0a465396-c80c-4c43-b56a-c6954bb67542 0xc002e81327 0xc002e81328}] [] [{kube-controller-manager Update v1 2023-09-04 15:18:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a465396-c80c-4c43-b56a-c6954bb67542\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:18:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-84hxb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-84hxb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:100.64.0.154,StartTime:2023-09-04 15:18:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:18:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://720172d5e686b27af26615b523ea07948ef9ab80f2ec54a098ac30ad44f5a6fe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  4 15:18:19.601: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-5894  1af8cda0-b763-4599-8c05-8cf784bd42c4 22017 4 2023-09-04 15:18:15 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 7920e068-289c-4d2d-9747-d8f1ab5b9807 0xc002e80cd7 0xc002e80cd8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7920e068-289c-4d2d-9747-d8f1ab5b9807\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:18:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e80d60 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Sep  4 15:18:19.613: INFO: pod: "test-deployment-7df74c55ff-8z6d2":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-8z6d2 test-deployment-7df74c55ff- deployment-5894  12aa363a-2b76-4b8f-95d0-b9f2513fdd91 22015 0 2023-09-04 15:18:15 +0000 UTC 2023-09-04 15:18:20 +0000 UTC 0xc00325a1b8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:f0b6108e4b7c3699154d25c6d402d4c103df8e4a0a5ecd99612fa61f46fa0c25 cni.projectcalico.org/podIP:100.64.1.231/32 cni.projectcalico.org/podIPs:100.64.1.231/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1af8cda0-b763-4599-8c05-8cf784bd42c4 0xc00325a207 0xc00325a208}] [] [{kube-controller-manager Update v1 2023-09-04 15:18:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1af8cda0-b763-4599-8c05-8cf784bd42c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:18:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:18:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwsrs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwsrs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.231,StartTime:2023-09-04 15:18:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:18:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://7ed71e112e9c9be81fb7ff7154282241c885f5074cadc1b5c543906527929658,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  4 15:18:19.613: INFO: pod: "test-deployment-7df74c55ff-pp8b7":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-pp8b7 test-deployment-7df74c55ff- deployment-5894  7c477aad-b440-4cf8-a916-839b515b4f64 22002 0 2023-09-04 15:18:16 +0000 UTC 2023-09-04 15:18:18 +0000 UTC 0xc00325a3e0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:622ad88571cacb1f2f98c1768615398d5ae11aeaf4ef16130ee254f953c361f7 cni.projectcalico.org/podIP:100.64.0.153/32 cni.projectcalico.org/podIPs:100.64.0.153/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1af8cda0-b763-4599-8c05-8cf784bd42c4 0xc00325a437 0xc00325a438}] [] [{kube-controller-manager Update v1 2023-09-04 15:18:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1af8cda0-b763-4599-8c05-8cf784bd42c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:18:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:18:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7v957,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7v957,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:100.64.0.153,StartTime:2023-09-04 15:18:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:18:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://31c92b56ab118e17a0d4b5d8bc062ddcc407b96a52d1fa35b74fcaf73755522d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  4 15:18:19.613: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-5894  bf6db6fa-0115-4b60-82e0-c6008302bd69 21950 3 2023-09-04 15:18:14 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 7920e068-289c-4d2d-9747-d8f1ab5b9807 0xc002e80dc7 0xc002e80dc8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:18:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7920e068-289c-4d2d-9747-d8f1ab5b9807\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:18:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e80e50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 15:18:19.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5894" for this suite. 09/04/23 15:18:19.637
------------------------------
• [5.188 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:18:14.461
    Sep  4 15:18:14.461: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 09/04/23 15:18:14.462
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:18:14.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:18:14.52
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 09/04/23 15:18:14.555
    STEP: waiting for Deployment to be created 09/04/23 15:18:14.569
    STEP: waiting for all Replicas to be Ready 09/04/23 15:18:14.581
    Sep  4 15:18:14.592: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 15:18:14.592: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 15:18:14.592: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 15:18:14.592: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 15:18:14.592: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 15:18:14.592: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 15:18:14.606: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 15:18:14.606: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  4 15:18:15.446: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep  4 15:18:15.446: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep  4 15:18:15.941: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 09/04/23 15:18:15.941
    W0904 15:18:15.960107    7754 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  4 15:18:15.970: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 09/04/23 15:18:15.97
    Sep  4 15:18:15.981: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
    Sep  4 15:18:15.981: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
    Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
    Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
    Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
    Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
    Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
    Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 0
    Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
    Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
    Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:15.982: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:15.989: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:15.989: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:15.989: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
    Sep  4 15:18:15.989: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
    Sep  4 15:18:15.989: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
    Sep  4 15:18:15.989: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
    Sep  4 15:18:16.950: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:16.950: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:16.957: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
    STEP: listing Deployments 09/04/23 15:18:16.957
    Sep  4 15:18:16.970: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 09/04/23 15:18:16.97
    Sep  4 15:18:16.998: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 09/04/23 15:18:16.998
    Sep  4 15:18:17.022: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 15:18:17.022: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 15:18:17.022: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 15:18:17.022: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 15:18:17.022: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 15:18:17.022: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 15:18:17.949: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 15:18:17.958: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 15:18:17.963: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 15:18:17.971: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 15:18:17.977: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  4 15:18:19.482: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 09/04/23 15:18:19.491
    STEP: fetching the DeploymentStatus 09/04/23 15:18:19.516
    Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
    Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
    Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
    Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
    Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
    Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 1
    Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:19.539: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:19.540: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:19.540: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 2
    Sep  4 15:18:19.540: INFO: observed Deployment test-deployment in namespace deployment-5894 with ReadyReplicas 3
    STEP: deleting the Deployment 09/04/23 15:18:19.54
    Sep  4 15:18:19.564: INFO: observed event type MODIFIED
    Sep  4 15:18:19.564: INFO: observed event type MODIFIED
    Sep  4 15:18:19.564: INFO: observed event type MODIFIED
    Sep  4 15:18:19.564: INFO: observed event type MODIFIED
    Sep  4 15:18:19.564: INFO: observed event type MODIFIED
    Sep  4 15:18:19.564: INFO: observed event type MODIFIED
    Sep  4 15:18:19.564: INFO: observed event type MODIFIED
    Sep  4 15:18:19.564: INFO: observed event type MODIFIED
    Sep  4 15:18:19.564: INFO: observed event type MODIFIED
    Sep  4 15:18:19.564: INFO: observed event type MODIFIED
    Sep  4 15:18:19.564: INFO: observed event type MODIFIED
    Sep  4 15:18:19.564: INFO: observed event type MODIFIED
    Sep  4 15:18:19.564: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 15:18:19.577: INFO: Log out all the ReplicaSets if there is no deployment created
    Sep  4 15:18:19.589: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-5894  0a465396-c80c-4c43-b56a-c6954bb67542 22012 2 2023-09-04 15:18:16 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 7920e068-289c-4d2d-9747-d8f1ab5b9807 0xc002e80be7 0xc002e80be8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:18:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7920e068-289c-4d2d-9747-d8f1ab5b9807\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:18:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e80c70 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Sep  4 15:18:19.601: INFO: pod: "test-deployment-7b7876f9d6-dxpzd":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-dxpzd test-deployment-7b7876f9d6- deployment-5894  03274848-3591-4e4b-90fa-1f2113caccc4 21979 0 2023-09-04 15:18:16 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:2563e469926581786446f0036d0163e19af3524930cd0d3538d0873265e3d6a4 cni.projectcalico.org/podIP:100.64.1.232/32 cni.projectcalico.org/podIPs:100.64.1.232/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 0a465396-c80c-4c43-b56a-c6954bb67542 0xc002e81107 0xc002e81108}] [] [{kube-controller-manager Update v1 2023-09-04 15:18:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a465396-c80c-4c43-b56a-c6954bb67542\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:18:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:18:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vgm84,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vgm84,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.232,StartTime:2023-09-04 15:18:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:18:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2e62a431c2ea52f3cb2dfa8f087d10762eac4abac6ec80b9abf9dcbe85b3efee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  4 15:18:19.601: INFO: pod: "test-deployment-7b7876f9d6-qf7wg":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-qf7wg test-deployment-7b7876f9d6- deployment-5894  185f2a94-d61c-43ed-901a-e1ce9cf61970 22011 0 2023-09-04 15:18:17 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:286825e12948853acdaece7a102579380904157f451b1a4d33b99f38d3965d09 cni.projectcalico.org/podIP:100.64.0.154/32 cni.projectcalico.org/podIPs:100.64.0.154/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 0a465396-c80c-4c43-b56a-c6954bb67542 0xc002e81327 0xc002e81328}] [] [{kube-controller-manager Update v1 2023-09-04 15:18:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a465396-c80c-4c43-b56a-c6954bb67542\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:18:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-84hxb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-84hxb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:100.64.0.154,StartTime:2023-09-04 15:18:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:18:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://720172d5e686b27af26615b523ea07948ef9ab80f2ec54a098ac30ad44f5a6fe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  4 15:18:19.601: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-5894  1af8cda0-b763-4599-8c05-8cf784bd42c4 22017 4 2023-09-04 15:18:15 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 7920e068-289c-4d2d-9747-d8f1ab5b9807 0xc002e80cd7 0xc002e80cd8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7920e068-289c-4d2d-9747-d8f1ab5b9807\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:18:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e80d60 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Sep  4 15:18:19.613: INFO: pod: "test-deployment-7df74c55ff-8z6d2":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-8z6d2 test-deployment-7df74c55ff- deployment-5894  12aa363a-2b76-4b8f-95d0-b9f2513fdd91 22015 0 2023-09-04 15:18:15 +0000 UTC 2023-09-04 15:18:20 +0000 UTC 0xc00325a1b8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:f0b6108e4b7c3699154d25c6d402d4c103df8e4a0a5ecd99612fa61f46fa0c25 cni.projectcalico.org/podIP:100.64.1.231/32 cni.projectcalico.org/podIPs:100.64.1.231/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1af8cda0-b763-4599-8c05-8cf784bd42c4 0xc00325a207 0xc00325a208}] [] [{kube-controller-manager Update v1 2023-09-04 15:18:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1af8cda0-b763-4599-8c05-8cf784bd42c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:18:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:18:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwsrs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwsrs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.231,StartTime:2023-09-04 15:18:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:18:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://7ed71e112e9c9be81fb7ff7154282241c885f5074cadc1b5c543906527929658,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  4 15:18:19.613: INFO: pod: "test-deployment-7df74c55ff-pp8b7":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-pp8b7 test-deployment-7df74c55ff- deployment-5894  7c477aad-b440-4cf8-a916-839b515b4f64 22002 0 2023-09-04 15:18:16 +0000 UTC 2023-09-04 15:18:18 +0000 UTC 0xc00325a3e0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:622ad88571cacb1f2f98c1768615398d5ae11aeaf4ef16130ee254f953c361f7 cni.projectcalico.org/podIP:100.64.0.153/32 cni.projectcalico.org/podIPs:100.64.0.153/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1af8cda0-b763-4599-8c05-8cf784bd42c4 0xc00325a437 0xc00325a438}] [] [{kube-controller-manager Update v1 2023-09-04 15:18:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1af8cda0-b763-4599-8c05-8cf784bd42c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:18:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:18:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7v957,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7v957,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:18:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:100.64.0.153,StartTime:2023-09-04 15:18:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:18:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://31c92b56ab118e17a0d4b5d8bc062ddcc407b96a52d1fa35b74fcaf73755522d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  4 15:18:19.613: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-5894  bf6db6fa-0115-4b60-82e0-c6008302bd69 21950 3 2023-09-04 15:18:14 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 7920e068-289c-4d2d-9747-d8f1ab5b9807 0xc002e80dc7 0xc002e80dc8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:18:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7920e068-289c-4d2d-9747-d8f1ab5b9807\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:18:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e80e50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:18:19.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5894" for this suite. 09/04/23 15:18:19.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:18:19.65
Sep  4 15:18:19.650: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 15:18:19.651
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:18:19.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:18:19.708
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/04/23 15:18:19.742
Sep  4 15:18:19.762: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5601" to be "running and ready"
Sep  4 15:18:19.774: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.107577ms
Sep  4 15:18:19.774: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:18:21.788: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.026450374s
Sep  4 15:18:21.788: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  4 15:18:21.788: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 09/04/23 15:18:21.801
Sep  4 15:18:21.818: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5601" to be "running and ready"
Sep  4 15:18:21.831: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 12.662565ms
Sep  4 15:18:21.831: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:18:23.845: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.026322206s
Sep  4 15:18:23.845: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Sep  4 15:18:23.845: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/04/23 15:18:23.858
STEP: delete the pod with lifecycle hook 09/04/23 15:18:23.892
Sep  4 15:18:23.906: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  4 15:18:23.918: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  4 15:18:25.918: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  4 15:18:25.932: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  4 15:18:27.919: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  4 15:18:27.932: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  4 15:18:27.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5601" for this suite. 09/04/23 15:18:27.955
------------------------------
• [8.319 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:18:19.65
    Sep  4 15:18:19.650: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 15:18:19.651
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:18:19.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:18:19.708
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/04/23 15:18:19.742
    Sep  4 15:18:19.762: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5601" to be "running and ready"
    Sep  4 15:18:19.774: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.107577ms
    Sep  4 15:18:19.774: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:18:21.788: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.026450374s
    Sep  4 15:18:21.788: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  4 15:18:21.788: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 09/04/23 15:18:21.801
    Sep  4 15:18:21.818: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5601" to be "running and ready"
    Sep  4 15:18:21.831: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 12.662565ms
    Sep  4 15:18:21.831: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:18:23.845: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.026322206s
    Sep  4 15:18:23.845: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Sep  4 15:18:23.845: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/04/23 15:18:23.858
    STEP: delete the pod with lifecycle hook 09/04/23 15:18:23.892
    Sep  4 15:18:23.906: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  4 15:18:23.918: INFO: Pod pod-with-poststart-exec-hook still exists
    Sep  4 15:18:25.918: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  4 15:18:25.932: INFO: Pod pod-with-poststart-exec-hook still exists
    Sep  4 15:18:27.919: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  4 15:18:27.932: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:18:27.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5601" for this suite. 09/04/23 15:18:27.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:18:27.97
Sep  4 15:18:27.970: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 09/04/23 15:18:27.971
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:18:28.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:18:28.031
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 09/04/23 15:18:28.054
STEP: Creating a ResourceQuota 09/04/23 15:18:33.068
STEP: Ensuring resource quota status is calculated 09/04/23 15:18:33.08
STEP: Creating a Service 09/04/23 15:18:35.094
STEP: Creating a NodePort Service 09/04/23 15:18:35.114
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/04/23 15:18:35.138
STEP: Ensuring resource quota status captures service creation 09/04/23 15:18:35.162
STEP: Deleting Services 09/04/23 15:18:37.176
STEP: Ensuring resource quota status released usage 09/04/23 15:18:37.22
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 15:18:39.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2242" for this suite. 09/04/23 15:18:39.256
------------------------------
• [11.299 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:18:27.97
    Sep  4 15:18:27.970: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 09/04/23 15:18:27.971
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:18:28.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:18:28.031
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 09/04/23 15:18:28.054
    STEP: Creating a ResourceQuota 09/04/23 15:18:33.068
    STEP: Ensuring resource quota status is calculated 09/04/23 15:18:33.08
    STEP: Creating a Service 09/04/23 15:18:35.094
    STEP: Creating a NodePort Service 09/04/23 15:18:35.114
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/04/23 15:18:35.138
    STEP: Ensuring resource quota status captures service creation 09/04/23 15:18:35.162
    STEP: Deleting Services 09/04/23 15:18:37.176
    STEP: Ensuring resource quota status released usage 09/04/23 15:18:37.22
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:18:39.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2242" for this suite. 09/04/23 15:18:39.256
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:18:39.269
Sep  4 15:18:39.270: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 15:18:39.27
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:18:39.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:18:39.331
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/04/23 15:18:39.367
Sep  4 15:18:39.385: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6102" to be "running and ready"
Sep  4 15:18:39.397: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.056196ms
Sep  4 15:18:39.397: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:18:41.412: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.026860791s
Sep  4 15:18:41.412: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  4 15:18:41.412: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 09/04/23 15:18:41.425
Sep  4 15:18:41.442: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6102" to be "running and ready"
Sep  4 15:18:41.462: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 19.664857ms
Sep  4 15:18:41.462: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:18:43.475: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.033544456s
Sep  4 15:18:43.475: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Sep  4 15:18:43.475: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/04/23 15:18:43.488
STEP: delete the pod with lifecycle hook 09/04/23 15:18:43.564
Sep  4 15:18:43.579: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  4 15:18:43.592: INFO: Pod pod-with-poststart-http-hook still exists
Sep  4 15:18:45.592: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  4 15:18:45.606: INFO: Pod pod-with-poststart-http-hook still exists
Sep  4 15:18:47.592: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  4 15:18:47.605: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  4 15:18:47.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6102" for this suite. 09/04/23 15:18:47.628
------------------------------
• [8.372 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:18:39.269
    Sep  4 15:18:39.270: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/04/23 15:18:39.27
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:18:39.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:18:39.331
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/04/23 15:18:39.367
    Sep  4 15:18:39.385: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6102" to be "running and ready"
    Sep  4 15:18:39.397: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.056196ms
    Sep  4 15:18:39.397: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:18:41.412: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.026860791s
    Sep  4 15:18:41.412: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  4 15:18:41.412: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 09/04/23 15:18:41.425
    Sep  4 15:18:41.442: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6102" to be "running and ready"
    Sep  4 15:18:41.462: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 19.664857ms
    Sep  4 15:18:41.462: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:18:43.475: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.033544456s
    Sep  4 15:18:43.475: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Sep  4 15:18:43.475: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/04/23 15:18:43.488
    STEP: delete the pod with lifecycle hook 09/04/23 15:18:43.564
    Sep  4 15:18:43.579: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  4 15:18:43.592: INFO: Pod pod-with-poststart-http-hook still exists
    Sep  4 15:18:45.592: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  4 15:18:45.606: INFO: Pod pod-with-poststart-http-hook still exists
    Sep  4 15:18:47.592: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  4 15:18:47.605: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:18:47.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6102" for this suite. 09/04/23 15:18:47.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:18:47.646
Sep  4 15:18:47.646: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop 09/04/23 15:18:47.646
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:18:47.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:18:47.705
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9315 09/04/23 15:18:47.728
STEP: Waiting for pods to come up. 09/04/23 15:18:47.746
Sep  4 15:18:47.746: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9315" to be "running"
Sep  4 15:18:47.758: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 11.838338ms
Sep  4 15:18:49.771: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.025070314s
Sep  4 15:18:49.771: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9315 09/04/23 15:18:49.784
Sep  4 15:18:49.800: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9315" to be "running"
Sep  4 15:18:49.812: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 11.835844ms
Sep  4 15:18:51.825: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.025247557s
Sep  4 15:18:51.825: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 09/04/23 15:18:51.825
Sep  4 15:18:56.980: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 09/04/23 15:18:56.98
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Sep  4 15:18:56.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-9315" for this suite. 09/04/23 15:18:57.018
------------------------------
• [9.388 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:18:47.646
    Sep  4 15:18:47.646: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename prestop 09/04/23 15:18:47.646
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:18:47.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:18:47.705
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9315 09/04/23 15:18:47.728
    STEP: Waiting for pods to come up. 09/04/23 15:18:47.746
    Sep  4 15:18:47.746: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9315" to be "running"
    Sep  4 15:18:47.758: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 11.838338ms
    Sep  4 15:18:49.771: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.025070314s
    Sep  4 15:18:49.771: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9315 09/04/23 15:18:49.784
    Sep  4 15:18:49.800: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9315" to be "running"
    Sep  4 15:18:49.812: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 11.835844ms
    Sep  4 15:18:51.825: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.025247557s
    Sep  4 15:18:51.825: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 09/04/23 15:18:51.825
    Sep  4 15:18:56.980: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 09/04/23 15:18:56.98
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:18:56.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-9315" for this suite. 09/04/23 15:18:57.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:18:57.034
Sep  4 15:18:57.034: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:18:57.035
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:18:57.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:18:57.094
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-f2b2e562-4eeb-4e40-8fb4-acd9a9810382 09/04/23 15:18:57.116
STEP: Creating a pod to test consume secrets 09/04/23 15:18:57.129
Sep  4 15:18:57.147: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3" in namespace "projected-8265" to be "Succeeded or Failed"
Sep  4 15:18:57.160: INFO: Pod "pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.04098ms
Sep  4 15:18:59.173: INFO: Pod "pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025784135s
Sep  4 15:19:01.174: INFO: Pod "pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026001504s
STEP: Saw pod success 09/04/23 15:19:01.174
Sep  4 15:19:01.174: INFO: Pod "pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3" satisfied condition "Succeeded or Failed"
Sep  4 15:19:01.186: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/04/23 15:19:01.268
Sep  4 15:19:01.284: INFO: Waiting for pod pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3 to disappear
Sep  4 15:19:01.296: INFO: Pod pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  4 15:19:01.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8265" for this suite. 09/04/23 15:19:01.321
------------------------------
• [4.300 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:18:57.034
    Sep  4 15:18:57.034: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:18:57.035
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:18:57.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:18:57.094
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-f2b2e562-4eeb-4e40-8fb4-acd9a9810382 09/04/23 15:18:57.116
    STEP: Creating a pod to test consume secrets 09/04/23 15:18:57.129
    Sep  4 15:18:57.147: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3" in namespace "projected-8265" to be "Succeeded or Failed"
    Sep  4 15:18:57.160: INFO: Pod "pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.04098ms
    Sep  4 15:18:59.173: INFO: Pod "pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025784135s
    Sep  4 15:19:01.174: INFO: Pod "pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026001504s
    STEP: Saw pod success 09/04/23 15:19:01.174
    Sep  4 15:19:01.174: INFO: Pod "pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3" satisfied condition "Succeeded or Failed"
    Sep  4 15:19:01.186: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 15:19:01.268
    Sep  4 15:19:01.284: INFO: Waiting for pod pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3 to disappear
    Sep  4 15:19:01.296: INFO: Pod pod-projected-secrets-649da22c-cc19-4cd1-b5b9-f450dc6a4bb3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:19:01.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8265" for this suite. 09/04/23 15:19:01.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:19:01.335
Sep  4 15:19:01.335: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 15:19:01.336
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:01.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:01.392
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-9180 09/04/23 15:19:01.414
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9180 to expose endpoints map[] 09/04/23 15:19:01.431
Sep  4 15:19:01.465: INFO: successfully validated that service endpoint-test2 in namespace services-9180 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9180 09/04/23 15:19:01.465
Sep  4 15:19:01.484: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9180" to be "running and ready"
Sep  4 15:19:01.496: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.198087ms
Sep  4 15:19:01.496: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:19:03.509: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.025253957s
Sep  4 15:19:03.509: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  4 15:19:03.509: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9180 to expose endpoints map[pod1:[80]] 09/04/23 15:19:03.522
Sep  4 15:19:03.569: INFO: successfully validated that service endpoint-test2 in namespace services-9180 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 09/04/23 15:19:03.569
Sep  4 15:19:03.569: INFO: Creating new exec pod
Sep  4 15:19:03.587: INFO: Waiting up to 5m0s for pod "execpodd7nzn" in namespace "services-9180" to be "running"
Sep  4 15:19:03.599: INFO: Pod "execpodd7nzn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.199777ms
Sep  4 15:19:05.613: INFO: Pod "execpodd7nzn": Phase="Running", Reason="", readiness=true. Elapsed: 2.025891002s
Sep  4 15:19:05.613: INFO: Pod "execpodd7nzn" satisfied condition "running"
Sep  4 15:19:06.613: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9180 exec execpodd7nzn -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  4 15:19:07.198: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  4 15:19:07.198: INFO: stdout: ""
Sep  4 15:19:07.198: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9180 exec execpodd7nzn -- /bin/sh -x -c nc -v -z -w 2 100.107.40.115 80'
Sep  4 15:19:07.813: INFO: stderr: "+ nc -v -z -w 2 100.107.40.115 80\nConnection to 100.107.40.115 80 port [tcp/http] succeeded!\n"
Sep  4 15:19:07.813: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-9180 09/04/23 15:19:07.813
Sep  4 15:19:07.831: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9180" to be "running and ready"
Sep  4 15:19:07.842: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.490493ms
Sep  4 15:19:07.842: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:19:09.856: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.025060857s
Sep  4 15:19:09.856: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  4 15:19:09.856: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9180 to expose endpoints map[pod1:[80] pod2:[80]] 09/04/23 15:19:09.868
Sep  4 15:19:09.932: INFO: successfully validated that service endpoint-test2 in namespace services-9180 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 09/04/23 15:19:09.932
Sep  4 15:19:10.932: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9180 exec execpodd7nzn -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  4 15:19:11.359: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  4 15:19:11.360: INFO: stdout: ""
Sep  4 15:19:11.360: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9180 exec execpodd7nzn -- /bin/sh -x -c nc -v -z -w 2 100.107.40.115 80'
Sep  4 15:19:11.905: INFO: stderr: "+ nc -v -z -w 2 100.107.40.115 80\nConnection to 100.107.40.115 80 port [tcp/http] succeeded!\n"
Sep  4 15:19:11.905: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-9180 09/04/23 15:19:11.905
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9180 to expose endpoints map[pod2:[80]] 09/04/23 15:19:11.921
Sep  4 15:19:11.968: INFO: successfully validated that service endpoint-test2 in namespace services-9180 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 09/04/23 15:19:11.968
Sep  4 15:19:12.969: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9180 exec execpodd7nzn -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  4 15:19:13.477: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  4 15:19:13.477: INFO: stdout: ""
Sep  4 15:19:13.477: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9180 exec execpodd7nzn -- /bin/sh -x -c nc -v -z -w 2 100.107.40.115 80'
Sep  4 15:19:14.065: INFO: stderr: "+ nc -v -z -w 2 100.107.40.115 80\nConnection to 100.107.40.115 80 port [tcp/http] succeeded!\n"
Sep  4 15:19:14.065: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-9180 09/04/23 15:19:14.065
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9180 to expose endpoints map[] 09/04/23 15:19:14.082
Sep  4 15:19:14.118: INFO: successfully validated that service endpoint-test2 in namespace services-9180 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 15:19:14.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9180" for this suite. 09/04/23 15:19:14.157
------------------------------
• [12.836 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:19:01.335
    Sep  4 15:19:01.335: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 15:19:01.336
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:01.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:01.392
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-9180 09/04/23 15:19:01.414
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9180 to expose endpoints map[] 09/04/23 15:19:01.431
    Sep  4 15:19:01.465: INFO: successfully validated that service endpoint-test2 in namespace services-9180 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9180 09/04/23 15:19:01.465
    Sep  4 15:19:01.484: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9180" to be "running and ready"
    Sep  4 15:19:01.496: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.198087ms
    Sep  4 15:19:01.496: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:19:03.509: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.025253957s
    Sep  4 15:19:03.509: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  4 15:19:03.509: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9180 to expose endpoints map[pod1:[80]] 09/04/23 15:19:03.522
    Sep  4 15:19:03.569: INFO: successfully validated that service endpoint-test2 in namespace services-9180 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 09/04/23 15:19:03.569
    Sep  4 15:19:03.569: INFO: Creating new exec pod
    Sep  4 15:19:03.587: INFO: Waiting up to 5m0s for pod "execpodd7nzn" in namespace "services-9180" to be "running"
    Sep  4 15:19:03.599: INFO: Pod "execpodd7nzn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.199777ms
    Sep  4 15:19:05.613: INFO: Pod "execpodd7nzn": Phase="Running", Reason="", readiness=true. Elapsed: 2.025891002s
    Sep  4 15:19:05.613: INFO: Pod "execpodd7nzn" satisfied condition "running"
    Sep  4 15:19:06.613: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9180 exec execpodd7nzn -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  4 15:19:07.198: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  4 15:19:07.198: INFO: stdout: ""
    Sep  4 15:19:07.198: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9180 exec execpodd7nzn -- /bin/sh -x -c nc -v -z -w 2 100.107.40.115 80'
    Sep  4 15:19:07.813: INFO: stderr: "+ nc -v -z -w 2 100.107.40.115 80\nConnection to 100.107.40.115 80 port [tcp/http] succeeded!\n"
    Sep  4 15:19:07.813: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-9180 09/04/23 15:19:07.813
    Sep  4 15:19:07.831: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9180" to be "running and ready"
    Sep  4 15:19:07.842: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.490493ms
    Sep  4 15:19:07.842: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:19:09.856: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.025060857s
    Sep  4 15:19:09.856: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  4 15:19:09.856: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9180 to expose endpoints map[pod1:[80] pod2:[80]] 09/04/23 15:19:09.868
    Sep  4 15:19:09.932: INFO: successfully validated that service endpoint-test2 in namespace services-9180 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 09/04/23 15:19:09.932
    Sep  4 15:19:10.932: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9180 exec execpodd7nzn -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  4 15:19:11.359: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  4 15:19:11.360: INFO: stdout: ""
    Sep  4 15:19:11.360: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9180 exec execpodd7nzn -- /bin/sh -x -c nc -v -z -w 2 100.107.40.115 80'
    Sep  4 15:19:11.905: INFO: stderr: "+ nc -v -z -w 2 100.107.40.115 80\nConnection to 100.107.40.115 80 port [tcp/http] succeeded!\n"
    Sep  4 15:19:11.905: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-9180 09/04/23 15:19:11.905
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9180 to expose endpoints map[pod2:[80]] 09/04/23 15:19:11.921
    Sep  4 15:19:11.968: INFO: successfully validated that service endpoint-test2 in namespace services-9180 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 09/04/23 15:19:11.968
    Sep  4 15:19:12.969: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9180 exec execpodd7nzn -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  4 15:19:13.477: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  4 15:19:13.477: INFO: stdout: ""
    Sep  4 15:19:13.477: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9180 exec execpodd7nzn -- /bin/sh -x -c nc -v -z -w 2 100.107.40.115 80'
    Sep  4 15:19:14.065: INFO: stderr: "+ nc -v -z -w 2 100.107.40.115 80\nConnection to 100.107.40.115 80 port [tcp/http] succeeded!\n"
    Sep  4 15:19:14.065: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-9180 09/04/23 15:19:14.065
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9180 to expose endpoints map[] 09/04/23 15:19:14.082
    Sep  4 15:19:14.118: INFO: successfully validated that service endpoint-test2 in namespace services-9180 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:19:14.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9180" for this suite. 09/04/23 15:19:14.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:19:14.173
Sep  4 15:19:14.173: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 15:19:14.174
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:14.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:14.231
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-7780 09/04/23 15:19:14.253
STEP: creating replication controller nodeport-test in namespace services-7780 09/04/23 15:19:14.271
I0904 15:19:14.284670    7754 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7780, replica count: 2
I0904 15:19:17.335566    7754 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 15:19:17.335: INFO: Creating new exec pod
Sep  4 15:19:17.352: INFO: Waiting up to 5m0s for pod "execpodq6v9s" in namespace "services-7780" to be "running"
Sep  4 15:19:17.364: INFO: Pod "execpodq6v9s": Phase="Pending", Reason="", readiness=false. Elapsed: 11.791552ms
Sep  4 15:19:19.377: INFO: Pod "execpodq6v9s": Phase="Running", Reason="", readiness=true. Elapsed: 2.025254127s
Sep  4 15:19:19.377: INFO: Pod "execpodq6v9s" satisfied condition "running"
Sep  4 15:19:20.400: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7780 exec execpodq6v9s -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Sep  4 15:19:21.004: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep  4 15:19:21.004: INFO: stdout: ""
Sep  4 15:19:21.004: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7780 exec execpodq6v9s -- /bin/sh -x -c nc -v -z -w 2 100.111.137.211 80'
Sep  4 15:19:21.608: INFO: stderr: "+ nc -v -z -w 2 100.111.137.211 80\nConnection to 100.111.137.211 80 port [tcp/http] succeeded!\n"
Sep  4 15:19:21.608: INFO: stdout: ""
Sep  4 15:19:21.608: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7780 exec execpodq6v9s -- /bin/sh -x -c nc -v -z -w 2 10.250.1.105 30571'
Sep  4 15:19:22.217: INFO: stderr: "+ nc -v -z -w 2 10.250.1.105 30571\nConnection to 10.250.1.105 30571 port [tcp/*] succeeded!\n"
Sep  4 15:19:22.217: INFO: stdout: ""
Sep  4 15:19:22.217: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7780 exec execpodq6v9s -- /bin/sh -x -c nc -v -z -w 2 10.250.1.231 30571'
Sep  4 15:19:22.745: INFO: stderr: "+ nc -v -z -w 2 10.250.1.231 30571\nConnection to 10.250.1.231 30571 port [tcp/*] succeeded!\n"
Sep  4 15:19:22.745: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 15:19:22.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7780" for this suite. 09/04/23 15:19:22.768
------------------------------
• [8.609 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:19:14.173
    Sep  4 15:19:14.173: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 15:19:14.174
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:14.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:14.231
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-7780 09/04/23 15:19:14.253
    STEP: creating replication controller nodeport-test in namespace services-7780 09/04/23 15:19:14.271
    I0904 15:19:14.284670    7754 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7780, replica count: 2
    I0904 15:19:17.335566    7754 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 15:19:17.335: INFO: Creating new exec pod
    Sep  4 15:19:17.352: INFO: Waiting up to 5m0s for pod "execpodq6v9s" in namespace "services-7780" to be "running"
    Sep  4 15:19:17.364: INFO: Pod "execpodq6v9s": Phase="Pending", Reason="", readiness=false. Elapsed: 11.791552ms
    Sep  4 15:19:19.377: INFO: Pod "execpodq6v9s": Phase="Running", Reason="", readiness=true. Elapsed: 2.025254127s
    Sep  4 15:19:19.377: INFO: Pod "execpodq6v9s" satisfied condition "running"
    Sep  4 15:19:20.400: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7780 exec execpodq6v9s -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Sep  4 15:19:21.004: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Sep  4 15:19:21.004: INFO: stdout: ""
    Sep  4 15:19:21.004: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7780 exec execpodq6v9s -- /bin/sh -x -c nc -v -z -w 2 100.111.137.211 80'
    Sep  4 15:19:21.608: INFO: stderr: "+ nc -v -z -w 2 100.111.137.211 80\nConnection to 100.111.137.211 80 port [tcp/http] succeeded!\n"
    Sep  4 15:19:21.608: INFO: stdout: ""
    Sep  4 15:19:21.608: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7780 exec execpodq6v9s -- /bin/sh -x -c nc -v -z -w 2 10.250.1.105 30571'
    Sep  4 15:19:22.217: INFO: stderr: "+ nc -v -z -w 2 10.250.1.105 30571\nConnection to 10.250.1.105 30571 port [tcp/*] succeeded!\n"
    Sep  4 15:19:22.217: INFO: stdout: ""
    Sep  4 15:19:22.217: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7780 exec execpodq6v9s -- /bin/sh -x -c nc -v -z -w 2 10.250.1.231 30571'
    Sep  4 15:19:22.745: INFO: stderr: "+ nc -v -z -w 2 10.250.1.231 30571\nConnection to 10.250.1.231 30571 port [tcp/*] succeeded!\n"
    Sep  4 15:19:22.745: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:19:22.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7780" for this suite. 09/04/23 15:19:22.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:19:22.782
Sep  4 15:19:22.782: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 09/04/23 15:19:22.783
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:22.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:22.845
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-892e28bb-ca3d-4ce4-9db1-89b3636fb5ad 09/04/23 15:19:22.867
STEP: Creating a pod to test consume secrets 09/04/23 15:19:22.88
Sep  4 15:19:22.900: INFO: Waiting up to 5m0s for pod "pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22" in namespace "secrets-4450" to be "Succeeded or Failed"
Sep  4 15:19:22.912: INFO: Pod "pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22": Phase="Pending", Reason="", readiness=false. Elapsed: 12.10205ms
Sep  4 15:19:24.927: INFO: Pod "pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026445357s
Sep  4 15:19:26.926: INFO: Pod "pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026078965s
STEP: Saw pod success 09/04/23 15:19:26.926
Sep  4 15:19:26.926: INFO: Pod "pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22" satisfied condition "Succeeded or Failed"
Sep  4 15:19:26.939: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22 container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 15:19:26.976
Sep  4 15:19:26.998: INFO: Waiting for pod pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22 to disappear
Sep  4 15:19:27.010: INFO: Pod pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 15:19:27.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4450" for this suite. 09/04/23 15:19:27.034
------------------------------
• [4.267 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:19:22.782
    Sep  4 15:19:22.782: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 09/04/23 15:19:22.783
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:22.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:22.845
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-892e28bb-ca3d-4ce4-9db1-89b3636fb5ad 09/04/23 15:19:22.867
    STEP: Creating a pod to test consume secrets 09/04/23 15:19:22.88
    Sep  4 15:19:22.900: INFO: Waiting up to 5m0s for pod "pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22" in namespace "secrets-4450" to be "Succeeded or Failed"
    Sep  4 15:19:22.912: INFO: Pod "pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22": Phase="Pending", Reason="", readiness=false. Elapsed: 12.10205ms
    Sep  4 15:19:24.927: INFO: Pod "pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026445357s
    Sep  4 15:19:26.926: INFO: Pod "pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026078965s
    STEP: Saw pod success 09/04/23 15:19:26.926
    Sep  4 15:19:26.926: INFO: Pod "pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22" satisfied condition "Succeeded or Failed"
    Sep  4 15:19:26.939: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22 container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 15:19:26.976
    Sep  4 15:19:26.998: INFO: Waiting for pod pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22 to disappear
    Sep  4 15:19:27.010: INFO: Pod pod-secrets-275c8b93-b4fc-4b49-8faf-6ce068e27d22 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:19:27.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4450" for this suite. 09/04/23 15:19:27.034
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:19:27.049
Sep  4 15:19:27.049: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 09/04/23 15:19:27.05
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:27.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:27.112
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
STEP: Creating a simple DaemonSet "daemon-set" 09/04/23 15:19:27.19
STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 15:19:27.204
Sep  4 15:19:27.232: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:19:27.232: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:19:28.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 15:19:28.268: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:19:29.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 15:19:29.268: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/04/23 15:19:29.28
Sep  4 15:19:29.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 15:19:29.340: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:19:30.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 15:19:30.377: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:19:31.378: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 15:19:31.378: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 09/04/23 15:19:31.378
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/04/23 15:19:31.404
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7589, will wait for the garbage collector to delete the pods 09/04/23 15:19:31.404
Sep  4 15:19:31.482: INFO: Deleting DaemonSet.extensions daemon-set took: 14.936514ms
Sep  4 15:19:31.582: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.40716ms
Sep  4 15:19:33.695: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:19:33.695: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  4 15:19:33.709: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22744"},"items":null}

Sep  4 15:19:33.721: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22744"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:19:33.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7589" for this suite. 09/04/23 15:19:33.783
------------------------------
• [6.748 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:19:27.049
    Sep  4 15:19:27.049: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 09/04/23 15:19:27.05
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:27.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:27.112
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:305
    STEP: Creating a simple DaemonSet "daemon-set" 09/04/23 15:19:27.19
    STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 15:19:27.204
    Sep  4 15:19:27.232: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:19:27.232: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:19:28.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 15:19:28.268: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:19:29.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 15:19:29.268: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/04/23 15:19:29.28
    Sep  4 15:19:29.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 15:19:29.340: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:19:30.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 15:19:30.377: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:19:31.378: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 15:19:31.378: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 09/04/23 15:19:31.378
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/04/23 15:19:31.404
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7589, will wait for the garbage collector to delete the pods 09/04/23 15:19:31.404
    Sep  4 15:19:31.482: INFO: Deleting DaemonSet.extensions daemon-set took: 14.936514ms
    Sep  4 15:19:31.582: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.40716ms
    Sep  4 15:19:33.695: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:19:33.695: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  4 15:19:33.709: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22744"},"items":null}

    Sep  4 15:19:33.721: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22744"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:19:33.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7589" for this suite. 09/04/23 15:19:33.783
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:19:33.797
Sep  4 15:19:33.797: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 09/04/23 15:19:33.798
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:33.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:33.859
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-13e994d6-1c8f-464d-8f8e-526a2cacf3a6 09/04/23 15:19:33.882
STEP: Creating a pod to test consume secrets 09/04/23 15:19:33.895
Sep  4 15:19:33.914: INFO: Waiting up to 5m0s for pod "pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9" in namespace "secrets-8135" to be "Succeeded or Failed"
Sep  4 15:19:33.926: INFO: Pod "pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.122825ms
Sep  4 15:19:35.940: INFO: Pod "pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026053828s
Sep  4 15:19:37.940: INFO: Pod "pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026320477s
STEP: Saw pod success 09/04/23 15:19:37.941
Sep  4 15:19:37.941: INFO: Pod "pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9" satisfied condition "Succeeded or Failed"
Sep  4 15:19:37.953: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9 container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 15:19:38.033
Sep  4 15:19:38.049: INFO: Waiting for pod pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9 to disappear
Sep  4 15:19:38.061: INFO: Pod pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 15:19:38.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8135" for this suite. 09/04/23 15:19:38.084
------------------------------
• [4.303 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:19:33.797
    Sep  4 15:19:33.797: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 09/04/23 15:19:33.798
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:33.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:33.859
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-13e994d6-1c8f-464d-8f8e-526a2cacf3a6 09/04/23 15:19:33.882
    STEP: Creating a pod to test consume secrets 09/04/23 15:19:33.895
    Sep  4 15:19:33.914: INFO: Waiting up to 5m0s for pod "pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9" in namespace "secrets-8135" to be "Succeeded or Failed"
    Sep  4 15:19:33.926: INFO: Pod "pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.122825ms
    Sep  4 15:19:35.940: INFO: Pod "pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026053828s
    Sep  4 15:19:37.940: INFO: Pod "pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026320477s
    STEP: Saw pod success 09/04/23 15:19:37.941
    Sep  4 15:19:37.941: INFO: Pod "pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9" satisfied condition "Succeeded or Failed"
    Sep  4 15:19:37.953: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9 container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 15:19:38.033
    Sep  4 15:19:38.049: INFO: Waiting for pod pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9 to disappear
    Sep  4 15:19:38.061: INFO: Pod pod-secrets-b2e583fb-5954-4750-905a-1b1b4bed57d9 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:19:38.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8135" for this suite. 09/04/23 15:19:38.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:19:38.1
Sep  4 15:19:38.100: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 15:19:38.101
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:38.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:38.161
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 09/04/23 15:19:38.197
STEP: watching for the Service to be added 09/04/23 15:19:38.215
Sep  4 15:19:38.226: INFO: Found Service test-service-gsvcz in namespace services-3848 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Sep  4 15:19:38.226: INFO: Service test-service-gsvcz created
STEP: Getting /status 09/04/23 15:19:38.226
Sep  4 15:19:38.239: INFO: Service test-service-gsvcz has LoadBalancer: {[]}
STEP: patching the ServiceStatus 09/04/23 15:19:38.239
STEP: watching for the Service to be patched 09/04/23 15:19:38.253
Sep  4 15:19:38.265: INFO: observed Service test-service-gsvcz in namespace services-3848 with annotations: map[] & LoadBalancer: {[]}
Sep  4 15:19:38.265: INFO: Found Service test-service-gsvcz in namespace services-3848 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Sep  4 15:19:38.265: INFO: Service test-service-gsvcz has service status patched
STEP: updating the ServiceStatus 09/04/23 15:19:38.265
Sep  4 15:19:38.290: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 09/04/23 15:19:38.29
Sep  4 15:19:38.302: INFO: Observed Service test-service-gsvcz in namespace services-3848 with annotations: map[] & Conditions: {[]}
Sep  4 15:19:38.302: INFO: Observed event: &Service{ObjectMeta:{test-service-gsvcz  services-3848  9856da9d-7135-4522-9274-15867d75dd0a 22786 0 2023-09-04 15:19:38 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-04 15:19:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-04 15:19:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.111.136.173,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[100.111.136.173],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Sep  4 15:19:38.302: INFO: Found Service test-service-gsvcz in namespace services-3848 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  4 15:19:38.302: INFO: Service test-service-gsvcz has service status updated
STEP: patching the service 09/04/23 15:19:38.302
STEP: watching for the Service to be patched 09/04/23 15:19:38.317
Sep  4 15:19:38.328: INFO: observed Service test-service-gsvcz in namespace services-3848 with labels: map[test-service-static:true]
Sep  4 15:19:38.328: INFO: observed Service test-service-gsvcz in namespace services-3848 with labels: map[test-service-static:true]
Sep  4 15:19:38.328: INFO: observed Service test-service-gsvcz in namespace services-3848 with labels: map[test-service-static:true]
Sep  4 15:19:38.328: INFO: Found Service test-service-gsvcz in namespace services-3848 with labels: map[test-service:patched test-service-static:true]
Sep  4 15:19:38.328: INFO: Service test-service-gsvcz patched
STEP: deleting the service 09/04/23 15:19:38.328
STEP: watching for the Service to be deleted 09/04/23 15:19:38.346
Sep  4 15:19:38.357: INFO: Observed event: ADDED
Sep  4 15:19:38.357: INFO: Observed event: MODIFIED
Sep  4 15:19:38.357: INFO: Observed event: MODIFIED
Sep  4 15:19:38.357: INFO: Observed event: MODIFIED
Sep  4 15:19:38.357: INFO: Found Service test-service-gsvcz in namespace services-3848 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Sep  4 15:19:38.357: INFO: Service test-service-gsvcz deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 15:19:38.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3848" for this suite. 09/04/23 15:19:38.371
------------------------------
• [0.285 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:19:38.1
    Sep  4 15:19:38.100: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 15:19:38.101
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:38.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:38.161
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 09/04/23 15:19:38.197
    STEP: watching for the Service to be added 09/04/23 15:19:38.215
    Sep  4 15:19:38.226: INFO: Found Service test-service-gsvcz in namespace services-3848 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Sep  4 15:19:38.226: INFO: Service test-service-gsvcz created
    STEP: Getting /status 09/04/23 15:19:38.226
    Sep  4 15:19:38.239: INFO: Service test-service-gsvcz has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 09/04/23 15:19:38.239
    STEP: watching for the Service to be patched 09/04/23 15:19:38.253
    Sep  4 15:19:38.265: INFO: observed Service test-service-gsvcz in namespace services-3848 with annotations: map[] & LoadBalancer: {[]}
    Sep  4 15:19:38.265: INFO: Found Service test-service-gsvcz in namespace services-3848 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Sep  4 15:19:38.265: INFO: Service test-service-gsvcz has service status patched
    STEP: updating the ServiceStatus 09/04/23 15:19:38.265
    Sep  4 15:19:38.290: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 09/04/23 15:19:38.29
    Sep  4 15:19:38.302: INFO: Observed Service test-service-gsvcz in namespace services-3848 with annotations: map[] & Conditions: {[]}
    Sep  4 15:19:38.302: INFO: Observed event: &Service{ObjectMeta:{test-service-gsvcz  services-3848  9856da9d-7135-4522-9274-15867d75dd0a 22786 0 2023-09-04 15:19:38 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-04 15:19:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-04 15:19:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.111.136.173,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[100.111.136.173],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Sep  4 15:19:38.302: INFO: Found Service test-service-gsvcz in namespace services-3848 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  4 15:19:38.302: INFO: Service test-service-gsvcz has service status updated
    STEP: patching the service 09/04/23 15:19:38.302
    STEP: watching for the Service to be patched 09/04/23 15:19:38.317
    Sep  4 15:19:38.328: INFO: observed Service test-service-gsvcz in namespace services-3848 with labels: map[test-service-static:true]
    Sep  4 15:19:38.328: INFO: observed Service test-service-gsvcz in namespace services-3848 with labels: map[test-service-static:true]
    Sep  4 15:19:38.328: INFO: observed Service test-service-gsvcz in namespace services-3848 with labels: map[test-service-static:true]
    Sep  4 15:19:38.328: INFO: Found Service test-service-gsvcz in namespace services-3848 with labels: map[test-service:patched test-service-static:true]
    Sep  4 15:19:38.328: INFO: Service test-service-gsvcz patched
    STEP: deleting the service 09/04/23 15:19:38.328
    STEP: watching for the Service to be deleted 09/04/23 15:19:38.346
    Sep  4 15:19:38.357: INFO: Observed event: ADDED
    Sep  4 15:19:38.357: INFO: Observed event: MODIFIED
    Sep  4 15:19:38.357: INFO: Observed event: MODIFIED
    Sep  4 15:19:38.357: INFO: Observed event: MODIFIED
    Sep  4 15:19:38.357: INFO: Found Service test-service-gsvcz in namespace services-3848 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Sep  4 15:19:38.357: INFO: Service test-service-gsvcz deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:19:38.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3848" for this suite. 09/04/23 15:19:38.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:19:38.386
Sep  4 15:19:38.386: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 15:19:38.387
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:38.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:38.445
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-1623 09/04/23 15:19:38.468
STEP: creating service affinity-clusterip-transition in namespace services-1623 09/04/23 15:19:38.468
STEP: creating replication controller affinity-clusterip-transition in namespace services-1623 09/04/23 15:19:38.486
I0904 15:19:38.499797    7754 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1623, replica count: 3
I0904 15:19:41.551389    7754 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 15:19:41.575: INFO: Creating new exec pod
Sep  4 15:19:41.592: INFO: Waiting up to 5m0s for pod "execpod-affinitynrr82" in namespace "services-1623" to be "running"
Sep  4 15:19:41.605: INFO: Pod "execpod-affinitynrr82": Phase="Pending", Reason="", readiness=false. Elapsed: 12.598152ms
Sep  4 15:19:43.619: INFO: Pod "execpod-affinitynrr82": Phase="Running", Reason="", readiness=true. Elapsed: 2.02741389s
Sep  4 15:19:43.619: INFO: Pod "execpod-affinitynrr82" satisfied condition "running"
Sep  4 15:19:44.620: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1623 exec execpod-affinitynrr82 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Sep  4 15:19:45.226: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep  4 15:19:45.226: INFO: stdout: ""
Sep  4 15:19:45.226: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1623 exec execpod-affinitynrr82 -- /bin/sh -x -c nc -v -z -w 2 100.108.253.178 80'
Sep  4 15:19:45.849: INFO: stderr: "+ nc -v -z -w 2 100.108.253.178 80\nConnection to 100.108.253.178 80 port [tcp/http] succeeded!\n"
Sep  4 15:19:45.849: INFO: stdout: ""
Sep  4 15:19:45.875: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1623 exec execpod-affinitynrr82 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.108.253.178:80/ ; done'
Sep  4 15:19:46.440: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n"
Sep  4 15:19:46.440: INFO: stdout: "\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-bwc27\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-bwc27\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-gp8vm\naffinity-clusterip-transition-gp8vm\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-gp8vm\naffinity-clusterip-transition-gp8vm\naffinity-clusterip-transition-bwc27\naffinity-clusterip-transition-bwc27\naffinity-clusterip-transition-bwc27\naffinity-clusterip-transition-bwc27\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-gp8vm"
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-bwc27
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-bwc27
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-gp8vm
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-gp8vm
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-gp8vm
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-gp8vm
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-bwc27
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-bwc27
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-bwc27
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-bwc27
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-gp8vm
Sep  4 15:19:46.465: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1623 exec execpod-affinitynrr82 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.108.253.178:80/ ; done'
Sep  4 15:19:46.927: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n"
Sep  4 15:19:46.927: INFO: stdout: "\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp"
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
Sep  4 15:19:46.927: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1623, will wait for the garbage collector to delete the pods 09/04/23 15:19:46.943
Sep  4 15:19:47.020: INFO: Deleting ReplicationController affinity-clusterip-transition took: 14.642556ms
Sep  4 15:19:47.121: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.027522ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 15:19:49.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1623" for this suite. 09/04/23 15:19:49.363
------------------------------
• [10.990 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:19:38.386
    Sep  4 15:19:38.386: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 15:19:38.387
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:38.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:38.445
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-1623 09/04/23 15:19:38.468
    STEP: creating service affinity-clusterip-transition in namespace services-1623 09/04/23 15:19:38.468
    STEP: creating replication controller affinity-clusterip-transition in namespace services-1623 09/04/23 15:19:38.486
    I0904 15:19:38.499797    7754 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1623, replica count: 3
    I0904 15:19:41.551389    7754 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 15:19:41.575: INFO: Creating new exec pod
    Sep  4 15:19:41.592: INFO: Waiting up to 5m0s for pod "execpod-affinitynrr82" in namespace "services-1623" to be "running"
    Sep  4 15:19:41.605: INFO: Pod "execpod-affinitynrr82": Phase="Pending", Reason="", readiness=false. Elapsed: 12.598152ms
    Sep  4 15:19:43.619: INFO: Pod "execpod-affinitynrr82": Phase="Running", Reason="", readiness=true. Elapsed: 2.02741389s
    Sep  4 15:19:43.619: INFO: Pod "execpod-affinitynrr82" satisfied condition "running"
    Sep  4 15:19:44.620: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1623 exec execpod-affinitynrr82 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Sep  4 15:19:45.226: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Sep  4 15:19:45.226: INFO: stdout: ""
    Sep  4 15:19:45.226: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1623 exec execpod-affinitynrr82 -- /bin/sh -x -c nc -v -z -w 2 100.108.253.178 80'
    Sep  4 15:19:45.849: INFO: stderr: "+ nc -v -z -w 2 100.108.253.178 80\nConnection to 100.108.253.178 80 port [tcp/http] succeeded!\n"
    Sep  4 15:19:45.849: INFO: stdout: ""
    Sep  4 15:19:45.875: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1623 exec execpod-affinitynrr82 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.108.253.178:80/ ; done'
    Sep  4 15:19:46.440: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n"
    Sep  4 15:19:46.440: INFO: stdout: "\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-bwc27\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-bwc27\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-gp8vm\naffinity-clusterip-transition-gp8vm\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-gp8vm\naffinity-clusterip-transition-gp8vm\naffinity-clusterip-transition-bwc27\naffinity-clusterip-transition-bwc27\naffinity-clusterip-transition-bwc27\naffinity-clusterip-transition-bwc27\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-gp8vm"
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-bwc27
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-bwc27
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-gp8vm
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-gp8vm
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-gp8vm
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-gp8vm
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-bwc27
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-bwc27
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-bwc27
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-bwc27
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.440: INFO: Received response from host: affinity-clusterip-transition-gp8vm
    Sep  4 15:19:46.465: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1623 exec execpod-affinitynrr82 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.108.253.178:80/ ; done'
    Sep  4 15:19:46.927: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.108.253.178:80/\n"
    Sep  4 15:19:46.927: INFO: stdout: "\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp\naffinity-clusterip-transition-pznxp"
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Received response from host: affinity-clusterip-transition-pznxp
    Sep  4 15:19:46.927: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1623, will wait for the garbage collector to delete the pods 09/04/23 15:19:46.943
    Sep  4 15:19:47.020: INFO: Deleting ReplicationController affinity-clusterip-transition took: 14.642556ms
    Sep  4 15:19:47.121: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.027522ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:19:49.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1623" for this suite. 09/04/23 15:19:49.363
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:19:49.376
Sep  4 15:19:49.376: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 15:19:49.377
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:49.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:49.437
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 09/04/23 15:19:49.459
Sep  4 15:19:49.478: INFO: Waiting up to 5m0s for pod "pod-90f42d17-cfbc-4539-88ac-5a5196168056" in namespace "emptydir-6669" to be "Succeeded or Failed"
Sep  4 15:19:49.490: INFO: Pod "pod-90f42d17-cfbc-4539-88ac-5a5196168056": Phase="Pending", Reason="", readiness=false. Elapsed: 12.100029ms
Sep  4 15:19:51.504: INFO: Pod "pod-90f42d17-cfbc-4539-88ac-5a5196168056": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025718657s
Sep  4 15:19:53.504: INFO: Pod "pod-90f42d17-cfbc-4539-88ac-5a5196168056": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025645428s
STEP: Saw pod success 09/04/23 15:19:53.504
Sep  4 15:19:53.504: INFO: Pod "pod-90f42d17-cfbc-4539-88ac-5a5196168056" satisfied condition "Succeeded or Failed"
Sep  4 15:19:53.517: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-90f42d17-cfbc-4539-88ac-5a5196168056 container test-container: <nil>
STEP: delete the pod 09/04/23 15:19:53.592
Sep  4 15:19:53.608: INFO: Waiting for pod pod-90f42d17-cfbc-4539-88ac-5a5196168056 to disappear
Sep  4 15:19:53.621: INFO: Pod pod-90f42d17-cfbc-4539-88ac-5a5196168056 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 15:19:53.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6669" for this suite. 09/04/23 15:19:53.644
------------------------------
• [4.282 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:19:49.376
    Sep  4 15:19:49.376: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 15:19:49.377
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:49.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:49.437
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 09/04/23 15:19:49.459
    Sep  4 15:19:49.478: INFO: Waiting up to 5m0s for pod "pod-90f42d17-cfbc-4539-88ac-5a5196168056" in namespace "emptydir-6669" to be "Succeeded or Failed"
    Sep  4 15:19:49.490: INFO: Pod "pod-90f42d17-cfbc-4539-88ac-5a5196168056": Phase="Pending", Reason="", readiness=false. Elapsed: 12.100029ms
    Sep  4 15:19:51.504: INFO: Pod "pod-90f42d17-cfbc-4539-88ac-5a5196168056": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025718657s
    Sep  4 15:19:53.504: INFO: Pod "pod-90f42d17-cfbc-4539-88ac-5a5196168056": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025645428s
    STEP: Saw pod success 09/04/23 15:19:53.504
    Sep  4 15:19:53.504: INFO: Pod "pod-90f42d17-cfbc-4539-88ac-5a5196168056" satisfied condition "Succeeded or Failed"
    Sep  4 15:19:53.517: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-90f42d17-cfbc-4539-88ac-5a5196168056 container test-container: <nil>
    STEP: delete the pod 09/04/23 15:19:53.592
    Sep  4 15:19:53.608: INFO: Waiting for pod pod-90f42d17-cfbc-4539-88ac-5a5196168056 to disappear
    Sep  4 15:19:53.621: INFO: Pod pod-90f42d17-cfbc-4539-88ac-5a5196168056 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:19:53.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6669" for this suite. 09/04/23 15:19:53.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:19:53.658
Sep  4 15:19:53.658: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 15:19:53.659
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:53.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:53.719
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-981fa00e-8afa-4bfc-9ff8-e1a01406a6fc 09/04/23 15:19:53.741
STEP: Creating a pod to test consume configMaps 09/04/23 15:19:53.754
Sep  4 15:19:53.773: INFO: Waiting up to 5m0s for pod "pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917" in namespace "configmap-7120" to be "Succeeded or Failed"
Sep  4 15:19:53.784: INFO: Pod "pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917": Phase="Pending", Reason="", readiness=false. Elapsed: 11.408933ms
Sep  4 15:19:55.798: INFO: Pod "pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025564598s
Sep  4 15:19:57.798: INFO: Pod "pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024793169s
STEP: Saw pod success 09/04/23 15:19:57.798
Sep  4 15:19:57.798: INFO: Pod "pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917" satisfied condition "Succeeded or Failed"
Sep  4 15:19:57.810: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 15:19:57.845
Sep  4 15:19:57.861: INFO: Waiting for pod pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917 to disappear
Sep  4 15:19:57.873: INFO: Pod pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:19:57.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7120" for this suite. 09/04/23 15:19:57.895
------------------------------
• [4.251 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:19:53.658
    Sep  4 15:19:53.658: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 15:19:53.659
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:53.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:53.719
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-981fa00e-8afa-4bfc-9ff8-e1a01406a6fc 09/04/23 15:19:53.741
    STEP: Creating a pod to test consume configMaps 09/04/23 15:19:53.754
    Sep  4 15:19:53.773: INFO: Waiting up to 5m0s for pod "pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917" in namespace "configmap-7120" to be "Succeeded or Failed"
    Sep  4 15:19:53.784: INFO: Pod "pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917": Phase="Pending", Reason="", readiness=false. Elapsed: 11.408933ms
    Sep  4 15:19:55.798: INFO: Pod "pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025564598s
    Sep  4 15:19:57.798: INFO: Pod "pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024793169s
    STEP: Saw pod success 09/04/23 15:19:57.798
    Sep  4 15:19:57.798: INFO: Pod "pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917" satisfied condition "Succeeded or Failed"
    Sep  4 15:19:57.810: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 15:19:57.845
    Sep  4 15:19:57.861: INFO: Waiting for pod pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917 to disappear
    Sep  4 15:19:57.873: INFO: Pod pod-configmaps-3deb5d17-fbca-423d-9525-c2be5c741917 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:19:57.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7120" for this suite. 09/04/23 15:19:57.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:19:57.91
Sep  4 15:19:57.910: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 15:19:57.911
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:57.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:57.969
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-80f53eed-571b-4945-91d4-cd75c40f0c9f 09/04/23 15:19:58.005
STEP: Creating the pod 09/04/23 15:19:58.018
Sep  4 15:19:58.036: INFO: Waiting up to 5m0s for pod "pod-configmaps-0255f82f-8959-4da7-a70c-2685c49968b0" in namespace "configmap-5036" to be "running and ready"
Sep  4 15:19:58.048: INFO: Pod "pod-configmaps-0255f82f-8959-4da7-a70c-2685c49968b0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.891434ms
Sep  4 15:19:58.048: INFO: The phase of Pod pod-configmaps-0255f82f-8959-4da7-a70c-2685c49968b0 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:20:00.062: INFO: Pod "pod-configmaps-0255f82f-8959-4da7-a70c-2685c49968b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.025593934s
Sep  4 15:20:00.062: INFO: The phase of Pod pod-configmaps-0255f82f-8959-4da7-a70c-2685c49968b0 is Running (Ready = true)
Sep  4 15:20:00.062: INFO: Pod "pod-configmaps-0255f82f-8959-4da7-a70c-2685c49968b0" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-80f53eed-571b-4945-91d4-cd75c40f0c9f 09/04/23 15:20:00.108
STEP: waiting to observe update in volume 09/04/23 15:20:00.122
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:20:02.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5036" for this suite. 09/04/23 15:20:02.274
------------------------------
• [4.379 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:19:57.91
    Sep  4 15:19:57.910: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 15:19:57.911
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:19:57.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:19:57.969
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-80f53eed-571b-4945-91d4-cd75c40f0c9f 09/04/23 15:19:58.005
    STEP: Creating the pod 09/04/23 15:19:58.018
    Sep  4 15:19:58.036: INFO: Waiting up to 5m0s for pod "pod-configmaps-0255f82f-8959-4da7-a70c-2685c49968b0" in namespace "configmap-5036" to be "running and ready"
    Sep  4 15:19:58.048: INFO: Pod "pod-configmaps-0255f82f-8959-4da7-a70c-2685c49968b0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.891434ms
    Sep  4 15:19:58.048: INFO: The phase of Pod pod-configmaps-0255f82f-8959-4da7-a70c-2685c49968b0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:20:00.062: INFO: Pod "pod-configmaps-0255f82f-8959-4da7-a70c-2685c49968b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.025593934s
    Sep  4 15:20:00.062: INFO: The phase of Pod pod-configmaps-0255f82f-8959-4da7-a70c-2685c49968b0 is Running (Ready = true)
    Sep  4 15:20:00.062: INFO: Pod "pod-configmaps-0255f82f-8959-4da7-a70c-2685c49968b0" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-80f53eed-571b-4945-91d4-cd75c40f0c9f 09/04/23 15:20:00.108
    STEP: waiting to observe update in volume 09/04/23 15:20:00.122
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:20:02.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5036" for this suite. 09/04/23 15:20:02.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:20:02.289
Sep  4 15:20:02.289: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 09/04/23 15:20:02.29
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:20:02.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:20:02.347
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  4 15:20:06.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6342" for this suite. 09/04/23 15:20:06.443
------------------------------
• [4.167 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:20:02.289
    Sep  4 15:20:02.289: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 09/04/23 15:20:02.29
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:20:02.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:20:02.347
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:20:06.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6342" for this suite. 09/04/23 15:20:06.443
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:20:06.456
Sep  4 15:20:06.456: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostport 09/04/23 15:20:06.456
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:20:06.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:20:06.516
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/04/23 15:20:06.551
Sep  4 15:20:06.569: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6545" to be "running and ready"
Sep  4 15:20:06.582: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.565006ms
Sep  4 15:20:06.582: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:20:08.595: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.026282937s
Sep  4 15:20:08.595: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  4 15:20:08.595: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.250.1.105 on the node which pod1 resides and expect scheduled 09/04/23 15:20:08.595
Sep  4 15:20:08.613: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6545" to be "running and ready"
Sep  4 15:20:08.625: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.448493ms
Sep  4 15:20:08.626: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:20:10.643: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.029632151s
Sep  4 15:20:10.643: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  4 15:20:10.643: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.250.1.105 but use UDP protocol on the node which pod2 resides 09/04/23 15:20:10.643
Sep  4 15:20:10.659: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6545" to be "running and ready"
Sep  4 15:20:10.673: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006147ms
Sep  4 15:20:10.674: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:20:12.688: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.028156441s
Sep  4 15:20:12.688: INFO: The phase of Pod pod3 is Running (Ready = true)
Sep  4 15:20:12.688: INFO: Pod "pod3" satisfied condition "running and ready"
Sep  4 15:20:12.703: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6545" to be "running and ready"
Sep  4 15:20:12.715: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 11.919205ms
Sep  4 15:20:12.715: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:20:14.729: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.025688538s
Sep  4 15:20:14.729: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Sep  4 15:20:14.729: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/04/23 15:20:14.741
Sep  4 15:20:14.741: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.1.105 http://127.0.0.1:54323/hostname] Namespace:hostport-6545 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:20:14.741: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:20:14.742: INFO: ExecWithOptions: Clientset creation
Sep  4 15:20:14.742: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6545/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.250.1.105+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.1.105, port: 54323 09/04/23 15:20:15.206
Sep  4 15:20:15.206: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.1.105:54323/hostname] Namespace:hostport-6545 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:20:15.206: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:20:15.207: INFO: ExecWithOptions: Clientset creation
Sep  4 15:20:15.207: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6545/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.250.1.105%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.1.105, port: 54323 UDP 09/04/23 15:20:15.803
Sep  4 15:20:15.803: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.250.1.105 54323] Namespace:hostport-6545 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:20:15.803: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:20:15.804: INFO: ExecWithOptions: Clientset creation
Sep  4 15:20:15.804: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6545/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.250.1.105+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Sep  4 15:20:21.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-6545" for this suite. 09/04/23 15:20:21.378
------------------------------
• [14.936 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:20:06.456
    Sep  4 15:20:06.456: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename hostport 09/04/23 15:20:06.456
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:20:06.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:20:06.516
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/04/23 15:20:06.551
    Sep  4 15:20:06.569: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6545" to be "running and ready"
    Sep  4 15:20:06.582: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.565006ms
    Sep  4 15:20:06.582: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:20:08.595: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.026282937s
    Sep  4 15:20:08.595: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  4 15:20:08.595: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.250.1.105 on the node which pod1 resides and expect scheduled 09/04/23 15:20:08.595
    Sep  4 15:20:08.613: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6545" to be "running and ready"
    Sep  4 15:20:08.625: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.448493ms
    Sep  4 15:20:08.626: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:20:10.643: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.029632151s
    Sep  4 15:20:10.643: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  4 15:20:10.643: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.250.1.105 but use UDP protocol on the node which pod2 resides 09/04/23 15:20:10.643
    Sep  4 15:20:10.659: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6545" to be "running and ready"
    Sep  4 15:20:10.673: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006147ms
    Sep  4 15:20:10.674: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:20:12.688: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.028156441s
    Sep  4 15:20:12.688: INFO: The phase of Pod pod3 is Running (Ready = true)
    Sep  4 15:20:12.688: INFO: Pod "pod3" satisfied condition "running and ready"
    Sep  4 15:20:12.703: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6545" to be "running and ready"
    Sep  4 15:20:12.715: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 11.919205ms
    Sep  4 15:20:12.715: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:20:14.729: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.025688538s
    Sep  4 15:20:14.729: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Sep  4 15:20:14.729: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/04/23 15:20:14.741
    Sep  4 15:20:14.741: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.1.105 http://127.0.0.1:54323/hostname] Namespace:hostport-6545 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:20:14.741: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:20:14.742: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:20:14.742: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6545/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.250.1.105+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.1.105, port: 54323 09/04/23 15:20:15.206
    Sep  4 15:20:15.206: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.1.105:54323/hostname] Namespace:hostport-6545 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:20:15.206: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:20:15.207: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:20:15.207: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6545/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.250.1.105%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.1.105, port: 54323 UDP 09/04/23 15:20:15.803
    Sep  4 15:20:15.803: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.250.1.105 54323] Namespace:hostport-6545 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:20:15.803: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:20:15.804: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:20:15.804: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6545/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.250.1.105+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:20:21.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-6545" for this suite. 09/04/23 15:20:21.378
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:20:21.392
Sep  4 15:20:21.392: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 09/04/23 15:20:21.393
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:20:21.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:20:21.45
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  4 15:20:21.472: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  4 15:20:21.497: INFO: Waiting for terminating namespaces to be deleted...
Sep  4 15:20:21.509: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx before test
Sep  4 15:20:21.528: INFO: e2e-host-exec from hostport-6545 started at 2023-09-04 15:20:12 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container e2e-host-exec ready: true, restart count 0
Sep  4 15:20:21.528: INFO: pod1 from hostport-6545 started at 2023-09-04 15:20:06 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container agnhost ready: true, restart count 0
Sep  4 15:20:21.528: INFO: pod2 from hostport-6545 started at 2023-09-04 15:20:08 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container agnhost ready: true, restart count 0
Sep  4 15:20:21.528: INFO: pod3 from hostport-6545 started at 2023-09-04 15:20:10 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container agnhost ready: true, restart count 0
Sep  4 15:20:21.528: INFO: addons-nginx-ingress-controller-56b5dc8f6c-tr9n6 from kube-system started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep  4 15:20:21.528: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-58685978db-gpxdq from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Sep  4 15:20:21.528: INFO: apiserver-proxy-bc5jh from kube-system started at 2023-09-04 14:34:44 +0000 UTC (2 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container proxy ready: true, restart count 0
Sep  4 15:20:21.528: INFO: 	Container sidecar ready: true, restart count 0
Sep  4 15:20:21.528: INFO: blackbox-exporter-585854d657-fvtbg from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container blackbox-exporter ready: true, restart count 0
Sep  4 15:20:21.528: INFO: blackbox-exporter-585854d657-vz8rw from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container blackbox-exporter ready: true, restart count 0
Sep  4 15:20:21.528: INFO: calico-kube-controllers-684b9f4889-f24wb from kube-system started at 2023-09-04 14:34:42 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  4 15:20:21.528: INFO: calico-node-959qs from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 15:20:21.528: INFO: calico-node-vertical-autoscaler-7bbd54698f-zb6st from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container autoscaler ready: true, restart count 0
Sep  4 15:20:21.528: INFO: calico-typha-deploy-6f4475c6d5-z7t2m from kube-system started at 2023-09-04 14:35:50 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container calico-typha ready: true, restart count 0
Sep  4 15:20:21.528: INFO: calico-typha-horizontal-autoscaler-7b89d5ff97-gdvkd from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container autoscaler ready: true, restart count 0
Sep  4 15:20:21.528: INFO: calico-typha-vertical-autoscaler-656479b7b5-khd8g from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container autoscaler ready: true, restart count 0
Sep  4 15:20:21.528: INFO: coredns-89679867b-2fr9s from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container coredns ready: true, restart count 0
Sep  4 15:20:21.528: INFO: coredns-89679867b-5qtl7 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container coredns ready: true, restart count 0
Sep  4 15:20:21.528: INFO: csi-driver-node-5kfkx from kube-system started at 2023-09-04 14:34:46 +0000 UTC (3 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container csi-driver ready: true, restart count 0
Sep  4 15:20:21.528: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Sep  4 15:20:21.528: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Sep  4 15:20:21.528: INFO: egress-filter-applier-n4kfc from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container egress-filter-applier ready: true, restart count 0
Sep  4 15:20:21.528: INFO: kube-proxy-worker-1-v1.26.8-ch6px from kube-system started at 2023-09-04 15:01:11 +0000 UTC (2 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container conntrack-fix ready: true, restart count 0
Sep  4 15:20:21.528: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 15:20:21.528: INFO: metrics-server-78947f8d7c-9cdcs from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container metrics-server ready: true, restart count 0
Sep  4 15:20:21.528: INFO: metrics-server-78947f8d7c-w8gr6 from kube-system started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container metrics-server ready: true, restart count 0
Sep  4 15:20:21.528: INFO: network-problem-detector-host-xf9l5 from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Sep  4 15:20:21.528: INFO: network-problem-detector-pod-lbp4l from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Sep  4 15:20:21.528: INFO: node-exporter-ktjnf from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container node-exporter ready: true, restart count 0
Sep  4 15:20:21.528: INFO: node-local-dns-c8s84 from kube-system started at 2023-09-04 14:44:10 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container node-cache ready: true, restart count 0
Sep  4 15:20:21.528: INFO: node-problem-detector-rqmbk from kube-system started at 2023-09-04 14:41:10 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep  4 15:20:21.528: INFO: vpn-shoot-5d596dbb88-5vx52 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container vpn-shoot ready: true, restart count 0
Sep  4 15:20:21.528: INFO: dashboard-metrics-scraper-6c889fdd54-555v8 from kubernetes-dashboard started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Sep  4 15:20:21.528: INFO: kubernetes-dashboard-b9859c4d7-bq9mj from kubernetes-dashboard started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.528: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep  4 15:20:21.528: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh before test
Sep  4 15:20:21.554: INFO: apiserver-proxy-4xjvl from kube-system started at 2023-09-04 14:35:03 +0000 UTC (2 container statuses recorded)
Sep  4 15:20:21.554: INFO: 	Container proxy ready: true, restart count 0
Sep  4 15:20:21.554: INFO: 	Container sidecar ready: true, restart count 0
Sep  4 15:20:21.554: INFO: calico-node-dqrx8 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.554: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 15:20:21.554: INFO: calico-typha-deploy-6f4475c6d5-p54fg from kube-system started at 2023-09-04 14:51:41 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.554: INFO: 	Container calico-typha ready: true, restart count 0
Sep  4 15:20:21.554: INFO: csi-driver-node-fcbsh from kube-system started at 2023-09-04 14:35:03 +0000 UTC (3 container statuses recorded)
Sep  4 15:20:21.554: INFO: 	Container csi-driver ready: true, restart count 0
Sep  4 15:20:21.554: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Sep  4 15:20:21.554: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Sep  4 15:20:21.554: INFO: egress-filter-applier-42dhz from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.554: INFO: 	Container egress-filter-applier ready: true, restart count 0
Sep  4 15:20:21.554: INFO: kube-proxy-worker-1-v1.26.8-zppm4 from kube-system started at 2023-09-04 14:36:10 +0000 UTC (2 container statuses recorded)
Sep  4 15:20:21.554: INFO: 	Container conntrack-fix ready: true, restart count 0
Sep  4 15:20:21.554: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 15:20:21.554: INFO: network-problem-detector-host-4k5k5 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.554: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Sep  4 15:20:21.554: INFO: network-problem-detector-pod-sv5cs from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.554: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Sep  4 15:20:21.554: INFO: node-exporter-mp64n from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.554: INFO: 	Container node-exporter ready: true, restart count 0
Sep  4 15:20:21.554: INFO: node-local-dns-fjhw2 from kube-system started at 2023-09-04 14:43:11 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.554: INFO: 	Container node-cache ready: true, restart count 0
Sep  4 15:20:21.554: INFO: node-problem-detector-gqcc9 from kube-system started at 2023-09-04 14:42:11 +0000 UTC (1 container statuses recorded)
Sep  4 15:20:21.554: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx 09/04/23 15:20:21.595
STEP: verifying the node has the label node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh 09/04/23 15:20:21.627
Sep  4 15:20:21.661: INFO: Pod e2e-host-exec requesting resource cpu=0m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod pod1 requesting resource cpu=0m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod pod2 requesting resource cpu=0m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod pod3 requesting resource cpu=0m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod addons-nginx-ingress-controller-56b5dc8f6c-tr9n6 requesting resource cpu=11m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-58685978db-gpxdq requesting resource cpu=20m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod apiserver-proxy-4xjvl requesting resource cpu=40m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
Sep  4 15:20:21.661: INFO: Pod apiserver-proxy-bc5jh requesting resource cpu=40m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod blackbox-exporter-585854d657-fvtbg requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod blackbox-exporter-585854d657-vz8rw requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod calico-kube-controllers-684b9f4889-f24wb requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod calico-node-959qs requesting resource cpu=250m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod calico-node-dqrx8 requesting resource cpu=250m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
Sep  4 15:20:21.661: INFO: Pod calico-node-vertical-autoscaler-7bbd54698f-zb6st requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod calico-typha-deploy-6f4475c6d5-p54fg requesting resource cpu=320m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
Sep  4 15:20:21.661: INFO: Pod calico-typha-deploy-6f4475c6d5-z7t2m requesting resource cpu=320m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod calico-typha-horizontal-autoscaler-7b89d5ff97-gdvkd requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod calico-typha-vertical-autoscaler-656479b7b5-khd8g requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod coredns-89679867b-2fr9s requesting resource cpu=50m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod coredns-89679867b-5qtl7 requesting resource cpu=50m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod csi-driver-node-5kfkx requesting resource cpu=37m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod csi-driver-node-fcbsh requesting resource cpu=37m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
Sep  4 15:20:21.661: INFO: Pod egress-filter-applier-42dhz requesting resource cpu=50m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
Sep  4 15:20:21.661: INFO: Pod egress-filter-applier-n4kfc requesting resource cpu=50m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod kube-proxy-worker-1-v1.26.8-ch6px requesting resource cpu=34m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod kube-proxy-worker-1-v1.26.8-zppm4 requesting resource cpu=70m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
Sep  4 15:20:21.661: INFO: Pod metrics-server-78947f8d7c-9cdcs requesting resource cpu=50m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod metrics-server-78947f8d7c-w8gr6 requesting resource cpu=11m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod network-problem-detector-host-4k5k5 requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
Sep  4 15:20:21.661: INFO: Pod network-problem-detector-host-xf9l5 requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod network-problem-detector-pod-lbp4l requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod network-problem-detector-pod-sv5cs requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
Sep  4 15:20:21.661: INFO: Pod node-exporter-ktjnf requesting resource cpu=50m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod node-exporter-mp64n requesting resource cpu=50m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
Sep  4 15:20:21.661: INFO: Pod node-local-dns-c8s84 requesting resource cpu=11m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod node-local-dns-fjhw2 requesting resource cpu=11m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
Sep  4 15:20:21.661: INFO: Pod node-problem-detector-gqcc9 requesting resource cpu=35m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
Sep  4 15:20:21.661: INFO: Pod node-problem-detector-rqmbk requesting resource cpu=49m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod vpn-shoot-5d596dbb88-5vx52 requesting resource cpu=100m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod dashboard-metrics-scraper-6c889fdd54-555v8 requesting resource cpu=0m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.661: INFO: Pod kubernetes-dashboard-b9859c4d7-bq9mj requesting resource cpu=11m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
STEP: Starting Pods to consume most of the cluster CPU. 09/04/23 15:20:21.661
Sep  4 15:20:21.661: INFO: Creating a pod which consumes cpu=487m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
Sep  4 15:20:21.679: INFO: Creating a pod which consumes cpu=725m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
Sep  4 15:20:21.696: INFO: Waiting up to 5m0s for pod "filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6" in namespace "sched-pred-479" to be "running"
Sep  4 15:20:21.708: INFO: Pod "filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.995623ms
Sep  4 15:20:23.722: INFO: Pod "filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.02646411s
Sep  4 15:20:23.722: INFO: Pod "filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6" satisfied condition "running"
Sep  4 15:20:23.722: INFO: Waiting up to 5m0s for pod "filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7" in namespace "sched-pred-479" to be "running"
Sep  4 15:20:23.735: INFO: Pod "filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7": Phase="Running", Reason="", readiness=true. Elapsed: 12.690333ms
Sep  4 15:20:23.735: INFO: Pod "filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 09/04/23 15:20:23.735
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6.1781bb40c224e675], Reason = [Scheduled], Message = [Successfully assigned sched-pred-479/filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6 to shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx] 09/04/23 15:20:23.748
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6.1781bb40e677425a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/04/23 15:20:23.748
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6.1781bb40e729e604], Reason = [Created], Message = [Created container filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6] 09/04/23 15:20:23.748
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6.1781bb40eb4c512a], Reason = [Started], Message = [Started container filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6] 09/04/23 15:20:23.748
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7.1781bb40c30dc02a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-479/filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7 to shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh] 09/04/23 15:20:23.748
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7.1781bb40e60114f3], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/04/23 15:20:23.748
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7.1781bb40e6c9944c], Reason = [Created], Message = [Created container filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7] 09/04/23 15:20:23.748
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7.1781bb40eadf658d], Reason = [Started], Message = [Started container filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7] 09/04/23 15:20:23.748
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1781bb413efdd98f], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 09/04/23 15:20:23.781
STEP: removing the label node off the node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh 09/04/23 15:20:24.791
STEP: verifying the node doesn't have the label node 09/04/23 15:20:24.833
STEP: removing the label node off the node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx 09/04/23 15:20:24.845
STEP: verifying the node doesn't have the label node 09/04/23 15:20:24.878
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:20:24.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-479" for this suite. 09/04/23 15:20:24.903
------------------------------
• [3.524 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:20:21.392
    Sep  4 15:20:21.392: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 09/04/23 15:20:21.393
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:20:21.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:20:21.45
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  4 15:20:21.472: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  4 15:20:21.497: INFO: Waiting for terminating namespaces to be deleted...
    Sep  4 15:20:21.509: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx before test
    Sep  4 15:20:21.528: INFO: e2e-host-exec from hostport-6545 started at 2023-09-04 15:20:12 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container e2e-host-exec ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: pod1 from hostport-6545 started at 2023-09-04 15:20:06 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container agnhost ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: pod2 from hostport-6545 started at 2023-09-04 15:20:08 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container agnhost ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: pod3 from hostport-6545 started at 2023-09-04 15:20:10 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container agnhost ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: addons-nginx-ingress-controller-56b5dc8f6c-tr9n6 from kube-system started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-58685978db-gpxdq from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: apiserver-proxy-bc5jh from kube-system started at 2023-09-04 14:34:44 +0000 UTC (2 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container proxy ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: 	Container sidecar ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: blackbox-exporter-585854d657-fvtbg from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: blackbox-exporter-585854d657-vz8rw from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: calico-kube-controllers-684b9f4889-f24wb from kube-system started at 2023-09-04 14:34:42 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: calico-node-959qs from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: calico-node-vertical-autoscaler-7bbd54698f-zb6st from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: calico-typha-deploy-6f4475c6d5-z7t2m from kube-system started at 2023-09-04 14:35:50 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container calico-typha ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: calico-typha-horizontal-autoscaler-7b89d5ff97-gdvkd from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: calico-typha-vertical-autoscaler-656479b7b5-khd8g from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: coredns-89679867b-2fr9s from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: coredns-89679867b-5qtl7 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: csi-driver-node-5kfkx from kube-system started at 2023-09-04 14:34:46 +0000 UTC (3 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container csi-driver ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: egress-filter-applier-n4kfc from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: kube-proxy-worker-1-v1.26.8-ch6px from kube-system started at 2023-09-04 15:01:11 +0000 UTC (2 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container conntrack-fix ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: metrics-server-78947f8d7c-9cdcs from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container metrics-server ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: metrics-server-78947f8d7c-w8gr6 from kube-system started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container metrics-server ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: network-problem-detector-host-xf9l5 from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: network-problem-detector-pod-lbp4l from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: node-exporter-ktjnf from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: node-local-dns-c8s84 from kube-system started at 2023-09-04 14:44:10 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container node-cache ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: node-problem-detector-rqmbk from kube-system started at 2023-09-04 14:41:10 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: vpn-shoot-5d596dbb88-5vx52 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container vpn-shoot ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: dashboard-metrics-scraper-6c889fdd54-555v8 from kubernetes-dashboard started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: kubernetes-dashboard-b9859c4d7-bq9mj from kubernetes-dashboard started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.528: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Sep  4 15:20:21.528: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh before test
    Sep  4 15:20:21.554: INFO: apiserver-proxy-4xjvl from kube-system started at 2023-09-04 14:35:03 +0000 UTC (2 container statuses recorded)
    Sep  4 15:20:21.554: INFO: 	Container proxy ready: true, restart count 0
    Sep  4 15:20:21.554: INFO: 	Container sidecar ready: true, restart count 0
    Sep  4 15:20:21.554: INFO: calico-node-dqrx8 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.554: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 15:20:21.554: INFO: calico-typha-deploy-6f4475c6d5-p54fg from kube-system started at 2023-09-04 14:51:41 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.554: INFO: 	Container calico-typha ready: true, restart count 0
    Sep  4 15:20:21.554: INFO: csi-driver-node-fcbsh from kube-system started at 2023-09-04 14:35:03 +0000 UTC (3 container statuses recorded)
    Sep  4 15:20:21.554: INFO: 	Container csi-driver ready: true, restart count 0
    Sep  4 15:20:21.554: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Sep  4 15:20:21.554: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Sep  4 15:20:21.554: INFO: egress-filter-applier-42dhz from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.554: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Sep  4 15:20:21.554: INFO: kube-proxy-worker-1-v1.26.8-zppm4 from kube-system started at 2023-09-04 14:36:10 +0000 UTC (2 container statuses recorded)
    Sep  4 15:20:21.554: INFO: 	Container conntrack-fix ready: true, restart count 0
    Sep  4 15:20:21.554: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 15:20:21.554: INFO: network-problem-detector-host-4k5k5 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.554: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Sep  4 15:20:21.554: INFO: network-problem-detector-pod-sv5cs from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.554: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Sep  4 15:20:21.554: INFO: node-exporter-mp64n from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.554: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  4 15:20:21.554: INFO: node-local-dns-fjhw2 from kube-system started at 2023-09-04 14:43:11 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.554: INFO: 	Container node-cache ready: true, restart count 0
    Sep  4 15:20:21.554: INFO: node-problem-detector-gqcc9 from kube-system started at 2023-09-04 14:42:11 +0000 UTC (1 container statuses recorded)
    Sep  4 15:20:21.554: INFO: 	Container node-problem-detector ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx 09/04/23 15:20:21.595
    STEP: verifying the node has the label node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh 09/04/23 15:20:21.627
    Sep  4 15:20:21.661: INFO: Pod e2e-host-exec requesting resource cpu=0m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod pod1 requesting resource cpu=0m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod pod2 requesting resource cpu=0m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod pod3 requesting resource cpu=0m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod addons-nginx-ingress-controller-56b5dc8f6c-tr9n6 requesting resource cpu=11m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-58685978db-gpxdq requesting resource cpu=20m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod apiserver-proxy-4xjvl requesting resource cpu=40m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
    Sep  4 15:20:21.661: INFO: Pod apiserver-proxy-bc5jh requesting resource cpu=40m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod blackbox-exporter-585854d657-fvtbg requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod blackbox-exporter-585854d657-vz8rw requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod calico-kube-controllers-684b9f4889-f24wb requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod calico-node-959qs requesting resource cpu=250m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod calico-node-dqrx8 requesting resource cpu=250m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
    Sep  4 15:20:21.661: INFO: Pod calico-node-vertical-autoscaler-7bbd54698f-zb6st requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod calico-typha-deploy-6f4475c6d5-p54fg requesting resource cpu=320m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
    Sep  4 15:20:21.661: INFO: Pod calico-typha-deploy-6f4475c6d5-z7t2m requesting resource cpu=320m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod calico-typha-horizontal-autoscaler-7b89d5ff97-gdvkd requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod calico-typha-vertical-autoscaler-656479b7b5-khd8g requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod coredns-89679867b-2fr9s requesting resource cpu=50m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod coredns-89679867b-5qtl7 requesting resource cpu=50m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod csi-driver-node-5kfkx requesting resource cpu=37m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod csi-driver-node-fcbsh requesting resource cpu=37m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
    Sep  4 15:20:21.661: INFO: Pod egress-filter-applier-42dhz requesting resource cpu=50m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
    Sep  4 15:20:21.661: INFO: Pod egress-filter-applier-n4kfc requesting resource cpu=50m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod kube-proxy-worker-1-v1.26.8-ch6px requesting resource cpu=34m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod kube-proxy-worker-1-v1.26.8-zppm4 requesting resource cpu=70m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
    Sep  4 15:20:21.661: INFO: Pod metrics-server-78947f8d7c-9cdcs requesting resource cpu=50m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod metrics-server-78947f8d7c-w8gr6 requesting resource cpu=11m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod network-problem-detector-host-4k5k5 requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
    Sep  4 15:20:21.661: INFO: Pod network-problem-detector-host-xf9l5 requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod network-problem-detector-pod-lbp4l requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod network-problem-detector-pod-sv5cs requesting resource cpu=10m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
    Sep  4 15:20:21.661: INFO: Pod node-exporter-ktjnf requesting resource cpu=50m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod node-exporter-mp64n requesting resource cpu=50m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
    Sep  4 15:20:21.661: INFO: Pod node-local-dns-c8s84 requesting resource cpu=11m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod node-local-dns-fjhw2 requesting resource cpu=11m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
    Sep  4 15:20:21.661: INFO: Pod node-problem-detector-gqcc9 requesting resource cpu=35m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
    Sep  4 15:20:21.661: INFO: Pod node-problem-detector-rqmbk requesting resource cpu=49m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod vpn-shoot-5d596dbb88-5vx52 requesting resource cpu=100m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod dashboard-metrics-scraper-6c889fdd54-555v8 requesting resource cpu=0m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.661: INFO: Pod kubernetes-dashboard-b9859c4d7-bq9mj requesting resource cpu=11m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    STEP: Starting Pods to consume most of the cluster CPU. 09/04/23 15:20:21.661
    Sep  4 15:20:21.661: INFO: Creating a pod which consumes cpu=487m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx
    Sep  4 15:20:21.679: INFO: Creating a pod which consumes cpu=725m on Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
    Sep  4 15:20:21.696: INFO: Waiting up to 5m0s for pod "filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6" in namespace "sched-pred-479" to be "running"
    Sep  4 15:20:21.708: INFO: Pod "filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.995623ms
    Sep  4 15:20:23.722: INFO: Pod "filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.02646411s
    Sep  4 15:20:23.722: INFO: Pod "filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6" satisfied condition "running"
    Sep  4 15:20:23.722: INFO: Waiting up to 5m0s for pod "filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7" in namespace "sched-pred-479" to be "running"
    Sep  4 15:20:23.735: INFO: Pod "filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7": Phase="Running", Reason="", readiness=true. Elapsed: 12.690333ms
    Sep  4 15:20:23.735: INFO: Pod "filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 09/04/23 15:20:23.735
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6.1781bb40c224e675], Reason = [Scheduled], Message = [Successfully assigned sched-pred-479/filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6 to shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx] 09/04/23 15:20:23.748
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6.1781bb40e677425a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/04/23 15:20:23.748
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6.1781bb40e729e604], Reason = [Created], Message = [Created container filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6] 09/04/23 15:20:23.748
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6.1781bb40eb4c512a], Reason = [Started], Message = [Started container filler-pod-ba0c4033-f158-4c3b-87f2-77f37f5152e6] 09/04/23 15:20:23.748
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7.1781bb40c30dc02a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-479/filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7 to shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh] 09/04/23 15:20:23.748
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7.1781bb40e60114f3], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/04/23 15:20:23.748
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7.1781bb40e6c9944c], Reason = [Created], Message = [Created container filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7] 09/04/23 15:20:23.748
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7.1781bb40eadf658d], Reason = [Started], Message = [Started container filler-pod-f0cc69e5-b36e-4e53-9c04-268fcb626ed7] 09/04/23 15:20:23.748
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1781bb413efdd98f], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 09/04/23 15:20:23.781
    STEP: removing the label node off the node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh 09/04/23 15:20:24.791
    STEP: verifying the node doesn't have the label node 09/04/23 15:20:24.833
    STEP: removing the label node off the node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx 09/04/23 15:20:24.845
    STEP: verifying the node doesn't have the label node 09/04/23 15:20:24.878
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:20:24.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-479" for this suite. 09/04/23 15:20:24.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:20:24.916
Sep  4 15:20:24.916: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-watch 09/04/23 15:20:24.917
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:20:24.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:20:24.977
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Sep  4 15:20:25.000: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating first CR  09/04/23 15:20:27.115
Sep  4 15:20:27.129: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T15:20:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T15:20:27Z]] name:name1 resourceVersion:23286 uid:7aa48c47-65e7-44c7-b735-fc6667745c9a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 09/04/23 15:20:37.13
Sep  4 15:20:37.144: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T15:20:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T15:20:37Z]] name:name2 resourceVersion:23368 uid:7f57e85a-e011-4e2c-9e5b-36dc867258c1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 09/04/23 15:20:47.145
Sep  4 15:20:47.160: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T15:20:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T15:20:47Z]] name:name1 resourceVersion:23418 uid:7aa48c47-65e7-44c7-b735-fc6667745c9a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 09/04/23 15:20:57.16
Sep  4 15:20:57.174: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T15:20:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T15:20:57Z]] name:name2 resourceVersion:23469 uid:7f57e85a-e011-4e2c-9e5b-36dc867258c1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 09/04/23 15:21:07.175
Sep  4 15:21:07.190: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T15:20:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T15:20:47Z]] name:name1 resourceVersion:23519 uid:7aa48c47-65e7-44c7-b735-fc6667745c9a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 09/04/23 15:21:17.19
Sep  4 15:21:17.204: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T15:20:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T15:20:57Z]] name:name2 resourceVersion:23600 uid:7f57e85a-e011-4e2c-9e5b-36dc867258c1] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:21:27.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-6312" for this suite. 09/04/23 15:21:27.258
------------------------------
• [62.355 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:20:24.916
    Sep  4 15:20:24.916: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-watch 09/04/23 15:20:24.917
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:20:24.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:20:24.977
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Sep  4 15:20:25.000: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Creating first CR  09/04/23 15:20:27.115
    Sep  4 15:20:27.129: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T15:20:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T15:20:27Z]] name:name1 resourceVersion:23286 uid:7aa48c47-65e7-44c7-b735-fc6667745c9a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 09/04/23 15:20:37.13
    Sep  4 15:20:37.144: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T15:20:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T15:20:37Z]] name:name2 resourceVersion:23368 uid:7f57e85a-e011-4e2c-9e5b-36dc867258c1] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 09/04/23 15:20:47.145
    Sep  4 15:20:47.160: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T15:20:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T15:20:47Z]] name:name1 resourceVersion:23418 uid:7aa48c47-65e7-44c7-b735-fc6667745c9a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 09/04/23 15:20:57.16
    Sep  4 15:20:57.174: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T15:20:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T15:20:57Z]] name:name2 resourceVersion:23469 uid:7f57e85a-e011-4e2c-9e5b-36dc867258c1] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 09/04/23 15:21:07.175
    Sep  4 15:21:07.190: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T15:20:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T15:20:47Z]] name:name1 resourceVersion:23519 uid:7aa48c47-65e7-44c7-b735-fc6667745c9a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 09/04/23 15:21:17.19
    Sep  4 15:21:17.204: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-04T15:20:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-04T15:20:57Z]] name:name2 resourceVersion:23600 uid:7f57e85a-e011-4e2c-9e5b-36dc867258c1] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:21:27.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-6312" for this suite. 09/04/23 15:21:27.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:21:27.273
Sep  4 15:21:27.273: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 09/04/23 15:21:27.273
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:27.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:27.333
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:21:27.354
Sep  4 15:21:27.355: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption-2 09/04/23 15:21:27.355
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:27.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:27.413
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 09/04/23 15:21:27.448
STEP: Waiting for the pdb to be processed 09/04/23 15:21:29.489
STEP: Waiting for the pdb to be processed 09/04/23 15:21:29.513
STEP: listing a collection of PDBs across all namespaces 09/04/23 15:21:29.525
STEP: listing a collection of PDBs in namespace disruption-7164 09/04/23 15:21:29.537
STEP: deleting a collection of PDBs 09/04/23 15:21:29.55
STEP: Waiting for the PDB collection to be deleted 09/04/23 15:21:29.573
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Sep  4 15:21:29.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  4 15:21:29.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-9540" for this suite. 09/04/23 15:21:29.62
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7164" for this suite. 09/04/23 15:21:29.633
------------------------------
• [2.375 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:21:27.273
    Sep  4 15:21:27.273: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 09/04/23 15:21:27.273
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:27.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:27.333
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:21:27.354
    Sep  4 15:21:27.355: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption-2 09/04/23 15:21:27.355
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:27.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:27.413
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 09/04/23 15:21:27.448
    STEP: Waiting for the pdb to be processed 09/04/23 15:21:29.489
    STEP: Waiting for the pdb to be processed 09/04/23 15:21:29.513
    STEP: listing a collection of PDBs across all namespaces 09/04/23 15:21:29.525
    STEP: listing a collection of PDBs in namespace disruption-7164 09/04/23 15:21:29.537
    STEP: deleting a collection of PDBs 09/04/23 15:21:29.55
    STEP: Waiting for the PDB collection to be deleted 09/04/23 15:21:29.573
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:21:29.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:21:29.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-9540" for this suite. 09/04/23 15:21:29.62
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7164" for this suite. 09/04/23 15:21:29.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:21:29.648
Sep  4 15:21:29.648: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:21:29.649
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:29.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:29.706
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-db4be17b-27c6-4594-8eaa-bfa72c13bc92 09/04/23 15:21:29.728
STEP: Creating secret with name secret-projected-all-test-volume-b396bf37-ddf6-4dee-a7ac-2f4cc2a86663 09/04/23 15:21:29.741
STEP: Creating a pod to test Check all projections for projected volume plugin 09/04/23 15:21:29.754
Sep  4 15:21:29.773: INFO: Waiting up to 5m0s for pod "projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83" in namespace "projected-8724" to be "Succeeded or Failed"
Sep  4 15:21:29.785: INFO: Pod "projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83": Phase="Pending", Reason="", readiness=false. Elapsed: 11.773097ms
Sep  4 15:21:31.798: INFO: Pod "projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025209766s
Sep  4 15:21:33.798: INFO: Pod "projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025228761s
STEP: Saw pod success 09/04/23 15:21:33.799
Sep  4 15:21:33.799: INFO: Pod "projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83" satisfied condition "Succeeded or Failed"
Sep  4 15:21:33.811: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83 container projected-all-volume-test: <nil>
STEP: delete the pod 09/04/23 15:21:33.888
Sep  4 15:21:33.905: INFO: Waiting for pod projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83 to disappear
Sep  4 15:21:33.917: INFO: Pod projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Sep  4 15:21:33.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8724" for this suite. 09/04/23 15:21:33.94
------------------------------
• [4.307 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:21:29.648
    Sep  4 15:21:29.648: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:21:29.649
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:29.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:29.706
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-db4be17b-27c6-4594-8eaa-bfa72c13bc92 09/04/23 15:21:29.728
    STEP: Creating secret with name secret-projected-all-test-volume-b396bf37-ddf6-4dee-a7ac-2f4cc2a86663 09/04/23 15:21:29.741
    STEP: Creating a pod to test Check all projections for projected volume plugin 09/04/23 15:21:29.754
    Sep  4 15:21:29.773: INFO: Waiting up to 5m0s for pod "projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83" in namespace "projected-8724" to be "Succeeded or Failed"
    Sep  4 15:21:29.785: INFO: Pod "projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83": Phase="Pending", Reason="", readiness=false. Elapsed: 11.773097ms
    Sep  4 15:21:31.798: INFO: Pod "projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025209766s
    Sep  4 15:21:33.798: INFO: Pod "projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025228761s
    STEP: Saw pod success 09/04/23 15:21:33.799
    Sep  4 15:21:33.799: INFO: Pod "projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83" satisfied condition "Succeeded or Failed"
    Sep  4 15:21:33.811: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83 container projected-all-volume-test: <nil>
    STEP: delete the pod 09/04/23 15:21:33.888
    Sep  4 15:21:33.905: INFO: Waiting for pod projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83 to disappear
    Sep  4 15:21:33.917: INFO: Pod projected-volume-8e2c89b7-3d6b-4542-a407-9d67fc98ce83 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:21:33.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8724" for this suite. 09/04/23 15:21:33.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:21:33.955
Sep  4 15:21:33.955: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 09/04/23 15:21:33.956
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:33.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:34.016
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 09/04/23 15:21:34.039
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:34.079
STEP: Creating a service in the namespace 09/04/23 15:21:34.101
STEP: Deleting the namespace 09/04/23 15:21:34.119
STEP: Waiting for the namespace to be removed. 09/04/23 15:21:34.132
STEP: Recreating the namespace 09/04/23 15:21:40.146
STEP: Verifying there is no service in the namespace 09/04/23 15:21:40.183
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:21:40.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6314" for this suite. 09/04/23 15:21:40.217
STEP: Destroying namespace "nsdeletetest-1154" for this suite. 09/04/23 15:21:40.231
Sep  4 15:21:40.244: INFO: Namespace nsdeletetest-1154 was already deleted
STEP: Destroying namespace "nsdeletetest-3533" for this suite. 09/04/23 15:21:40.244
------------------------------
• [6.302 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:21:33.955
    Sep  4 15:21:33.955: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 09/04/23 15:21:33.956
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:33.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:34.016
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 09/04/23 15:21:34.039
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:34.079
    STEP: Creating a service in the namespace 09/04/23 15:21:34.101
    STEP: Deleting the namespace 09/04/23 15:21:34.119
    STEP: Waiting for the namespace to be removed. 09/04/23 15:21:34.132
    STEP: Recreating the namespace 09/04/23 15:21:40.146
    STEP: Verifying there is no service in the namespace 09/04/23 15:21:40.183
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:21:40.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6314" for this suite. 09/04/23 15:21:40.217
    STEP: Destroying namespace "nsdeletetest-1154" for this suite. 09/04/23 15:21:40.231
    Sep  4 15:21:40.244: INFO: Namespace nsdeletetest-1154 was already deleted
    STEP: Destroying namespace "nsdeletetest-3533" for this suite. 09/04/23 15:21:40.244
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:21:40.257
Sep  4 15:21:40.257: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:21:40.258
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:40.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:40.317
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 09/04/23 15:21:40.34
Sep  4 15:21:40.361: INFO: Waiting up to 5m0s for pod "annotationupdate45adc4af-5d9e-4b29-8a70-4b3908fb5067" in namespace "projected-3653" to be "running and ready"
Sep  4 15:21:40.373: INFO: Pod "annotationupdate45adc4af-5d9e-4b29-8a70-4b3908fb5067": Phase="Pending", Reason="", readiness=false. Elapsed: 12.809953ms
Sep  4 15:21:40.374: INFO: The phase of Pod annotationupdate45adc4af-5d9e-4b29-8a70-4b3908fb5067 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:21:42.388: INFO: Pod "annotationupdate45adc4af-5d9e-4b29-8a70-4b3908fb5067": Phase="Running", Reason="", readiness=true. Elapsed: 2.027772979s
Sep  4 15:21:42.388: INFO: The phase of Pod annotationupdate45adc4af-5d9e-4b29-8a70-4b3908fb5067 is Running (Ready = true)
Sep  4 15:21:42.388: INFO: Pod "annotationupdate45adc4af-5d9e-4b29-8a70-4b3908fb5067" satisfied condition "running and ready"
Sep  4 15:21:43.019: INFO: Successfully updated pod "annotationupdate45adc4af-5d9e-4b29-8a70-4b3908fb5067"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 15:21:45.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3653" for this suite. 09/04/23 15:21:45.155
------------------------------
• [4.912 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:21:40.257
    Sep  4 15:21:40.257: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:21:40.258
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:40.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:40.317
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 09/04/23 15:21:40.34
    Sep  4 15:21:40.361: INFO: Waiting up to 5m0s for pod "annotationupdate45adc4af-5d9e-4b29-8a70-4b3908fb5067" in namespace "projected-3653" to be "running and ready"
    Sep  4 15:21:40.373: INFO: Pod "annotationupdate45adc4af-5d9e-4b29-8a70-4b3908fb5067": Phase="Pending", Reason="", readiness=false. Elapsed: 12.809953ms
    Sep  4 15:21:40.374: INFO: The phase of Pod annotationupdate45adc4af-5d9e-4b29-8a70-4b3908fb5067 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:21:42.388: INFO: Pod "annotationupdate45adc4af-5d9e-4b29-8a70-4b3908fb5067": Phase="Running", Reason="", readiness=true. Elapsed: 2.027772979s
    Sep  4 15:21:42.388: INFO: The phase of Pod annotationupdate45adc4af-5d9e-4b29-8a70-4b3908fb5067 is Running (Ready = true)
    Sep  4 15:21:42.388: INFO: Pod "annotationupdate45adc4af-5d9e-4b29-8a70-4b3908fb5067" satisfied condition "running and ready"
    Sep  4 15:21:43.019: INFO: Successfully updated pod "annotationupdate45adc4af-5d9e-4b29-8a70-4b3908fb5067"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:21:45.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3653" for this suite. 09/04/23 15:21:45.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:21:45.17
Sep  4 15:21:45.170: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 15:21:45.171
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:45.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:45.232
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-1639/configmap-test-56883ec4-97f1-4a0b-acce-55d29b377c13 09/04/23 15:21:45.254
STEP: Creating a pod to test consume configMaps 09/04/23 15:21:45.268
Sep  4 15:21:45.286: INFO: Waiting up to 5m0s for pod "pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94" in namespace "configmap-1639" to be "Succeeded or Failed"
Sep  4 15:21:45.298: INFO: Pod "pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94": Phase="Pending", Reason="", readiness=false. Elapsed: 11.830774ms
Sep  4 15:21:47.313: INFO: Pod "pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026690764s
Sep  4 15:21:49.312: INFO: Pod "pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025891436s
STEP: Saw pod success 09/04/23 15:21:49.312
Sep  4 15:21:49.312: INFO: Pod "pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94" satisfied condition "Succeeded or Failed"
Sep  4 15:21:49.326: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94 container env-test: <nil>
STEP: delete the pod 09/04/23 15:21:49.361
Sep  4 15:21:49.377: INFO: Waiting for pod pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94 to disappear
Sep  4 15:21:49.389: INFO: Pod pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:21:49.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1639" for this suite. 09/04/23 15:21:49.412
------------------------------
• [4.256 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:21:45.17
    Sep  4 15:21:45.170: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 15:21:45.171
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:45.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:45.232
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-1639/configmap-test-56883ec4-97f1-4a0b-acce-55d29b377c13 09/04/23 15:21:45.254
    STEP: Creating a pod to test consume configMaps 09/04/23 15:21:45.268
    Sep  4 15:21:45.286: INFO: Waiting up to 5m0s for pod "pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94" in namespace "configmap-1639" to be "Succeeded or Failed"
    Sep  4 15:21:45.298: INFO: Pod "pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94": Phase="Pending", Reason="", readiness=false. Elapsed: 11.830774ms
    Sep  4 15:21:47.313: INFO: Pod "pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026690764s
    Sep  4 15:21:49.312: INFO: Pod "pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025891436s
    STEP: Saw pod success 09/04/23 15:21:49.312
    Sep  4 15:21:49.312: INFO: Pod "pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94" satisfied condition "Succeeded or Failed"
    Sep  4 15:21:49.326: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94 container env-test: <nil>
    STEP: delete the pod 09/04/23 15:21:49.361
    Sep  4 15:21:49.377: INFO: Waiting for pod pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94 to disappear
    Sep  4 15:21:49.389: INFO: Pod pod-configmaps-a24fa0f4-9e00-4f93-a3ae-4b7ebf97ca94 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:21:49.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1639" for this suite. 09/04/23 15:21:49.412
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:21:49.426
Sep  4 15:21:49.426: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events 09/04/23 15:21:49.427
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:49.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:49.488
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 09/04/23 15:21:49.51
STEP: listing events in all namespaces 09/04/23 15:21:49.524
STEP: listing events in test namespace 09/04/23 15:21:49.547
STEP: listing events with field selection filtering on source 09/04/23 15:21:49.559
STEP: listing events with field selection filtering on reportingController 09/04/23 15:21:49.572
STEP: getting the test event 09/04/23 15:21:49.584
STEP: patching the test event 09/04/23 15:21:49.596
STEP: getting the test event 09/04/23 15:21:49.614
STEP: updating the test event 09/04/23 15:21:49.626
STEP: getting the test event 09/04/23 15:21:49.64
STEP: deleting the test event 09/04/23 15:21:49.652
STEP: listing events in all namespaces 09/04/23 15:21:49.665
STEP: listing events in test namespace 09/04/23 15:21:49.686
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Sep  4 15:21:49.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3487" for this suite. 09/04/23 15:21:49.713
------------------------------
• [0.302 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:21:49.426
    Sep  4 15:21:49.426: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename events 09/04/23 15:21:49.427
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:49.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:49.488
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 09/04/23 15:21:49.51
    STEP: listing events in all namespaces 09/04/23 15:21:49.524
    STEP: listing events in test namespace 09/04/23 15:21:49.547
    STEP: listing events with field selection filtering on source 09/04/23 15:21:49.559
    STEP: listing events with field selection filtering on reportingController 09/04/23 15:21:49.572
    STEP: getting the test event 09/04/23 15:21:49.584
    STEP: patching the test event 09/04/23 15:21:49.596
    STEP: getting the test event 09/04/23 15:21:49.614
    STEP: updating the test event 09/04/23 15:21:49.626
    STEP: getting the test event 09/04/23 15:21:49.64
    STEP: deleting the test event 09/04/23 15:21:49.652
    STEP: listing events in all namespaces 09/04/23 15:21:49.665
    STEP: listing events in test namespace 09/04/23 15:21:49.686
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:21:49.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3487" for this suite. 09/04/23 15:21:49.713
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:21:49.728
Sep  4 15:21:49.728: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 09/04/23 15:21:49.729
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:49.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:49.791
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-4195/secret-test-e3b7ffe7-97de-44bd-9c5c-84ed36c956c0 09/04/23 15:21:49.813
STEP: Creating a pod to test consume secrets 09/04/23 15:21:49.827
Sep  4 15:21:49.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f" in namespace "secrets-4195" to be "Succeeded or Failed"
Sep  4 15:21:49.859: INFO: Pod "pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.462208ms
Sep  4 15:21:51.873: INFO: Pod "pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027285861s
Sep  4 15:21:53.874: INFO: Pod "pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027657906s
STEP: Saw pod success 09/04/23 15:21:53.874
Sep  4 15:21:53.874: INFO: Pod "pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f" satisfied condition "Succeeded or Failed"
Sep  4 15:21:53.887: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f container env-test: <nil>
STEP: delete the pod 09/04/23 15:21:53.913
Sep  4 15:21:53.929: INFO: Waiting for pod pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f to disappear
Sep  4 15:21:53.942: INFO: Pod pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 15:21:53.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4195" for this suite. 09/04/23 15:21:53.964
------------------------------
• [4.273 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:21:49.728
    Sep  4 15:21:49.728: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 09/04/23 15:21:49.729
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:49.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:49.791
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-4195/secret-test-e3b7ffe7-97de-44bd-9c5c-84ed36c956c0 09/04/23 15:21:49.813
    STEP: Creating a pod to test consume secrets 09/04/23 15:21:49.827
    Sep  4 15:21:49.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f" in namespace "secrets-4195" to be "Succeeded or Failed"
    Sep  4 15:21:49.859: INFO: Pod "pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.462208ms
    Sep  4 15:21:51.873: INFO: Pod "pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027285861s
    Sep  4 15:21:53.874: INFO: Pod "pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027657906s
    STEP: Saw pod success 09/04/23 15:21:53.874
    Sep  4 15:21:53.874: INFO: Pod "pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f" satisfied condition "Succeeded or Failed"
    Sep  4 15:21:53.887: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f container env-test: <nil>
    STEP: delete the pod 09/04/23 15:21:53.913
    Sep  4 15:21:53.929: INFO: Waiting for pod pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f to disappear
    Sep  4 15:21:53.942: INFO: Pod pod-configmaps-9e75b1e9-81be-4d96-bb43-055b4ca38b9f no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:21:53.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4195" for this suite. 09/04/23 15:21:53.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:21:54.001
Sep  4 15:21:54.001: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 15:21:54.002
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:54.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:54.062
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 09/04/23 15:21:54.085
Sep  4 15:21:54.105: INFO: Waiting up to 5m0s for pod "downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2" in namespace "downward-api-6092" to be "Succeeded or Failed"
Sep  4 15:21:54.118: INFO: Pod "downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.500306ms
Sep  4 15:21:56.132: INFO: Pod "downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026380606s
Sep  4 15:21:58.134: INFO: Pod "downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028257107s
STEP: Saw pod success 09/04/23 15:21:58.134
Sep  4 15:21:58.134: INFO: Pod "downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2" satisfied condition "Succeeded or Failed"
Sep  4 15:21:58.147: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2 container dapi-container: <nil>
STEP: delete the pod 09/04/23 15:21:58.186
Sep  4 15:21:58.202: INFO: Waiting for pod downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2 to disappear
Sep  4 15:21:58.216: INFO: Pod downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  4 15:21:58.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6092" for this suite. 09/04/23 15:21:58.238
------------------------------
• [4.251 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:21:54.001
    Sep  4 15:21:54.001: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 15:21:54.002
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:54.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:54.062
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 09/04/23 15:21:54.085
    Sep  4 15:21:54.105: INFO: Waiting up to 5m0s for pod "downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2" in namespace "downward-api-6092" to be "Succeeded or Failed"
    Sep  4 15:21:54.118: INFO: Pod "downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.500306ms
    Sep  4 15:21:56.132: INFO: Pod "downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026380606s
    Sep  4 15:21:58.134: INFO: Pod "downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028257107s
    STEP: Saw pod success 09/04/23 15:21:58.134
    Sep  4 15:21:58.134: INFO: Pod "downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2" satisfied condition "Succeeded or Failed"
    Sep  4 15:21:58.147: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2 container dapi-container: <nil>
    STEP: delete the pod 09/04/23 15:21:58.186
    Sep  4 15:21:58.202: INFO: Waiting for pod downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2 to disappear
    Sep  4 15:21:58.216: INFO: Pod downward-api-f7d3054e-4b1a-405e-a7e3-fb9f93d982f2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:21:58.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6092" for this suite. 09/04/23 15:21:58.238
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:21:58.252
Sep  4 15:21:58.252: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename conformance-tests 09/04/23 15:21:58.253
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:58.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:58.313
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 09/04/23 15:21:58.337
Sep  4 15:21:58.337: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Sep  4 15:21:58.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-9588" for this suite. 09/04/23 15:21:58.381
------------------------------
• [0.142 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:21:58.252
    Sep  4 15:21:58.252: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename conformance-tests 09/04/23 15:21:58.253
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:58.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:58.313
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 09/04/23 15:21:58.337
    Sep  4 15:21:58.337: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:21:58.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-9588" for this suite. 09/04/23 15:21:58.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:21:58.396
Sep  4 15:21:58.396: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 15:21:58.397
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:58.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:58.457
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/04/23 15:21:58.479
Sep  4 15:21:58.501: INFO: Waiting up to 5m0s for pod "pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66" in namespace "emptydir-8277" to be "Succeeded or Failed"
Sep  4 15:21:58.514: INFO: Pod "pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66": Phase="Pending", Reason="", readiness=false. Elapsed: 12.271697ms
Sep  4 15:22:00.528: INFO: Pod "pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027001862s
Sep  4 15:22:02.526: INFO: Pod "pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024897622s
STEP: Saw pod success 09/04/23 15:22:02.526
Sep  4 15:22:02.526: INFO: Pod "pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66" satisfied condition "Succeeded or Failed"
Sep  4 15:22:02.538: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66 container test-container: <nil>
STEP: delete the pod 09/04/23 15:22:02.571
Sep  4 15:22:02.589: INFO: Waiting for pod pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66 to disappear
Sep  4 15:22:02.601: INFO: Pod pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 15:22:02.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8277" for this suite. 09/04/23 15:22:02.624
------------------------------
• [4.241 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:21:58.396
    Sep  4 15:21:58.396: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 15:21:58.397
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:21:58.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:21:58.457
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/04/23 15:21:58.479
    Sep  4 15:21:58.501: INFO: Waiting up to 5m0s for pod "pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66" in namespace "emptydir-8277" to be "Succeeded or Failed"
    Sep  4 15:21:58.514: INFO: Pod "pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66": Phase="Pending", Reason="", readiness=false. Elapsed: 12.271697ms
    Sep  4 15:22:00.528: INFO: Pod "pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027001862s
    Sep  4 15:22:02.526: INFO: Pod "pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024897622s
    STEP: Saw pod success 09/04/23 15:22:02.526
    Sep  4 15:22:02.526: INFO: Pod "pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66" satisfied condition "Succeeded or Failed"
    Sep  4 15:22:02.538: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66 container test-container: <nil>
    STEP: delete the pod 09/04/23 15:22:02.571
    Sep  4 15:22:02.589: INFO: Waiting for pod pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66 to disappear
    Sep  4 15:22:02.601: INFO: Pod pod-ce1d4ca7-d18c-4b3b-8fef-3e9e81cb8d66 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:22:02.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8277" for this suite. 09/04/23 15:22:02.624
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:22:02.637
Sep  4 15:22:02.637: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sysctl 09/04/23 15:22:02.638
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:22:02.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:22:02.699
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 09/04/23 15:22:02.723
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:22:02.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-9569" for this suite. 09/04/23 15:22:02.754
------------------------------
• [0.131 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:22:02.637
    Sep  4 15:22:02.637: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sysctl 09/04/23 15:22:02.638
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:22:02.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:22:02.699
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 09/04/23 15:22:02.723
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:22:02.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-9569" for this suite. 09/04/23 15:22:02.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:22:02.769
Sep  4 15:22:02.769: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 09/04/23 15:22:02.77
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:22:02.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:22:02.83
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 09/04/23 15:22:02.853
STEP: Creating a ResourceQuota 09/04/23 15:22:07.867
STEP: Ensuring resource quota status is calculated 09/04/23 15:22:07.88
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 15:22:09.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6191" for this suite. 09/04/23 15:22:09.917
------------------------------
• [7.163 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:22:02.769
    Sep  4 15:22:02.769: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 09/04/23 15:22:02.77
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:22:02.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:22:02.83
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 09/04/23 15:22:02.853
    STEP: Creating a ResourceQuota 09/04/23 15:22:07.867
    STEP: Ensuring resource quota status is calculated 09/04/23 15:22:07.88
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:22:09.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6191" for this suite. 09/04/23 15:22:09.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:22:09.932
Sep  4 15:22:09.932: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 15:22:09.933
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:22:09.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:22:10.002
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 09/04/23 15:22:10.025
Sep  4 15:22:10.046: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174" in namespace "downward-api-8727" to be "Succeeded or Failed"
Sep  4 15:22:10.058: INFO: Pod "downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174": Phase="Pending", Reason="", readiness=false. Elapsed: 12.782185ms
Sep  4 15:22:12.072: INFO: Pod "downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026205702s
Sep  4 15:22:14.072: INFO: Pod "downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026540792s
STEP: Saw pod success 09/04/23 15:22:14.072
Sep  4 15:22:14.072: INFO: Pod "downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174" satisfied condition "Succeeded or Failed"
Sep  4 15:22:14.085: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174 container client-container: <nil>
STEP: delete the pod 09/04/23 15:22:14.117
Sep  4 15:22:14.135: INFO: Waiting for pod downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174 to disappear
Sep  4 15:22:14.147: INFO: Pod downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 15:22:14.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8727" for this suite. 09/04/23 15:22:14.176
------------------------------
• [4.260 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:22:09.932
    Sep  4 15:22:09.932: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 15:22:09.933
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:22:09.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:22:10.002
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 09/04/23 15:22:10.025
    Sep  4 15:22:10.046: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174" in namespace "downward-api-8727" to be "Succeeded or Failed"
    Sep  4 15:22:10.058: INFO: Pod "downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174": Phase="Pending", Reason="", readiness=false. Elapsed: 12.782185ms
    Sep  4 15:22:12.072: INFO: Pod "downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026205702s
    Sep  4 15:22:14.072: INFO: Pod "downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026540792s
    STEP: Saw pod success 09/04/23 15:22:14.072
    Sep  4 15:22:14.072: INFO: Pod "downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174" satisfied condition "Succeeded or Failed"
    Sep  4 15:22:14.085: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174 container client-container: <nil>
    STEP: delete the pod 09/04/23 15:22:14.117
    Sep  4 15:22:14.135: INFO: Waiting for pod downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174 to disappear
    Sep  4 15:22:14.147: INFO: Pod downwardapi-volume-bd954f2f-2c96-4644-8614-1aa65e33c174 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:22:14.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8727" for this suite. 09/04/23 15:22:14.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:22:14.192
Sep  4 15:22:14.192: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 09/04/23 15:22:14.193
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:22:14.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:22:14.253
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 09/04/23 15:22:14.289
STEP: Verify that the required pods have come up. 09/04/23 15:22:14.303
Sep  4 15:22:14.315: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  4 15:22:19.328: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/04/23 15:22:19.328
STEP: Getting /status 09/04/23 15:22:19.328
Sep  4 15:22:19.340: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 09/04/23 15:22:19.34
Sep  4 15:22:19.366: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 09/04/23 15:22:19.366
Sep  4 15:22:19.378: INFO: Observed &ReplicaSet event: ADDED
Sep  4 15:22:19.378: INFO: Observed &ReplicaSet event: MODIFIED
Sep  4 15:22:19.378: INFO: Observed &ReplicaSet event: MODIFIED
Sep  4 15:22:19.378: INFO: Observed &ReplicaSet event: MODIFIED
Sep  4 15:22:19.378: INFO: Found replicaset test-rs in namespace replicaset-1607 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  4 15:22:19.378: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 09/04/23 15:22:19.378
Sep  4 15:22:19.378: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  4 15:22:19.392: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 09/04/23 15:22:19.392
Sep  4 15:22:19.403: INFO: Observed &ReplicaSet event: ADDED
Sep  4 15:22:19.403: INFO: Observed &ReplicaSet event: MODIFIED
Sep  4 15:22:19.403: INFO: Observed &ReplicaSet event: MODIFIED
Sep  4 15:22:19.403: INFO: Observed &ReplicaSet event: MODIFIED
Sep  4 15:22:19.403: INFO: Observed replicaset test-rs in namespace replicaset-1607 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  4 15:22:19.403: INFO: Observed &ReplicaSet event: MODIFIED
Sep  4 15:22:19.403: INFO: Found replicaset test-rs in namespace replicaset-1607 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Sep  4 15:22:19.403: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  4 15:22:19.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1607" for this suite. 09/04/23 15:22:19.425
------------------------------
• [5.247 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:22:14.192
    Sep  4 15:22:14.192: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 09/04/23 15:22:14.193
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:22:14.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:22:14.253
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 09/04/23 15:22:14.289
    STEP: Verify that the required pods have come up. 09/04/23 15:22:14.303
    Sep  4 15:22:14.315: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  4 15:22:19.328: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/04/23 15:22:19.328
    STEP: Getting /status 09/04/23 15:22:19.328
    Sep  4 15:22:19.340: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 09/04/23 15:22:19.34
    Sep  4 15:22:19.366: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 09/04/23 15:22:19.366
    Sep  4 15:22:19.378: INFO: Observed &ReplicaSet event: ADDED
    Sep  4 15:22:19.378: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  4 15:22:19.378: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  4 15:22:19.378: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  4 15:22:19.378: INFO: Found replicaset test-rs in namespace replicaset-1607 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  4 15:22:19.378: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 09/04/23 15:22:19.378
    Sep  4 15:22:19.378: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  4 15:22:19.392: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 09/04/23 15:22:19.392
    Sep  4 15:22:19.403: INFO: Observed &ReplicaSet event: ADDED
    Sep  4 15:22:19.403: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  4 15:22:19.403: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  4 15:22:19.403: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  4 15:22:19.403: INFO: Observed replicaset test-rs in namespace replicaset-1607 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  4 15:22:19.403: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  4 15:22:19.403: INFO: Found replicaset test-rs in namespace replicaset-1607 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Sep  4 15:22:19.403: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:22:19.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1607" for this suite. 09/04/23 15:22:19.425
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:22:19.439
Sep  4 15:22:19.439: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 09/04/23 15:22:19.44
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:22:19.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:22:19.499
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 09/04/23 15:22:19.522
STEP: setting up watch 09/04/23 15:22:19.522
STEP: submitting the pod to kubernetes 09/04/23 15:22:19.635
STEP: verifying the pod is in kubernetes 09/04/23 15:22:19.655
STEP: verifying pod creation was observed 09/04/23 15:22:19.667
Sep  4 15:22:19.667: INFO: Waiting up to 5m0s for pod "pod-submit-remove-fea34272-c040-4d6c-bc9e-ad960103506c" in namespace "pods-5104" to be "running"
Sep  4 15:22:19.679: INFO: Pod "pod-submit-remove-fea34272-c040-4d6c-bc9e-ad960103506c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.794723ms
Sep  4 15:22:21.692: INFO: Pod "pod-submit-remove-fea34272-c040-4d6c-bc9e-ad960103506c": Phase="Running", Reason="", readiness=true. Elapsed: 2.024335789s
Sep  4 15:22:21.692: INFO: Pod "pod-submit-remove-fea34272-c040-4d6c-bc9e-ad960103506c" satisfied condition "running"
STEP: deleting the pod gracefully 09/04/23 15:22:21.703
STEP: verifying pod deletion was observed 09/04/23 15:22:21.718
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 15:22:24.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5104" for this suite. 09/04/23 15:22:24.564
------------------------------
• [5.138 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:22:19.439
    Sep  4 15:22:19.439: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 09/04/23 15:22:19.44
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:22:19.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:22:19.499
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 09/04/23 15:22:19.522
    STEP: setting up watch 09/04/23 15:22:19.522
    STEP: submitting the pod to kubernetes 09/04/23 15:22:19.635
    STEP: verifying the pod is in kubernetes 09/04/23 15:22:19.655
    STEP: verifying pod creation was observed 09/04/23 15:22:19.667
    Sep  4 15:22:19.667: INFO: Waiting up to 5m0s for pod "pod-submit-remove-fea34272-c040-4d6c-bc9e-ad960103506c" in namespace "pods-5104" to be "running"
    Sep  4 15:22:19.679: INFO: Pod "pod-submit-remove-fea34272-c040-4d6c-bc9e-ad960103506c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.794723ms
    Sep  4 15:22:21.692: INFO: Pod "pod-submit-remove-fea34272-c040-4d6c-bc9e-ad960103506c": Phase="Running", Reason="", readiness=true. Elapsed: 2.024335789s
    Sep  4 15:22:21.692: INFO: Pod "pod-submit-remove-fea34272-c040-4d6c-bc9e-ad960103506c" satisfied condition "running"
    STEP: deleting the pod gracefully 09/04/23 15:22:21.703
    STEP: verifying pod deletion was observed 09/04/23 15:22:21.718
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:22:24.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5104" for this suite. 09/04/23 15:22:24.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:22:24.577
Sep  4 15:22:24.577: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container 09/04/23 15:22:24.577
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:22:24.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:22:24.635
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 09/04/23 15:22:24.657
Sep  4 15:22:24.657: INFO: PodSpec: initContainers in spec.initContainers
Sep  4 15:23:05.605: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1cbbdf2f-caa7-4b49-9dd5-63e449273bf5", GenerateName:"", Namespace:"init-container-546", SelfLink:"", UID:"c4ecb742-189e-4ff6-ba4f-99542526459a", ResourceVersion:"24449", Generation:0, CreationTimestamp:time.Date(2023, time.September, 4, 15, 22, 24, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"657860162"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"08a6c321d82ea04ed20e8c72598677a43e6b33436d23d1e8fdd5c478fdf6525c", "cni.projectcalico.org/podIP":"100.64.1.12/32", "cni.projectcalico.org/podIPs":"100.64.1.12/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 4, 15, 22, 24, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a345d0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 4, 15, 22, 25, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a34630), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 4, 15, 23, 5, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a34678), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-nf2r5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003ef9e80), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmud5-dd2.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nf2r5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmud5-dd2.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nf2r5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmud5-dd2.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nf2r5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003ddde88), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000213f10), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003dddf00)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003dddf20)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003dddf28), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003dddf2c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0016be9c0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 15, 22, 24, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 15, 22, 24, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 15, 22, 24, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 15, 22, 24, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.1.231", PodIP:"100.64.1.12", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.64.1.12"}}, StartTime:time.Date(2023, time.September, 4, 15, 22, 24, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000e7e0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000e7e150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://7503657d689b9eaf3f5fbd67de1757cf8be3bcf20402378451ef66a122d667fc", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003ef9f00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003ef9ee0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003dddfaf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:23:05.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-546" for this suite. 09/04/23 15:23:05.628
------------------------------
• [41.065 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:22:24.577
    Sep  4 15:22:24.577: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename init-container 09/04/23 15:22:24.577
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:22:24.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:22:24.635
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 09/04/23 15:22:24.657
    Sep  4 15:22:24.657: INFO: PodSpec: initContainers in spec.initContainers
    Sep  4 15:23:05.605: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1cbbdf2f-caa7-4b49-9dd5-63e449273bf5", GenerateName:"", Namespace:"init-container-546", SelfLink:"", UID:"c4ecb742-189e-4ff6-ba4f-99542526459a", ResourceVersion:"24449", Generation:0, CreationTimestamp:time.Date(2023, time.September, 4, 15, 22, 24, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"657860162"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"08a6c321d82ea04ed20e8c72598677a43e6b33436d23d1e8fdd5c478fdf6525c", "cni.projectcalico.org/podIP":"100.64.1.12/32", "cni.projectcalico.org/podIPs":"100.64.1.12/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 4, 15, 22, 24, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a345d0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 4, 15, 22, 25, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a34630), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 4, 15, 23, 5, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a34678), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-nf2r5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003ef9e80), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmud5-dd2.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nf2r5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmud5-dd2.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nf2r5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmud5-dd2.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nf2r5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003ddde88), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000213f10), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003dddf00)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003dddf20)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003dddf28), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003dddf2c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0016be9c0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 15, 22, 24, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 15, 22, 24, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 15, 22, 24, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 4, 15, 22, 24, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.1.231", PodIP:"100.64.1.12", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.64.1.12"}}, StartTime:time.Date(2023, time.September, 4, 15, 22, 24, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000e7e0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000e7e150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://7503657d689b9eaf3f5fbd67de1757cf8be3bcf20402378451ef66a122d667fc", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003ef9f00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003ef9ee0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003dddfaf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:23:05.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-546" for this suite. 09/04/23 15:23:05.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:23:05.642
Sep  4 15:23:05.642: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 15:23:05.643
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:23:05.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:23:05.705
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 09/04/23 15:23:05.727
Sep  4 15:23:05.728: INFO: namespace kubectl-5843
Sep  4 15:23:05.728: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5843 create -f -'
Sep  4 15:23:06.566: INFO: stderr: ""
Sep  4 15:23:06.566: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/04/23 15:23:06.566
Sep  4 15:23:07.582: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 15:23:07.582: INFO: Found 0 / 1
Sep  4 15:23:08.580: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 15:23:08.580: INFO: Found 1 / 1
Sep  4 15:23:08.580: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  4 15:23:08.592: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 15:23:08.592: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  4 15:23:08.592: INFO: wait on agnhost-primary startup in kubectl-5843 
Sep  4 15:23:08.593: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5843 logs agnhost-primary-9psf5 agnhost-primary'
Sep  4 15:23:08.868: INFO: stderr: ""
Sep  4 15:23:08.868: INFO: stdout: "Paused\n"
STEP: exposing RC 09/04/23 15:23:08.868
Sep  4 15:23:08.868: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5843 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Sep  4 15:23:09.001: INFO: stderr: ""
Sep  4 15:23:09.002: INFO: stdout: "service/rm2 exposed\n"
Sep  4 15:23:09.013: INFO: Service rm2 in namespace kubectl-5843 found.
STEP: exposing service 09/04/23 15:23:11.039
Sep  4 15:23:11.040: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5843 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Sep  4 15:23:11.162: INFO: stderr: ""
Sep  4 15:23:11.162: INFO: stdout: "service/rm3 exposed\n"
Sep  4 15:23:11.174: INFO: Service rm3 in namespace kubectl-5843 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 15:23:13.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5843" for this suite. 09/04/23 15:23:13.222
------------------------------
• [7.594 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:23:05.642
    Sep  4 15:23:05.642: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 15:23:05.643
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:23:05.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:23:05.705
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 09/04/23 15:23:05.727
    Sep  4 15:23:05.728: INFO: namespace kubectl-5843
    Sep  4 15:23:05.728: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5843 create -f -'
    Sep  4 15:23:06.566: INFO: stderr: ""
    Sep  4 15:23:06.566: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/04/23 15:23:06.566
    Sep  4 15:23:07.582: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 15:23:07.582: INFO: Found 0 / 1
    Sep  4 15:23:08.580: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 15:23:08.580: INFO: Found 1 / 1
    Sep  4 15:23:08.580: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep  4 15:23:08.592: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 15:23:08.592: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  4 15:23:08.592: INFO: wait on agnhost-primary startup in kubectl-5843 
    Sep  4 15:23:08.593: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5843 logs agnhost-primary-9psf5 agnhost-primary'
    Sep  4 15:23:08.868: INFO: stderr: ""
    Sep  4 15:23:08.868: INFO: stdout: "Paused\n"
    STEP: exposing RC 09/04/23 15:23:08.868
    Sep  4 15:23:08.868: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5843 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Sep  4 15:23:09.001: INFO: stderr: ""
    Sep  4 15:23:09.002: INFO: stdout: "service/rm2 exposed\n"
    Sep  4 15:23:09.013: INFO: Service rm2 in namespace kubectl-5843 found.
    STEP: exposing service 09/04/23 15:23:11.039
    Sep  4 15:23:11.040: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5843 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Sep  4 15:23:11.162: INFO: stderr: ""
    Sep  4 15:23:11.162: INFO: stdout: "service/rm3 exposed\n"
    Sep  4 15:23:11.174: INFO: Service rm3 in namespace kubectl-5843 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:23:13.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5843" for this suite. 09/04/23 15:23:13.222
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:23:13.236
Sep  4 15:23:13.236: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 09/04/23 15:23:13.238
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:23:13.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:23:13.297
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 09/04/23 15:23:13.334
STEP: Patching the Job 09/04/23 15:23:13.348
STEP: Watching for Job to be patched 09/04/23 15:23:13.365
Sep  4 15:23:13.377: INFO: Event ADDED observed for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking:]
Sep  4 15:23:13.377: INFO: Event MODIFIED observed for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking:]
Sep  4 15:23:13.378: INFO: Event MODIFIED found for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 09/04/23 15:23:13.378
STEP: Watching for Job to be updated 09/04/23 15:23:13.404
Sep  4 15:23:13.415: INFO: Event MODIFIED found for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  4 15:23:13.415: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 09/04/23 15:23:13.415
Sep  4 15:23:13.426: INFO: Job: e2e-dsds4 as labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4]
STEP: Waiting for job to complete 09/04/23 15:23:13.426
STEP: Delete a job collection with a labelselector 09/04/23 15:23:21.44
STEP: Watching for Job to be deleted 09/04/23 15:23:21.455
Sep  4 15:23:21.467: INFO: Event MODIFIED observed for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  4 15:23:21.467: INFO: Event MODIFIED observed for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  4 15:23:21.467: INFO: Event MODIFIED observed for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  4 15:23:21.467: INFO: Event MODIFIED observed for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  4 15:23:21.467: INFO: Event MODIFIED observed for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  4 15:23:21.467: INFO: Event DELETED found for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 09/04/23 15:23:21.467
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  4 15:23:21.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3928" for this suite. 09/04/23 15:23:21.501
------------------------------
• [8.277 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:23:13.236
    Sep  4 15:23:13.236: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 09/04/23 15:23:13.238
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:23:13.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:23:13.297
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 09/04/23 15:23:13.334
    STEP: Patching the Job 09/04/23 15:23:13.348
    STEP: Watching for Job to be patched 09/04/23 15:23:13.365
    Sep  4 15:23:13.377: INFO: Event ADDED observed for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking:]
    Sep  4 15:23:13.377: INFO: Event MODIFIED observed for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking:]
    Sep  4 15:23:13.378: INFO: Event MODIFIED found for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 09/04/23 15:23:13.378
    STEP: Watching for Job to be updated 09/04/23 15:23:13.404
    Sep  4 15:23:13.415: INFO: Event MODIFIED found for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  4 15:23:13.415: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 09/04/23 15:23:13.415
    Sep  4 15:23:13.426: INFO: Job: e2e-dsds4 as labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4]
    STEP: Waiting for job to complete 09/04/23 15:23:13.426
    STEP: Delete a job collection with a labelselector 09/04/23 15:23:21.44
    STEP: Watching for Job to be deleted 09/04/23 15:23:21.455
    Sep  4 15:23:21.467: INFO: Event MODIFIED observed for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  4 15:23:21.467: INFO: Event MODIFIED observed for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  4 15:23:21.467: INFO: Event MODIFIED observed for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  4 15:23:21.467: INFO: Event MODIFIED observed for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  4 15:23:21.467: INFO: Event MODIFIED observed for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  4 15:23:21.467: INFO: Event DELETED found for Job e2e-dsds4 in namespace job-3928 with labels: map[e2e-dsds4:patched e2e-job-label:e2e-dsds4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 09/04/23 15:23:21.467
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:23:21.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3928" for this suite. 09/04/23 15:23:21.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:23:21.515
Sep  4 15:23:21.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 15:23:21.516
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:23:21.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:23:21.575
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Sep  4 15:23:21.598: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3011 create -f -'
Sep  4 15:23:22.504: INFO: stderr: ""
Sep  4 15:23:22.504: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep  4 15:23:22.504: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3011 create -f -'
Sep  4 15:23:22.723: INFO: stderr: ""
Sep  4 15:23:22.723: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/04/23 15:23:22.723
Sep  4 15:23:23.736: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 15:23:23.736: INFO: Found 1 / 1
Sep  4 15:23:23.736: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  4 15:23:23.749: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 15:23:23.749: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  4 15:23:23.749: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3011 describe pod agnhost-primary-2bj7d'
Sep  4 15:23:23.860: INFO: stderr: ""
Sep  4 15:23:23.860: INFO: stdout: "Name:             agnhost-primary-2bj7d\nNamespace:        kubectl-3011\nPriority:         0\nService Account:  default\nNode:             shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh/10.250.1.231\nStart Time:       Mon, 04 Sep 2023 15:23:22 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 730d2318b3c4df378623b947d351b4c8cdd6ea8028ca52054a6ed3349b1d18e6\n                  cni.projectcalico.org/podIP: 100.64.1.18/32\n                  cni.projectcalico.org/podIPs: 100.64.1.18/32\nStatus:           Running\nIP:               100.64.1.18\nIPs:\n  IP:           100.64.1.18\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://2a773b36b08b069d0ce242c6a9a8d2c520fb5b59aa83cf6e90bd370262af91a2\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 04 Sep 2023 15:23:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBERNETES_SERVICE_HOST:  api.tmud5-dd2.it.internal.staging.k8s.ondemand.com\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8jtm2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-8jtm2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-3011/agnhost-primary-2bj7d to shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh\n  Normal  Pulled     0s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Sep  4 15:23:23.860: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3011 describe rc agnhost-primary'
Sep  4 15:23:24.006: INFO: stderr: ""
Sep  4 15:23:24.006: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3011\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-2bj7d\n"
Sep  4 15:23:24.006: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3011 describe service agnhost-primary'
Sep  4 15:23:24.165: INFO: stderr: ""
Sep  4 15:23:24.165: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3011\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.104.37.53\nIPs:               100.104.37.53\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.64.1.18:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  4 15:23:24.188: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3011 describe node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx'
Sep  4 15:23:24.400: INFO: stderr: ""
Sep  4 15:23:24.400: INFO: stdout: "Name:               shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=g_c2_m4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-nl-1\n                    failure-domain.beta.kubernetes.io/zone=eu-nl-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx\n                    kubernetes.io/os=linux\n                    networking.gardener.cloud/node-local-dns-enabled=true\n                    node.kubernetes.io/instance-type=g_c2_m4\n                    node.kubernetes.io/role=node\n                    topology.cinder.csi.openstack.org/zone=eu-nl-1a\n                    topology.kubernetes.io/region=eu-nl-1\n                    topology.kubernetes.io/zone=eu-nl-1a\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/cri-name=containerd\n                    worker.gardener.cloud/image-name=gardenlinux\n                    worker.gardener.cloud/image-version=934.10.0\n                    worker.gardener.cloud/kubernetes-version=1.26.8\n                    worker.gardener.cloud/pool=worker-1\n                    worker.gardener.cloud/system-components=true\nAnnotations:        checksum/cloud-config-data: 0feb5c6e921ccd73834b5ac833849002acd76626441b9a150f91c7f8c8e59824\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"c33c7dc2-0749-4aba-b85f-ac574d3e2add\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.machine.sapcloud.io/last-applied-anno-labels-taints:\n                      {\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"kubernetes.io/arch\":\"amd64\",\"networking.gardener.cloud/node-local-dns-enabled\":\"true\",\"no...\n                    projectcalico.org/IPv4Address: 10.250.1.105/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.64.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 04 Sep 2023 14:34:42 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 04 Sep 2023 15:23:17 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  ClusterNetworkProblem         False   Mon, 04 Sep 2023 15:20:54 +0000   Mon, 04 Sep 2023 14:38:10 +0000   NoNetworkProblems               no cluster network problems\n  HostNetworkProblem            False   Mon, 04 Sep 2023 15:21:49 +0000   Mon, 04 Sep 2023 14:36:00 +0000   NoNetworkProblems               no host network problems\n  KernelDeadlock                False   Mon, 04 Sep 2023 15:21:18 +0000   Mon, 04 Sep 2023 14:41:11 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Mon, 04 Sep 2023 15:21:18 +0000   Mon, 04 Sep 2023 14:41:11 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  FrequentKubeletRestart        False   Mon, 04 Sep 2023 15:21:18 +0000   Mon, 04 Sep 2023 14:41:11 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Mon, 04 Sep 2023 15:21:18 +0000   Mon, 04 Sep 2023 14:41:11 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Mon, 04 Sep 2023 15:21:18 +0000   Mon, 04 Sep 2023 14:41:11 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  FrequentUnregisterNetDevice   False   Mon, 04 Sep 2023 15:21:18 +0000   Mon, 04 Sep 2023 14:41:11 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  CorruptDockerOverlay2         False   Mon, 04 Sep 2023 15:21:18 +0000   Mon, 04 Sep 2023 14:41:11 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  NetworkUnavailable            False   Mon, 04 Sep 2023 14:35:06 +0000   Mon, 04 Sep 2023 14:35:06 +0000   CalicoIsUp                      Calico is running on this node\n  MemoryPressure                False   Mon, 04 Sep 2023 15:19:11 +0000   Mon, 04 Sep 2023 14:34:42 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Mon, 04 Sep 2023 15:19:11 +0000   Mon, 04 Sep 2023 14:34:42 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Mon, 04 Sep 2023 15:19:11 +0000   Mon, 04 Sep 2023 14:34:42 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Mon, 04 Sep 2023 15:19:11 +0000   Mon, 04 Sep 2023 14:34:51 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.250.1.105\n  Hostname:    shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx\nCapacity:\n  cpu:                2\n  ephemeral-storage:  64455612Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4008148Ki\n  pods:               110\nAllocatable:\n  cpu:                1920m\n  ephemeral-storage:  62702419305\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2857172Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 e53c6370ad8448bfacd7f410dd7dd40f\n  System UUID:                0b310142-5859-59e9-3ded-33db568fac7d\n  Boot ID:                    96b62fee-161f-42b3-8395-a5ae1b4ceed3\n  Kernel Version:             5.15.125-gardenlinux-cloud-amd64\n  OS Image:                   Garden Linux 934.10\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.20\n  Kubelet Version:            v1.26.8\n  Kube-Proxy Version:         v1.26.8\nPodCIDR:                      100.64.0.0/24\nPodCIDRs:                     100.64.0.0/24\nProviderID:                   openstack:///c33c7dc2-0749-4aba-b85f-ac574d3e2add\nNon-terminated Pods:          (26 in total)\n  Namespace                   Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits    Age\n  ---------                   ----                                                               ------------  ----------  ---------------  -------------    ---\n  kube-system                 addons-nginx-ingress-controller-56b5dc8f6c-tr9n6                   11m (0%)      0 (0%)      100Mi (3%)       4Gi (146%)       32m\n  kube-system                 addons-nginx-ingress-nginx-ingress-k8s-backend-58685978db-gpxdq    20m (1%)      0 (0%)      20Mi (0%)        100Mi (3%)       49m\n  kube-system                 apiserver-proxy-bc5jh                                              40m (2%)      0 (0%)      40Mi (1%)        1114Mi (39%)     48m\n  kube-system                 blackbox-exporter-585854d657-fvtbg                                 10m (0%)      0 (0%)      25Mi (0%)        128Mi (4%)       50m\n  kube-system                 blackbox-exporter-585854d657-vz8rw                                 10m (0%)      0 (0%)      25Mi (0%)        128Mi (4%)       50m\n  kube-system                 calico-kube-controllers-684b9f4889-f24wb                           10m (0%)      0 (0%)      50Mi (1%)        2Gi (73%)        49m\n  kube-system                 calico-node-959qs                                                  250m (13%)    0 (0%)      100Mi (3%)       2800Mi (100%)    48m\n  kube-system                 calico-node-vertical-autoscaler-7bbd54698f-zb6st                   10m (0%)      0 (0%)      50Mi (1%)        130Mi (4%)       49m\n  kube-system                 calico-typha-deploy-6f4475c6d5-z7t2m                               320m (16%)    0 (0%)      262144k (8%)     4194304k (143%)  47m\n  kube-system                 calico-typha-horizontal-autoscaler-7b89d5ff97-gdvkd                10m (0%)      0 (0%)      50Mi (1%)        100Mi (3%)       49m\n  kube-system                 calico-typha-vertical-autoscaler-656479b7b5-khd8g                  10m (0%)      0 (0%)      50Mi (1%)        130Mi (4%)       49m\n  kube-system                 coredns-89679867b-2fr9s                                            50m (2%)      0 (0%)      15Mi (0%)        1500Mi (53%)     49m\n  kube-system                 coredns-89679867b-5qtl7                                            50m (2%)      0 (0%)      15Mi (0%)        1500Mi (53%)     49m\n  kube-system                 csi-driver-node-5kfkx                                              37m (1%)      0 (0%)      106Mi (3%)       0 (0%)           48m\n  kube-system                 egress-filter-applier-n4kfc                                        50m (2%)      0 (0%)      64Mi (2%)        256Mi (9%)       48m\n  kube-system                 kube-proxy-worker-1-v1.26.8-ch6px                                  34m (1%)      0 (0%)      61066436 (2%)    2Gi (73%)        22m\n  kube-system                 metrics-server-78947f8d7c-9cdcs                                    50m (2%)      0 (0%)      60Mi (2%)        1Gi (36%)        49m\n  kube-system                 metrics-server-78947f8d7c-w8gr6                                    11m (0%)      0 (0%)      60Mi (2%)        1Gi (36%)        32m\n  kube-system                 network-problem-detector-host-xf9l5                                10m (0%)      50m (2%)    32Mi (1%)        64Mi (2%)        48m\n  kube-system                 network-problem-detector-pod-lbp4l                                 10m (0%)      50m (2%)    32Mi (1%)        64Mi (2%)        48m\n  kube-system                 node-exporter-ktjnf                                                50m (2%)      0 (0%)      50Mi (1%)        250Mi (8%)       48m\n  kube-system                 node-local-dns-c8s84                                               11m (0%)      0 (0%)      23574998 (0%)    94299992 (3%)    39m\n  kube-system                 node-problem-detector-rqmbk                                        49m (2%)      0 (0%)      36253748 (1%)    120Mi (4%)       42m\n  kube-system                 vpn-shoot-5d596dbb88-5vx52                                         100m (5%)     0 (0%)      100Mi (3%)       100Mi (3%)       50m\n  kubernetes-dashboard        dashboard-metrics-scraper-6c889fdd54-555v8                         0 (0%)        0 (0%)      0 (0%)           0 (0%)           50m\n  kubernetes-dashboard        kubernetes-dashboard-b9859c4d7-bq9mj                               11m (0%)      0 (0%)      23574998 (0%)    256Mi (9%)       32m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests          Limits\n  --------           --------          ------\n  cpu                1224m (63%)       100m (5%)\n  memory             1501327524 (51%)  24190576472 (826%)\n  ephemeral-storage  0 (0%)            0 (0%)\n  hugepages-1Gi      0 (0%)            0 (0%)\n  hugepages-2Mi      0 (0%)            0 (0%)\nEvents:\n  Type     Reason                             Age                From                          Message\n  ----     ------                             ----               ----                          -------\n  Normal   Starting                           22m                kube-proxy                    \n  Normal   Starting                           46m                kube-proxy                    \n  Normal   Starting                           48m                kube-proxy                    \n  Normal   Starting                           48m                kubelet                       Starting kubelet.\n  Warning  InvalidDiskCapacity                48m                kubelet                       invalid capacity 0 on image filesystem\n  Warning  UnscheduledNodeCriticalDaemonSets  48m                gardener-node-controller      Node-critical DaemonSets found that were not scheduled to Node yet: kube-system/apiserver-proxy, kube-system/calico-node, kube-system/csi-driver-node, kube-system/kube-proxy-worker-1-v1.26.8\n  Normal   NodeHasSufficientMemory            48m (x2 over 48m)  kubelet                       Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure              48m (x2 over 48m)  kubelet                       Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID               48m (x2 over 48m)  kubelet                       Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced            48m                kubelet                       Updated Node Allocatable limit across pods\n  Normal   Synced                             48m                cloud-node-controller         Node synced successfully\n  Normal   RegisteredNode                     48m                node-controller               Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx event: Registered Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx in Controller\n  Normal   NodeReady                          48m                kubelet                       Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx status is now: NodeReady\n  Warning  UnreadyNodeCriticalPods            48m (x2 over 48m)  gardener-node-controller      Unready node-critical Pods found on Node: kube-system/apiserver-proxy-bc5jh, kube-system/calico-node-959qs, kube-system/csi-driver-node-5kfkx, kube-system/kube-proxy-worker-1-v1.26.8-x7pxl\n  Warning  UnreadyNodeCriticalPods            48m                gardener-node-controller      Unready node-critical Pods found on Node: kube-system/calico-node-959qs, kube-system/csi-driver-node-5kfkx\n  Warning  ContainerdStart                    48m (x2 over 48m)  systemd-monitor               Starting containerd container runtime...\n  Warning  DockerStart                        48m (x2 over 48m)  systemd-monitor               Starting Docker Application Container Engine...\n  Normal   NodeCriticalComponentsReady        48m                gardener-node-controller      All node-critical components got ready, removing taint\n  Warning  FailedNetworkChecks                47m                network-problem-detector-pod  cluster network problems for 4 pairs of jobIDs (https-p2api-int,nslookup-p) and destinations (api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com,eu.gcr.io,kubernetes.default.svc.cluster.local)\n"
Sep  4 15:23:24.400: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3011 describe namespace kubectl-3011'
Sep  4 15:23:24.537: INFO: stderr: ""
Sep  4 15:23:24.537: INFO: stdout: "Name:         kubectl-3011\nLabels:       e2e-framework=kubectl\n              e2e-run=0be53abf-8976-49ee-9130-04032f304ae9\n              kubernetes.io/metadata.name=kubectl-3011\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 15:23:24.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3011" for this suite. 09/04/23 15:23:24.56
------------------------------
• [3.058 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:23:21.515
    Sep  4 15:23:21.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 15:23:21.516
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:23:21.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:23:21.575
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Sep  4 15:23:21.598: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3011 create -f -'
    Sep  4 15:23:22.504: INFO: stderr: ""
    Sep  4 15:23:22.504: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Sep  4 15:23:22.504: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3011 create -f -'
    Sep  4 15:23:22.723: INFO: stderr: ""
    Sep  4 15:23:22.723: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/04/23 15:23:22.723
    Sep  4 15:23:23.736: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 15:23:23.736: INFO: Found 1 / 1
    Sep  4 15:23:23.736: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep  4 15:23:23.749: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 15:23:23.749: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  4 15:23:23.749: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3011 describe pod agnhost-primary-2bj7d'
    Sep  4 15:23:23.860: INFO: stderr: ""
    Sep  4 15:23:23.860: INFO: stdout: "Name:             agnhost-primary-2bj7d\nNamespace:        kubectl-3011\nPriority:         0\nService Account:  default\nNode:             shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh/10.250.1.231\nStart Time:       Mon, 04 Sep 2023 15:23:22 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 730d2318b3c4df378623b947d351b4c8cdd6ea8028ca52054a6ed3349b1d18e6\n                  cni.projectcalico.org/podIP: 100.64.1.18/32\n                  cni.projectcalico.org/podIPs: 100.64.1.18/32\nStatus:           Running\nIP:               100.64.1.18\nIPs:\n  IP:           100.64.1.18\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://2a773b36b08b069d0ce242c6a9a8d2c520fb5b59aa83cf6e90bd370262af91a2\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 04 Sep 2023 15:23:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBERNETES_SERVICE_HOST:  api.tmud5-dd2.it.internal.staging.k8s.ondemand.com\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8jtm2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-8jtm2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-3011/agnhost-primary-2bj7d to shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh\n  Normal  Pulled     0s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
    Sep  4 15:23:23.860: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3011 describe rc agnhost-primary'
    Sep  4 15:23:24.006: INFO: stderr: ""
    Sep  4 15:23:24.006: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3011\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-2bj7d\n"
    Sep  4 15:23:24.006: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3011 describe service agnhost-primary'
    Sep  4 15:23:24.165: INFO: stderr: ""
    Sep  4 15:23:24.165: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3011\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.104.37.53\nIPs:               100.104.37.53\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.64.1.18:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Sep  4 15:23:24.188: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3011 describe node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx'
    Sep  4 15:23:24.400: INFO: stderr: ""
    Sep  4 15:23:24.400: INFO: stdout: "Name:               shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=g_c2_m4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-nl-1\n                    failure-domain.beta.kubernetes.io/zone=eu-nl-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx\n                    kubernetes.io/os=linux\n                    networking.gardener.cloud/node-local-dns-enabled=true\n                    node.kubernetes.io/instance-type=g_c2_m4\n                    node.kubernetes.io/role=node\n                    topology.cinder.csi.openstack.org/zone=eu-nl-1a\n                    topology.kubernetes.io/region=eu-nl-1\n                    topology.kubernetes.io/zone=eu-nl-1a\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/cri-name=containerd\n                    worker.gardener.cloud/image-name=gardenlinux\n                    worker.gardener.cloud/image-version=934.10.0\n                    worker.gardener.cloud/kubernetes-version=1.26.8\n                    worker.gardener.cloud/pool=worker-1\n                    worker.gardener.cloud/system-components=true\nAnnotations:        checksum/cloud-config-data: 0feb5c6e921ccd73834b5ac833849002acd76626441b9a150f91c7f8c8e59824\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"c33c7dc2-0749-4aba-b85f-ac574d3e2add\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.machine.sapcloud.io/last-applied-anno-labels-taints:\n                      {\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"kubernetes.io/arch\":\"amd64\",\"networking.gardener.cloud/node-local-dns-enabled\":\"true\",\"no...\n                    projectcalico.org/IPv4Address: 10.250.1.105/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.64.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 04 Sep 2023 14:34:42 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 04 Sep 2023 15:23:17 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  ClusterNetworkProblem         False   Mon, 04 Sep 2023 15:20:54 +0000   Mon, 04 Sep 2023 14:38:10 +0000   NoNetworkProblems               no cluster network problems\n  HostNetworkProblem            False   Mon, 04 Sep 2023 15:21:49 +0000   Mon, 04 Sep 2023 14:36:00 +0000   NoNetworkProblems               no host network problems\n  KernelDeadlock                False   Mon, 04 Sep 2023 15:21:18 +0000   Mon, 04 Sep 2023 14:41:11 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Mon, 04 Sep 2023 15:21:18 +0000   Mon, 04 Sep 2023 14:41:11 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  FrequentKubeletRestart        False   Mon, 04 Sep 2023 15:21:18 +0000   Mon, 04 Sep 2023 14:41:11 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Mon, 04 Sep 2023 15:21:18 +0000   Mon, 04 Sep 2023 14:41:11 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Mon, 04 Sep 2023 15:21:18 +0000   Mon, 04 Sep 2023 14:41:11 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  FrequentUnregisterNetDevice   False   Mon, 04 Sep 2023 15:21:18 +0000   Mon, 04 Sep 2023 14:41:11 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  CorruptDockerOverlay2         False   Mon, 04 Sep 2023 15:21:18 +0000   Mon, 04 Sep 2023 14:41:11 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  NetworkUnavailable            False   Mon, 04 Sep 2023 14:35:06 +0000   Mon, 04 Sep 2023 14:35:06 +0000   CalicoIsUp                      Calico is running on this node\n  MemoryPressure                False   Mon, 04 Sep 2023 15:19:11 +0000   Mon, 04 Sep 2023 14:34:42 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Mon, 04 Sep 2023 15:19:11 +0000   Mon, 04 Sep 2023 14:34:42 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Mon, 04 Sep 2023 15:19:11 +0000   Mon, 04 Sep 2023 14:34:42 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Mon, 04 Sep 2023 15:19:11 +0000   Mon, 04 Sep 2023 14:34:51 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.250.1.105\n  Hostname:    shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx\nCapacity:\n  cpu:                2\n  ephemeral-storage:  64455612Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4008148Ki\n  pods:               110\nAllocatable:\n  cpu:                1920m\n  ephemeral-storage:  62702419305\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2857172Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 e53c6370ad8448bfacd7f410dd7dd40f\n  System UUID:                0b310142-5859-59e9-3ded-33db568fac7d\n  Boot ID:                    96b62fee-161f-42b3-8395-a5ae1b4ceed3\n  Kernel Version:             5.15.125-gardenlinux-cloud-amd64\n  OS Image:                   Garden Linux 934.10\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.20\n  Kubelet Version:            v1.26.8\n  Kube-Proxy Version:         v1.26.8\nPodCIDR:                      100.64.0.0/24\nPodCIDRs:                     100.64.0.0/24\nProviderID:                   openstack:///c33c7dc2-0749-4aba-b85f-ac574d3e2add\nNon-terminated Pods:          (26 in total)\n  Namespace                   Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits    Age\n  ---------                   ----                                                               ------------  ----------  ---------------  -------------    ---\n  kube-system                 addons-nginx-ingress-controller-56b5dc8f6c-tr9n6                   11m (0%)      0 (0%)      100Mi (3%)       4Gi (146%)       32m\n  kube-system                 addons-nginx-ingress-nginx-ingress-k8s-backend-58685978db-gpxdq    20m (1%)      0 (0%)      20Mi (0%)        100Mi (3%)       49m\n  kube-system                 apiserver-proxy-bc5jh                                              40m (2%)      0 (0%)      40Mi (1%)        1114Mi (39%)     48m\n  kube-system                 blackbox-exporter-585854d657-fvtbg                                 10m (0%)      0 (0%)      25Mi (0%)        128Mi (4%)       50m\n  kube-system                 blackbox-exporter-585854d657-vz8rw                                 10m (0%)      0 (0%)      25Mi (0%)        128Mi (4%)       50m\n  kube-system                 calico-kube-controllers-684b9f4889-f24wb                           10m (0%)      0 (0%)      50Mi (1%)        2Gi (73%)        49m\n  kube-system                 calico-node-959qs                                                  250m (13%)    0 (0%)      100Mi (3%)       2800Mi (100%)    48m\n  kube-system                 calico-node-vertical-autoscaler-7bbd54698f-zb6st                   10m (0%)      0 (0%)      50Mi (1%)        130Mi (4%)       49m\n  kube-system                 calico-typha-deploy-6f4475c6d5-z7t2m                               320m (16%)    0 (0%)      262144k (8%)     4194304k (143%)  47m\n  kube-system                 calico-typha-horizontal-autoscaler-7b89d5ff97-gdvkd                10m (0%)      0 (0%)      50Mi (1%)        100Mi (3%)       49m\n  kube-system                 calico-typha-vertical-autoscaler-656479b7b5-khd8g                  10m (0%)      0 (0%)      50Mi (1%)        130Mi (4%)       49m\n  kube-system                 coredns-89679867b-2fr9s                                            50m (2%)      0 (0%)      15Mi (0%)        1500Mi (53%)     49m\n  kube-system                 coredns-89679867b-5qtl7                                            50m (2%)      0 (0%)      15Mi (0%)        1500Mi (53%)     49m\n  kube-system                 csi-driver-node-5kfkx                                              37m (1%)      0 (0%)      106Mi (3%)       0 (0%)           48m\n  kube-system                 egress-filter-applier-n4kfc                                        50m (2%)      0 (0%)      64Mi (2%)        256Mi (9%)       48m\n  kube-system                 kube-proxy-worker-1-v1.26.8-ch6px                                  34m (1%)      0 (0%)      61066436 (2%)    2Gi (73%)        22m\n  kube-system                 metrics-server-78947f8d7c-9cdcs                                    50m (2%)      0 (0%)      60Mi (2%)        1Gi (36%)        49m\n  kube-system                 metrics-server-78947f8d7c-w8gr6                                    11m (0%)      0 (0%)      60Mi (2%)        1Gi (36%)        32m\n  kube-system                 network-problem-detector-host-xf9l5                                10m (0%)      50m (2%)    32Mi (1%)        64Mi (2%)        48m\n  kube-system                 network-problem-detector-pod-lbp4l                                 10m (0%)      50m (2%)    32Mi (1%)        64Mi (2%)        48m\n  kube-system                 node-exporter-ktjnf                                                50m (2%)      0 (0%)      50Mi (1%)        250Mi (8%)       48m\n  kube-system                 node-local-dns-c8s84                                               11m (0%)      0 (0%)      23574998 (0%)    94299992 (3%)    39m\n  kube-system                 node-problem-detector-rqmbk                                        49m (2%)      0 (0%)      36253748 (1%)    120Mi (4%)       42m\n  kube-system                 vpn-shoot-5d596dbb88-5vx52                                         100m (5%)     0 (0%)      100Mi (3%)       100Mi (3%)       50m\n  kubernetes-dashboard        dashboard-metrics-scraper-6c889fdd54-555v8                         0 (0%)        0 (0%)      0 (0%)           0 (0%)           50m\n  kubernetes-dashboard        kubernetes-dashboard-b9859c4d7-bq9mj                               11m (0%)      0 (0%)      23574998 (0%)    256Mi (9%)       32m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests          Limits\n  --------           --------          ------\n  cpu                1224m (63%)       100m (5%)\n  memory             1501327524 (51%)  24190576472 (826%)\n  ephemeral-storage  0 (0%)            0 (0%)\n  hugepages-1Gi      0 (0%)            0 (0%)\n  hugepages-2Mi      0 (0%)            0 (0%)\nEvents:\n  Type     Reason                             Age                From                          Message\n  ----     ------                             ----               ----                          -------\n  Normal   Starting                           22m                kube-proxy                    \n  Normal   Starting                           46m                kube-proxy                    \n  Normal   Starting                           48m                kube-proxy                    \n  Normal   Starting                           48m                kubelet                       Starting kubelet.\n  Warning  InvalidDiskCapacity                48m                kubelet                       invalid capacity 0 on image filesystem\n  Warning  UnscheduledNodeCriticalDaemonSets  48m                gardener-node-controller      Node-critical DaemonSets found that were not scheduled to Node yet: kube-system/apiserver-proxy, kube-system/calico-node, kube-system/csi-driver-node, kube-system/kube-proxy-worker-1-v1.26.8\n  Normal   NodeHasSufficientMemory            48m (x2 over 48m)  kubelet                       Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure              48m (x2 over 48m)  kubelet                       Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID               48m (x2 over 48m)  kubelet                       Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced            48m                kubelet                       Updated Node Allocatable limit across pods\n  Normal   Synced                             48m                cloud-node-controller         Node synced successfully\n  Normal   RegisteredNode                     48m                node-controller               Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx event: Registered Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx in Controller\n  Normal   NodeReady                          48m                kubelet                       Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx status is now: NodeReady\n  Warning  UnreadyNodeCriticalPods            48m (x2 over 48m)  gardener-node-controller      Unready node-critical Pods found on Node: kube-system/apiserver-proxy-bc5jh, kube-system/calico-node-959qs, kube-system/csi-driver-node-5kfkx, kube-system/kube-proxy-worker-1-v1.26.8-x7pxl\n  Warning  UnreadyNodeCriticalPods            48m                gardener-node-controller      Unready node-critical Pods found on Node: kube-system/calico-node-959qs, kube-system/csi-driver-node-5kfkx\n  Warning  ContainerdStart                    48m (x2 over 48m)  systemd-monitor               Starting containerd container runtime...\n  Warning  DockerStart                        48m (x2 over 48m)  systemd-monitor               Starting Docker Application Container Engine...\n  Normal   NodeCriticalComponentsReady        48m                gardener-node-controller      All node-critical components got ready, removing taint\n  Warning  FailedNetworkChecks                47m                network-problem-detector-pod  cluster network problems for 4 pairs of jobIDs (https-p2api-int,nslookup-p) and destinations (api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com,eu.gcr.io,kubernetes.default.svc.cluster.local)\n"
    Sep  4 15:23:24.400: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3011 describe namespace kubectl-3011'
    Sep  4 15:23:24.537: INFO: stderr: ""
    Sep  4 15:23:24.537: INFO: stdout: "Name:         kubectl-3011\nLabels:       e2e-framework=kubectl\n              e2e-run=0be53abf-8976-49ee-9130-04032f304ae9\n              kubernetes.io/metadata.name=kubectl-3011\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:23:24.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3011" for this suite. 09/04/23 15:23:24.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:23:24.573
Sep  4 15:23:24.573: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:23:24.574
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:23:24.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:23:24.634
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 09/04/23 15:23:24.656
Sep  4 15:23:24.674: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b" in namespace "projected-6096" to be "Succeeded or Failed"
Sep  4 15:23:24.686: INFO: Pod "downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.711476ms
Sep  4 15:23:26.698: INFO: Pod "downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023947136s
Sep  4 15:23:28.700: INFO: Pod "downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025199331s
STEP: Saw pod success 09/04/23 15:23:28.7
Sep  4 15:23:28.700: INFO: Pod "downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b" satisfied condition "Succeeded or Failed"
Sep  4 15:23:28.713: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b container client-container: <nil>
STEP: delete the pod 09/04/23 15:23:28.799
Sep  4 15:23:28.819: INFO: Waiting for pod downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b to disappear
Sep  4 15:23:28.832: INFO: Pod downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 15:23:28.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6096" for this suite. 09/04/23 15:23:28.855
------------------------------
• [4.296 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:23:24.573
    Sep  4 15:23:24.573: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:23:24.574
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:23:24.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:23:24.634
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 09/04/23 15:23:24.656
    Sep  4 15:23:24.674: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b" in namespace "projected-6096" to be "Succeeded or Failed"
    Sep  4 15:23:24.686: INFO: Pod "downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.711476ms
    Sep  4 15:23:26.698: INFO: Pod "downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023947136s
    Sep  4 15:23:28.700: INFO: Pod "downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025199331s
    STEP: Saw pod success 09/04/23 15:23:28.7
    Sep  4 15:23:28.700: INFO: Pod "downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b" satisfied condition "Succeeded or Failed"
    Sep  4 15:23:28.713: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b container client-container: <nil>
    STEP: delete the pod 09/04/23 15:23:28.799
    Sep  4 15:23:28.819: INFO: Waiting for pod downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b to disappear
    Sep  4 15:23:28.832: INFO: Pod downwardapi-volume-99d68e1d-8571-42c0-87fb-31362ed9f73b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:23:28.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6096" for this suite. 09/04/23 15:23:28.855
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:23:28.869
Sep  4 15:23:28.869: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 09/04/23 15:23:28.87
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:23:28.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:23:28.931
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-950 09/04/23 15:23:28.954
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 09/04/23 15:23:28.967
STEP: Creating pod with conflicting port in namespace statefulset-950 09/04/23 15:23:29.044
STEP: Waiting until pod test-pod will start running in namespace statefulset-950 09/04/23 15:23:29.063
Sep  4 15:23:29.064: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-950" to be "running"
Sep  4 15:23:29.076: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.640696ms
Sep  4 15:23:31.090: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02638382s
Sep  4 15:23:31.090: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-950 09/04/23 15:23:31.09
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-950 09/04/23 15:23:31.104
Sep  4 15:23:31.127: INFO: Observed stateful pod in namespace: statefulset-950, name: ss-0, uid: 5258ff98-5098-450e-9d45-fcf287a93757, status phase: Pending. Waiting for statefulset controller to delete.
Sep  4 15:23:31.145: INFO: Observed stateful pod in namespace: statefulset-950, name: ss-0, uid: 5258ff98-5098-450e-9d45-fcf287a93757, status phase: Failed. Waiting for statefulset controller to delete.
Sep  4 15:23:31.149: INFO: Observed stateful pod in namespace: statefulset-950, name: ss-0, uid: 5258ff98-5098-450e-9d45-fcf287a93757, status phase: Failed. Waiting for statefulset controller to delete.
Sep  4 15:23:31.151: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-950
STEP: Removing pod with conflicting port in namespace statefulset-950 09/04/23 15:23:31.151
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-950 and will be in running state 09/04/23 15:23:31.166
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 15:23:33.191: INFO: Deleting all statefulset in ns statefulset-950
Sep  4 15:23:33.203: INFO: Scaling statefulset ss to 0
Sep  4 15:23:43.254: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 15:23:43.267: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 15:23:43.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-950" for this suite. 09/04/23 15:23:43.325
------------------------------
• [14.470 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:23:28.869
    Sep  4 15:23:28.869: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 09/04/23 15:23:28.87
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:23:28.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:23:28.931
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-950 09/04/23 15:23:28.954
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 09/04/23 15:23:28.967
    STEP: Creating pod with conflicting port in namespace statefulset-950 09/04/23 15:23:29.044
    STEP: Waiting until pod test-pod will start running in namespace statefulset-950 09/04/23 15:23:29.063
    Sep  4 15:23:29.064: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-950" to be "running"
    Sep  4 15:23:29.076: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.640696ms
    Sep  4 15:23:31.090: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02638382s
    Sep  4 15:23:31.090: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-950 09/04/23 15:23:31.09
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-950 09/04/23 15:23:31.104
    Sep  4 15:23:31.127: INFO: Observed stateful pod in namespace: statefulset-950, name: ss-0, uid: 5258ff98-5098-450e-9d45-fcf287a93757, status phase: Pending. Waiting for statefulset controller to delete.
    Sep  4 15:23:31.145: INFO: Observed stateful pod in namespace: statefulset-950, name: ss-0, uid: 5258ff98-5098-450e-9d45-fcf287a93757, status phase: Failed. Waiting for statefulset controller to delete.
    Sep  4 15:23:31.149: INFO: Observed stateful pod in namespace: statefulset-950, name: ss-0, uid: 5258ff98-5098-450e-9d45-fcf287a93757, status phase: Failed. Waiting for statefulset controller to delete.
    Sep  4 15:23:31.151: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-950
    STEP: Removing pod with conflicting port in namespace statefulset-950 09/04/23 15:23:31.151
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-950 and will be in running state 09/04/23 15:23:31.166
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 15:23:33.191: INFO: Deleting all statefulset in ns statefulset-950
    Sep  4 15:23:33.203: INFO: Scaling statefulset ss to 0
    Sep  4 15:23:43.254: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 15:23:43.267: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:23:43.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-950" for this suite. 09/04/23 15:23:43.325
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:23:43.339
Sep  4 15:23:43.339: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 09/04/23 15:23:43.34
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:23:43.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:23:43.398
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/04/23 15:23:43.42
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-lr4l 09/04/23 15:23:43.445
STEP: Creating a pod to test atomic-volume-subpath 09/04/23 15:23:43.445
Sep  4 15:23:43.474: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lr4l" in namespace "subpath-8257" to be "Succeeded or Failed"
Sep  4 15:23:43.486: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019988ms
Sep  4 15:23:45.499: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 2.02570782s
Sep  4 15:23:47.499: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 4.025405801s
Sep  4 15:23:49.500: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 6.026411647s
Sep  4 15:23:51.499: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 8.0259333s
Sep  4 15:23:53.501: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 10.026983481s
Sep  4 15:23:55.500: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 12.02681059s
Sep  4 15:23:57.499: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 14.02557739s
Sep  4 15:23:59.499: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 16.025265294s
Sep  4 15:24:01.498: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 18.024880571s
Sep  4 15:24:03.500: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 20.026718228s
Sep  4 15:24:05.500: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=false. Elapsed: 22.026015771s
Sep  4 15:24:07.500: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.026355108s
STEP: Saw pod success 09/04/23 15:24:07.5
Sep  4 15:24:07.500: INFO: Pod "pod-subpath-test-configmap-lr4l" satisfied condition "Succeeded or Failed"
Sep  4 15:24:07.512: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-subpath-test-configmap-lr4l container test-container-subpath-configmap-lr4l: <nil>
STEP: delete the pod 09/04/23 15:24:07.559
Sep  4 15:24:07.575: INFO: Waiting for pod pod-subpath-test-configmap-lr4l to disappear
Sep  4 15:24:07.588: INFO: Pod pod-subpath-test-configmap-lr4l no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lr4l 09/04/23 15:24:07.588
Sep  4 15:24:07.588: INFO: Deleting pod "pod-subpath-test-configmap-lr4l" in namespace "subpath-8257"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  4 15:24:07.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8257" for this suite. 09/04/23 15:24:07.623
------------------------------
• [24.298 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:23:43.339
    Sep  4 15:23:43.339: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 09/04/23 15:23:43.34
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:23:43.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:23:43.398
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/04/23 15:23:43.42
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-lr4l 09/04/23 15:23:43.445
    STEP: Creating a pod to test atomic-volume-subpath 09/04/23 15:23:43.445
    Sep  4 15:23:43.474: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lr4l" in namespace "subpath-8257" to be "Succeeded or Failed"
    Sep  4 15:23:43.486: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019988ms
    Sep  4 15:23:45.499: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 2.02570782s
    Sep  4 15:23:47.499: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 4.025405801s
    Sep  4 15:23:49.500: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 6.026411647s
    Sep  4 15:23:51.499: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 8.0259333s
    Sep  4 15:23:53.501: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 10.026983481s
    Sep  4 15:23:55.500: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 12.02681059s
    Sep  4 15:23:57.499: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 14.02557739s
    Sep  4 15:23:59.499: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 16.025265294s
    Sep  4 15:24:01.498: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 18.024880571s
    Sep  4 15:24:03.500: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=true. Elapsed: 20.026718228s
    Sep  4 15:24:05.500: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Running", Reason="", readiness=false. Elapsed: 22.026015771s
    Sep  4 15:24:07.500: INFO: Pod "pod-subpath-test-configmap-lr4l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.026355108s
    STEP: Saw pod success 09/04/23 15:24:07.5
    Sep  4 15:24:07.500: INFO: Pod "pod-subpath-test-configmap-lr4l" satisfied condition "Succeeded or Failed"
    Sep  4 15:24:07.512: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-subpath-test-configmap-lr4l container test-container-subpath-configmap-lr4l: <nil>
    STEP: delete the pod 09/04/23 15:24:07.559
    Sep  4 15:24:07.575: INFO: Waiting for pod pod-subpath-test-configmap-lr4l to disappear
    Sep  4 15:24:07.588: INFO: Pod pod-subpath-test-configmap-lr4l no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-lr4l 09/04/23 15:24:07.588
    Sep  4 15:24:07.588: INFO: Deleting pod "pod-subpath-test-configmap-lr4l" in namespace "subpath-8257"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:24:07.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8257" for this suite. 09/04/23 15:24:07.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:24:07.638
Sep  4 15:24:07.638: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 15:24:07.639
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:07.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:07.698
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 09/04/23 15:24:07.721
Sep  4 15:24:07.721: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5706 api-versions'
Sep  4 15:24:07.854: INFO: stderr: ""
Sep  4 15:24:07.854: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.k8s.io/v1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndns.gardener.cloud/v1alpha1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 15:24:07.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5706" for this suite. 09/04/23 15:24:07.868
------------------------------
• [0.243 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:24:07.638
    Sep  4 15:24:07.638: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 15:24:07.639
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:07.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:07.698
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 09/04/23 15:24:07.721
    Sep  4 15:24:07.721: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5706 api-versions'
    Sep  4 15:24:07.854: INFO: stderr: ""
    Sep  4 15:24:07.854: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.k8s.io/v1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndns.gardener.cloud/v1alpha1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:24:07.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5706" for this suite. 09/04/23 15:24:07.868
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:24:07.881
Sep  4 15:24:07.882: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 15:24:07.882
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:07.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:07.948
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-4335 09/04/23 15:24:07.971
STEP: creating service affinity-nodeport in namespace services-4335 09/04/23 15:24:07.971
STEP: creating replication controller affinity-nodeport in namespace services-4335 09/04/23 15:24:07.994
I0904 15:24:08.010497    7754 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4335, replica count: 3
I0904 15:24:11.061587    7754 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 15:24:11.112: INFO: Creating new exec pod
Sep  4 15:24:11.130: INFO: Waiting up to 5m0s for pod "execpod-affinityl2vqb" in namespace "services-4335" to be "running"
Sep  4 15:24:11.143: INFO: Pod "execpod-affinityl2vqb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.237212ms
Sep  4 15:24:13.155: INFO: Pod "execpod-affinityl2vqb": Phase="Running", Reason="", readiness=true. Elapsed: 2.024814707s
Sep  4 15:24:13.155: INFO: Pod "execpod-affinityl2vqb" satisfied condition "running"
Sep  4 15:24:14.179: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4335 exec execpod-affinityl2vqb -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Sep  4 15:24:14.732: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep  4 15:24:14.732: INFO: stdout: ""
Sep  4 15:24:14.732: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4335 exec execpod-affinityl2vqb -- /bin/sh -x -c nc -v -z -w 2 100.106.129.38 80'
Sep  4 15:24:15.392: INFO: stderr: "+ nc -v -z -w 2 100.106.129.38 80\nConnection to 100.106.129.38 80 port [tcp/http] succeeded!\n"
Sep  4 15:24:15.392: INFO: stdout: ""
Sep  4 15:24:15.392: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4335 exec execpod-affinityl2vqb -- /bin/sh -x -c nc -v -z -w 2 10.250.1.105 30534'
Sep  4 15:24:16.013: INFO: stderr: "+ nc -v -z -w 2 10.250.1.105 30534\nConnection to 10.250.1.105 30534 port [tcp/*] succeeded!\n"
Sep  4 15:24:16.013: INFO: stdout: ""
Sep  4 15:24:16.013: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4335 exec execpod-affinityl2vqb -- /bin/sh -x -c nc -v -z -w 2 10.250.1.231 30534'
Sep  4 15:24:16.694: INFO: stderr: "+ nc -v -z -w 2 10.250.1.231 30534\nConnection to 10.250.1.231 30534 port [tcp/*] succeeded!\n"
Sep  4 15:24:16.694: INFO: stdout: ""
Sep  4 15:24:16.694: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4335 exec execpod-affinityl2vqb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.1.105:30534/ ; done'
Sep  4 15:24:17.392: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n"
Sep  4 15:24:17.392: INFO: stdout: "\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf"
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
Sep  4 15:24:17.392: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4335, will wait for the garbage collector to delete the pods 09/04/23 15:24:17.415
Sep  4 15:24:17.492: INFO: Deleting ReplicationController affinity-nodeport took: 14.354162ms
Sep  4 15:24:17.593: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.802886ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 15:24:19.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4335" for this suite. 09/04/23 15:24:19.938
------------------------------
• [12.072 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:24:07.881
    Sep  4 15:24:07.882: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 15:24:07.882
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:07.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:07.948
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-4335 09/04/23 15:24:07.971
    STEP: creating service affinity-nodeport in namespace services-4335 09/04/23 15:24:07.971
    STEP: creating replication controller affinity-nodeport in namespace services-4335 09/04/23 15:24:07.994
    I0904 15:24:08.010497    7754 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4335, replica count: 3
    I0904 15:24:11.061587    7754 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 15:24:11.112: INFO: Creating new exec pod
    Sep  4 15:24:11.130: INFO: Waiting up to 5m0s for pod "execpod-affinityl2vqb" in namespace "services-4335" to be "running"
    Sep  4 15:24:11.143: INFO: Pod "execpod-affinityl2vqb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.237212ms
    Sep  4 15:24:13.155: INFO: Pod "execpod-affinityl2vqb": Phase="Running", Reason="", readiness=true. Elapsed: 2.024814707s
    Sep  4 15:24:13.155: INFO: Pod "execpod-affinityl2vqb" satisfied condition "running"
    Sep  4 15:24:14.179: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4335 exec execpod-affinityl2vqb -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Sep  4 15:24:14.732: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Sep  4 15:24:14.732: INFO: stdout: ""
    Sep  4 15:24:14.732: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4335 exec execpod-affinityl2vqb -- /bin/sh -x -c nc -v -z -w 2 100.106.129.38 80'
    Sep  4 15:24:15.392: INFO: stderr: "+ nc -v -z -w 2 100.106.129.38 80\nConnection to 100.106.129.38 80 port [tcp/http] succeeded!\n"
    Sep  4 15:24:15.392: INFO: stdout: ""
    Sep  4 15:24:15.392: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4335 exec execpod-affinityl2vqb -- /bin/sh -x -c nc -v -z -w 2 10.250.1.105 30534'
    Sep  4 15:24:16.013: INFO: stderr: "+ nc -v -z -w 2 10.250.1.105 30534\nConnection to 10.250.1.105 30534 port [tcp/*] succeeded!\n"
    Sep  4 15:24:16.013: INFO: stdout: ""
    Sep  4 15:24:16.013: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4335 exec execpod-affinityl2vqb -- /bin/sh -x -c nc -v -z -w 2 10.250.1.231 30534'
    Sep  4 15:24:16.694: INFO: stderr: "+ nc -v -z -w 2 10.250.1.231 30534\nConnection to 10.250.1.231 30534 port [tcp/*] succeeded!\n"
    Sep  4 15:24:16.694: INFO: stdout: ""
    Sep  4 15:24:16.694: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4335 exec execpod-affinityl2vqb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.1.105:30534/ ; done'
    Sep  4 15:24:17.392: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30534/\n"
    Sep  4 15:24:17.392: INFO: stdout: "\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf\naffinity-nodeport-wr8zf"
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Received response from host: affinity-nodeport-wr8zf
    Sep  4 15:24:17.392: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-4335, will wait for the garbage collector to delete the pods 09/04/23 15:24:17.415
    Sep  4 15:24:17.492: INFO: Deleting ReplicationController affinity-nodeport took: 14.354162ms
    Sep  4 15:24:17.593: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.802886ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:24:19.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4335" for this suite. 09/04/23 15:24:19.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:24:19.957
Sep  4 15:24:19.957: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:24:19.958
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:19.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:20.023
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-f1983a4d-b83e-4544-94f5-d5fb2ccbe593 09/04/23 15:24:20.045
STEP: Creating a pod to test consume secrets 09/04/23 15:24:20.059
Sep  4 15:24:20.081: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6" in namespace "projected-4623" to be "Succeeded or Failed"
Sep  4 15:24:20.093: INFO: Pod "pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.683998ms
Sep  4 15:24:22.107: INFO: Pod "pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026382853s
Sep  4 15:24:24.107: INFO: Pod "pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02627015s
STEP: Saw pod success 09/04/23 15:24:24.107
Sep  4 15:24:24.107: INFO: Pod "pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6" satisfied condition "Succeeded or Failed"
Sep  4 15:24:24.120: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/04/23 15:24:24.154
Sep  4 15:24:24.170: INFO: Waiting for pod pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6 to disappear
Sep  4 15:24:24.182: INFO: Pod pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  4 15:24:24.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4623" for this suite. 09/04/23 15:24:24.205
------------------------------
• [4.261 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:24:19.957
    Sep  4 15:24:19.957: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:24:19.958
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:19.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:20.023
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-f1983a4d-b83e-4544-94f5-d5fb2ccbe593 09/04/23 15:24:20.045
    STEP: Creating a pod to test consume secrets 09/04/23 15:24:20.059
    Sep  4 15:24:20.081: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6" in namespace "projected-4623" to be "Succeeded or Failed"
    Sep  4 15:24:20.093: INFO: Pod "pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.683998ms
    Sep  4 15:24:22.107: INFO: Pod "pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026382853s
    Sep  4 15:24:24.107: INFO: Pod "pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02627015s
    STEP: Saw pod success 09/04/23 15:24:24.107
    Sep  4 15:24:24.107: INFO: Pod "pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6" satisfied condition "Succeeded or Failed"
    Sep  4 15:24:24.120: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 15:24:24.154
    Sep  4 15:24:24.170: INFO: Waiting for pod pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6 to disappear
    Sep  4 15:24:24.182: INFO: Pod pod-projected-secrets-d397e836-f0fe-4e86-877c-572ae6de3db6 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:24:24.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4623" for this suite. 09/04/23 15:24:24.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:24:24.218
Sep  4 15:24:24.219: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 15:24:24.219
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:24.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:24.278
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 09/04/23 15:24:24.301
Sep  4 15:24:24.301: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9462 cluster-info'
Sep  4 15:24:24.407: INFO: stderr: ""
Sep  4 15:24:24.407: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 15:24:24.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9462" for this suite. 09/04/23 15:24:24.42
------------------------------
• [0.215 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:24:24.218
    Sep  4 15:24:24.219: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 15:24:24.219
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:24.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:24.278
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 09/04/23 15:24:24.301
    Sep  4 15:24:24.301: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9462 cluster-info'
    Sep  4 15:24:24.407: INFO: stderr: ""
    Sep  4 15:24:24.407: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:24:24.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9462" for this suite. 09/04/23 15:24:24.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:24:24.434
Sep  4 15:24:24.434: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 09/04/23 15:24:24.435
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:24.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:24.495
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 09/04/23 15:24:24.518
STEP: creating a new configmap 09/04/23 15:24:24.529
STEP: modifying the configmap once 09/04/23 15:24:24.542
STEP: changing the label value of the configmap 09/04/23 15:24:24.569
STEP: Expecting to observe a delete notification for the watched object 09/04/23 15:24:24.595
Sep  4 15:24:24.595: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8063  26e6d6ba-f6a8-4dc4-b669-1cc8752b765f 25251 0 2023-09-04 15:24:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 15:24:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 15:24:24.596: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8063  26e6d6ba-f6a8-4dc4-b669-1cc8752b765f 25252 0 2023-09-04 15:24:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 15:24:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 15:24:24.596: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8063  26e6d6ba-f6a8-4dc4-b669-1cc8752b765f 25253 0 2023-09-04 15:24:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 15:24:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 09/04/23 15:24:24.596
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/04/23 15:24:24.622
STEP: changing the label value of the configmap back 09/04/23 15:24:34.623
STEP: modifying the configmap a third time 09/04/23 15:24:34.65
STEP: deleting the configmap 09/04/23 15:24:34.676
STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/04/23 15:24:34.689
Sep  4 15:24:34.689: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8063  26e6d6ba-f6a8-4dc4-b669-1cc8752b765f 25317 0 2023-09-04 15:24:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 15:24:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 15:24:34.689: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8063  26e6d6ba-f6a8-4dc4-b669-1cc8752b765f 25319 0 2023-09-04 15:24:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 15:24:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  4 15:24:34.690: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8063  26e6d6ba-f6a8-4dc4-b669-1cc8752b765f 25320 0 2023-09-04 15:24:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 15:24:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  4 15:24:34.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8063" for this suite. 09/04/23 15:24:34.712
------------------------------
• [10.292 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:24:24.434
    Sep  4 15:24:24.434: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 09/04/23 15:24:24.435
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:24.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:24.495
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 09/04/23 15:24:24.518
    STEP: creating a new configmap 09/04/23 15:24:24.529
    STEP: modifying the configmap once 09/04/23 15:24:24.542
    STEP: changing the label value of the configmap 09/04/23 15:24:24.569
    STEP: Expecting to observe a delete notification for the watched object 09/04/23 15:24:24.595
    Sep  4 15:24:24.595: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8063  26e6d6ba-f6a8-4dc4-b669-1cc8752b765f 25251 0 2023-09-04 15:24:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 15:24:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 15:24:24.596: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8063  26e6d6ba-f6a8-4dc4-b669-1cc8752b765f 25252 0 2023-09-04 15:24:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 15:24:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 15:24:24.596: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8063  26e6d6ba-f6a8-4dc4-b669-1cc8752b765f 25253 0 2023-09-04 15:24:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 15:24:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 09/04/23 15:24:24.596
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/04/23 15:24:24.622
    STEP: changing the label value of the configmap back 09/04/23 15:24:34.623
    STEP: modifying the configmap a third time 09/04/23 15:24:34.65
    STEP: deleting the configmap 09/04/23 15:24:34.676
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/04/23 15:24:34.689
    Sep  4 15:24:34.689: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8063  26e6d6ba-f6a8-4dc4-b669-1cc8752b765f 25317 0 2023-09-04 15:24:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 15:24:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 15:24:34.689: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8063  26e6d6ba-f6a8-4dc4-b669-1cc8752b765f 25319 0 2023-09-04 15:24:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 15:24:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  4 15:24:34.690: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8063  26e6d6ba-f6a8-4dc4-b669-1cc8752b765f 25320 0 2023-09-04 15:24:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-04 15:24:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:24:34.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8063" for this suite. 09/04/23 15:24:34.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:24:34.727
Sep  4 15:24:34.727: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename limitrange 09/04/23 15:24:34.727
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:34.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:34.786
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-6c6dc" in namespace "limitrange-3839" 09/04/23 15:24:34.809
STEP: Creating another limitRange in another namespace 09/04/23 15:24:34.822
Sep  4 15:24:34.860: INFO: Namespace "e2e-limitrange-6c6dc-8409" created
Sep  4 15:24:34.860: INFO: Creating LimitRange "e2e-limitrange-6c6dc" in namespace "e2e-limitrange-6c6dc-8409"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-6c6dc" 09/04/23 15:24:34.873
Sep  4 15:24:34.885: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-6c6dc" in "limitrange-3839" namespace 09/04/23 15:24:34.885
Sep  4 15:24:34.901: INFO: LimitRange "e2e-limitrange-6c6dc" has been patched
STEP: Delete LimitRange "e2e-limitrange-6c6dc" by Collection with labelSelector: "e2e-limitrange-6c6dc=patched" 09/04/23 15:24:34.901
STEP: Confirm that the limitRange "e2e-limitrange-6c6dc" has been deleted 09/04/23 15:24:34.917
Sep  4 15:24:34.917: INFO: Requesting list of LimitRange to confirm quantity
Sep  4 15:24:34.929: INFO: Found 0 LimitRange with label "e2e-limitrange-6c6dc=patched"
Sep  4 15:24:34.929: INFO: LimitRange "e2e-limitrange-6c6dc" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-6c6dc" 09/04/23 15:24:34.929
Sep  4 15:24:34.941: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Sep  4 15:24:34.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-3839" for this suite. 09/04/23 15:24:34.955
STEP: Destroying namespace "e2e-limitrange-6c6dc-8409" for this suite. 09/04/23 15:24:34.969
------------------------------
• [0.266 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:24:34.727
    Sep  4 15:24:34.727: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename limitrange 09/04/23 15:24:34.727
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:34.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:34.786
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-6c6dc" in namespace "limitrange-3839" 09/04/23 15:24:34.809
    STEP: Creating another limitRange in another namespace 09/04/23 15:24:34.822
    Sep  4 15:24:34.860: INFO: Namespace "e2e-limitrange-6c6dc-8409" created
    Sep  4 15:24:34.860: INFO: Creating LimitRange "e2e-limitrange-6c6dc" in namespace "e2e-limitrange-6c6dc-8409"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-6c6dc" 09/04/23 15:24:34.873
    Sep  4 15:24:34.885: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-6c6dc" in "limitrange-3839" namespace 09/04/23 15:24:34.885
    Sep  4 15:24:34.901: INFO: LimitRange "e2e-limitrange-6c6dc" has been patched
    STEP: Delete LimitRange "e2e-limitrange-6c6dc" by Collection with labelSelector: "e2e-limitrange-6c6dc=patched" 09/04/23 15:24:34.901
    STEP: Confirm that the limitRange "e2e-limitrange-6c6dc" has been deleted 09/04/23 15:24:34.917
    Sep  4 15:24:34.917: INFO: Requesting list of LimitRange to confirm quantity
    Sep  4 15:24:34.929: INFO: Found 0 LimitRange with label "e2e-limitrange-6c6dc=patched"
    Sep  4 15:24:34.929: INFO: LimitRange "e2e-limitrange-6c6dc" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-6c6dc" 09/04/23 15:24:34.929
    Sep  4 15:24:34.941: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:24:34.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-3839" for this suite. 09/04/23 15:24:34.955
    STEP: Destroying namespace "e2e-limitrange-6c6dc-8409" for this suite. 09/04/23 15:24:34.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:24:34.993
Sep  4 15:24:34.993: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 15:24:34.994
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:35.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:35.054
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 15:24:35.077
Sep  4 15:24:35.077: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-150 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Sep  4 15:24:35.180: INFO: stderr: ""
Sep  4 15:24:35.180: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 09/04/23 15:24:35.18
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Sep  4 15:24:35.193: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-150 delete pods e2e-test-httpd-pod'
Sep  4 15:24:37.848: INFO: stderr: ""
Sep  4 15:24:37.848: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 15:24:37.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-150" for this suite. 09/04/23 15:24:37.87
------------------------------
• [2.891 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:24:34.993
    Sep  4 15:24:34.993: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 15:24:34.994
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:35.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:35.054
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 15:24:35.077
    Sep  4 15:24:35.077: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-150 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Sep  4 15:24:35.180: INFO: stderr: ""
    Sep  4 15:24:35.180: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 09/04/23 15:24:35.18
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Sep  4 15:24:35.193: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-150 delete pods e2e-test-httpd-pod'
    Sep  4 15:24:37.848: INFO: stderr: ""
    Sep  4 15:24:37.848: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:24:37.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-150" for this suite. 09/04/23 15:24:37.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:24:37.884
Sep  4 15:24:37.884: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 09/04/23 15:24:37.885
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:37.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:37.945
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 09/04/23 15:24:37.967
STEP: Wait for the Deployment to create new ReplicaSet 09/04/23 15:24:37.993
STEP: delete the deployment 09/04/23 15:24:38.518
STEP: wait for all rs to be garbage collected 09/04/23 15:24:38.532
STEP: expected 0 rs, got 1 rs 09/04/23 15:24:38.557
STEP: expected 0 pods, got 2 pods 09/04/23 15:24:38.569
STEP: Gathering metrics 09/04/23 15:24:39.106
W0904 15:24:39.136205    7754 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep  4 15:24:39.136: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  4 15:24:39.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1428" for this suite. 09/04/23 15:24:39.149
------------------------------
• [1.278 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:24:37.884
    Sep  4 15:24:37.884: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 09/04/23 15:24:37.885
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:37.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:37.945
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 09/04/23 15:24:37.967
    STEP: Wait for the Deployment to create new ReplicaSet 09/04/23 15:24:37.993
    STEP: delete the deployment 09/04/23 15:24:38.518
    STEP: wait for all rs to be garbage collected 09/04/23 15:24:38.532
    STEP: expected 0 rs, got 1 rs 09/04/23 15:24:38.557
    STEP: expected 0 pods, got 2 pods 09/04/23 15:24:38.569
    STEP: Gathering metrics 09/04/23 15:24:39.106
    W0904 15:24:39.136205    7754 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep  4 15:24:39.136: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:24:39.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1428" for this suite. 09/04/23 15:24:39.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:24:39.163
Sep  4 15:24:39.163: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 15:24:39.164
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:39.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:39.223
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Sep  4 15:24:39.245: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:24:39.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5207" for this suite. 09/04/23 15:24:39.832
------------------------------
• [0.682 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:24:39.163
    Sep  4 15:24:39.163: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 15:24:39.164
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:39.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:39.223
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Sep  4 15:24:39.245: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:24:39.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5207" for this suite. 09/04/23 15:24:39.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:24:39.845
Sep  4 15:24:39.845: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test 09/04/23 15:24:39.846
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:39.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:39.907
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-6581 09/04/23 15:24:39.929
STEP: creating a selector 09/04/23 15:24:39.929
STEP: Creating the service pods in kubernetes 09/04/23 15:24:39.929
Sep  4 15:24:39.929: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  4 15:24:40.001: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6581" to be "running and ready"
Sep  4 15:24:40.014: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.219929ms
Sep  4 15:24:40.014: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:24:42.027: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.025463355s
Sep  4 15:24:42.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:24:44.027: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.025849268s
Sep  4 15:24:44.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:24:46.028: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.026454131s
Sep  4 15:24:46.028: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:24:48.028: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.026414033s
Sep  4 15:24:48.028: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:24:50.027: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.025635773s
Sep  4 15:24:50.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:24:52.028: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.026117712s
Sep  4 15:24:52.028: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  4 15:24:52.028: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  4 15:24:52.040: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6581" to be "running and ready"
Sep  4 15:24:52.053: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.972352ms
Sep  4 15:24:52.053: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  4 15:24:52.053: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/04/23 15:24:52.066
Sep  4 15:24:52.083: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6581" to be "running"
Sep  4 15:24:52.095: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.984689ms
Sep  4 15:24:54.108: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025288784s
Sep  4 15:24:54.108: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  4 15:24:54.120: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  4 15:24:54.120: INFO: Breadth first check of 100.64.0.167 on host 10.250.1.105...
Sep  4 15:24:54.132: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.30:9080/dial?request=hostname&protocol=http&host=100.64.0.167&port=8083&tries=1'] Namespace:pod-network-test-6581 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:24:54.132: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:24:54.133: INFO: ExecWithOptions: Clientset creation
Sep  4 15:24:54.133: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-6581/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.30%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.64.0.167%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  4 15:24:54.575: INFO: Waiting for responses: map[]
Sep  4 15:24:54.575: INFO: reached 100.64.0.167 after 0/1 tries
Sep  4 15:24:54.575: INFO: Breadth first check of 100.64.1.29 on host 10.250.1.231...
Sep  4 15:24:54.587: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.30:9080/dial?request=hostname&protocol=http&host=100.64.1.29&port=8083&tries=1'] Namespace:pod-network-test-6581 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:24:54.587: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:24:54.588: INFO: ExecWithOptions: Clientset creation
Sep  4 15:24:54.588: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-6581/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.30%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.64.1.29%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  4 15:24:54.996: INFO: Waiting for responses: map[]
Sep  4 15:24:54.996: INFO: reached 100.64.1.29 after 0/1 tries
Sep  4 15:24:54.996: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  4 15:24:54.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6581" for this suite. 09/04/23 15:24:55.019
------------------------------
• [15.188 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:24:39.845
    Sep  4 15:24:39.845: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pod-network-test 09/04/23 15:24:39.846
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:39.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:39.907
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-6581 09/04/23 15:24:39.929
    STEP: creating a selector 09/04/23 15:24:39.929
    STEP: Creating the service pods in kubernetes 09/04/23 15:24:39.929
    Sep  4 15:24:39.929: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  4 15:24:40.001: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6581" to be "running and ready"
    Sep  4 15:24:40.014: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.219929ms
    Sep  4 15:24:40.014: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:24:42.027: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.025463355s
    Sep  4 15:24:42.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:24:44.027: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.025849268s
    Sep  4 15:24:44.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:24:46.028: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.026454131s
    Sep  4 15:24:46.028: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:24:48.028: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.026414033s
    Sep  4 15:24:48.028: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:24:50.027: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.025635773s
    Sep  4 15:24:50.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:24:52.028: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.026117712s
    Sep  4 15:24:52.028: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  4 15:24:52.028: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  4 15:24:52.040: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6581" to be "running and ready"
    Sep  4 15:24:52.053: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.972352ms
    Sep  4 15:24:52.053: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  4 15:24:52.053: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/04/23 15:24:52.066
    Sep  4 15:24:52.083: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6581" to be "running"
    Sep  4 15:24:52.095: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.984689ms
    Sep  4 15:24:54.108: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025288784s
    Sep  4 15:24:54.108: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  4 15:24:54.120: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  4 15:24:54.120: INFO: Breadth first check of 100.64.0.167 on host 10.250.1.105...
    Sep  4 15:24:54.132: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.30:9080/dial?request=hostname&protocol=http&host=100.64.0.167&port=8083&tries=1'] Namespace:pod-network-test-6581 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:24:54.132: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:24:54.133: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:24:54.133: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-6581/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.30%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.64.0.167%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  4 15:24:54.575: INFO: Waiting for responses: map[]
    Sep  4 15:24:54.575: INFO: reached 100.64.0.167 after 0/1 tries
    Sep  4 15:24:54.575: INFO: Breadth first check of 100.64.1.29 on host 10.250.1.231...
    Sep  4 15:24:54.587: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.30:9080/dial?request=hostname&protocol=http&host=100.64.1.29&port=8083&tries=1'] Namespace:pod-network-test-6581 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:24:54.587: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:24:54.588: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:24:54.588: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-6581/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.30%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.64.1.29%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  4 15:24:54.996: INFO: Waiting for responses: map[]
    Sep  4 15:24:54.996: INFO: reached 100.64.1.29 after 0/1 tries
    Sep  4 15:24:54.996: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:24:54.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6581" for this suite. 09/04/23 15:24:55.019
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:24:55.033
Sep  4 15:24:55.034: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 09/04/23 15:24:55.034
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:55.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:55.094
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Sep  4 15:24:55.136: INFO: Waiting up to 5m0s for pod "server-envvars-64822f72-ba36-40c0-a18d-cfd825b384f6" in namespace "pods-1791" to be "running and ready"
Sep  4 15:24:55.148: INFO: Pod "server-envvars-64822f72-ba36-40c0-a18d-cfd825b384f6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.09094ms
Sep  4 15:24:55.148: INFO: The phase of Pod server-envvars-64822f72-ba36-40c0-a18d-cfd825b384f6 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:24:57.161: INFO: Pod "server-envvars-64822f72-ba36-40c0-a18d-cfd825b384f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.025119178s
Sep  4 15:24:57.161: INFO: The phase of Pod server-envvars-64822f72-ba36-40c0-a18d-cfd825b384f6 is Running (Ready = true)
Sep  4 15:24:57.161: INFO: Pod "server-envvars-64822f72-ba36-40c0-a18d-cfd825b384f6" satisfied condition "running and ready"
Sep  4 15:24:57.206: INFO: Waiting up to 5m0s for pod "client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2" in namespace "pods-1791" to be "Succeeded or Failed"
Sep  4 15:24:57.218: INFO: Pod "client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.500123ms
Sep  4 15:24:59.230: INFO: Pod "client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024240451s
Sep  4 15:25:01.231: INFO: Pod "client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025110707s
STEP: Saw pod success 09/04/23 15:25:01.231
Sep  4 15:25:01.231: INFO: Pod "client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2" satisfied condition "Succeeded or Failed"
Sep  4 15:25:01.244: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2 container env3cont: <nil>
STEP: delete the pod 09/04/23 15:25:01.28
Sep  4 15:25:01.295: INFO: Waiting for pod client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2 to disappear
Sep  4 15:25:01.310: INFO: Pod client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 15:25:01.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1791" for this suite. 09/04/23 15:25:01.332
------------------------------
• [6.313 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:24:55.033
    Sep  4 15:24:55.034: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 09/04/23 15:24:55.034
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:24:55.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:24:55.094
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Sep  4 15:24:55.136: INFO: Waiting up to 5m0s for pod "server-envvars-64822f72-ba36-40c0-a18d-cfd825b384f6" in namespace "pods-1791" to be "running and ready"
    Sep  4 15:24:55.148: INFO: Pod "server-envvars-64822f72-ba36-40c0-a18d-cfd825b384f6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.09094ms
    Sep  4 15:24:55.148: INFO: The phase of Pod server-envvars-64822f72-ba36-40c0-a18d-cfd825b384f6 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:24:57.161: INFO: Pod "server-envvars-64822f72-ba36-40c0-a18d-cfd825b384f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.025119178s
    Sep  4 15:24:57.161: INFO: The phase of Pod server-envvars-64822f72-ba36-40c0-a18d-cfd825b384f6 is Running (Ready = true)
    Sep  4 15:24:57.161: INFO: Pod "server-envvars-64822f72-ba36-40c0-a18d-cfd825b384f6" satisfied condition "running and ready"
    Sep  4 15:24:57.206: INFO: Waiting up to 5m0s for pod "client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2" in namespace "pods-1791" to be "Succeeded or Failed"
    Sep  4 15:24:57.218: INFO: Pod "client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.500123ms
    Sep  4 15:24:59.230: INFO: Pod "client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024240451s
    Sep  4 15:25:01.231: INFO: Pod "client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025110707s
    STEP: Saw pod success 09/04/23 15:25:01.231
    Sep  4 15:25:01.231: INFO: Pod "client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2" satisfied condition "Succeeded or Failed"
    Sep  4 15:25:01.244: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2 container env3cont: <nil>
    STEP: delete the pod 09/04/23 15:25:01.28
    Sep  4 15:25:01.295: INFO: Waiting for pod client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2 to disappear
    Sep  4 15:25:01.310: INFO: Pod client-envvars-1f118ef4-53a0-4aa7-bfa1-bcfadee06ce2 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:25:01.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1791" for this suite. 09/04/23 15:25:01.332
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:25:01.347
Sep  4 15:25:01.347: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 09/04/23 15:25:01.347
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:25:01.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:25:01.408
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-d466e02f-77f1-41d6-a75a-493f882a057d in namespace container-probe-9791 09/04/23 15:25:01.431
Sep  4 15:25:01.450: INFO: Waiting up to 5m0s for pod "liveness-d466e02f-77f1-41d6-a75a-493f882a057d" in namespace "container-probe-9791" to be "not pending"
Sep  4 15:25:01.464: INFO: Pod "liveness-d466e02f-77f1-41d6-a75a-493f882a057d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.913468ms
Sep  4 15:25:03.476: INFO: Pod "liveness-d466e02f-77f1-41d6-a75a-493f882a057d": Phase="Running", Reason="", readiness=true. Elapsed: 2.025754177s
Sep  4 15:25:03.476: INFO: Pod "liveness-d466e02f-77f1-41d6-a75a-493f882a057d" satisfied condition "not pending"
Sep  4 15:25:03.476: INFO: Started pod liveness-d466e02f-77f1-41d6-a75a-493f882a057d in namespace container-probe-9791
STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 15:25:03.476
Sep  4 15:25:03.487: INFO: Initial restart count of pod liveness-d466e02f-77f1-41d6-a75a-493f882a057d is 0
Sep  4 15:25:23.634: INFO: Restart count of pod container-probe-9791/liveness-d466e02f-77f1-41d6-a75a-493f882a057d is now 1 (20.146760448s elapsed)
Sep  4 15:25:43.770: INFO: Restart count of pod container-probe-9791/liveness-d466e02f-77f1-41d6-a75a-493f882a057d is now 2 (40.28254071s elapsed)
Sep  4 15:26:03.907: INFO: Restart count of pod container-probe-9791/liveness-d466e02f-77f1-41d6-a75a-493f882a057d is now 3 (1m0.419541702s elapsed)
Sep  4 15:26:24.069: INFO: Restart count of pod container-probe-9791/liveness-d466e02f-77f1-41d6-a75a-493f882a057d is now 4 (1m20.581572516s elapsed)
Sep  4 15:27:34.571: INFO: Restart count of pod container-probe-9791/liveness-d466e02f-77f1-41d6-a75a-493f882a057d is now 5 (2m31.083691991s elapsed)
STEP: deleting the pod 09/04/23 15:27:34.571
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 15:27:34.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9791" for this suite. 09/04/23 15:27:34.611
------------------------------
• [153.279 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:25:01.347
    Sep  4 15:25:01.347: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 09/04/23 15:25:01.347
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:25:01.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:25:01.408
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-d466e02f-77f1-41d6-a75a-493f882a057d in namespace container-probe-9791 09/04/23 15:25:01.431
    Sep  4 15:25:01.450: INFO: Waiting up to 5m0s for pod "liveness-d466e02f-77f1-41d6-a75a-493f882a057d" in namespace "container-probe-9791" to be "not pending"
    Sep  4 15:25:01.464: INFO: Pod "liveness-d466e02f-77f1-41d6-a75a-493f882a057d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.913468ms
    Sep  4 15:25:03.476: INFO: Pod "liveness-d466e02f-77f1-41d6-a75a-493f882a057d": Phase="Running", Reason="", readiness=true. Elapsed: 2.025754177s
    Sep  4 15:25:03.476: INFO: Pod "liveness-d466e02f-77f1-41d6-a75a-493f882a057d" satisfied condition "not pending"
    Sep  4 15:25:03.476: INFO: Started pod liveness-d466e02f-77f1-41d6-a75a-493f882a057d in namespace container-probe-9791
    STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 15:25:03.476
    Sep  4 15:25:03.487: INFO: Initial restart count of pod liveness-d466e02f-77f1-41d6-a75a-493f882a057d is 0
    Sep  4 15:25:23.634: INFO: Restart count of pod container-probe-9791/liveness-d466e02f-77f1-41d6-a75a-493f882a057d is now 1 (20.146760448s elapsed)
    Sep  4 15:25:43.770: INFO: Restart count of pod container-probe-9791/liveness-d466e02f-77f1-41d6-a75a-493f882a057d is now 2 (40.28254071s elapsed)
    Sep  4 15:26:03.907: INFO: Restart count of pod container-probe-9791/liveness-d466e02f-77f1-41d6-a75a-493f882a057d is now 3 (1m0.419541702s elapsed)
    Sep  4 15:26:24.069: INFO: Restart count of pod container-probe-9791/liveness-d466e02f-77f1-41d6-a75a-493f882a057d is now 4 (1m20.581572516s elapsed)
    Sep  4 15:27:34.571: INFO: Restart count of pod container-probe-9791/liveness-d466e02f-77f1-41d6-a75a-493f882a057d is now 5 (2m31.083691991s elapsed)
    STEP: deleting the pod 09/04/23 15:27:34.571
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:27:34.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9791" for this suite. 09/04/23 15:27:34.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:27:34.626
Sep  4 15:27:34.626: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 15:27:34.627
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:27:34.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:27:34.687
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 09/04/23 15:27:34.71
Sep  4 15:27:34.739: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10" in namespace "downward-api-7725" to be "Succeeded or Failed"
Sep  4 15:27:34.751: INFO: Pod "downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10": Phase="Pending", Reason="", readiness=false. Elapsed: 12.330009ms
Sep  4 15:27:36.766: INFO: Pod "downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02722371s
Sep  4 15:27:38.765: INFO: Pod "downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026234056s
STEP: Saw pod success 09/04/23 15:27:38.765
Sep  4 15:27:38.765: INFO: Pod "downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10" satisfied condition "Succeeded or Failed"
Sep  4 15:27:38.778: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10 container client-container: <nil>
STEP: delete the pod 09/04/23 15:27:38.855
Sep  4 15:27:38.871: INFO: Waiting for pod downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10 to disappear
Sep  4 15:27:38.883: INFO: Pod downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 15:27:38.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7725" for this suite. 09/04/23 15:27:38.906
------------------------------
• [4.294 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:27:34.626
    Sep  4 15:27:34.626: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 15:27:34.627
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:27:34.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:27:34.687
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 09/04/23 15:27:34.71
    Sep  4 15:27:34.739: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10" in namespace "downward-api-7725" to be "Succeeded or Failed"
    Sep  4 15:27:34.751: INFO: Pod "downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10": Phase="Pending", Reason="", readiness=false. Elapsed: 12.330009ms
    Sep  4 15:27:36.766: INFO: Pod "downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02722371s
    Sep  4 15:27:38.765: INFO: Pod "downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026234056s
    STEP: Saw pod success 09/04/23 15:27:38.765
    Sep  4 15:27:38.765: INFO: Pod "downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10" satisfied condition "Succeeded or Failed"
    Sep  4 15:27:38.778: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10 container client-container: <nil>
    STEP: delete the pod 09/04/23 15:27:38.855
    Sep  4 15:27:38.871: INFO: Waiting for pod downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10 to disappear
    Sep  4 15:27:38.883: INFO: Pod downwardapi-volume-6989f588-ccc7-4bc4-b549-2d8cbc50fe10 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:27:38.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7725" for this suite. 09/04/23 15:27:38.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:27:38.921
Sep  4 15:27:38.921: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 15:27:38.922
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:27:38.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:27:38.993
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 09/04/23 15:27:39.016
STEP: fetching the ConfigMap 09/04/23 15:27:39.029
STEP: patching the ConfigMap 09/04/23 15:27:39.042
STEP: listing all ConfigMaps in all namespaces with a label selector 09/04/23 15:27:39.057
STEP: deleting the ConfigMap by collection with a label selector 09/04/23 15:27:39.07
STEP: listing all ConfigMaps in test namespace 09/04/23 15:27:39.085
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:27:39.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4849" for this suite. 09/04/23 15:27:39.112
------------------------------
• [0.204 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:27:38.921
    Sep  4 15:27:38.921: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 15:27:38.922
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:27:38.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:27:38.993
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 09/04/23 15:27:39.016
    STEP: fetching the ConfigMap 09/04/23 15:27:39.029
    STEP: patching the ConfigMap 09/04/23 15:27:39.042
    STEP: listing all ConfigMaps in all namespaces with a label selector 09/04/23 15:27:39.057
    STEP: deleting the ConfigMap by collection with a label selector 09/04/23 15:27:39.07
    STEP: listing all ConfigMaps in test namespace 09/04/23 15:27:39.085
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:27:39.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4849" for this suite. 09/04/23 15:27:39.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:27:39.126
Sep  4 15:27:39.126: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 15:27:39.127
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:27:39.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:27:39.188
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-eee26d01-1922-4015-b680-fac7c5a540db 09/04/23 15:27:39.211
STEP: Creating a pod to test consume configMaps 09/04/23 15:27:39.224
Sep  4 15:27:39.244: INFO: Waiting up to 5m0s for pod "pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134" in namespace "configmap-9970" to be "Succeeded or Failed"
Sep  4 15:27:39.256: INFO: Pod "pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134": Phase="Pending", Reason="", readiness=false. Elapsed: 12.201807ms
Sep  4 15:27:41.270: INFO: Pod "pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026249466s
Sep  4 15:27:43.269: INFO: Pod "pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025752988s
STEP: Saw pod success 09/04/23 15:27:43.269
Sep  4 15:27:43.269: INFO: Pod "pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134" satisfied condition "Succeeded or Failed"
Sep  4 15:27:43.282: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134 container configmap-volume-test: <nil>
STEP: delete the pod 09/04/23 15:27:43.317
Sep  4 15:27:43.335: INFO: Waiting for pod pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134 to disappear
Sep  4 15:27:43.357: INFO: Pod pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:27:43.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9970" for this suite. 09/04/23 15:27:43.381
------------------------------
• [4.269 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:27:39.126
    Sep  4 15:27:39.126: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 15:27:39.127
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:27:39.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:27:39.188
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-eee26d01-1922-4015-b680-fac7c5a540db 09/04/23 15:27:39.211
    STEP: Creating a pod to test consume configMaps 09/04/23 15:27:39.224
    Sep  4 15:27:39.244: INFO: Waiting up to 5m0s for pod "pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134" in namespace "configmap-9970" to be "Succeeded or Failed"
    Sep  4 15:27:39.256: INFO: Pod "pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134": Phase="Pending", Reason="", readiness=false. Elapsed: 12.201807ms
    Sep  4 15:27:41.270: INFO: Pod "pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026249466s
    Sep  4 15:27:43.269: INFO: Pod "pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025752988s
    STEP: Saw pod success 09/04/23 15:27:43.269
    Sep  4 15:27:43.269: INFO: Pod "pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134" satisfied condition "Succeeded or Failed"
    Sep  4 15:27:43.282: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134 container configmap-volume-test: <nil>
    STEP: delete the pod 09/04/23 15:27:43.317
    Sep  4 15:27:43.335: INFO: Waiting for pod pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134 to disappear
    Sep  4 15:27:43.357: INFO: Pod pod-configmaps-791b8f4c-08e2-4eae-996c-98f5507e5134 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:27:43.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9970" for this suite. 09/04/23 15:27:43.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:27:43.396
Sep  4 15:27:43.396: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 15:27:43.397
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:27:43.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:27:43.461
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 09/04/23 15:27:43.484
Sep  4 15:27:43.484: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep  4 15:27:43.612: INFO: stderr: ""
Sep  4 15:27:43.612: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 09/04/23 15:27:43.612
Sep  4 15:27:43.612: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep  4 15:27:43.612: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-341" to be "running and ready, or succeeded"
Sep  4 15:27:43.625: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.870357ms
Sep  4 15:27:43.625: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh' to be 'Running' but was 'Pending'
Sep  4 15:27:45.639: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.026765383s
Sep  4 15:27:45.639: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep  4 15:27:45.639: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 09/04/23 15:27:45.639
Sep  4 15:27:45.639: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 logs logs-generator logs-generator'
Sep  4 15:27:45.792: INFO: stderr: ""
Sep  4 15:27:45.792: INFO: stdout: "I0904 15:27:44.348069       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/5q88 338\nI0904 15:27:44.548153       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/wqd7 546\nI0904 15:27:44.749135       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/95k 583\nI0904 15:27:44.948423       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/mwg 256\nI0904 15:27:45.148752       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/bdkh 310\nI0904 15:27:45.348875       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/g99 404\nI0904 15:27:45.548134       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/z5t 255\nI0904 15:27:45.748426       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5xw 435\n"
STEP: limiting log lines 09/04/23 15:27:45.793
Sep  4 15:27:45.793: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 logs logs-generator logs-generator --tail=1'
Sep  4 15:27:45.935: INFO: stderr: ""
Sep  4 15:27:45.935: INFO: stdout: "I0904 15:27:45.748426       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5xw 435\n"
Sep  4 15:27:45.935: INFO: got output "I0904 15:27:45.748426       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5xw 435\n"
STEP: limiting log bytes 09/04/23 15:27:45.935
Sep  4 15:27:45.935: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 logs logs-generator logs-generator --limit-bytes=1'
Sep  4 15:27:46.234: INFO: stderr: ""
Sep  4 15:27:46.234: INFO: stdout: "I"
Sep  4 15:27:46.234: INFO: got output "I"
STEP: exposing timestamps 09/04/23 15:27:46.234
Sep  4 15:27:46.234: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 logs logs-generator logs-generator --tail=1 --timestamps'
Sep  4 15:27:46.429: INFO: stderr: ""
Sep  4 15:27:46.429: INFO: stdout: "2023-09-04T15:27:46.348364903Z I0904 15:27:46.348276       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/4dzn 513\n"
Sep  4 15:27:46.429: INFO: got output "2023-09-04T15:27:46.348364903Z I0904 15:27:46.348276       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/4dzn 513\n"
STEP: restricting to a time range 09/04/23 15:27:46.429
Sep  4 15:27:48.930: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 logs logs-generator logs-generator --since=1s'
Sep  4 15:27:49.144: INFO: stderr: ""
Sep  4 15:27:49.144: INFO: stdout: "I0904 15:27:48.148786       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/kbzl 273\nI0904 15:27:48.349080       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/vrft 332\nI0904 15:27:48.548390       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/r7l 245\nI0904 15:27:48.748699       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/22f 299\nI0904 15:27:48.949030       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/k526 506\n"
Sep  4 15:27:49.144: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 logs logs-generator logs-generator --since=24h'
Sep  4 15:27:49.313: INFO: stderr: ""
Sep  4 15:27:49.313: INFO: stdout: "I0904 15:27:44.348069       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/5q88 338\nI0904 15:27:44.548153       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/wqd7 546\nI0904 15:27:44.749135       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/95k 583\nI0904 15:27:44.948423       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/mwg 256\nI0904 15:27:45.148752       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/bdkh 310\nI0904 15:27:45.348875       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/g99 404\nI0904 15:27:45.548134       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/z5t 255\nI0904 15:27:45.748426       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5xw 435\nI0904 15:27:45.948732       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/7vbm 329\nI0904 15:27:46.149031       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/h5j6 393\nI0904 15:27:46.348276       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/4dzn 513\nI0904 15:27:46.548597       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/m4v 508\nI0904 15:27:46.748919       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/ppj5 591\nI0904 15:27:46.948113       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/ktp 439\nI0904 15:27:47.148417       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/45w 383\nI0904 15:27:47.348726       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/qr82 382\nI0904 15:27:47.549022       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/68md 320\nI0904 15:27:47.748179       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/s5ht 566\nI0904 15:27:47.948474       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/ghm 203\nI0904 15:27:48.148786       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/kbzl 273\nI0904 15:27:48.349080       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/vrft 332\nI0904 15:27:48.548390       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/r7l 245\nI0904 15:27:48.748699       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/22f 299\nI0904 15:27:48.949030       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/k526 506\nI0904 15:27:49.148161       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/5dsv 521\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Sep  4 15:27:49.313: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 delete pod logs-generator'
Sep  4 15:27:50.266: INFO: stderr: ""
Sep  4 15:27:50.266: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 15:27:50.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-341" for this suite. 09/04/23 15:27:50.289
------------------------------
• [6.907 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:27:43.396
    Sep  4 15:27:43.396: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 15:27:43.397
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:27:43.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:27:43.461
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 09/04/23 15:27:43.484
    Sep  4 15:27:43.484: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Sep  4 15:27:43.612: INFO: stderr: ""
    Sep  4 15:27:43.612: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 09/04/23 15:27:43.612
    Sep  4 15:27:43.612: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Sep  4 15:27:43.612: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-341" to be "running and ready, or succeeded"
    Sep  4 15:27:43.625: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.870357ms
    Sep  4 15:27:43.625: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh' to be 'Running' but was 'Pending'
    Sep  4 15:27:45.639: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.026765383s
    Sep  4 15:27:45.639: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Sep  4 15:27:45.639: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 09/04/23 15:27:45.639
    Sep  4 15:27:45.639: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 logs logs-generator logs-generator'
    Sep  4 15:27:45.792: INFO: stderr: ""
    Sep  4 15:27:45.792: INFO: stdout: "I0904 15:27:44.348069       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/5q88 338\nI0904 15:27:44.548153       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/wqd7 546\nI0904 15:27:44.749135       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/95k 583\nI0904 15:27:44.948423       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/mwg 256\nI0904 15:27:45.148752       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/bdkh 310\nI0904 15:27:45.348875       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/g99 404\nI0904 15:27:45.548134       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/z5t 255\nI0904 15:27:45.748426       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5xw 435\n"
    STEP: limiting log lines 09/04/23 15:27:45.793
    Sep  4 15:27:45.793: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 logs logs-generator logs-generator --tail=1'
    Sep  4 15:27:45.935: INFO: stderr: ""
    Sep  4 15:27:45.935: INFO: stdout: "I0904 15:27:45.748426       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5xw 435\n"
    Sep  4 15:27:45.935: INFO: got output "I0904 15:27:45.748426       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5xw 435\n"
    STEP: limiting log bytes 09/04/23 15:27:45.935
    Sep  4 15:27:45.935: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 logs logs-generator logs-generator --limit-bytes=1'
    Sep  4 15:27:46.234: INFO: stderr: ""
    Sep  4 15:27:46.234: INFO: stdout: "I"
    Sep  4 15:27:46.234: INFO: got output "I"
    STEP: exposing timestamps 09/04/23 15:27:46.234
    Sep  4 15:27:46.234: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 logs logs-generator logs-generator --tail=1 --timestamps'
    Sep  4 15:27:46.429: INFO: stderr: ""
    Sep  4 15:27:46.429: INFO: stdout: "2023-09-04T15:27:46.348364903Z I0904 15:27:46.348276       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/4dzn 513\n"
    Sep  4 15:27:46.429: INFO: got output "2023-09-04T15:27:46.348364903Z I0904 15:27:46.348276       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/4dzn 513\n"
    STEP: restricting to a time range 09/04/23 15:27:46.429
    Sep  4 15:27:48.930: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 logs logs-generator logs-generator --since=1s'
    Sep  4 15:27:49.144: INFO: stderr: ""
    Sep  4 15:27:49.144: INFO: stdout: "I0904 15:27:48.148786       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/kbzl 273\nI0904 15:27:48.349080       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/vrft 332\nI0904 15:27:48.548390       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/r7l 245\nI0904 15:27:48.748699       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/22f 299\nI0904 15:27:48.949030       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/k526 506\n"
    Sep  4 15:27:49.144: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 logs logs-generator logs-generator --since=24h'
    Sep  4 15:27:49.313: INFO: stderr: ""
    Sep  4 15:27:49.313: INFO: stdout: "I0904 15:27:44.348069       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/5q88 338\nI0904 15:27:44.548153       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/wqd7 546\nI0904 15:27:44.749135       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/95k 583\nI0904 15:27:44.948423       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/mwg 256\nI0904 15:27:45.148752       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/bdkh 310\nI0904 15:27:45.348875       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/g99 404\nI0904 15:27:45.548134       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/z5t 255\nI0904 15:27:45.748426       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5xw 435\nI0904 15:27:45.948732       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/7vbm 329\nI0904 15:27:46.149031       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/h5j6 393\nI0904 15:27:46.348276       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/4dzn 513\nI0904 15:27:46.548597       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/m4v 508\nI0904 15:27:46.748919       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/ppj5 591\nI0904 15:27:46.948113       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/ktp 439\nI0904 15:27:47.148417       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/45w 383\nI0904 15:27:47.348726       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/qr82 382\nI0904 15:27:47.549022       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/68md 320\nI0904 15:27:47.748179       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/s5ht 566\nI0904 15:27:47.948474       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/ghm 203\nI0904 15:27:48.148786       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/kbzl 273\nI0904 15:27:48.349080       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/vrft 332\nI0904 15:27:48.548390       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/r7l 245\nI0904 15:27:48.748699       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/22f 299\nI0904 15:27:48.949030       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/k526 506\nI0904 15:27:49.148161       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/5dsv 521\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Sep  4 15:27:49.313: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-341 delete pod logs-generator'
    Sep  4 15:27:50.266: INFO: stderr: ""
    Sep  4 15:27:50.266: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:27:50.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-341" for this suite. 09/04/23 15:27:50.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:27:50.303
Sep  4 15:27:50.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 09/04/23 15:27:50.304
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:27:50.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:27:50.378
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 09/04/23 15:27:50.4
STEP: Ensuring no jobs are scheduled 09/04/23 15:27:50.415
------------------------------
Automatically polling progress:
  [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance] (Spec Runtime: 5m0.097s)
    test/e2e/apps/cronjob.go:96
    In [It] (Node Runtime: 5m0s)
      test/e2e/apps/cronjob.go:96
      At [By Step] Ensuring no jobs are scheduled (Step Runtime: 4m59.986s)
        test/e2e/apps/cronjob.go:105

      Spec Goroutine
      goroutine 16161 [select]
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.WaitForWithContext({0x817c768, 0xc0001a6000}, 0xc004eeab70, 0x306620a?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:660
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.poll({0x817c768, 0xc0001a6000}, 0xd8?, 0x3064da5?, 0x68c0340?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:596
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.PollWithContext({0x817c768, 0xc0001a6000}, 0x0?, 0xc000fade28?, 0x26724e7?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:460
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.Poll(0x0?, 0xc1359bbd98c33bde?, 0x26eaccce30e?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:445
      > k8s.io/kubernetes/test/e2e/apps.waitForNoJobs({0x81ba5a8?, 0xc004c824e0}, {0xc000adbf90, 0xc}, {0xc004e791b0, 0x9}, 0x0)
          test/e2e/apps/cronjob.go:607
      > k8s.io/kubernetes/test/e2e/apps.glob..func2.2()
          test/e2e/apps/cronjob.go:106
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0xc0045b7560, 0xc0041b0960})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
STEP: Ensuring no job exists by listing jobs explicitly 09/04/23 15:32:50.44
STEP: Removing cronjob 09/04/23 15:32:50.452
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  4 15:32:50.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1054" for this suite. 09/04/23 15:32:50.487
• [SLOW TEST] [300.197 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:27:50.303
    Sep  4 15:27:50.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 09/04/23 15:27:50.304
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:27:50.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:27:50.378
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 09/04/23 15:27:50.4
    STEP: Ensuring no jobs are scheduled 09/04/23 15:27:50.415
    STEP: Ensuring no job exists by listing jobs explicitly 09/04/23 15:32:50.44
    STEP: Removing cronjob 09/04/23 15:32:50.452
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:32:50.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1054" for this suite. 09/04/23 15:32:50.487
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:32:50.501
Sep  4 15:32:50.501: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test 09/04/23 15:32:50.502
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:32:50.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:32:50.56
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Sep  4 15:32:50.606: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e603eab1-da2d-4dd1-95ae-b56b5db16aae" in namespace "security-context-test-2278" to be "Succeeded or Failed"
Sep  4 15:32:50.618: INFO: Pod "busybox-readonly-false-e603eab1-da2d-4dd1-95ae-b56b5db16aae": Phase="Pending", Reason="", readiness=false. Elapsed: 11.5841ms
Sep  4 15:32:52.632: INFO: Pod "busybox-readonly-false-e603eab1-da2d-4dd1-95ae-b56b5db16aae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025596572s
Sep  4 15:32:54.632: INFO: Pod "busybox-readonly-false-e603eab1-da2d-4dd1-95ae-b56b5db16aae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025541641s
Sep  4 15:32:54.632: INFO: Pod "busybox-readonly-false-e603eab1-da2d-4dd1-95ae-b56b5db16aae" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  4 15:32:54.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2278" for this suite. 09/04/23 15:32:54.655
------------------------------
• [4.167 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:32:50.501
    Sep  4 15:32:50.501: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context-test 09/04/23 15:32:50.502
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:32:50.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:32:50.56
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Sep  4 15:32:50.606: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e603eab1-da2d-4dd1-95ae-b56b5db16aae" in namespace "security-context-test-2278" to be "Succeeded or Failed"
    Sep  4 15:32:50.618: INFO: Pod "busybox-readonly-false-e603eab1-da2d-4dd1-95ae-b56b5db16aae": Phase="Pending", Reason="", readiness=false. Elapsed: 11.5841ms
    Sep  4 15:32:52.632: INFO: Pod "busybox-readonly-false-e603eab1-da2d-4dd1-95ae-b56b5db16aae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025596572s
    Sep  4 15:32:54.632: INFO: Pod "busybox-readonly-false-e603eab1-da2d-4dd1-95ae-b56b5db16aae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025541641s
    Sep  4 15:32:54.632: INFO: Pod "busybox-readonly-false-e603eab1-da2d-4dd1-95ae-b56b5db16aae" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:32:54.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2278" for this suite. 09/04/23 15:32:54.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:32:54.669
Sep  4 15:32:54.669: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 15:32:54.67
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:32:54.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:32:54.728
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 15:32:54.778
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:32:55.164
STEP: Deploying the webhook pod 09/04/23 15:32:55.179
STEP: Wait for the deployment to be ready 09/04/23 15:32:55.205
Sep  4 15:32:55.240: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 32, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 32, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 32, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 32, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 15:32:57.255
STEP: Verifying the service has paired with the endpoint 09/04/23 15:32:57.272
Sep  4 15:32:58.272: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 09/04/23 15:32:58.414
STEP: Creating a configMap that should be mutated 09/04/23 15:32:58.651
STEP: Deleting the collection of validation webhooks 09/04/23 15:32:59.38
STEP: Creating a configMap that should not be mutated 09/04/23 15:32:59.423
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:32:59.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8897" for this suite. 09/04/23 15:32:59.525
STEP: Destroying namespace "webhook-8897-markers" for this suite. 09/04/23 15:32:59.538
------------------------------
• [4.882 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:32:54.669
    Sep  4 15:32:54.669: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 15:32:54.67
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:32:54.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:32:54.728
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 15:32:54.778
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:32:55.164
    STEP: Deploying the webhook pod 09/04/23 15:32:55.179
    STEP: Wait for the deployment to be ready 09/04/23 15:32:55.205
    Sep  4 15:32:55.240: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 32, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 32, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 32, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 32, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 15:32:57.255
    STEP: Verifying the service has paired with the endpoint 09/04/23 15:32:57.272
    Sep  4 15:32:58.272: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 09/04/23 15:32:58.414
    STEP: Creating a configMap that should be mutated 09/04/23 15:32:58.651
    STEP: Deleting the collection of validation webhooks 09/04/23 15:32:59.38
    STEP: Creating a configMap that should not be mutated 09/04/23 15:32:59.423
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:32:59.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8897" for this suite. 09/04/23 15:32:59.525
    STEP: Destroying namespace "webhook-8897-markers" for this suite. 09/04/23 15:32:59.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:32:59.553
Sep  4 15:32:59.553: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 09/04/23 15:32:59.554
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:32:59.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:32:59.611
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 09/04/23 15:32:59.632
STEP: Ensuring a job is scheduled 09/04/23 15:32:59.645
STEP: Ensuring exactly one is scheduled 09/04/23 15:33:01.658
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/04/23 15:33:01.671
STEP: Ensuring the job is replaced with a new one 09/04/23 15:33:01.683
STEP: Removing cronjob 09/04/23 15:34:01.696
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  4 15:34:01.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5057" for this suite. 09/04/23 15:34:01.732
------------------------------
• [62.193 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:32:59.553
    Sep  4 15:32:59.553: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 09/04/23 15:32:59.554
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:32:59.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:32:59.611
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 09/04/23 15:32:59.632
    STEP: Ensuring a job is scheduled 09/04/23 15:32:59.645
    STEP: Ensuring exactly one is scheduled 09/04/23 15:33:01.658
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/04/23 15:33:01.671
    STEP: Ensuring the job is replaced with a new one 09/04/23 15:33:01.683
    STEP: Removing cronjob 09/04/23 15:34:01.696
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:34:01.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5057" for this suite. 09/04/23 15:34:01.732
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:34:01.746
Sep  4 15:34:01.746: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 09/04/23 15:34:01.747
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:34:01.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:34:01.806
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 09/04/23 15:34:01.828
STEP: submitting the pod to kubernetes 09/04/23 15:34:01.828
Sep  4 15:34:01.846: INFO: Waiting up to 5m0s for pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112" in namespace "pods-3410" to be "running and ready"
Sep  4 15:34:01.859: INFO: Pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112": Phase="Pending", Reason="", readiness=false. Elapsed: 12.554755ms
Sep  4 15:34:01.859: INFO: The phase of Pod pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:34:03.871: INFO: Pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112": Phase="Running", Reason="", readiness=true. Elapsed: 2.0250012s
Sep  4 15:34:03.871: INFO: The phase of Pod pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112 is Running (Ready = true)
Sep  4 15:34:03.871: INFO: Pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/04/23 15:34:03.884
STEP: updating the pod 09/04/23 15:34:03.896
Sep  4 15:34:04.423: INFO: Successfully updated pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112"
Sep  4 15:34:04.423: INFO: Waiting up to 5m0s for pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112" in namespace "pods-3410" to be "running"
Sep  4 15:34:04.434: INFO: Pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112": Phase="Running", Reason="", readiness=true. Elapsed: 11.30801ms
Sep  4 15:34:04.435: INFO: Pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 09/04/23 15:34:04.435
Sep  4 15:34:04.446: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 15:34:04.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3410" for this suite. 09/04/23 15:34:04.468
------------------------------
• [2.735 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:34:01.746
    Sep  4 15:34:01.746: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 09/04/23 15:34:01.747
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:34:01.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:34:01.806
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 09/04/23 15:34:01.828
    STEP: submitting the pod to kubernetes 09/04/23 15:34:01.828
    Sep  4 15:34:01.846: INFO: Waiting up to 5m0s for pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112" in namespace "pods-3410" to be "running and ready"
    Sep  4 15:34:01.859: INFO: Pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112": Phase="Pending", Reason="", readiness=false. Elapsed: 12.554755ms
    Sep  4 15:34:01.859: INFO: The phase of Pod pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:34:03.871: INFO: Pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112": Phase="Running", Reason="", readiness=true. Elapsed: 2.0250012s
    Sep  4 15:34:03.871: INFO: The phase of Pod pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112 is Running (Ready = true)
    Sep  4 15:34:03.871: INFO: Pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/04/23 15:34:03.884
    STEP: updating the pod 09/04/23 15:34:03.896
    Sep  4 15:34:04.423: INFO: Successfully updated pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112"
    Sep  4 15:34:04.423: INFO: Waiting up to 5m0s for pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112" in namespace "pods-3410" to be "running"
    Sep  4 15:34:04.434: INFO: Pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112": Phase="Running", Reason="", readiness=true. Elapsed: 11.30801ms
    Sep  4 15:34:04.435: INFO: Pod "pod-update-c2ba41b6-e9bd-4251-97c5-36da09f06112" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 09/04/23 15:34:04.435
    Sep  4 15:34:04.446: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:34:04.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3410" for this suite. 09/04/23 15:34:04.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:34:04.481
Sep  4 15:34:04.481: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 09/04/23 15:34:04.482
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:34:04.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:34:04.54
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Sep  4 15:34:04.562: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod 09/04/23 15:34:04.562
STEP: submitting the pod to kubernetes 09/04/23 15:34:04.562
Sep  4 15:34:04.580: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5cd2b18c-6e82-40c5-9f74-bf9b2131a377" in namespace "pods-1739" to be "running and ready"
Sep  4 15:34:04.592: INFO: Pod "pod-logs-websocket-5cd2b18c-6e82-40c5-9f74-bf9b2131a377": Phase="Pending", Reason="", readiness=false. Elapsed: 11.464984ms
Sep  4 15:34:04.592: INFO: The phase of Pod pod-logs-websocket-5cd2b18c-6e82-40c5-9f74-bf9b2131a377 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:34:06.605: INFO: Pod "pod-logs-websocket-5cd2b18c-6e82-40c5-9f74-bf9b2131a377": Phase="Running", Reason="", readiness=true. Elapsed: 2.024826111s
Sep  4 15:34:06.605: INFO: The phase of Pod pod-logs-websocket-5cd2b18c-6e82-40c5-9f74-bf9b2131a377 is Running (Ready = true)
Sep  4 15:34:06.605: INFO: Pod "pod-logs-websocket-5cd2b18c-6e82-40c5-9f74-bf9b2131a377" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 15:34:06.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1739" for this suite. 09/04/23 15:34:06.719
------------------------------
• [2.251 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:34:04.481
    Sep  4 15:34:04.481: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 09/04/23 15:34:04.482
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:34:04.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:34:04.54
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Sep  4 15:34:04.562: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: creating the pod 09/04/23 15:34:04.562
    STEP: submitting the pod to kubernetes 09/04/23 15:34:04.562
    Sep  4 15:34:04.580: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5cd2b18c-6e82-40c5-9f74-bf9b2131a377" in namespace "pods-1739" to be "running and ready"
    Sep  4 15:34:04.592: INFO: Pod "pod-logs-websocket-5cd2b18c-6e82-40c5-9f74-bf9b2131a377": Phase="Pending", Reason="", readiness=false. Elapsed: 11.464984ms
    Sep  4 15:34:04.592: INFO: The phase of Pod pod-logs-websocket-5cd2b18c-6e82-40c5-9f74-bf9b2131a377 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:34:06.605: INFO: Pod "pod-logs-websocket-5cd2b18c-6e82-40c5-9f74-bf9b2131a377": Phase="Running", Reason="", readiness=true. Elapsed: 2.024826111s
    Sep  4 15:34:06.605: INFO: The phase of Pod pod-logs-websocket-5cd2b18c-6e82-40c5-9f74-bf9b2131a377 is Running (Ready = true)
    Sep  4 15:34:06.605: INFO: Pod "pod-logs-websocket-5cd2b18c-6e82-40c5-9f74-bf9b2131a377" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:34:06.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1739" for this suite. 09/04/23 15:34:06.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:34:06.733
Sep  4 15:34:06.733: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 15:34:06.734
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:34:06.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:34:06.792
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Sep  4 15:34:06.814: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/04/23 15:34:08.405
Sep  4 15:34:08.406: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3906 --namespace=crd-publish-openapi-3906 create -f -'
Sep  4 15:34:09.149: INFO: stderr: ""
Sep  4 15:34:09.149: INFO: stdout: "e2e-test-crd-publish-openapi-6028-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  4 15:34:09.149: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3906 --namespace=crd-publish-openapi-3906 delete e2e-test-crd-publish-openapi-6028-crds test-cr'
Sep  4 15:34:09.289: INFO: stderr: ""
Sep  4 15:34:09.289: INFO: stdout: "e2e-test-crd-publish-openapi-6028-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep  4 15:34:09.289: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3906 --namespace=crd-publish-openapi-3906 apply -f -'
Sep  4 15:34:09.522: INFO: stderr: ""
Sep  4 15:34:09.522: INFO: stdout: "e2e-test-crd-publish-openapi-6028-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  4 15:34:09.522: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3906 --namespace=crd-publish-openapi-3906 delete e2e-test-crd-publish-openapi-6028-crds test-cr'
Sep  4 15:34:09.630: INFO: stderr: ""
Sep  4 15:34:09.630: INFO: stdout: "e2e-test-crd-publish-openapi-6028-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/04/23 15:34:09.63
Sep  4 15:34:09.630: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3906 explain e2e-test-crd-publish-openapi-6028-crds'
Sep  4 15:34:10.261: INFO: stderr: ""
Sep  4 15:34:10.261: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6028-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:34:12.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3906" for this suite. 09/04/23 15:34:12.463
------------------------------
• [5.745 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:34:06.733
    Sep  4 15:34:06.733: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 15:34:06.734
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:34:06.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:34:06.792
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Sep  4 15:34:06.814: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/04/23 15:34:08.405
    Sep  4 15:34:08.406: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3906 --namespace=crd-publish-openapi-3906 create -f -'
    Sep  4 15:34:09.149: INFO: stderr: ""
    Sep  4 15:34:09.149: INFO: stdout: "e2e-test-crd-publish-openapi-6028-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep  4 15:34:09.149: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3906 --namespace=crd-publish-openapi-3906 delete e2e-test-crd-publish-openapi-6028-crds test-cr'
    Sep  4 15:34:09.289: INFO: stderr: ""
    Sep  4 15:34:09.289: INFO: stdout: "e2e-test-crd-publish-openapi-6028-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Sep  4 15:34:09.289: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3906 --namespace=crd-publish-openapi-3906 apply -f -'
    Sep  4 15:34:09.522: INFO: stderr: ""
    Sep  4 15:34:09.522: INFO: stdout: "e2e-test-crd-publish-openapi-6028-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep  4 15:34:09.522: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3906 --namespace=crd-publish-openapi-3906 delete e2e-test-crd-publish-openapi-6028-crds test-cr'
    Sep  4 15:34:09.630: INFO: stderr: ""
    Sep  4 15:34:09.630: INFO: stdout: "e2e-test-crd-publish-openapi-6028-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/04/23 15:34:09.63
    Sep  4 15:34:09.630: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3906 explain e2e-test-crd-publish-openapi-6028-crds'
    Sep  4 15:34:10.261: INFO: stderr: ""
    Sep  4 15:34:10.261: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6028-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:34:12.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3906" for this suite. 09/04/23 15:34:12.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:34:12.479
Sep  4 15:34:12.479: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 09/04/23 15:34:12.48
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:34:12.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:34:12.546
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-035a87f4-4ac5-487d-b65b-d50dd0da1c81 09/04/23 15:34:12.625
STEP: Creating a pod to test consume secrets 09/04/23 15:34:12.639
Sep  4 15:34:12.659: INFO: Waiting up to 5m0s for pod "pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593" in namespace "secrets-2962" to be "Succeeded or Failed"
Sep  4 15:34:12.672: INFO: Pod "pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593": Phase="Pending", Reason="", readiness=false. Elapsed: 13.494049ms
Sep  4 15:34:14.686: INFO: Pod "pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027332584s
Sep  4 15:34:16.688: INFO: Pod "pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028805671s
STEP: Saw pod success 09/04/23 15:34:16.688
Sep  4 15:34:16.688: INFO: Pod "pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593" satisfied condition "Succeeded or Failed"
Sep  4 15:34:16.702: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593 container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 15:34:16.738
Sep  4 15:34:16.755: INFO: Waiting for pod pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593 to disappear
Sep  4 15:34:16.768: INFO: Pod pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 15:34:16.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2962" for this suite. 09/04/23 15:34:16.794
STEP: Destroying namespace "secret-namespace-4955" for this suite. 09/04/23 15:34:16.809
------------------------------
• [4.345 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:34:12.479
    Sep  4 15:34:12.479: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 09/04/23 15:34:12.48
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:34:12.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:34:12.546
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-035a87f4-4ac5-487d-b65b-d50dd0da1c81 09/04/23 15:34:12.625
    STEP: Creating a pod to test consume secrets 09/04/23 15:34:12.639
    Sep  4 15:34:12.659: INFO: Waiting up to 5m0s for pod "pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593" in namespace "secrets-2962" to be "Succeeded or Failed"
    Sep  4 15:34:12.672: INFO: Pod "pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593": Phase="Pending", Reason="", readiness=false. Elapsed: 13.494049ms
    Sep  4 15:34:14.686: INFO: Pod "pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027332584s
    Sep  4 15:34:16.688: INFO: Pod "pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028805671s
    STEP: Saw pod success 09/04/23 15:34:16.688
    Sep  4 15:34:16.688: INFO: Pod "pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593" satisfied condition "Succeeded or Failed"
    Sep  4 15:34:16.702: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593 container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 15:34:16.738
    Sep  4 15:34:16.755: INFO: Waiting for pod pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593 to disappear
    Sep  4 15:34:16.768: INFO: Pod pod-secrets-b3b41dff-3275-4193-ab06-05d99942a593 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:34:16.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2962" for this suite. 09/04/23 15:34:16.794
    STEP: Destroying namespace "secret-namespace-4955" for this suite. 09/04/23 15:34:16.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:34:16.826
Sep  4 15:34:16.826: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 09/04/23 15:34:16.827
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:34:16.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:34:16.896
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-033ff512-3c16-476c-a88a-ac02f606f8d8 09/04/23 15:34:16.921
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 15:34:16.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6007" for this suite. 09/04/23 15:34:16.948
------------------------------
• [0.137 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:34:16.826
    Sep  4 15:34:16.826: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 09/04/23 15:34:16.827
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:34:16.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:34:16.896
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-033ff512-3c16-476c-a88a-ac02f606f8d8 09/04/23 15:34:16.921
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:34:16.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6007" for this suite. 09/04/23 15:34:16.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:34:16.964
Sep  4 15:34:16.964: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 09/04/23 15:34:16.965
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:34:17.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:34:17.029
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf in namespace container-probe-2921 09/04/23 15:34:17.054
Sep  4 15:34:17.076: INFO: Waiting up to 5m0s for pod "busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf" in namespace "container-probe-2921" to be "not pending"
Sep  4 15:34:17.090: INFO: Pod "busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.709305ms
Sep  4 15:34:19.105: INFO: Pod "busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf": Phase="Running", Reason="", readiness=true. Elapsed: 2.028816936s
Sep  4 15:34:19.105: INFO: Pod "busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf" satisfied condition "not pending"
Sep  4 15:34:19.105: INFO: Started pod busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf in namespace container-probe-2921
STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 15:34:19.105
Sep  4 15:34:19.119: INFO: Initial restart count of pod busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf is 0
Sep  4 15:35:09.530: INFO: Restart count of pod container-probe-2921/busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf is now 1 (50.411204502s elapsed)
STEP: deleting the pod 09/04/23 15:35:09.53
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 15:35:09.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2921" for this suite. 09/04/23 15:35:09.574
------------------------------
• [52.627 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:34:16.964
    Sep  4 15:34:16.964: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 09/04/23 15:34:16.965
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:34:17.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:34:17.029
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf in namespace container-probe-2921 09/04/23 15:34:17.054
    Sep  4 15:34:17.076: INFO: Waiting up to 5m0s for pod "busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf" in namespace "container-probe-2921" to be "not pending"
    Sep  4 15:34:17.090: INFO: Pod "busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.709305ms
    Sep  4 15:34:19.105: INFO: Pod "busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf": Phase="Running", Reason="", readiness=true. Elapsed: 2.028816936s
    Sep  4 15:34:19.105: INFO: Pod "busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf" satisfied condition "not pending"
    Sep  4 15:34:19.105: INFO: Started pod busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf in namespace container-probe-2921
    STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 15:34:19.105
    Sep  4 15:34:19.119: INFO: Initial restart count of pod busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf is 0
    Sep  4 15:35:09.530: INFO: Restart count of pod container-probe-2921/busybox-3fc91e03-adc8-459f-81cb-5bed894f1acf is now 1 (50.411204502s elapsed)
    STEP: deleting the pod 09/04/23 15:35:09.53
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:35:09.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2921" for this suite. 09/04/23 15:35:09.574
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:35:09.591
Sep  4 15:35:09.591: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 09/04/23 15:35:09.592
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:09.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:09.661
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Sep  4 15:35:09.688: INFO: Creating deployment "test-recreate-deployment"
Sep  4 15:35:09.703: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  4 15:35:09.731: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  4 15:35:09.745: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 35, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 35, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 35, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 35, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 15:35:11.760: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  4 15:35:11.791: INFO: Updating deployment test-recreate-deployment
Sep  4 15:35:11.791: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 15:35:11.842: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4036  a51d4e70-ab0a-4f2e-9b6b-6d673b1ae29b 29338 2 2023-09-04 15:35:09 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059e2c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-04 15:35:11 +0000 UTC,LastTransitionTime:2023-09-04 15:35:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-09-04 15:35:11 +0000 UTC,LastTransitionTime:2023-09-04 15:35:09 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep  4 15:35:11.857: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-4036  db65b8f9-aca2-4166-a76e-4d90084b26f3 29337 1 2023-09-04 15:35:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment a51d4e70-ab0a-4f2e-9b6b-6d673b1ae29b 0xc0042134e0 0xc0042134e1}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a51d4e70-ab0a-4f2e-9b6b-6d673b1ae29b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004213578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  4 15:35:11.857: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  4 15:35:11.857: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-4036  c0c1bc2b-bf1f-48e6-8bf4-9f8d52b36318 29330 2 2023-09-04 15:35:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment a51d4e70-ab0a-4f2e-9b6b-6d673b1ae29b 0xc0042133c7 0xc0042133c8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a51d4e70-ab0a-4f2e-9b6b-6d673b1ae29b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004213478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  4 15:35:11.872: INFO: Pod "test-recreate-deployment-cff6dc657-pq6nb" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-pq6nb test-recreate-deployment-cff6dc657- deployment-4036  e5f861a2-eeb7-4b7a-a411-b7b7a6aee5c5 29339 0 2023-09-04 15:35:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 db65b8f9-aca2-4166-a76e-4d90084b26f3 0xc002c9a440 0xc002c9a441}] [] [{kube-controller-manager Update v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"db65b8f9-aca2-4166-a76e-4d90084b26f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rrjd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rrjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:,StartTime:2023-09-04 15:35:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 15:35:11.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4036" for this suite. 09/04/23 15:35:11.898
------------------------------
• [2.324 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:35:09.591
    Sep  4 15:35:09.591: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 09/04/23 15:35:09.592
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:09.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:09.661
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Sep  4 15:35:09.688: INFO: Creating deployment "test-recreate-deployment"
    Sep  4 15:35:09.703: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Sep  4 15:35:09.731: INFO: Waiting deployment "test-recreate-deployment" to complete
    Sep  4 15:35:09.745: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 35, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 35, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 35, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 35, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 15:35:11.760: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Sep  4 15:35:11.791: INFO: Updating deployment test-recreate-deployment
    Sep  4 15:35:11.791: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 15:35:11.842: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-4036  a51d4e70-ab0a-4f2e-9b6b-6d673b1ae29b 29338 2 2023-09-04 15:35:09 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059e2c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-04 15:35:11 +0000 UTC,LastTransitionTime:2023-09-04 15:35:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-09-04 15:35:11 +0000 UTC,LastTransitionTime:2023-09-04 15:35:09 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Sep  4 15:35:11.857: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-4036  db65b8f9-aca2-4166-a76e-4d90084b26f3 29337 1 2023-09-04 15:35:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment a51d4e70-ab0a-4f2e-9b6b-6d673b1ae29b 0xc0042134e0 0xc0042134e1}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a51d4e70-ab0a-4f2e-9b6b-6d673b1ae29b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004213578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 15:35:11.857: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Sep  4 15:35:11.857: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-4036  c0c1bc2b-bf1f-48e6-8bf4-9f8d52b36318 29330 2 2023-09-04 15:35:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment a51d4e70-ab0a-4f2e-9b6b-6d673b1ae29b 0xc0042133c7 0xc0042133c8}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a51d4e70-ab0a-4f2e-9b6b-6d673b1ae29b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004213478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 15:35:11.872: INFO: Pod "test-recreate-deployment-cff6dc657-pq6nb" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-pq6nb test-recreate-deployment-cff6dc657- deployment-4036  e5f861a2-eeb7-4b7a-a411-b7b7a6aee5c5 29339 0 2023-09-04 15:35:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 db65b8f9-aca2-4166-a76e-4d90084b26f3 0xc002c9a440 0xc002c9a441}] [] [{kube-controller-manager Update v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"db65b8f9-aca2-4166-a76e-4d90084b26f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:35:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rrjd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rrjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:,StartTime:2023-09-04 15:35:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:35:11.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4036" for this suite. 09/04/23 15:35:11.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:35:11.915
Sep  4 15:35:11.915: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 15:35:11.916
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:11.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:11.997
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 09/04/23 15:35:12.024
Sep  4 15:35:12.045: INFO: Waiting up to 5m0s for pod "pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c" in namespace "emptydir-4076" to be "Succeeded or Failed"
Sep  4 15:35:12.059: INFO: Pod "pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.818294ms
Sep  4 15:35:14.075: INFO: Pod "pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029434924s
Sep  4 15:35:16.076: INFO: Pod "pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030490622s
Sep  4 15:35:18.076: INFO: Pod "pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030712205s
STEP: Saw pod success 09/04/23 15:35:18.076
Sep  4 15:35:18.076: INFO: Pod "pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c" satisfied condition "Succeeded or Failed"
Sep  4 15:35:18.090: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c container test-container: <nil>
STEP: delete the pod 09/04/23 15:35:18.126
Sep  4 15:35:18.144: INFO: Waiting for pod pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c to disappear
Sep  4 15:35:18.158: INFO: Pod pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 15:35:18.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4076" for this suite. 09/04/23 15:35:18.185
------------------------------
• [6.285 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:35:11.915
    Sep  4 15:35:11.915: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 15:35:11.916
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:11.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:11.997
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/04/23 15:35:12.024
    Sep  4 15:35:12.045: INFO: Waiting up to 5m0s for pod "pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c" in namespace "emptydir-4076" to be "Succeeded or Failed"
    Sep  4 15:35:12.059: INFO: Pod "pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.818294ms
    Sep  4 15:35:14.075: INFO: Pod "pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029434924s
    Sep  4 15:35:16.076: INFO: Pod "pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030490622s
    Sep  4 15:35:18.076: INFO: Pod "pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030712205s
    STEP: Saw pod success 09/04/23 15:35:18.076
    Sep  4 15:35:18.076: INFO: Pod "pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c" satisfied condition "Succeeded or Failed"
    Sep  4 15:35:18.090: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c container test-container: <nil>
    STEP: delete the pod 09/04/23 15:35:18.126
    Sep  4 15:35:18.144: INFO: Waiting for pod pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c to disappear
    Sep  4 15:35:18.158: INFO: Pod pod-f68891b9-8423-40c0-9367-0f8bd83e6f9c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:35:18.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4076" for this suite. 09/04/23 15:35:18.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:35:18.2
Sep  4 15:35:18.200: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 09/04/23 15:35:18.201
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:18.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:18.271
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 09/04/23 15:35:18.312
Sep  4 15:35:18.313: INFO: Creating simple deployment test-deployment-z49pz
Sep  4 15:35:18.371: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 35, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 35, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 35, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 35, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-z49pz-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 09/04/23 15:35:20.401
Sep  4 15:35:20.416: INFO: Deployment test-deployment-z49pz has Conditions: [{Available True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z49pz-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 09/04/23 15:35:20.416
Sep  4 15:35:20.446: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 35, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 35, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 35, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 35, 18, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-z49pz-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 09/04/23 15:35:20.446
Sep  4 15:35:20.460: INFO: Observed &Deployment event: ADDED
Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-z49pz-54bc444df"}
Sep  4 15:35:20.460: INFO: Observed &Deployment event: MODIFIED
Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-z49pz-54bc444df"}
Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  4 15:35:20.460: INFO: Observed &Deployment event: MODIFIED
Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-z49pz-54bc444df" is progressing.}
Sep  4 15:35:20.460: INFO: Observed &Deployment event: MODIFIED
Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z49pz-54bc444df" has successfully progressed.}
Sep  4 15:35:20.460: INFO: Observed &Deployment event: MODIFIED
Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z49pz-54bc444df" has successfully progressed.}
Sep  4 15:35:20.460: INFO: Found Deployment test-deployment-z49pz in namespace deployment-546 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  4 15:35:20.460: INFO: Deployment test-deployment-z49pz has an updated status
STEP: patching the Statefulset Status 09/04/23 15:35:20.46
Sep  4 15:35:20.460: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  4 15:35:20.476: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 09/04/23 15:35:20.476
Sep  4 15:35:20.489: INFO: Observed &Deployment event: ADDED
Sep  4 15:35:20.489: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-z49pz-54bc444df"}
Sep  4 15:35:20.490: INFO: Observed &Deployment event: MODIFIED
Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-z49pz-54bc444df"}
Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  4 15:35:20.490: INFO: Observed &Deployment event: MODIFIED
Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-z49pz-54bc444df" is progressing.}
Sep  4 15:35:20.490: INFO: Observed &Deployment event: MODIFIED
Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z49pz-54bc444df" has successfully progressed.}
Sep  4 15:35:20.490: INFO: Observed &Deployment event: MODIFIED
Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z49pz-54bc444df" has successfully progressed.}
Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  4 15:35:20.490: INFO: Observed &Deployment event: MODIFIED
Sep  4 15:35:20.490: INFO: Found deployment test-deployment-z49pz in namespace deployment-546 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Sep  4 15:35:20.490: INFO: Deployment test-deployment-z49pz has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 15:35:20.505: INFO: Deployment "test-deployment-z49pz":
&Deployment{ObjectMeta:{test-deployment-z49pz  deployment-546  9627d847-7ecc-47e0-8652-a2521442a418 29434 1 2023-09-04 15:35:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-04 15:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-09-04 15:35:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-09-04 15:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046dd158 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-04 15:35:20 +0000 UTC,LastTransitionTime:2023-09-04 15:35:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-deployment-z49pz-54bc444df" has successfully progressed.,LastUpdateTime:2023-09-04 15:35:20 +0000 UTC,LastTransitionTime:2023-09-04 15:35:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  4 15:35:20.519: INFO: New ReplicaSet "test-deployment-z49pz-54bc444df" of Deployment "test-deployment-z49pz":
&ReplicaSet{ObjectMeta:{test-deployment-z49pz-54bc444df  deployment-546  00b574f7-140b-45b5-965a-b316a7eceaf3 29423 1 2023-09-04 15:35:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-z49pz 9627d847-7ecc-47e0-8652-a2521442a418 0xc005278510 0xc005278511}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9627d847-7ecc-47e0-8652-a2521442a418\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:35:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052785b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  4 15:35:20.533: INFO: Pod "test-deployment-z49pz-54bc444df-6nl26" is available:
&Pod{ObjectMeta:{test-deployment-z49pz-54bc444df-6nl26 test-deployment-z49pz-54bc444df- deployment-546  324a321a-d3ea-4888-9c8d-168408d5a4c8 29422 0 2023-09-04 15:35:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:16280f2e9cb50faa7c2d0c8fade3f4bdcbbf9f65f942e3583bb26363612ffc26 cni.projectcalico.org/podIP:100.64.1.47/32 cni.projectcalico.org/podIPs:100.64.1.47/32] [{apps/v1 ReplicaSet test-deployment-z49pz-54bc444df 00b574f7-140b-45b5-965a-b316a7eceaf3 0xc004310670 0xc004310671}] [] [{calico Update v1 2023-09-04 15:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-04 15:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00b574f7-140b-45b5-965a-b316a7eceaf3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:35:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lv4df,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lv4df,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.47,StartTime:2023-09-04 15:35:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:35:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://770f1f3961f6d86cd9a6c8f5a5529d653191e835972f021a8f8ceba10bf565ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 15:35:20.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-546" for this suite. 09/04/23 15:35:20.549
------------------------------
• [2.364 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:35:18.2
    Sep  4 15:35:18.200: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 09/04/23 15:35:18.201
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:18.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:18.271
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 09/04/23 15:35:18.312
    Sep  4 15:35:18.313: INFO: Creating simple deployment test-deployment-z49pz
    Sep  4 15:35:18.371: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 35, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 35, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 35, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 35, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-z49pz-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 09/04/23 15:35:20.401
    Sep  4 15:35:20.416: INFO: Deployment test-deployment-z49pz has Conditions: [{Available True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z49pz-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 09/04/23 15:35:20.416
    Sep  4 15:35:20.446: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 35, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 35, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 35, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 35, 18, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-z49pz-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 09/04/23 15:35:20.446
    Sep  4 15:35:20.460: INFO: Observed &Deployment event: ADDED
    Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-z49pz-54bc444df"}
    Sep  4 15:35:20.460: INFO: Observed &Deployment event: MODIFIED
    Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-z49pz-54bc444df"}
    Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  4 15:35:20.460: INFO: Observed &Deployment event: MODIFIED
    Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-z49pz-54bc444df" is progressing.}
    Sep  4 15:35:20.460: INFO: Observed &Deployment event: MODIFIED
    Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z49pz-54bc444df" has successfully progressed.}
    Sep  4 15:35:20.460: INFO: Observed &Deployment event: MODIFIED
    Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  4 15:35:20.460: INFO: Observed Deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z49pz-54bc444df" has successfully progressed.}
    Sep  4 15:35:20.460: INFO: Found Deployment test-deployment-z49pz in namespace deployment-546 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  4 15:35:20.460: INFO: Deployment test-deployment-z49pz has an updated status
    STEP: patching the Statefulset Status 09/04/23 15:35:20.46
    Sep  4 15:35:20.460: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  4 15:35:20.476: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 09/04/23 15:35:20.476
    Sep  4 15:35:20.489: INFO: Observed &Deployment event: ADDED
    Sep  4 15:35:20.489: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-z49pz-54bc444df"}
    Sep  4 15:35:20.490: INFO: Observed &Deployment event: MODIFIED
    Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-z49pz-54bc444df"}
    Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  4 15:35:20.490: INFO: Observed &Deployment event: MODIFIED
    Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:18 +0000 UTC 2023-09-04 15:35:18 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-z49pz-54bc444df" is progressing.}
    Sep  4 15:35:20.490: INFO: Observed &Deployment event: MODIFIED
    Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z49pz-54bc444df" has successfully progressed.}
    Sep  4 15:35:20.490: INFO: Observed &Deployment event: MODIFIED
    Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-04 15:35:19 +0000 UTC 2023-09-04 15:35:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z49pz-54bc444df" has successfully progressed.}
    Sep  4 15:35:20.490: INFO: Observed deployment test-deployment-z49pz in namespace deployment-546 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  4 15:35:20.490: INFO: Observed &Deployment event: MODIFIED
    Sep  4 15:35:20.490: INFO: Found deployment test-deployment-z49pz in namespace deployment-546 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Sep  4 15:35:20.490: INFO: Deployment test-deployment-z49pz has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 15:35:20.505: INFO: Deployment "test-deployment-z49pz":
    &Deployment{ObjectMeta:{test-deployment-z49pz  deployment-546  9627d847-7ecc-47e0-8652-a2521442a418 29434 1 2023-09-04 15:35:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-04 15:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-09-04 15:35:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-09-04 15:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046dd158 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-04 15:35:20 +0000 UTC,LastTransitionTime:2023-09-04 15:35:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-deployment-z49pz-54bc444df" has successfully progressed.,LastUpdateTime:2023-09-04 15:35:20 +0000 UTC,LastTransitionTime:2023-09-04 15:35:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  4 15:35:20.519: INFO: New ReplicaSet "test-deployment-z49pz-54bc444df" of Deployment "test-deployment-z49pz":
    &ReplicaSet{ObjectMeta:{test-deployment-z49pz-54bc444df  deployment-546  00b574f7-140b-45b5-965a-b316a7eceaf3 29423 1 2023-09-04 15:35:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-z49pz 9627d847-7ecc-47e0-8652-a2521442a418 0xc005278510 0xc005278511}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9627d847-7ecc-47e0-8652-a2521442a418\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:35:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052785b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 15:35:20.533: INFO: Pod "test-deployment-z49pz-54bc444df-6nl26" is available:
    &Pod{ObjectMeta:{test-deployment-z49pz-54bc444df-6nl26 test-deployment-z49pz-54bc444df- deployment-546  324a321a-d3ea-4888-9c8d-168408d5a4c8 29422 0 2023-09-04 15:35:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:16280f2e9cb50faa7c2d0c8fade3f4bdcbbf9f65f942e3583bb26363612ffc26 cni.projectcalico.org/podIP:100.64.1.47/32 cni.projectcalico.org/podIPs:100.64.1.47/32] [{apps/v1 ReplicaSet test-deployment-z49pz-54bc444df 00b574f7-140b-45b5-965a-b316a7eceaf3 0xc004310670 0xc004310671}] [] [{calico Update v1 2023-09-04 15:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-04 15:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00b574f7-140b-45b5-965a-b316a7eceaf3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:35:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lv4df,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lv4df,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:35:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.47,StartTime:2023-09-04 15:35:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:35:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://770f1f3961f6d86cd9a6c8f5a5529d653191e835972f021a8f8ceba10bf565ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:35:20.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-546" for this suite. 09/04/23 15:35:20.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:35:20.565
Sep  4 15:35:20.565: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 09/04/23 15:35:20.566
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:20.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:20.634
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
STEP: Creating simple DaemonSet "daemon-set" 09/04/23 15:35:20.718
STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 15:35:20.734
Sep  4 15:35:20.763: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:35:20.763: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:35:21.805: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 15:35:21.805: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh is running 0 daemon pod, expected 1
Sep  4 15:35:22.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 15:35:22.806: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 09/04/23 15:35:22.82
STEP: DeleteCollection of the DaemonSets 09/04/23 15:35:22.835
STEP: Verify that ReplicaSets have been deleted 09/04/23 15:35:22.851
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
Sep  4 15:35:22.893: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29470"},"items":null}

Sep  4 15:35:22.908: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29471"},"items":[{"metadata":{"name":"daemon-set-cp8ds","generateName":"daemon-set-","namespace":"daemonsets-2988","uid":"89a2ce4b-1725-48f0-9bcf-83d0bf2019e3","resourceVersion":"29459","creationTimestamp":"2023-09-04T15:35:20Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d38ed3fe0e6a1f52b280fa35729b9c8e07d5eaa2ca27d82de143c20f63f05864","cni.projectcalico.org/podIP":"100.64.1.48/32","cni.projectcalico.org/podIPs":"100.64.1.48/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d9c15c01-0cda-404c-8549-552ec3652e10","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-04T15:35:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9c15c01-0cda-404c-8549-552ec3652e10\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-04T15:35:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-04T15:35:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tssvn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tmud5-dd2.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tssvn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:20Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:22Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:22Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:20Z"}],"hostIP":"10.250.1.231","podIP":"100.64.1.48","podIPs":[{"ip":"100.64.1.48"}],"startTime":"2023-09-04T15:35:20Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-04T15:35:21Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4534a4732a79bfb81a00bdb6613ed8baff59f6cd2ec8fc9a2e2c98efd751f464","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qm5jj","generateName":"daemon-set-","namespace":"daemonsets-2988","uid":"0802f7d6-28da-4cd8-b845-fb39fb4d5111","resourceVersion":"29455","creationTimestamp":"2023-09-04T15:35:20Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"1105db090aa19309fc3e12f1abf434cd621905a87469fc762f6f20f5263dc274","cni.projectcalico.org/podIP":"100.64.0.168/32","cni.projectcalico.org/podIPs":"100.64.0.168/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d9c15c01-0cda-404c-8549-552ec3652e10","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-04T15:35:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9c15c01-0cda-404c-8549-552ec3652e10\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-04T15:35:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-04T15:35:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ndgh2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tmud5-dd2.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ndgh2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:20Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:21Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:21Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:20Z"}],"hostIP":"10.250.1.105","podIP":"100.64.0.168","podIPs":[{"ip":"100.64.0.168"}],"startTime":"2023-09-04T15:35:20Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-04T15:35:21Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a921b6e0b8fe9728c1f2d051a8f919377cf924352d2fd61ba3c6c0315e84e54e","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:35:22.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2988" for this suite. 09/04/23 15:35:22.966
------------------------------
• [2.417 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:35:20.565
    Sep  4 15:35:20.565: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 09/04/23 15:35:20.566
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:20.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:20.634
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:834
    STEP: Creating simple DaemonSet "daemon-set" 09/04/23 15:35:20.718
    STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 15:35:20.734
    Sep  4 15:35:20.763: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:35:20.763: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:35:21.805: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 15:35:21.805: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh is running 0 daemon pod, expected 1
    Sep  4 15:35:22.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 15:35:22.806: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 09/04/23 15:35:22.82
    STEP: DeleteCollection of the DaemonSets 09/04/23 15:35:22.835
    STEP: Verify that ReplicaSets have been deleted 09/04/23 15:35:22.851
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    Sep  4 15:35:22.893: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29470"},"items":null}

    Sep  4 15:35:22.908: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29471"},"items":[{"metadata":{"name":"daemon-set-cp8ds","generateName":"daemon-set-","namespace":"daemonsets-2988","uid":"89a2ce4b-1725-48f0-9bcf-83d0bf2019e3","resourceVersion":"29459","creationTimestamp":"2023-09-04T15:35:20Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d38ed3fe0e6a1f52b280fa35729b9c8e07d5eaa2ca27d82de143c20f63f05864","cni.projectcalico.org/podIP":"100.64.1.48/32","cni.projectcalico.org/podIPs":"100.64.1.48/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d9c15c01-0cda-404c-8549-552ec3652e10","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-04T15:35:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9c15c01-0cda-404c-8549-552ec3652e10\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-04T15:35:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-04T15:35:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tssvn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tmud5-dd2.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tssvn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:20Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:22Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:22Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:20Z"}],"hostIP":"10.250.1.231","podIP":"100.64.1.48","podIPs":[{"ip":"100.64.1.48"}],"startTime":"2023-09-04T15:35:20Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-04T15:35:21Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4534a4732a79bfb81a00bdb6613ed8baff59f6cd2ec8fc9a2e2c98efd751f464","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qm5jj","generateName":"daemon-set-","namespace":"daemonsets-2988","uid":"0802f7d6-28da-4cd8-b845-fb39fb4d5111","resourceVersion":"29455","creationTimestamp":"2023-09-04T15:35:20Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"1105db090aa19309fc3e12f1abf434cd621905a87469fc762f6f20f5263dc274","cni.projectcalico.org/podIP":"100.64.0.168/32","cni.projectcalico.org/podIPs":"100.64.0.168/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d9c15c01-0cda-404c-8549-552ec3652e10","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-04T15:35:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9c15c01-0cda-404c-8549-552ec3652e10\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-04T15:35:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-04T15:35:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ndgh2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tmud5-dd2.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ndgh2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:20Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:21Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:21Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-04T15:35:20Z"}],"hostIP":"10.250.1.105","podIP":"100.64.0.168","podIPs":[{"ip":"100.64.0.168"}],"startTime":"2023-09-04T15:35:20Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-04T15:35:21Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a921b6e0b8fe9728c1f2d051a8f919377cf924352d2fd61ba3c6c0315e84e54e","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:35:22.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2988" for this suite. 09/04/23 15:35:22.966
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:35:22.982
Sep  4 15:35:22.982: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename csistoragecapacity 09/04/23 15:35:22.983
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:23.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:23.06
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 09/04/23 15:35:23.086
STEP: getting /apis/storage.k8s.io 09/04/23 15:35:23.113
STEP: getting /apis/storage.k8s.io/v1 09/04/23 15:35:23.126
STEP: creating 09/04/23 15:35:23.139
STEP: watching 09/04/23 15:35:23.185
Sep  4 15:35:23.185: INFO: starting watch
STEP: getting 09/04/23 15:35:23.227
STEP: listing in namespace 09/04/23 15:35:23.241
STEP: listing across namespaces 09/04/23 15:35:23.254
STEP: patching 09/04/23 15:35:23.268
STEP: updating 09/04/23 15:35:23.283
Sep  4 15:35:23.298: INFO: waiting for watch events with expected annotations in namespace
Sep  4 15:35:23.298: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 09/04/23 15:35:23.298
STEP: deleting a collection 09/04/23 15:35:23.34
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Sep  4 15:35:23.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-93" for this suite. 09/04/23 15:35:23.392
------------------------------
• [0.426 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:35:22.982
    Sep  4 15:35:22.982: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename csistoragecapacity 09/04/23 15:35:22.983
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:23.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:23.06
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 09/04/23 15:35:23.086
    STEP: getting /apis/storage.k8s.io 09/04/23 15:35:23.113
    STEP: getting /apis/storage.k8s.io/v1 09/04/23 15:35:23.126
    STEP: creating 09/04/23 15:35:23.139
    STEP: watching 09/04/23 15:35:23.185
    Sep  4 15:35:23.185: INFO: starting watch
    STEP: getting 09/04/23 15:35:23.227
    STEP: listing in namespace 09/04/23 15:35:23.241
    STEP: listing across namespaces 09/04/23 15:35:23.254
    STEP: patching 09/04/23 15:35:23.268
    STEP: updating 09/04/23 15:35:23.283
    Sep  4 15:35:23.298: INFO: waiting for watch events with expected annotations in namespace
    Sep  4 15:35:23.298: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 09/04/23 15:35:23.298
    STEP: deleting a collection 09/04/23 15:35:23.34
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:35:23.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-93" for this suite. 09/04/23 15:35:23.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:35:23.409
Sep  4 15:35:23.409: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:35:23.409
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:23.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:23.478
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-648899bb-ea01-4ddc-abfe-07a9e1297903 09/04/23 15:35:23.505
STEP: Creating a pod to test consume configMaps 09/04/23 15:35:23.52
Sep  4 15:35:23.542: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221" in namespace "projected-1971" to be "Succeeded or Failed"
Sep  4 15:35:23.556: INFO: Pod "pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221": Phase="Pending", Reason="", readiness=false. Elapsed: 14.613226ms
Sep  4 15:35:25.572: INFO: Pod "pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029952835s
Sep  4 15:35:27.572: INFO: Pod "pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029800053s
STEP: Saw pod success 09/04/23 15:35:27.572
Sep  4 15:35:27.572: INFO: Pod "pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221" satisfied condition "Succeeded or Failed"
Sep  4 15:35:27.586: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 15:35:27.621
Sep  4 15:35:27.639: INFO: Waiting for pod pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221 to disappear
Sep  4 15:35:27.653: INFO: Pod pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:35:27.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1971" for this suite. 09/04/23 15:35:27.68
------------------------------
• [4.287 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:35:23.409
    Sep  4 15:35:23.409: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:35:23.409
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:23.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:23.478
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-648899bb-ea01-4ddc-abfe-07a9e1297903 09/04/23 15:35:23.505
    STEP: Creating a pod to test consume configMaps 09/04/23 15:35:23.52
    Sep  4 15:35:23.542: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221" in namespace "projected-1971" to be "Succeeded or Failed"
    Sep  4 15:35:23.556: INFO: Pod "pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221": Phase="Pending", Reason="", readiness=false. Elapsed: 14.613226ms
    Sep  4 15:35:25.572: INFO: Pod "pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029952835s
    Sep  4 15:35:27.572: INFO: Pod "pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029800053s
    STEP: Saw pod success 09/04/23 15:35:27.572
    Sep  4 15:35:27.572: INFO: Pod "pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221" satisfied condition "Succeeded or Failed"
    Sep  4 15:35:27.586: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 15:35:27.621
    Sep  4 15:35:27.639: INFO: Waiting for pod pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221 to disappear
    Sep  4 15:35:27.653: INFO: Pod pod-projected-configmaps-d8584993-0ad3-478c-a97c-3b4b4d5b1221 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:35:27.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1971" for this suite. 09/04/23 15:35:27.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:35:27.696
Sep  4 15:35:27.696: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 09/04/23 15:35:27.697
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:27.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:27.766
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-8646" 09/04/23 15:35:27.793
Sep  4 15:35:27.821: INFO: Namespace "namespaces-8646" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"0be53abf-8976-49ee-9130-04032f304ae9", "kubernetes.io/metadata.name":"namespaces-8646", "namespaces-8646":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:35:27.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8646" for this suite. 09/04/23 15:35:27.836
------------------------------
• [0.155 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:35:27.696
    Sep  4 15:35:27.696: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 09/04/23 15:35:27.697
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:27.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:27.766
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-8646" 09/04/23 15:35:27.793
    Sep  4 15:35:27.821: INFO: Namespace "namespaces-8646" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"0be53abf-8976-49ee-9130-04032f304ae9", "kubernetes.io/metadata.name":"namespaces-8646", "namespaces-8646":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:35:27.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8646" for this suite. 09/04/23 15:35:27.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:35:27.852
Sep  4 15:35:27.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice 09/04/23 15:35:27.853
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:27.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:27.922
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  4 15:35:28.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6795" for this suite. 09/04/23 15:35:28.057
------------------------------
• [0.220 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:35:27.852
    Sep  4 15:35:27.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslice 09/04/23 15:35:27.853
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:27.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:27.922
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:35:28.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6795" for this suite. 09/04/23 15:35:28.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:35:28.072
Sep  4 15:35:28.072: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 09/04/23 15:35:28.073
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:28.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:28.14
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 09/04/23 15:35:28.167
Sep  4 15:35:28.188: INFO: Waiting up to 5m0s for pod "pod-hostip-fde94f6f-fb74-4ab0-b494-ff114c248837" in namespace "pods-8321" to be "running and ready"
Sep  4 15:35:28.202: INFO: Pod "pod-hostip-fde94f6f-fb74-4ab0-b494-ff114c248837": Phase="Pending", Reason="", readiness=false. Elapsed: 13.967337ms
Sep  4 15:35:28.202: INFO: The phase of Pod pod-hostip-fde94f6f-fb74-4ab0-b494-ff114c248837 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:35:30.217: INFO: Pod "pod-hostip-fde94f6f-fb74-4ab0-b494-ff114c248837": Phase="Running", Reason="", readiness=true. Elapsed: 2.029189068s
Sep  4 15:35:30.217: INFO: The phase of Pod pod-hostip-fde94f6f-fb74-4ab0-b494-ff114c248837 is Running (Ready = true)
Sep  4 15:35:30.217: INFO: Pod "pod-hostip-fde94f6f-fb74-4ab0-b494-ff114c248837" satisfied condition "running and ready"
Sep  4 15:35:30.245: INFO: Pod pod-hostip-fde94f6f-fb74-4ab0-b494-ff114c248837 has hostIP: 10.250.1.231
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 15:35:30.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8321" for this suite. 09/04/23 15:35:30.272
------------------------------
• [2.215 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:35:28.072
    Sep  4 15:35:28.072: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 09/04/23 15:35:28.073
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:28.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:28.14
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 09/04/23 15:35:28.167
    Sep  4 15:35:28.188: INFO: Waiting up to 5m0s for pod "pod-hostip-fde94f6f-fb74-4ab0-b494-ff114c248837" in namespace "pods-8321" to be "running and ready"
    Sep  4 15:35:28.202: INFO: Pod "pod-hostip-fde94f6f-fb74-4ab0-b494-ff114c248837": Phase="Pending", Reason="", readiness=false. Elapsed: 13.967337ms
    Sep  4 15:35:28.202: INFO: The phase of Pod pod-hostip-fde94f6f-fb74-4ab0-b494-ff114c248837 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:35:30.217: INFO: Pod "pod-hostip-fde94f6f-fb74-4ab0-b494-ff114c248837": Phase="Running", Reason="", readiness=true. Elapsed: 2.029189068s
    Sep  4 15:35:30.217: INFO: The phase of Pod pod-hostip-fde94f6f-fb74-4ab0-b494-ff114c248837 is Running (Ready = true)
    Sep  4 15:35:30.217: INFO: Pod "pod-hostip-fde94f6f-fb74-4ab0-b494-ff114c248837" satisfied condition "running and ready"
    Sep  4 15:35:30.245: INFO: Pod pod-hostip-fde94f6f-fb74-4ab0-b494-ff114c248837 has hostIP: 10.250.1.231
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:35:30.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8321" for this suite. 09/04/23 15:35:30.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:35:30.288
Sep  4 15:35:30.288: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 09/04/23 15:35:30.289
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:30.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:30.358
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 09/04/23 15:35:30.385
STEP: Ensuring job reaches completions 09/04/23 15:35:30.4
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  4 15:35:40.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3557" for this suite. 09/04/23 15:35:40.443
------------------------------
• [10.172 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:35:30.288
    Sep  4 15:35:30.288: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 09/04/23 15:35:30.289
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:30.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:30.358
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 09/04/23 15:35:30.385
    STEP: Ensuring job reaches completions 09/04/23 15:35:30.4
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:35:40.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3557" for this suite. 09/04/23 15:35:40.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:35:40.461
Sep  4 15:35:40.461: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 09/04/23 15:35:40.462
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:40.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:40.537
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 09/04/23 15:35:40.563
STEP: watching for the ServiceAccount to be added 09/04/23 15:35:40.591
STEP: patching the ServiceAccount 09/04/23 15:35:40.605
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/04/23 15:35:40.621
STEP: deleting the ServiceAccount 09/04/23 15:35:40.636
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  4 15:35:40.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9982" for this suite. 09/04/23 15:35:40.671
------------------------------
• [0.225 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:35:40.461
    Sep  4 15:35:40.461: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 09/04/23 15:35:40.462
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:40.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:40.537
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 09/04/23 15:35:40.563
    STEP: watching for the ServiceAccount to be added 09/04/23 15:35:40.591
    STEP: patching the ServiceAccount 09/04/23 15:35:40.605
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/04/23 15:35:40.621
    STEP: deleting the ServiceAccount 09/04/23 15:35:40.636
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:35:40.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9982" for this suite. 09/04/23 15:35:40.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:35:40.687
Sep  4 15:35:40.687: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 09/04/23 15:35:40.688
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:40.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:40.759
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 09/04/23 15:35:40.787
STEP: submitting the pod to kubernetes 09/04/23 15:35:40.787
STEP: verifying QOS class is set on the pod 09/04/23 15:35:40.809
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Sep  4 15:35:40.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8110" for this suite. 09/04/23 15:35:40.841
------------------------------
• [0.170 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:35:40.687
    Sep  4 15:35:40.687: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 09/04/23 15:35:40.688
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:40.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:40.759
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 09/04/23 15:35:40.787
    STEP: submitting the pod to kubernetes 09/04/23 15:35:40.787
    STEP: verifying QOS class is set on the pod 09/04/23 15:35:40.809
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:35:40.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8110" for this suite. 09/04/23 15:35:40.841
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:35:40.857
Sep  4 15:35:40.857: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 09/04/23 15:35:40.858
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:40.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:40.932
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 09/04/23 15:35:40.96
STEP: Ensuring a job is scheduled 09/04/23 15:35:40.976
STEP: Ensuring exactly one is scheduled 09/04/23 15:36:01.005
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/04/23 15:36:01.021
STEP: Ensuring no more jobs are scheduled 09/04/23 15:36:01.036
------------------------------
Automatically polling progress:
  [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance] (Spec Runtime: 5m0.104s)
    test/e2e/apps/cronjob.go:124
    In [It] (Node Runtime: 5m0.002s)
      test/e2e/apps/cronjob.go:124
      At [By Step] Ensuring no more jobs are scheduled (Step Runtime: 4m39.925s)
        test/e2e/apps/cronjob.go:146

      Spec Goroutine
      goroutine 18196 [select]
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.WaitForWithContext({0x817c768, 0xc0001a6000}, 0xc00321d1b8, 0x306620a?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:660
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.poll({0x817c768, 0xc0001a6000}, 0x10?, 0x3064da5?, 0x68c0340?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:596
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.PollWithContext({0x817c768, 0xc0001a6000}, 0x0?, 0xc0066add60?, 0x26724e7?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:460
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.Poll(0x0?, 0xc1359c38422b1e89?, 0x2e0e81633bb?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:445
      > k8s.io/kubernetes/test/e2e/apps.waitForActiveJobs({0x81ba5a8?, 0xc004162680}, {0xc00690a1c0, 0xc}, {0xc0065f3968, 0x6}, 0x2)
          test/e2e/apps/cronjob.go:593
      > k8s.io/kubernetes/test/e2e/apps.glob..func2.3()
          test/e2e/apps/cronjob.go:147
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0xc0068870e0, 0xc00689a1e0})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
Automatically polling progress:
  [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance] (Spec Runtime: 5m20.107s)
    test/e2e/apps/cronjob.go:124
    In [It] (Node Runtime: 5m20.004s)
      test/e2e/apps/cronjob.go:124
      At [By Step] Ensuring no more jobs are scheduled (Step Runtime: 4m59.927s)
        test/e2e/apps/cronjob.go:146

      Spec Goroutine
      goroutine 18196 [select]
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.WaitForWithContext({0x817c768, 0xc0001a6000}, 0xc00321d1b8, 0x306620a?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:660
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.poll({0x817c768, 0xc0001a6000}, 0x10?, 0x3064da5?, 0x68c0340?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:596
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.PollWithContext({0x817c768, 0xc0001a6000}, 0x0?, 0xc0066add60?, 0x26724e7?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:460
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.Poll(0x0?, 0xc1359c38422b1e89?, 0x2e0e81633bb?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:445
      > k8s.io/kubernetes/test/e2e/apps.waitForActiveJobs({0x81ba5a8?, 0xc004162680}, {0xc00690a1c0, 0xc}, {0xc0065f3968, 0x6}, 0x2)
          test/e2e/apps/cronjob.go:593
      > k8s.io/kubernetes/test/e2e/apps.glob..func2.3()
          test/e2e/apps/cronjob.go:147
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0xc0068870e0, 0xc00689a1e0})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
STEP: Removing cronjob 09/04/23 15:41:01.066
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  4 15:41:01.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5305" for this suite. 09/04/23 15:41:01.11
• [SLOW TEST] [320.269 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:35:40.857
    Sep  4 15:35:40.857: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 09/04/23 15:35:40.858
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:35:40.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:35:40.932
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 09/04/23 15:35:40.96
    STEP: Ensuring a job is scheduled 09/04/23 15:35:40.976
    STEP: Ensuring exactly one is scheduled 09/04/23 15:36:01.005
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/04/23 15:36:01.021
    STEP: Ensuring no more jobs are scheduled 09/04/23 15:36:01.036
    STEP: Removing cronjob 09/04/23 15:41:01.066
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:41:01.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5305" for this suite. 09/04/23 15:41:01.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:41:01.127
Sep  4 15:41:01.127: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 09/04/23 15:41:01.128
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:41:01.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:41:01.198
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 09/04/23 15:41:01.225
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7007 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7007;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7007 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7007;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7007.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7007.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7007.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7007.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7007.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7007.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7007.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7007.svc;check="$$(dig +notcp +noall +answer +search 125.164.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.164.125_udp@PTR;check="$$(dig +tcp +noall +answer +search 125.164.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.164.125_tcp@PTR;sleep 1; done
 09/04/23 15:41:01.26
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7007 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7007;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7007 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7007;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7007.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7007.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7007.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7007.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7007.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7007.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7007.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7007.svc;check="$$(dig +notcp +noall +answer +search 125.164.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.164.125_udp@PTR;check="$$(dig +tcp +noall +answer +search 125.164.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.164.125_tcp@PTR;sleep 1; done
 09/04/23 15:41:01.26
STEP: creating a pod to probe DNS 09/04/23 15:41:01.26
STEP: submitting the pod to kubernetes 09/04/23 15:41:01.26
Sep  4 15:41:01.290: INFO: Waiting up to 15m0s for pod "dns-test-99729a1c-5b55-419d-91d5-cbc696180304" in namespace "dns-7007" to be "running"
Sep  4 15:41:01.304: INFO: Pod "dns-test-99729a1c-5b55-419d-91d5-cbc696180304": Phase="Pending", Reason="", readiness=false. Elapsed: 14.394742ms
Sep  4 15:41:03.321: INFO: Pod "dns-test-99729a1c-5b55-419d-91d5-cbc696180304": Phase="Running", Reason="", readiness=true. Elapsed: 2.031108442s
Sep  4 15:41:03.321: INFO: Pod "dns-test-99729a1c-5b55-419d-91d5-cbc696180304" satisfied condition "running"
STEP: retrieving the pod 09/04/23 15:41:03.321
STEP: looking for the results for each expected name from probers 09/04/23 15:41:03.336
Sep  4 15:41:03.470: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:03.524: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:03.552: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:03.578: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:03.604: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:03.634: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:03.664: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:03.690: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:03.817: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:03.842: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:03.867: INFO: Unable to read jessie_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:03.893: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:03.919: INFO: Unable to read jessie_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:03.946: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:03.972: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:04.016: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:04.121: INFO: Lookups using dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7007 wheezy_tcp@dns-test-service.dns-7007 wheezy_udp@dns-test-service.dns-7007.svc wheezy_tcp@dns-test-service.dns-7007.svc wheezy_udp@_http._tcp.dns-test-service.dns-7007.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7007.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7007 jessie_tcp@dns-test-service.dns-7007 jessie_udp@dns-test-service.dns-7007.svc jessie_tcp@dns-test-service.dns-7007.svc jessie_udp@_http._tcp.dns-test-service.dns-7007.svc jessie_tcp@_http._tcp.dns-test-service.dns-7007.svc]

Sep  4 15:41:09.148: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.199: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.225: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.257: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.312: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.338: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.364: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.391: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.521: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.547: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.574: INFO: Unable to read jessie_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.600: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.626: INFO: Unable to read jessie_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.652: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.677: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.702: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:09.803: INFO: Lookups using dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7007 wheezy_tcp@dns-test-service.dns-7007 wheezy_udp@dns-test-service.dns-7007.svc wheezy_tcp@dns-test-service.dns-7007.svc wheezy_udp@_http._tcp.dns-test-service.dns-7007.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7007.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7007 jessie_tcp@dns-test-service.dns-7007 jessie_udp@dns-test-service.dns-7007.svc jessie_tcp@dns-test-service.dns-7007.svc jessie_udp@_http._tcp.dns-test-service.dns-7007.svc jessie_tcp@_http._tcp.dns-test-service.dns-7007.svc]

Sep  4 15:41:14.150: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.203: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.229: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.255: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.280: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.342: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.396: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.528: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.553: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.580: INFO: Unable to read jessie_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.616: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.667: INFO: Unable to read jessie_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.694: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.721: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.747: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:14.888: INFO: Lookups using dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7007 wheezy_tcp@dns-test-service.dns-7007 wheezy_udp@dns-test-service.dns-7007.svc wheezy_tcp@dns-test-service.dns-7007.svc wheezy_udp@_http._tcp.dns-test-service.dns-7007.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7007.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7007 jessie_tcp@dns-test-service.dns-7007 jessie_udp@dns-test-service.dns-7007.svc jessie_tcp@dns-test-service.dns-7007.svc jessie_udp@_http._tcp.dns-test-service.dns-7007.svc jessie_tcp@_http._tcp.dns-test-service.dns-7007.svc]

Sep  4 15:41:19.150: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:19.208: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:19.234: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:19.259: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:19.284: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:19.310: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:19.489: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:19.516: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:19.540: INFO: Unable to read jessie_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:19.565: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:19.592: INFO: Unable to read jessie_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:19.618: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
Sep  4 15:41:19.767: INFO: Lookups using dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7007 wheezy_tcp@dns-test-service.dns-7007 wheezy_udp@dns-test-service.dns-7007.svc wheezy_tcp@dns-test-service.dns-7007.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7007 jessie_tcp@dns-test-service.dns-7007 jessie_udp@dns-test-service.dns-7007.svc jessie_tcp@dns-test-service.dns-7007.svc]

Sep  4 15:41:24.795: INFO: DNS probes using dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304 succeeded

STEP: deleting the pod 09/04/23 15:41:24.795
STEP: deleting the test service 09/04/23 15:41:24.813
STEP: deleting the test headless service 09/04/23 15:41:24.837
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 15:41:25.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7007" for this suite. 09/04/23 15:41:25.32
------------------------------
• [24.209 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:41:01.127
    Sep  4 15:41:01.127: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 09/04/23 15:41:01.128
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:41:01.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:41:01.198
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 09/04/23 15:41:01.225
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7007 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7007;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7007 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7007;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7007.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7007.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7007.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7007.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7007.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7007.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7007.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7007.svc;check="$$(dig +notcp +noall +answer +search 125.164.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.164.125_udp@PTR;check="$$(dig +tcp +noall +answer +search 125.164.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.164.125_tcp@PTR;sleep 1; done
     09/04/23 15:41:01.26
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7007 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7007;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7007 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7007;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7007.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7007.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7007.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7007.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7007.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7007.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7007.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7007.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7007.svc;check="$$(dig +notcp +noall +answer +search 125.164.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.164.125_udp@PTR;check="$$(dig +tcp +noall +answer +search 125.164.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.164.125_tcp@PTR;sleep 1; done
     09/04/23 15:41:01.26
    STEP: creating a pod to probe DNS 09/04/23 15:41:01.26
    STEP: submitting the pod to kubernetes 09/04/23 15:41:01.26
    Sep  4 15:41:01.290: INFO: Waiting up to 15m0s for pod "dns-test-99729a1c-5b55-419d-91d5-cbc696180304" in namespace "dns-7007" to be "running"
    Sep  4 15:41:01.304: INFO: Pod "dns-test-99729a1c-5b55-419d-91d5-cbc696180304": Phase="Pending", Reason="", readiness=false. Elapsed: 14.394742ms
    Sep  4 15:41:03.321: INFO: Pod "dns-test-99729a1c-5b55-419d-91d5-cbc696180304": Phase="Running", Reason="", readiness=true. Elapsed: 2.031108442s
    Sep  4 15:41:03.321: INFO: Pod "dns-test-99729a1c-5b55-419d-91d5-cbc696180304" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 15:41:03.321
    STEP: looking for the results for each expected name from probers 09/04/23 15:41:03.336
    Sep  4 15:41:03.470: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:03.524: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:03.552: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:03.578: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:03.604: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:03.634: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:03.664: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:03.690: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:03.817: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:03.842: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:03.867: INFO: Unable to read jessie_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:03.893: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:03.919: INFO: Unable to read jessie_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:03.946: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:03.972: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:04.016: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:04.121: INFO: Lookups using dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7007 wheezy_tcp@dns-test-service.dns-7007 wheezy_udp@dns-test-service.dns-7007.svc wheezy_tcp@dns-test-service.dns-7007.svc wheezy_udp@_http._tcp.dns-test-service.dns-7007.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7007.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7007 jessie_tcp@dns-test-service.dns-7007 jessie_udp@dns-test-service.dns-7007.svc jessie_tcp@dns-test-service.dns-7007.svc jessie_udp@_http._tcp.dns-test-service.dns-7007.svc jessie_tcp@_http._tcp.dns-test-service.dns-7007.svc]

    Sep  4 15:41:09.148: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.199: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.225: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.257: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.312: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.338: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.364: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.391: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.521: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.547: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.574: INFO: Unable to read jessie_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.600: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.626: INFO: Unable to read jessie_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.652: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.677: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.702: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:09.803: INFO: Lookups using dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7007 wheezy_tcp@dns-test-service.dns-7007 wheezy_udp@dns-test-service.dns-7007.svc wheezy_tcp@dns-test-service.dns-7007.svc wheezy_udp@_http._tcp.dns-test-service.dns-7007.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7007.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7007 jessie_tcp@dns-test-service.dns-7007 jessie_udp@dns-test-service.dns-7007.svc jessie_tcp@dns-test-service.dns-7007.svc jessie_udp@_http._tcp.dns-test-service.dns-7007.svc jessie_tcp@_http._tcp.dns-test-service.dns-7007.svc]

    Sep  4 15:41:14.150: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.203: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.229: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.255: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.280: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.342: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.396: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.528: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.553: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.580: INFO: Unable to read jessie_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.616: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.667: INFO: Unable to read jessie_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.694: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.721: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.747: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:14.888: INFO: Lookups using dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7007 wheezy_tcp@dns-test-service.dns-7007 wheezy_udp@dns-test-service.dns-7007.svc wheezy_tcp@dns-test-service.dns-7007.svc wheezy_udp@_http._tcp.dns-test-service.dns-7007.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7007.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7007 jessie_tcp@dns-test-service.dns-7007 jessie_udp@dns-test-service.dns-7007.svc jessie_tcp@dns-test-service.dns-7007.svc jessie_udp@_http._tcp.dns-test-service.dns-7007.svc jessie_tcp@_http._tcp.dns-test-service.dns-7007.svc]

    Sep  4 15:41:19.150: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:19.208: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:19.234: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:19.259: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:19.284: INFO: Unable to read wheezy_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:19.310: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:19.489: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:19.516: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:19.540: INFO: Unable to read jessie_udp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:19.565: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007 from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:19.592: INFO: Unable to read jessie_udp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:19.618: INFO: Unable to read jessie_tcp@dns-test-service.dns-7007.svc from pod dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304: the server could not find the requested resource (get pods dns-test-99729a1c-5b55-419d-91d5-cbc696180304)
    Sep  4 15:41:19.767: INFO: Lookups using dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7007 wheezy_tcp@dns-test-service.dns-7007 wheezy_udp@dns-test-service.dns-7007.svc wheezy_tcp@dns-test-service.dns-7007.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7007 jessie_tcp@dns-test-service.dns-7007 jessie_udp@dns-test-service.dns-7007.svc jessie_tcp@dns-test-service.dns-7007.svc]

    Sep  4 15:41:24.795: INFO: DNS probes using dns-7007/dns-test-99729a1c-5b55-419d-91d5-cbc696180304 succeeded

    STEP: deleting the pod 09/04/23 15:41:24.795
    STEP: deleting the test service 09/04/23 15:41:24.813
    STEP: deleting the test headless service 09/04/23 15:41:24.837
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:41:25.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7007" for this suite. 09/04/23 15:41:25.32
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:41:25.336
Sep  4 15:41:25.336: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 09/04/23 15:41:25.337
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:41:25.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:41:25.406
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 09/04/23 15:41:42.447
STEP: Creating a ResourceQuota 09/04/23 15:41:47.465
STEP: Ensuring resource quota status is calculated 09/04/23 15:41:47.481
STEP: Creating a ConfigMap 09/04/23 15:41:49.497
STEP: Ensuring resource quota status captures configMap creation 09/04/23 15:41:49.515
STEP: Deleting a ConfigMap 09/04/23 15:41:51.531
STEP: Ensuring resource quota status released usage 09/04/23 15:41:51.547
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 15:41:53.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9762" for this suite. 09/04/23 15:41:53.59
------------------------------
• [28.272 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:41:25.336
    Sep  4 15:41:25.336: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 09/04/23 15:41:25.337
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:41:25.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:41:25.406
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 09/04/23 15:41:42.447
    STEP: Creating a ResourceQuota 09/04/23 15:41:47.465
    STEP: Ensuring resource quota status is calculated 09/04/23 15:41:47.481
    STEP: Creating a ConfigMap 09/04/23 15:41:49.497
    STEP: Ensuring resource quota status captures configMap creation 09/04/23 15:41:49.515
    STEP: Deleting a ConfigMap 09/04/23 15:41:51.531
    STEP: Ensuring resource quota status released usage 09/04/23 15:41:51.547
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:41:53.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9762" for this suite. 09/04/23 15:41:53.59
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:41:53.608
Sep  4 15:41:53.608: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice 09/04/23 15:41:53.609
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:41:53.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:41:53.68
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Sep  4 15:41:53.751: INFO: Endpoints addresses: [10.243.19.124] , ports: [443]
Sep  4 15:41:53.751: INFO: EndpointSlices addresses: [10.243.19.124] , ports: [443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  4 15:41:53.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7042" for this suite. 09/04/23 15:41:53.766
------------------------------
• [0.174 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:41:53.608
    Sep  4 15:41:53.608: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslice 09/04/23 15:41:53.609
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:41:53.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:41:53.68
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Sep  4 15:41:53.751: INFO: Endpoints addresses: [10.243.19.124] , ports: [443]
    Sep  4 15:41:53.751: INFO: EndpointSlices addresses: [10.243.19.124] , ports: [443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:41:53.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7042" for this suite. 09/04/23 15:41:53.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:41:53.783
Sep  4 15:41:53.783: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:41:53.783
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:41:53.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:41:53.856
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-1c9280d5-56ff-46c1-aba4-15f79d39c777 09/04/23 15:41:53.884
STEP: Creating a pod to test consume configMaps 09/04/23 15:41:53.899
Sep  4 15:41:53.921: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead" in namespace "projected-9826" to be "Succeeded or Failed"
Sep  4 15:41:53.936: INFO: Pod "pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead": Phase="Pending", Reason="", readiness=false. Elapsed: 14.328419ms
Sep  4 15:41:55.952: INFO: Pod "pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030383353s
Sep  4 15:41:57.952: INFO: Pod "pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030917812s
STEP: Saw pod success 09/04/23 15:41:57.952
Sep  4 15:41:57.953: INFO: Pod "pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead" satisfied condition "Succeeded or Failed"
Sep  4 15:41:57.967: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead container agnhost-container: <nil>
STEP: delete the pod 09/04/23 15:41:58.058
Sep  4 15:41:58.078: INFO: Waiting for pod pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead to disappear
Sep  4 15:41:58.093: INFO: Pod pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:41:58.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9826" for this suite. 09/04/23 15:41:58.121
------------------------------
• [4.354 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:41:53.783
    Sep  4 15:41:53.783: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:41:53.783
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:41:53.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:41:53.856
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-1c9280d5-56ff-46c1-aba4-15f79d39c777 09/04/23 15:41:53.884
    STEP: Creating a pod to test consume configMaps 09/04/23 15:41:53.899
    Sep  4 15:41:53.921: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead" in namespace "projected-9826" to be "Succeeded or Failed"
    Sep  4 15:41:53.936: INFO: Pod "pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead": Phase="Pending", Reason="", readiness=false. Elapsed: 14.328419ms
    Sep  4 15:41:55.952: INFO: Pod "pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030383353s
    Sep  4 15:41:57.952: INFO: Pod "pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030917812s
    STEP: Saw pod success 09/04/23 15:41:57.952
    Sep  4 15:41:57.953: INFO: Pod "pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead" satisfied condition "Succeeded or Failed"
    Sep  4 15:41:57.967: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 15:41:58.058
    Sep  4 15:41:58.078: INFO: Waiting for pod pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead to disappear
    Sep  4 15:41:58.093: INFO: Pod pod-projected-configmaps-1cb0f4ef-83d9-4e01-b945-49cc4a110ead no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:41:58.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9826" for this suite. 09/04/23 15:41:58.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:41:58.138
Sep  4 15:41:58.138: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test 09/04/23 15:41:58.139
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:41:58.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:41:58.212
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-380 09/04/23 15:41:58.239
STEP: creating a selector 09/04/23 15:41:58.239
STEP: Creating the service pods in kubernetes 09/04/23 15:41:58.239
Sep  4 15:41:58.239: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  4 15:41:58.312: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-380" to be "running and ready"
Sep  4 15:41:58.327: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.799051ms
Sep  4 15:41:58.327: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:42:00.344: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.031619219s
Sep  4 15:42:00.344: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:42:02.343: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.030084262s
Sep  4 15:42:02.343: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:42:04.344: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.03118903s
Sep  4 15:42:04.344: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:42:06.343: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.030201238s
Sep  4 15:42:06.343: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:42:08.342: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.029537175s
Sep  4 15:42:08.342: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:42:10.342: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.030033757s
Sep  4 15:42:10.343: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  4 15:42:10.343: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  4 15:42:10.357: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-380" to be "running and ready"
Sep  4 15:42:10.372: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.92147ms
Sep  4 15:42:10.372: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  4 15:42:10.372: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/04/23 15:42:10.387
Sep  4 15:42:10.407: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-380" to be "running"
Sep  4 15:42:10.421: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.37035ms
Sep  4 15:42:12.438: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.031024095s
Sep  4 15:42:12.438: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  4 15:42:12.453: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  4 15:42:12.453: INFO: Breadth first check of 100.64.0.169 on host 10.250.1.105...
Sep  4 15:42:12.470: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.59:9080/dial?request=hostname&protocol=udp&host=100.64.0.169&port=8081&tries=1'] Namespace:pod-network-test-380 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:42:12.470: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:42:12.470: INFO: ExecWithOptions: Clientset creation
Sep  4 15:42:12.470: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-380/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.64.0.169%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  4 15:42:12.962: INFO: Waiting for responses: map[]
Sep  4 15:42:12.962: INFO: reached 100.64.0.169 after 0/1 tries
Sep  4 15:42:12.962: INFO: Breadth first check of 100.64.1.58 on host 10.250.1.231...
Sep  4 15:42:12.978: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.59:9080/dial?request=hostname&protocol=udp&host=100.64.1.58&port=8081&tries=1'] Namespace:pod-network-test-380 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:42:12.978: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:42:12.979: INFO: ExecWithOptions: Clientset creation
Sep  4 15:42:12.979: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-380/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.64.1.58%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  4 15:42:13.439: INFO: Waiting for responses: map[]
Sep  4 15:42:13.439: INFO: reached 100.64.1.58 after 0/1 tries
Sep  4 15:42:13.439: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  4 15:42:13.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-380" for this suite. 09/04/23 15:42:13.466
------------------------------
• [15.344 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:41:58.138
    Sep  4 15:41:58.138: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pod-network-test 09/04/23 15:41:58.139
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:41:58.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:41:58.212
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-380 09/04/23 15:41:58.239
    STEP: creating a selector 09/04/23 15:41:58.239
    STEP: Creating the service pods in kubernetes 09/04/23 15:41:58.239
    Sep  4 15:41:58.239: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  4 15:41:58.312: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-380" to be "running and ready"
    Sep  4 15:41:58.327: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.799051ms
    Sep  4 15:41:58.327: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:42:00.344: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.031619219s
    Sep  4 15:42:00.344: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:42:02.343: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.030084262s
    Sep  4 15:42:02.343: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:42:04.344: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.03118903s
    Sep  4 15:42:04.344: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:42:06.343: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.030201238s
    Sep  4 15:42:06.343: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:42:08.342: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.029537175s
    Sep  4 15:42:08.342: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:42:10.342: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.030033757s
    Sep  4 15:42:10.343: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  4 15:42:10.343: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  4 15:42:10.357: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-380" to be "running and ready"
    Sep  4 15:42:10.372: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.92147ms
    Sep  4 15:42:10.372: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  4 15:42:10.372: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/04/23 15:42:10.387
    Sep  4 15:42:10.407: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-380" to be "running"
    Sep  4 15:42:10.421: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.37035ms
    Sep  4 15:42:12.438: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.031024095s
    Sep  4 15:42:12.438: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  4 15:42:12.453: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  4 15:42:12.453: INFO: Breadth first check of 100.64.0.169 on host 10.250.1.105...
    Sep  4 15:42:12.470: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.59:9080/dial?request=hostname&protocol=udp&host=100.64.0.169&port=8081&tries=1'] Namespace:pod-network-test-380 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:42:12.470: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:42:12.470: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:42:12.470: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-380/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.64.0.169%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  4 15:42:12.962: INFO: Waiting for responses: map[]
    Sep  4 15:42:12.962: INFO: reached 100.64.0.169 after 0/1 tries
    Sep  4 15:42:12.962: INFO: Breadth first check of 100.64.1.58 on host 10.250.1.231...
    Sep  4 15:42:12.978: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.59:9080/dial?request=hostname&protocol=udp&host=100.64.1.58&port=8081&tries=1'] Namespace:pod-network-test-380 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:42:12.978: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:42:12.979: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:42:12.979: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-380/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.64.1.58%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  4 15:42:13.439: INFO: Waiting for responses: map[]
    Sep  4 15:42:13.439: INFO: reached 100.64.1.58 after 0/1 tries
    Sep  4 15:42:13.439: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:42:13.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-380" for this suite. 09/04/23 15:42:13.466
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:42:13.482
Sep  4 15:42:13.483: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:42:13.483
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:13.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:13.554
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 09/04/23 15:42:13.583
Sep  4 15:42:13.605: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23" in namespace "projected-8595" to be "Succeeded or Failed"
Sep  4 15:42:13.620: INFO: Pod "downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23": Phase="Pending", Reason="", readiness=false. Elapsed: 14.496776ms
Sep  4 15:42:15.636: INFO: Pod "downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030764018s
Sep  4 15:42:17.636: INFO: Pod "downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030999373s
STEP: Saw pod success 09/04/23 15:42:17.636
Sep  4 15:42:17.636: INFO: Pod "downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23" satisfied condition "Succeeded or Failed"
Sep  4 15:42:17.651: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23 container client-container: <nil>
STEP: delete the pod 09/04/23 15:42:17.687
Sep  4 15:42:17.707: INFO: Waiting for pod downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23 to disappear
Sep  4 15:42:17.721: INFO: Pod downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 15:42:17.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8595" for this suite. 09/04/23 15:42:17.749
------------------------------
• [4.282 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:42:13.482
    Sep  4 15:42:13.483: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:42:13.483
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:13.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:13.554
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 09/04/23 15:42:13.583
    Sep  4 15:42:13.605: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23" in namespace "projected-8595" to be "Succeeded or Failed"
    Sep  4 15:42:13.620: INFO: Pod "downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23": Phase="Pending", Reason="", readiness=false. Elapsed: 14.496776ms
    Sep  4 15:42:15.636: INFO: Pod "downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030764018s
    Sep  4 15:42:17.636: INFO: Pod "downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030999373s
    STEP: Saw pod success 09/04/23 15:42:17.636
    Sep  4 15:42:17.636: INFO: Pod "downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23" satisfied condition "Succeeded or Failed"
    Sep  4 15:42:17.651: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23 container client-container: <nil>
    STEP: delete the pod 09/04/23 15:42:17.687
    Sep  4 15:42:17.707: INFO: Waiting for pod downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23 to disappear
    Sep  4 15:42:17.721: INFO: Pod downwardapi-volume-e2b6f1c5-7a53-40f1-8511-aa7822e93f23 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:42:17.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8595" for this suite. 09/04/23 15:42:17.749
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:42:17.764
Sep  4 15:42:17.765: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 09/04/23 15:42:17.766
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:17.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:17.838
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
STEP: Creating simple DaemonSet "daemon-set" 09/04/23 15:42:17.928
STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 15:42:17.944
Sep  4 15:42:17.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:42:17.974: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:42:19.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 15:42:19.023: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:42:20.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 15:42:20.017: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 09/04/23 15:42:20.031
Sep  4 15:42:20.091: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 15:42:20.091: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:42:21.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 15:42:21.134: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:42:22.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 15:42:22.134: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:42:23.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 15:42:23.134: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:42:24.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 15:42:24.134: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/04/23 15:42:24.149
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2897, will wait for the garbage collector to delete the pods 09/04/23 15:42:24.149
Sep  4 15:42:24.231: INFO: Deleting DaemonSet.extensions daemon-set took: 16.37496ms
Sep  4 15:42:24.331: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.818906ms
Sep  4 15:42:26.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:42:26.847: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  4 15:42:26.861: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32201"},"items":null}

Sep  4 15:42:26.876: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32201"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:42:26.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2897" for this suite. 09/04/23 15:42:26.947
------------------------------
• [9.199 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:42:17.764
    Sep  4 15:42:17.765: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 09/04/23 15:42:17.766
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:17.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:17.838
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:177
    STEP: Creating simple DaemonSet "daemon-set" 09/04/23 15:42:17.928
    STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 15:42:17.944
    Sep  4 15:42:17.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:42:17.974: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:42:19.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 15:42:19.023: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:42:20.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 15:42:20.017: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 09/04/23 15:42:20.031
    Sep  4 15:42:20.091: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 15:42:20.091: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:42:21.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 15:42:21.134: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:42:22.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 15:42:22.134: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:42:23.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 15:42:23.134: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:42:24.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 15:42:24.134: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/04/23 15:42:24.149
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2897, will wait for the garbage collector to delete the pods 09/04/23 15:42:24.149
    Sep  4 15:42:24.231: INFO: Deleting DaemonSet.extensions daemon-set took: 16.37496ms
    Sep  4 15:42:24.331: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.818906ms
    Sep  4 15:42:26.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:42:26.847: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  4 15:42:26.861: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32201"},"items":null}

    Sep  4 15:42:26.876: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32201"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:42:26.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2897" for this suite. 09/04/23 15:42:26.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:42:26.964
Sep  4 15:42:26.964: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 15:42:26.965
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:27.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:27.035
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 15:42:27.095
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:42:27.45
STEP: Deploying the webhook pod 09/04/23 15:42:27.468
STEP: Wait for the deployment to be ready 09/04/23 15:42:27.5
Sep  4 15:42:27.546: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 42, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 42, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 42, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 42, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 15:42:29.562
STEP: Verifying the service has paired with the endpoint 09/04/23 15:42:29.582
Sep  4 15:42:30.583: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 09/04/23 15:42:30.599
STEP: create a pod 09/04/23 15:42:30.739
Sep  4 15:42:30.760: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7803" to be "running"
Sep  4 15:42:30.775: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.193484ms
Sep  4 15:42:32.791: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.031169947s
Sep  4 15:42:32.791: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 09/04/23 15:42:32.791
Sep  4 15:42:32.791: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=webhook-7803 attach --namespace=webhook-7803 to-be-attached-pod -i -c=container1'
Sep  4 15:42:33.168: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:42:33.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7803" for this suite. 09/04/23 15:42:33.284
STEP: Destroying namespace "webhook-7803-markers" for this suite. 09/04/23 15:42:33.299
------------------------------
• [6.351 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:42:26.964
    Sep  4 15:42:26.964: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 15:42:26.965
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:27.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:27.035
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 15:42:27.095
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:42:27.45
    STEP: Deploying the webhook pod 09/04/23 15:42:27.468
    STEP: Wait for the deployment to be ready 09/04/23 15:42:27.5
    Sep  4 15:42:27.546: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 42, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 42, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 42, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 42, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 15:42:29.562
    STEP: Verifying the service has paired with the endpoint 09/04/23 15:42:29.582
    Sep  4 15:42:30.583: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 09/04/23 15:42:30.599
    STEP: create a pod 09/04/23 15:42:30.739
    Sep  4 15:42:30.760: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7803" to be "running"
    Sep  4 15:42:30.775: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.193484ms
    Sep  4 15:42:32.791: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.031169947s
    Sep  4 15:42:32.791: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 09/04/23 15:42:32.791
    Sep  4 15:42:32.791: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=webhook-7803 attach --namespace=webhook-7803 to-be-attached-pod -i -c=container1'
    Sep  4 15:42:33.168: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:42:33.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7803" for this suite. 09/04/23 15:42:33.284
    STEP: Destroying namespace "webhook-7803-markers" for this suite. 09/04/23 15:42:33.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:42:33.315
Sep  4 15:42:33.315: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 15:42:33.316
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:33.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:33.386
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-e06209ab-d0dd-41a0-a838-7cc5701337cd 09/04/23 15:42:33.429
STEP: Creating configMap with name cm-test-opt-upd-f9f7f559-650f-4347-ba79-f877354426e9 09/04/23 15:42:33.444
STEP: Creating the pod 09/04/23 15:42:33.46
Sep  4 15:42:33.484: INFO: Waiting up to 5m0s for pod "pod-configmaps-72f8772f-b8d0-4678-b554-4a026268dfc0" in namespace "configmap-6038" to be "running and ready"
Sep  4 15:42:33.498: INFO: Pod "pod-configmaps-72f8772f-b8d0-4678-b554-4a026268dfc0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.125774ms
Sep  4 15:42:33.498: INFO: The phase of Pod pod-configmaps-72f8772f-b8d0-4678-b554-4a026268dfc0 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:42:35.514: INFO: Pod "pod-configmaps-72f8772f-b8d0-4678-b554-4a026268dfc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.029910272s
Sep  4 15:42:35.514: INFO: The phase of Pod pod-configmaps-72f8772f-b8d0-4678-b554-4a026268dfc0 is Running (Ready = true)
Sep  4 15:42:35.514: INFO: Pod "pod-configmaps-72f8772f-b8d0-4678-b554-4a026268dfc0" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-e06209ab-d0dd-41a0-a838-7cc5701337cd 09/04/23 15:42:35.774
STEP: Updating configmap cm-test-opt-upd-f9f7f559-650f-4347-ba79-f877354426e9 09/04/23 15:42:35.79
STEP: Creating configMap with name cm-test-opt-create-171bbacc-41bf-4d79-9715-52f1a533d8ad 09/04/23 15:42:35.805
STEP: waiting to observe update in volume 09/04/23 15:42:35.821
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:42:38.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6038" for this suite. 09/04/23 15:42:38.173
------------------------------
• [4.874 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:42:33.315
    Sep  4 15:42:33.315: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 15:42:33.316
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:33.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:33.386
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-e06209ab-d0dd-41a0-a838-7cc5701337cd 09/04/23 15:42:33.429
    STEP: Creating configMap with name cm-test-opt-upd-f9f7f559-650f-4347-ba79-f877354426e9 09/04/23 15:42:33.444
    STEP: Creating the pod 09/04/23 15:42:33.46
    Sep  4 15:42:33.484: INFO: Waiting up to 5m0s for pod "pod-configmaps-72f8772f-b8d0-4678-b554-4a026268dfc0" in namespace "configmap-6038" to be "running and ready"
    Sep  4 15:42:33.498: INFO: Pod "pod-configmaps-72f8772f-b8d0-4678-b554-4a026268dfc0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.125774ms
    Sep  4 15:42:33.498: INFO: The phase of Pod pod-configmaps-72f8772f-b8d0-4678-b554-4a026268dfc0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:42:35.514: INFO: Pod "pod-configmaps-72f8772f-b8d0-4678-b554-4a026268dfc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.029910272s
    Sep  4 15:42:35.514: INFO: The phase of Pod pod-configmaps-72f8772f-b8d0-4678-b554-4a026268dfc0 is Running (Ready = true)
    Sep  4 15:42:35.514: INFO: Pod "pod-configmaps-72f8772f-b8d0-4678-b554-4a026268dfc0" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-e06209ab-d0dd-41a0-a838-7cc5701337cd 09/04/23 15:42:35.774
    STEP: Updating configmap cm-test-opt-upd-f9f7f559-650f-4347-ba79-f877354426e9 09/04/23 15:42:35.79
    STEP: Creating configMap with name cm-test-opt-create-171bbacc-41bf-4d79-9715-52f1a533d8ad 09/04/23 15:42:35.805
    STEP: waiting to observe update in volume 09/04/23 15:42:35.821
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:42:38.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6038" for this suite. 09/04/23 15:42:38.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:42:38.19
Sep  4 15:42:38.190: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:42:38.191
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:38.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:38.265
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-5eae6fa8-5443-4028-acae-6f65b372e0b1 09/04/23 15:42:38.308
STEP: Creating the pod 09/04/23 15:42:38.322
Sep  4 15:42:38.346: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ce012741-ac15-401e-8bf5-6b3e853e268e" in namespace "projected-191" to be "running and ready"
Sep  4 15:42:38.360: INFO: Pod "pod-projected-configmaps-ce012741-ac15-401e-8bf5-6b3e853e268e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.234357ms
Sep  4 15:42:38.360: INFO: The phase of Pod pod-projected-configmaps-ce012741-ac15-401e-8bf5-6b3e853e268e is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:42:40.376: INFO: Pod "pod-projected-configmaps-ce012741-ac15-401e-8bf5-6b3e853e268e": Phase="Running", Reason="", readiness=true. Elapsed: 2.030504816s
Sep  4 15:42:40.376: INFO: The phase of Pod pod-projected-configmaps-ce012741-ac15-401e-8bf5-6b3e853e268e is Running (Ready = true)
Sep  4 15:42:40.376: INFO: Pod "pod-projected-configmaps-ce012741-ac15-401e-8bf5-6b3e853e268e" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-5eae6fa8-5443-4028-acae-6f65b372e0b1 09/04/23 15:42:40.427
STEP: waiting to observe update in volume 09/04/23 15:42:40.443
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:42:42.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-191" for this suite. 09/04/23 15:42:42.597
------------------------------
• [4.423 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:42:38.19
    Sep  4 15:42:38.190: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:42:38.191
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:38.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:38.265
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-5eae6fa8-5443-4028-acae-6f65b372e0b1 09/04/23 15:42:38.308
    STEP: Creating the pod 09/04/23 15:42:38.322
    Sep  4 15:42:38.346: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ce012741-ac15-401e-8bf5-6b3e853e268e" in namespace "projected-191" to be "running and ready"
    Sep  4 15:42:38.360: INFO: Pod "pod-projected-configmaps-ce012741-ac15-401e-8bf5-6b3e853e268e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.234357ms
    Sep  4 15:42:38.360: INFO: The phase of Pod pod-projected-configmaps-ce012741-ac15-401e-8bf5-6b3e853e268e is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:42:40.376: INFO: Pod "pod-projected-configmaps-ce012741-ac15-401e-8bf5-6b3e853e268e": Phase="Running", Reason="", readiness=true. Elapsed: 2.030504816s
    Sep  4 15:42:40.376: INFO: The phase of Pod pod-projected-configmaps-ce012741-ac15-401e-8bf5-6b3e853e268e is Running (Ready = true)
    Sep  4 15:42:40.376: INFO: Pod "pod-projected-configmaps-ce012741-ac15-401e-8bf5-6b3e853e268e" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-5eae6fa8-5443-4028-acae-6f65b372e0b1 09/04/23 15:42:40.427
    STEP: waiting to observe update in volume 09/04/23 15:42:40.443
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:42:42.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-191" for this suite. 09/04/23 15:42:42.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:42:42.618
Sep  4 15:42:42.618: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 15:42:42.618
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:42.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:42.691
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-0d92bf2e-cf09-480f-b7bf-77b9d5387468 09/04/23 15:42:42.719
STEP: Creating a pod to test consume configMaps 09/04/23 15:42:42.734
Sep  4 15:42:42.758: INFO: Waiting up to 5m0s for pod "pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553" in namespace "configmap-4826" to be "Succeeded or Failed"
Sep  4 15:42:42.772: INFO: Pod "pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553": Phase="Pending", Reason="", readiness=false. Elapsed: 14.193774ms
Sep  4 15:42:44.790: INFO: Pod "pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03140632s
Sep  4 15:42:46.790: INFO: Pod "pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03162818s
STEP: Saw pod success 09/04/23 15:42:46.79
Sep  4 15:42:46.790: INFO: Pod "pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553" satisfied condition "Succeeded or Failed"
Sep  4 15:42:46.805: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 15:42:46.841
Sep  4 15:42:46.861: INFO: Waiting for pod pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553 to disappear
Sep  4 15:42:46.875: INFO: Pod pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:42:46.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4826" for this suite. 09/04/23 15:42:46.905
------------------------------
• [4.305 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:42:42.618
    Sep  4 15:42:42.618: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 15:42:42.618
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:42.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:42.691
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-0d92bf2e-cf09-480f-b7bf-77b9d5387468 09/04/23 15:42:42.719
    STEP: Creating a pod to test consume configMaps 09/04/23 15:42:42.734
    Sep  4 15:42:42.758: INFO: Waiting up to 5m0s for pod "pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553" in namespace "configmap-4826" to be "Succeeded or Failed"
    Sep  4 15:42:42.772: INFO: Pod "pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553": Phase="Pending", Reason="", readiness=false. Elapsed: 14.193774ms
    Sep  4 15:42:44.790: INFO: Pod "pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03140632s
    Sep  4 15:42:46.790: INFO: Pod "pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03162818s
    STEP: Saw pod success 09/04/23 15:42:46.79
    Sep  4 15:42:46.790: INFO: Pod "pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553" satisfied condition "Succeeded or Failed"
    Sep  4 15:42:46.805: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 15:42:46.841
    Sep  4 15:42:46.861: INFO: Waiting for pod pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553 to disappear
    Sep  4 15:42:46.875: INFO: Pod pod-configmaps-16cc32f8-64c4-4194-8ee8-927197745553 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:42:46.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4826" for this suite. 09/04/23 15:42:46.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:42:46.923
Sep  4 15:42:46.923: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 09/04/23 15:42:46.924
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:46.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:46.995
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Sep  4 15:42:47.022: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/04/23 15:42:47.052
STEP: Checking rc "condition-test" has the desired failure condition set 09/04/23 15:42:47.068
STEP: Scaling down rc "condition-test" to satisfy pod quota 09/04/23 15:42:48.098
Sep  4 15:42:48.129: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 09/04/23 15:42:48.129
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  4 15:42:48.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-900" for this suite. 09/04/23 15:42:48.173
------------------------------
• [1.266 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:42:46.923
    Sep  4 15:42:46.923: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 09/04/23 15:42:46.924
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:46.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:46.995
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Sep  4 15:42:47.022: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/04/23 15:42:47.052
    STEP: Checking rc "condition-test" has the desired failure condition set 09/04/23 15:42:47.068
    STEP: Scaling down rc "condition-test" to satisfy pod quota 09/04/23 15:42:48.098
    Sep  4 15:42:48.129: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 09/04/23 15:42:48.129
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:42:48.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-900" for this suite. 09/04/23 15:42:48.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:42:48.189
Sep  4 15:42:48.189: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook 09/04/23 15:42:48.19
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:48.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:48.262
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/04/23 15:42:48.289
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/04/23 15:42:48.714
STEP: Deploying the custom resource conversion webhook pod 09/04/23 15:42:48.729
STEP: Wait for the deployment to be ready 09/04/23 15:42:48.76
Sep  4 15:42:48.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 42, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 42, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 42, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 42, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 15:42:50.818
STEP: Verifying the service has paired with the endpoint 09/04/23 15:42:50.837
Sep  4 15:42:51.838: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Sep  4 15:42:51.853: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource 09/04/23 15:42:54.292
STEP: v2 custom resource should be converted 09/04/23 15:42:54.311
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:42:54.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-3205" for this suite. 09/04/23 15:42:55.03
------------------------------
• [6.856 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:42:48.189
    Sep  4 15:42:48.189: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-webhook 09/04/23 15:42:48.19
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:48.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:48.262
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/04/23 15:42:48.289
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/04/23 15:42:48.714
    STEP: Deploying the custom resource conversion webhook pod 09/04/23 15:42:48.729
    STEP: Wait for the deployment to be ready 09/04/23 15:42:48.76
    Sep  4 15:42:48.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 42, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 42, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 42, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 42, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 15:42:50.818
    STEP: Verifying the service has paired with the endpoint 09/04/23 15:42:50.837
    Sep  4 15:42:51.838: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Sep  4 15:42:51.853: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Creating a v1 custom resource 09/04/23 15:42:54.292
    STEP: v2 custom resource should be converted 09/04/23 15:42:54.311
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:42:54.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-3205" for this suite. 09/04/23 15:42:55.03
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:42:55.045
Sep  4 15:42:55.045: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 09/04/23 15:42:55.046
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:55.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:55.115
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Sep  4 15:42:55.186: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/04/23 15:42:55.186
Sep  4 15:42:55.186: INFO: Waiting up to 5m0s for pod "test-rs-skhgb" in namespace "replicaset-5629" to be "running"
Sep  4 15:42:55.200: INFO: Pod "test-rs-skhgb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.799521ms
Sep  4 15:42:57.217: INFO: Pod "test-rs-skhgb": Phase="Running", Reason="", readiness=true. Elapsed: 2.030222261s
Sep  4 15:42:57.217: INFO: Pod "test-rs-skhgb" satisfied condition "running"
STEP: Scaling up "test-rs" replicaset  09/04/23 15:42:57.217
Sep  4 15:42:57.248: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 09/04/23 15:42:57.248
W0904 15:42:57.265858    7754 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  4 15:42:57.279: INFO: observed ReplicaSet test-rs in namespace replicaset-5629 with ReadyReplicas 1, AvailableReplicas 1
Sep  4 15:42:57.279: INFO: observed ReplicaSet test-rs in namespace replicaset-5629 with ReadyReplicas 1, AvailableReplicas 1
Sep  4 15:42:57.279: INFO: observed ReplicaSet test-rs in namespace replicaset-5629 with ReadyReplicas 1, AvailableReplicas 1
Sep  4 15:42:58.815: INFO: observed ReplicaSet test-rs in namespace replicaset-5629 with ReadyReplicas 2, AvailableReplicas 2
Sep  4 15:42:58.978: INFO: observed Replicaset test-rs in namespace replicaset-5629 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  4 15:42:58.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5629" for this suite. 09/04/23 15:42:59.005
------------------------------
• [3.975 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:42:55.045
    Sep  4 15:42:55.045: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 09/04/23 15:42:55.046
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:55.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:55.115
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Sep  4 15:42:55.186: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/04/23 15:42:55.186
    Sep  4 15:42:55.186: INFO: Waiting up to 5m0s for pod "test-rs-skhgb" in namespace "replicaset-5629" to be "running"
    Sep  4 15:42:55.200: INFO: Pod "test-rs-skhgb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.799521ms
    Sep  4 15:42:57.217: INFO: Pod "test-rs-skhgb": Phase="Running", Reason="", readiness=true. Elapsed: 2.030222261s
    Sep  4 15:42:57.217: INFO: Pod "test-rs-skhgb" satisfied condition "running"
    STEP: Scaling up "test-rs" replicaset  09/04/23 15:42:57.217
    Sep  4 15:42:57.248: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 09/04/23 15:42:57.248
    W0904 15:42:57.265858    7754 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  4 15:42:57.279: INFO: observed ReplicaSet test-rs in namespace replicaset-5629 with ReadyReplicas 1, AvailableReplicas 1
    Sep  4 15:42:57.279: INFO: observed ReplicaSet test-rs in namespace replicaset-5629 with ReadyReplicas 1, AvailableReplicas 1
    Sep  4 15:42:57.279: INFO: observed ReplicaSet test-rs in namespace replicaset-5629 with ReadyReplicas 1, AvailableReplicas 1
    Sep  4 15:42:58.815: INFO: observed ReplicaSet test-rs in namespace replicaset-5629 with ReadyReplicas 2, AvailableReplicas 2
    Sep  4 15:42:58.978: INFO: observed Replicaset test-rs in namespace replicaset-5629 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:42:58.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5629" for this suite. 09/04/23 15:42:59.005
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:42:59.021
Sep  4 15:42:59.021: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 15:42:59.022
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:59.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:59.092
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 15:42:59.119
Sep  4 15:42:59.119: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4455 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep  4 15:42:59.462: INFO: stderr: ""
Sep  4 15:42:59.462: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 09/04/23 15:42:59.462
STEP: verifying the pod e2e-test-httpd-pod was created 09/04/23 15:43:04.535
Sep  4 15:43:04.535: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4455 get pod e2e-test-httpd-pod -o json'
Sep  4 15:43:04.877: INFO: stderr: ""
Sep  4 15:43:04.877: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"61e43ddfa42d98dc1b528f4500100fc26fb8c2405dadcd914ab1cf8921b87c4d\",\n            \"cni.projectcalico.org/podIP\": \"100.64.1.71/32\",\n            \"cni.projectcalico.org/podIPs\": \"100.64.1.71/32\"\n        },\n        \"creationTimestamp\": \"2023-09-04T15:42:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4455\",\n        \"resourceVersion\": \"32623\",\n        \"uid\": \"2f981320-4f6a-46f7-9e8e-9f36767223d9\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBERNETES_SERVICE_HOST\",\n                        \"value\": \"api.tmud5-dd2.it.internal.staging.k8s.ondemand.com\"\n                    }\n                ],\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-wc4nh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-wc4nh\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T15:42:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T15:43:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T15:43:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T15:42:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://2abd92ec982b5473e36c85e983787158d9a536395fd461620d8bab2a25c37fe9\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-09-04T15:43:00Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.1.231\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.64.1.71\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.64.1.71\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-09-04T15:42:59Z\"\n    }\n}\n"
STEP: replace the image in the pod 09/04/23 15:43:04.877
Sep  4 15:43:04.877: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4455 replace -f -'
Sep  4 15:43:05.949: INFO: stderr: ""
Sep  4 15:43:05.949: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 09/04/23 15:43:05.949
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Sep  4 15:43:05.964: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4455 delete pods e2e-test-httpd-pod'
Sep  4 15:43:08.019: INFO: stderr: ""
Sep  4 15:43:08.019: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 15:43:08.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4455" for this suite. 09/04/23 15:43:08.047
------------------------------
• [9.042 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:42:59.021
    Sep  4 15:42:59.021: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 15:42:59.022
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:42:59.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:42:59.092
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/04/23 15:42:59.119
    Sep  4 15:42:59.119: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4455 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep  4 15:42:59.462: INFO: stderr: ""
    Sep  4 15:42:59.462: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 09/04/23 15:42:59.462
    STEP: verifying the pod e2e-test-httpd-pod was created 09/04/23 15:43:04.535
    Sep  4 15:43:04.535: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4455 get pod e2e-test-httpd-pod -o json'
    Sep  4 15:43:04.877: INFO: stderr: ""
    Sep  4 15:43:04.877: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"61e43ddfa42d98dc1b528f4500100fc26fb8c2405dadcd914ab1cf8921b87c4d\",\n            \"cni.projectcalico.org/podIP\": \"100.64.1.71/32\",\n            \"cni.projectcalico.org/podIPs\": \"100.64.1.71/32\"\n        },\n        \"creationTimestamp\": \"2023-09-04T15:42:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4455\",\n        \"resourceVersion\": \"32623\",\n        \"uid\": \"2f981320-4f6a-46f7-9e8e-9f36767223d9\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBERNETES_SERVICE_HOST\",\n                        \"value\": \"api.tmud5-dd2.it.internal.staging.k8s.ondemand.com\"\n                    }\n                ],\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-wc4nh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-wc4nh\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T15:42:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T15:43:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T15:43:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-04T15:42:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://2abd92ec982b5473e36c85e983787158d9a536395fd461620d8bab2a25c37fe9\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-09-04T15:43:00Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.1.231\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.64.1.71\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.64.1.71\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-09-04T15:42:59Z\"\n    }\n}\n"
    STEP: replace the image in the pod 09/04/23 15:43:04.877
    Sep  4 15:43:04.877: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4455 replace -f -'
    Sep  4 15:43:05.949: INFO: stderr: ""
    Sep  4 15:43:05.949: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 09/04/23 15:43:05.949
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Sep  4 15:43:05.964: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4455 delete pods e2e-test-httpd-pod'
    Sep  4 15:43:08.019: INFO: stderr: ""
    Sep  4 15:43:08.019: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:43:08.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4455" for this suite. 09/04/23 15:43:08.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:43:08.063
Sep  4 15:43:08.063: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 09/04/23 15:43:08.064
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:43:08.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:43:08.135
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1348 09/04/23 15:43:08.162
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 09/04/23 15:43:08.178
Sep  4 15:43:08.209: INFO: Found 0 stateful pods, waiting for 3
Sep  4 15:43:18.226: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 15:43:18.227: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 15:43:18.227: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 15:43:18.271: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1348 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 15:43:18.949: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 15:43:18.949: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 15:43:18.949: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/04/23 15:43:29.009
Sep  4 15:43:29.051: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/04/23 15:43:29.051
STEP: Updating Pods in reverse ordinal order 09/04/23 15:43:29.08
Sep  4 15:43:29.095: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1348 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 15:43:29.811: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  4 15:43:29.811: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 15:43:29.811: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 09/04/23 15:43:39.904
Sep  4 15:43:39.904: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1348 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  4 15:43:40.687: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  4 15:43:40.687: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  4 15:43:40.687: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  4 15:43:50.791: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 09/04/23 15:43:50.822
Sep  4 15:43:50.838: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1348 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  4 15:43:51.364: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  4 15:43:51.364: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  4 15:43:51.364: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 15:44:01.455: INFO: Deleting all statefulset in ns statefulset-1348
Sep  4 15:44:01.469: INFO: Scaling statefulset ss2 to 0
Sep  4 15:44:11.531: INFO: Waiting for statefulset status.replicas updated to 0
Sep  4 15:44:11.546: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 15:44:11.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1348" for this suite. 09/04/23 15:44:11.617
------------------------------
• [63.569 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:43:08.063
    Sep  4 15:43:08.063: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 09/04/23 15:43:08.064
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:43:08.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:43:08.135
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1348 09/04/23 15:43:08.162
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 09/04/23 15:43:08.178
    Sep  4 15:43:08.209: INFO: Found 0 stateful pods, waiting for 3
    Sep  4 15:43:18.226: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 15:43:18.227: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 15:43:18.227: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 15:43:18.271: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1348 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 15:43:18.949: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 15:43:18.949: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 15:43:18.949: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/04/23 15:43:29.009
    Sep  4 15:43:29.051: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/04/23 15:43:29.051
    STEP: Updating Pods in reverse ordinal order 09/04/23 15:43:29.08
    Sep  4 15:43:29.095: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1348 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 15:43:29.811: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  4 15:43:29.811: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 15:43:29.811: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 09/04/23 15:43:39.904
    Sep  4 15:43:39.904: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1348 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  4 15:43:40.687: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  4 15:43:40.687: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  4 15:43:40.687: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  4 15:43:50.791: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 09/04/23 15:43:50.822
    Sep  4 15:43:50.838: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1348 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  4 15:43:51.364: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  4 15:43:51.364: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  4 15:43:51.364: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 15:44:01.455: INFO: Deleting all statefulset in ns statefulset-1348
    Sep  4 15:44:01.469: INFO: Scaling statefulset ss2 to 0
    Sep  4 15:44:11.531: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  4 15:44:11.546: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:44:11.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1348" for this suite. 09/04/23 15:44:11.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:44:11.633
Sep  4 15:44:11.633: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 09/04/23 15:44:11.634
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:44:11.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:44:11.704
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 09/04/23 15:44:11.731
STEP: Wait for the Deployment to create new ReplicaSet 09/04/23 15:44:11.746
STEP: delete the deployment 09/04/23 15:44:11.775
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/04/23 15:44:11.791
STEP: Gathering metrics 09/04/23 15:44:12.378
W0904 15:44:12.411853    7754 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep  4 15:44:12.411: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  4 15:44:12.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9405" for this suite. 09/04/23 15:44:12.426
------------------------------
• [0.808 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:44:11.633
    Sep  4 15:44:11.633: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 09/04/23 15:44:11.634
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:44:11.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:44:11.704
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 09/04/23 15:44:11.731
    STEP: Wait for the Deployment to create new ReplicaSet 09/04/23 15:44:11.746
    STEP: delete the deployment 09/04/23 15:44:11.775
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/04/23 15:44:11.791
    STEP: Gathering metrics 09/04/23 15:44:12.378
    W0904 15:44:12.411853    7754 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep  4 15:44:12.411: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:44:12.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9405" for this suite. 09/04/23 15:44:12.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:44:12.442
Sep  4 15:44:12.442: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 09/04/23 15:44:12.443
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:44:12.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:44:12.513
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 09/04/23 15:44:12.54
STEP: Creating a ResourceQuota 09/04/23 15:44:17.554
STEP: Ensuring resource quota status is calculated 09/04/23 15:44:17.569
STEP: Creating a Pod that fits quota 09/04/23 15:44:19.585
STEP: Ensuring ResourceQuota status captures the pod usage 09/04/23 15:44:19.609
STEP: Not allowing a pod to be created that exceeds remaining quota 09/04/23 15:44:21.625
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/04/23 15:44:21.646
STEP: Ensuring a pod cannot update its resource requirements 09/04/23 15:44:21.664
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/04/23 15:44:21.68
STEP: Deleting the pod 09/04/23 15:44:23.696
STEP: Ensuring resource quota status released the pod usage 09/04/23 15:44:23.716
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 15:44:25.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1865" for this suite. 09/04/23 15:44:25.758
------------------------------
• [13.332 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:44:12.442
    Sep  4 15:44:12.442: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 09/04/23 15:44:12.443
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:44:12.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:44:12.513
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 09/04/23 15:44:12.54
    STEP: Creating a ResourceQuota 09/04/23 15:44:17.554
    STEP: Ensuring resource quota status is calculated 09/04/23 15:44:17.569
    STEP: Creating a Pod that fits quota 09/04/23 15:44:19.585
    STEP: Ensuring ResourceQuota status captures the pod usage 09/04/23 15:44:19.609
    STEP: Not allowing a pod to be created that exceeds remaining quota 09/04/23 15:44:21.625
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/04/23 15:44:21.646
    STEP: Ensuring a pod cannot update its resource requirements 09/04/23 15:44:21.664
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/04/23 15:44:21.68
    STEP: Deleting the pod 09/04/23 15:44:23.696
    STEP: Ensuring resource quota status released the pod usage 09/04/23 15:44:23.716
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:44:25.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1865" for this suite. 09/04/23 15:44:25.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:44:25.775
Sep  4 15:44:25.775: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 15:44:25.775
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:44:25.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:44:25.846
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-04115a0c-55b2-429f-94bf-491497a9236e 09/04/23 15:44:25.874
STEP: Creating a pod to test consume configMaps 09/04/23 15:44:25.888
Sep  4 15:44:25.909: INFO: Waiting up to 5m0s for pod "pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70" in namespace "configmap-2299" to be "Succeeded or Failed"
Sep  4 15:44:25.924: INFO: Pod "pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70": Phase="Pending", Reason="", readiness=false. Elapsed: 14.222683ms
Sep  4 15:44:27.939: INFO: Pod "pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029858949s
Sep  4 15:44:29.940: INFO: Pod "pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030844264s
STEP: Saw pod success 09/04/23 15:44:29.94
Sep  4 15:44:29.940: INFO: Pod "pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70" satisfied condition "Succeeded or Failed"
Sep  4 15:44:29.955: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 15:44:29.991
Sep  4 15:44:30.010: INFO: Waiting for pod pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70 to disappear
Sep  4 15:44:30.024: INFO: Pod pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:44:30.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2299" for this suite. 09/04/23 15:44:30.052
------------------------------
• [4.293 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:44:25.775
    Sep  4 15:44:25.775: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 15:44:25.775
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:44:25.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:44:25.846
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-04115a0c-55b2-429f-94bf-491497a9236e 09/04/23 15:44:25.874
    STEP: Creating a pod to test consume configMaps 09/04/23 15:44:25.888
    Sep  4 15:44:25.909: INFO: Waiting up to 5m0s for pod "pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70" in namespace "configmap-2299" to be "Succeeded or Failed"
    Sep  4 15:44:25.924: INFO: Pod "pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70": Phase="Pending", Reason="", readiness=false. Elapsed: 14.222683ms
    Sep  4 15:44:27.939: INFO: Pod "pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029858949s
    Sep  4 15:44:29.940: INFO: Pod "pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030844264s
    STEP: Saw pod success 09/04/23 15:44:29.94
    Sep  4 15:44:29.940: INFO: Pod "pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70" satisfied condition "Succeeded or Failed"
    Sep  4 15:44:29.955: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 15:44:29.991
    Sep  4 15:44:30.010: INFO: Waiting for pod pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70 to disappear
    Sep  4 15:44:30.024: INFO: Pod pod-configmaps-79dc147d-6062-4f17-bef1-00de8e4c7b70 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:44:30.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2299" for this suite. 09/04/23 15:44:30.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:44:30.068
Sep  4 15:44:30.068: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 09/04/23 15:44:30.069
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:44:30.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:44:30.141
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 09/04/23 15:44:30.167
STEP: Counting existing ResourceQuota 09/04/23 15:44:35.182
STEP: Creating a ResourceQuota 09/04/23 15:44:40.197
STEP: Ensuring resource quota status is calculated 09/04/23 15:44:40.212
STEP: Creating a Secret 09/04/23 15:44:42.228
STEP: Ensuring resource quota status captures secret creation 09/04/23 15:44:42.246
STEP: Deleting a secret 09/04/23 15:44:44.262
STEP: Ensuring resource quota status released usage 09/04/23 15:44:44.278
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 15:44:46.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4719" for this suite. 09/04/23 15:44:46.321
------------------------------
• [16.268 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:44:30.068
    Sep  4 15:44:30.068: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 09/04/23 15:44:30.069
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:44:30.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:44:30.141
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 09/04/23 15:44:30.167
    STEP: Counting existing ResourceQuota 09/04/23 15:44:35.182
    STEP: Creating a ResourceQuota 09/04/23 15:44:40.197
    STEP: Ensuring resource quota status is calculated 09/04/23 15:44:40.212
    STEP: Creating a Secret 09/04/23 15:44:42.228
    STEP: Ensuring resource quota status captures secret creation 09/04/23 15:44:42.246
    STEP: Deleting a secret 09/04/23 15:44:44.262
    STEP: Ensuring resource quota status released usage 09/04/23 15:44:44.278
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:44:46.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4719" for this suite. 09/04/23 15:44:46.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:44:46.337
Sep  4 15:44:46.337: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 09/04/23 15:44:46.338
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:44:46.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:44:46.408
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 09/04/23 15:44:46.434
STEP: wait for the container to reach Succeeded 09/04/23 15:44:46.455
STEP: get the container status 09/04/23 15:44:49.514
STEP: the container should be terminated 09/04/23 15:44:49.529
STEP: the termination message should be set 09/04/23 15:44:49.529
Sep  4 15:44:49.529: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/04/23 15:44:49.529
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  4 15:44:49.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-10" for this suite. 09/04/23 15:44:49.59
------------------------------
• [3.270 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:44:46.337
    Sep  4 15:44:46.337: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 09/04/23 15:44:46.338
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:44:46.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:44:46.408
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 09/04/23 15:44:46.434
    STEP: wait for the container to reach Succeeded 09/04/23 15:44:46.455
    STEP: get the container status 09/04/23 15:44:49.514
    STEP: the container should be terminated 09/04/23 15:44:49.529
    STEP: the termination message should be set 09/04/23 15:44:49.529
    Sep  4 15:44:49.529: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/04/23 15:44:49.529
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:44:49.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-10" for this suite. 09/04/23 15:44:49.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:44:49.607
Sep  4 15:44:49.608: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 09/04/23 15:44:49.608
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:44:49.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:44:49.68
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Sep  4 15:44:49.729: INFO: Waiting up to 2m0s for pod "var-expansion-c8c68fa1-3af9-4a2f-a88d-cb755404780a" in namespace "var-expansion-9209" to be "container 0 failed with reason CreateContainerConfigError"
Sep  4 15:44:49.743: INFO: Pod "var-expansion-c8c68fa1-3af9-4a2f-a88d-cb755404780a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.162082ms
Sep  4 15:44:51.760: INFO: Pod "var-expansion-c8c68fa1-3af9-4a2f-a88d-cb755404780a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030430238s
Sep  4 15:44:51.760: INFO: Pod "var-expansion-c8c68fa1-3af9-4a2f-a88d-cb755404780a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep  4 15:44:51.760: INFO: Deleting pod "var-expansion-c8c68fa1-3af9-4a2f-a88d-cb755404780a" in namespace "var-expansion-9209"
Sep  4 15:44:51.775: INFO: Wait up to 5m0s for pod "var-expansion-c8c68fa1-3af9-4a2f-a88d-cb755404780a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 15:44:55.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9209" for this suite. 09/04/23 15:44:55.841
------------------------------
• [6.249 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:44:49.607
    Sep  4 15:44:49.608: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 09/04/23 15:44:49.608
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:44:49.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:44:49.68
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Sep  4 15:44:49.729: INFO: Waiting up to 2m0s for pod "var-expansion-c8c68fa1-3af9-4a2f-a88d-cb755404780a" in namespace "var-expansion-9209" to be "container 0 failed with reason CreateContainerConfigError"
    Sep  4 15:44:49.743: INFO: Pod "var-expansion-c8c68fa1-3af9-4a2f-a88d-cb755404780a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.162082ms
    Sep  4 15:44:51.760: INFO: Pod "var-expansion-c8c68fa1-3af9-4a2f-a88d-cb755404780a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030430238s
    Sep  4 15:44:51.760: INFO: Pod "var-expansion-c8c68fa1-3af9-4a2f-a88d-cb755404780a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep  4 15:44:51.760: INFO: Deleting pod "var-expansion-c8c68fa1-3af9-4a2f-a88d-cb755404780a" in namespace "var-expansion-9209"
    Sep  4 15:44:51.775: INFO: Wait up to 5m0s for pod "var-expansion-c8c68fa1-3af9-4a2f-a88d-cb755404780a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:44:55.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9209" for this suite. 09/04/23 15:44:55.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:44:55.857
Sep  4 15:44:55.857: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 09/04/23 15:44:55.858
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:44:55.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:44:55.936
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-31421a39-73ff-4007-a216-ffcd73b638d2 09/04/23 15:44:55.963
STEP: Creating a pod to test consume secrets 09/04/23 15:44:55.979
Sep  4 15:44:56.002: INFO: Waiting up to 5m0s for pod "pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668" in namespace "secrets-541" to be "Succeeded or Failed"
Sep  4 15:44:56.016: INFO: Pod "pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668": Phase="Pending", Reason="", readiness=false. Elapsed: 14.338259ms
Sep  4 15:44:58.032: INFO: Pod "pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03010195s
Sep  4 15:45:00.032: INFO: Pod "pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030677375s
STEP: Saw pod success 09/04/23 15:45:00.032
Sep  4 15:45:00.032: INFO: Pod "pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668" satisfied condition "Succeeded or Failed"
Sep  4 15:45:00.047: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668 container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 15:45:00.126
Sep  4 15:45:00.145: INFO: Waiting for pod pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668 to disappear
Sep  4 15:45:00.159: INFO: Pod pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 15:45:00.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-541" for this suite. 09/04/23 15:45:00.187
------------------------------
• [4.345 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:44:55.857
    Sep  4 15:44:55.857: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 09/04/23 15:44:55.858
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:44:55.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:44:55.936
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-31421a39-73ff-4007-a216-ffcd73b638d2 09/04/23 15:44:55.963
    STEP: Creating a pod to test consume secrets 09/04/23 15:44:55.979
    Sep  4 15:44:56.002: INFO: Waiting up to 5m0s for pod "pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668" in namespace "secrets-541" to be "Succeeded or Failed"
    Sep  4 15:44:56.016: INFO: Pod "pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668": Phase="Pending", Reason="", readiness=false. Elapsed: 14.338259ms
    Sep  4 15:44:58.032: INFO: Pod "pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03010195s
    Sep  4 15:45:00.032: INFO: Pod "pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030677375s
    STEP: Saw pod success 09/04/23 15:45:00.032
    Sep  4 15:45:00.032: INFO: Pod "pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668" satisfied condition "Succeeded or Failed"
    Sep  4 15:45:00.047: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668 container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 15:45:00.126
    Sep  4 15:45:00.145: INFO: Waiting for pod pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668 to disappear
    Sep  4 15:45:00.159: INFO: Pod pod-secrets-dbb1a75a-ec20-4b9b-8895-ca9339253668 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:45:00.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-541" for this suite. 09/04/23 15:45:00.187
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:45:00.203
Sep  4 15:45:00.203: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 15:45:00.204
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:00.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:00.277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 15:45:00.338
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:45:00.635
STEP: Deploying the webhook pod 09/04/23 15:45:00.651
STEP: Wait for the deployment to be ready 09/04/23 15:45:00.683
Sep  4 15:45:00.729: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 45, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 45, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 45, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 45, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 15:45:02.744
STEP: Verifying the service has paired with the endpoint 09/04/23 15:45:02.763
Sep  4 15:45:03.764: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 09/04/23 15:45:03.779
STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 15:45:03.924
STEP: Updating a validating webhook configuration's rules to not include the create operation 09/04/23 15:45:04.051
STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 15:45:04.084
STEP: Patching a validating webhook configuration's rules to include the create operation 09/04/23 15:45:04.115
STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 15:45:04.132
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:45:04.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2321" for this suite. 09/04/23 15:45:04.286
STEP: Destroying namespace "webhook-2321-markers" for this suite. 09/04/23 15:45:04.301
------------------------------
• [4.117 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:45:00.203
    Sep  4 15:45:00.203: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 15:45:00.204
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:00.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:00.277
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 15:45:00.338
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:45:00.635
    STEP: Deploying the webhook pod 09/04/23 15:45:00.651
    STEP: Wait for the deployment to be ready 09/04/23 15:45:00.683
    Sep  4 15:45:00.729: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 45, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 45, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 45, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 45, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 15:45:02.744
    STEP: Verifying the service has paired with the endpoint 09/04/23 15:45:02.763
    Sep  4 15:45:03.764: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 09/04/23 15:45:03.779
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 15:45:03.924
    STEP: Updating a validating webhook configuration's rules to not include the create operation 09/04/23 15:45:04.051
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 15:45:04.084
    STEP: Patching a validating webhook configuration's rules to include the create operation 09/04/23 15:45:04.115
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 15:45:04.132
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:45:04.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2321" for this suite. 09/04/23 15:45:04.286
    STEP: Destroying namespace "webhook-2321-markers" for this suite. 09/04/23 15:45:04.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:45:04.32
Sep  4 15:45:04.320: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 15:45:04.321
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:04.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:04.396
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 15:45:04.465
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:45:04.949
STEP: Deploying the webhook pod 09/04/23 15:45:04.964
STEP: Wait for the deployment to be ready 09/04/23 15:45:04.996
Sep  4 15:45:05.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 45, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 45, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 45, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 45, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 15:45:07.055
STEP: Verifying the service has paired with the endpoint 09/04/23 15:45:07.076
Sep  4 15:45:08.076: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Sep  4 15:45:08.092: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6146-crds.webhook.example.com via the AdmissionRegistration API 09/04/23 15:45:08.137
STEP: Creating a custom resource that should be mutated by the webhook 09/04/23 15:45:08.277
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:45:11.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4075" for this suite. 09/04/23 15:45:11.148
STEP: Destroying namespace "webhook-4075-markers" for this suite. 09/04/23 15:45:11.164
------------------------------
• [6.860 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:45:04.32
    Sep  4 15:45:04.320: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 15:45:04.321
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:04.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:04.396
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 15:45:04.465
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:45:04.949
    STEP: Deploying the webhook pod 09/04/23 15:45:04.964
    STEP: Wait for the deployment to be ready 09/04/23 15:45:04.996
    Sep  4 15:45:05.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 45, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 45, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 45, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 45, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 15:45:07.055
    STEP: Verifying the service has paired with the endpoint 09/04/23 15:45:07.076
    Sep  4 15:45:08.076: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Sep  4 15:45:08.092: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6146-crds.webhook.example.com via the AdmissionRegistration API 09/04/23 15:45:08.137
    STEP: Creating a custom resource that should be mutated by the webhook 09/04/23 15:45:08.277
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:45:11.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4075" for this suite. 09/04/23 15:45:11.148
    STEP: Destroying namespace "webhook-4075-markers" for this suite. 09/04/23 15:45:11.164
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:45:11.18
Sep  4 15:45:11.180: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 09/04/23 15:45:11.181
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:11.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:11.253
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5293 09/04/23 15:45:11.281
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Sep  4 15:45:11.327: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  4 15:45:21.343: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 09/04/23 15:45:21.373
W0904 15:45:21.389703    7754 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  4 15:45:21.418: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 15:45:21.418: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Sep  4 15:45:31.434: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  4 15:45:31.434: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 09/04/23 15:45:31.463
STEP: Delete all of the StatefulSets 09/04/23 15:45:31.477
STEP: Verify that StatefulSets have been deleted 09/04/23 15:45:31.493
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  4 15:45:31.507: INFO: Deleting all statefulset in ns statefulset-5293
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  4 15:45:31.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5293" for this suite. 09/04/23 15:45:31.576
------------------------------
• [20.412 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:45:11.18
    Sep  4 15:45:11.180: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 09/04/23 15:45:11.181
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:11.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:11.253
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5293 09/04/23 15:45:11.281
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Sep  4 15:45:11.327: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
    Sep  4 15:45:21.343: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 09/04/23 15:45:21.373
    W0904 15:45:21.389703    7754 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  4 15:45:21.418: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 15:45:21.418: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Sep  4 15:45:31.434: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  4 15:45:31.434: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 09/04/23 15:45:31.463
    STEP: Delete all of the StatefulSets 09/04/23 15:45:31.477
    STEP: Verify that StatefulSets have been deleted 09/04/23 15:45:31.493
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  4 15:45:31.507: INFO: Deleting all statefulset in ns statefulset-5293
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:45:31.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5293" for this suite. 09/04/23 15:45:31.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:45:31.593
Sep  4 15:45:31.593: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 09/04/23 15:45:31.594
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:31.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:31.665
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Sep  4 15:45:31.728: INFO: Waiting up to 5m0s for pod "pod-service-account-22293ccf-c866-469a-a99e-1f4ac36a71a6" in namespace "svcaccounts-9236" to be "running"
Sep  4 15:45:31.742: INFO: Pod "pod-service-account-22293ccf-c866-469a-a99e-1f4ac36a71a6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.214915ms
Sep  4 15:45:33.758: INFO: Pod "pod-service-account-22293ccf-c866-469a-a99e-1f4ac36a71a6": Phase="Running", Reason="", readiness=true. Elapsed: 2.029642217s
Sep  4 15:45:33.758: INFO: Pod "pod-service-account-22293ccf-c866-469a-a99e-1f4ac36a71a6" satisfied condition "running"
STEP: reading a file in the container 09/04/23 15:45:33.758
Sep  4 15:45:33.758: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-9236 pod-service-account-22293ccf-c866-469a-a99e-1f4ac36a71a6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 09/04/23 15:45:34.389
Sep  4 15:45:34.389: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-9236 pod-service-account-22293ccf-c866-469a-a99e-1f4ac36a71a6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 09/04/23 15:45:35.114
Sep  4 15:45:35.114: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-9236 pod-service-account-22293ccf-c866-469a-a99e-1f4ac36a71a6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Sep  4 15:45:35.738: INFO: Got root ca configmap in namespace "svcaccounts-9236"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  4 15:45:35.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9236" for this suite. 09/04/23 15:45:35.781
------------------------------
• [4.204 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:45:31.593
    Sep  4 15:45:31.593: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 09/04/23 15:45:31.594
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:31.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:31.665
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Sep  4 15:45:31.728: INFO: Waiting up to 5m0s for pod "pod-service-account-22293ccf-c866-469a-a99e-1f4ac36a71a6" in namespace "svcaccounts-9236" to be "running"
    Sep  4 15:45:31.742: INFO: Pod "pod-service-account-22293ccf-c866-469a-a99e-1f4ac36a71a6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.214915ms
    Sep  4 15:45:33.758: INFO: Pod "pod-service-account-22293ccf-c866-469a-a99e-1f4ac36a71a6": Phase="Running", Reason="", readiness=true. Elapsed: 2.029642217s
    Sep  4 15:45:33.758: INFO: Pod "pod-service-account-22293ccf-c866-469a-a99e-1f4ac36a71a6" satisfied condition "running"
    STEP: reading a file in the container 09/04/23 15:45:33.758
    Sep  4 15:45:33.758: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-9236 pod-service-account-22293ccf-c866-469a-a99e-1f4ac36a71a6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 09/04/23 15:45:34.389
    Sep  4 15:45:34.389: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-9236 pod-service-account-22293ccf-c866-469a-a99e-1f4ac36a71a6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 09/04/23 15:45:35.114
    Sep  4 15:45:35.114: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-9236 pod-service-account-22293ccf-c866-469a-a99e-1f4ac36a71a6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Sep  4 15:45:35.738: INFO: Got root ca configmap in namespace "svcaccounts-9236"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:45:35.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9236" for this suite. 09/04/23 15:45:35.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:45:35.797
Sep  4 15:45:35.797: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 09/04/23 15:45:35.798
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:35.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:35.869
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 09/04/23 15:45:35.896
STEP: creating 09/04/23 15:45:35.896
STEP: getting 09/04/23 15:45:35.913
STEP: listing 09/04/23 15:45:35.931
STEP: watching 09/04/23 15:45:35.946
Sep  4 15:45:35.946: INFO: starting watch
STEP: cluster-wide listing 09/04/23 15:45:35.96
STEP: cluster-wide watching 09/04/23 15:45:35.974
Sep  4 15:45:35.974: INFO: starting watch
STEP: patching 09/04/23 15:45:35.988
STEP: updating 09/04/23 15:45:36.004
Sep  4 15:45:36.036: INFO: waiting for watch events with expected annotations
Sep  4 15:45:36.036: INFO: saw patched and updated annotations
STEP: patching /status 09/04/23 15:45:36.036
STEP: updating /status 09/04/23 15:45:36.053
STEP: get /status 09/04/23 15:45:36.084
STEP: deleting 09/04/23 15:45:36.099
STEP: deleting a collection 09/04/23 15:45:36.145
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  4 15:45:36.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9658" for this suite. 09/04/23 15:45:36.203
------------------------------
• [0.423 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:45:35.797
    Sep  4 15:45:35.797: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 09/04/23 15:45:35.798
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:35.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:35.869
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 09/04/23 15:45:35.896
    STEP: creating 09/04/23 15:45:35.896
    STEP: getting 09/04/23 15:45:35.913
    STEP: listing 09/04/23 15:45:35.931
    STEP: watching 09/04/23 15:45:35.946
    Sep  4 15:45:35.946: INFO: starting watch
    STEP: cluster-wide listing 09/04/23 15:45:35.96
    STEP: cluster-wide watching 09/04/23 15:45:35.974
    Sep  4 15:45:35.974: INFO: starting watch
    STEP: patching 09/04/23 15:45:35.988
    STEP: updating 09/04/23 15:45:36.004
    Sep  4 15:45:36.036: INFO: waiting for watch events with expected annotations
    Sep  4 15:45:36.036: INFO: saw patched and updated annotations
    STEP: patching /status 09/04/23 15:45:36.036
    STEP: updating /status 09/04/23 15:45:36.053
    STEP: get /status 09/04/23 15:45:36.084
    STEP: deleting 09/04/23 15:45:36.099
    STEP: deleting a collection 09/04/23 15:45:36.145
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:45:36.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9658" for this suite. 09/04/23 15:45:36.203
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:45:36.22
Sep  4 15:45:36.220: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 15:45:36.221
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:36.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:36.292
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 09/04/23 15:45:36.318
Sep  4 15:45:36.340: INFO: Waiting up to 5m0s for pod "pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68" in namespace "emptydir-7786" to be "Succeeded or Failed"
Sep  4 15:45:36.355: INFO: Pod "pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68": Phase="Pending", Reason="", readiness=false. Elapsed: 15.199057ms
Sep  4 15:45:38.371: INFO: Pod "pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030735359s
Sep  4 15:45:40.371: INFO: Pod "pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031284463s
STEP: Saw pod success 09/04/23 15:45:40.371
Sep  4 15:45:40.371: INFO: Pod "pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68" satisfied condition "Succeeded or Failed"
Sep  4 15:45:40.386: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68 container test-container: <nil>
STEP: delete the pod 09/04/23 15:45:40.461
Sep  4 15:45:40.481: INFO: Waiting for pod pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68 to disappear
Sep  4 15:45:40.496: INFO: Pod pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 15:45:40.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7786" for this suite. 09/04/23 15:45:40.523
------------------------------
• [4.319 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:45:36.22
    Sep  4 15:45:36.220: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 15:45:36.221
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:36.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:36.292
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/04/23 15:45:36.318
    Sep  4 15:45:36.340: INFO: Waiting up to 5m0s for pod "pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68" in namespace "emptydir-7786" to be "Succeeded or Failed"
    Sep  4 15:45:36.355: INFO: Pod "pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68": Phase="Pending", Reason="", readiness=false. Elapsed: 15.199057ms
    Sep  4 15:45:38.371: INFO: Pod "pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030735359s
    Sep  4 15:45:40.371: INFO: Pod "pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031284463s
    STEP: Saw pod success 09/04/23 15:45:40.371
    Sep  4 15:45:40.371: INFO: Pod "pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68" satisfied condition "Succeeded or Failed"
    Sep  4 15:45:40.386: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68 container test-container: <nil>
    STEP: delete the pod 09/04/23 15:45:40.461
    Sep  4 15:45:40.481: INFO: Waiting for pod pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68 to disappear
    Sep  4 15:45:40.496: INFO: Pod pod-a06e1b9c-c7f9-43b1-a68b-cae33c239b68 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:45:40.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7786" for this suite. 09/04/23 15:45:40.523
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:45:40.539
Sep  4 15:45:40.539: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 09/04/23 15:45:40.54
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:40.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:40.612
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Sep  4 15:45:40.639: INFO: Creating deployment "webserver-deployment"
Sep  4 15:45:40.654: INFO: Waiting for observed generation 1
Sep  4 15:45:40.669: INFO: Waiting for all required pods to come up
Sep  4 15:45:40.684: INFO: Pod name httpd: Found 4 pods out of 10
Sep  4 15:45:45.716: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 09/04/23 15:45:45.716
Sep  4 15:45:45.716: INFO: Waiting for deployment "webserver-deployment" to complete
Sep  4 15:45:45.746: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep  4 15:45:45.777: INFO: Updating deployment webserver-deployment
Sep  4 15:45:45.777: INFO: Waiting for observed generation 2
Sep  4 15:45:47.808: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  4 15:45:47.823: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  4 15:45:47.837: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  4 15:45:47.881: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  4 15:45:47.881: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  4 15:45:47.895: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  4 15:45:47.930: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep  4 15:45:47.930: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep  4 15:45:47.961: INFO: Updating deployment webserver-deployment
Sep  4 15:45:47.961: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep  4 15:45:47.994: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  4 15:45:50.068: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 15:45:50.099: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1487  507eaa73-8264-4bc3-9f86-90595c0c6aaf 34454 3 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ddb418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:13,UnavailableReplicas:20,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-04 15:45:47 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-09-04 15:45:49 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,},},ReadyReplicas:13,CollisionCount:nil,},}

Sep  4 15:45:50.113: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-1487  4b9a070b-6622-480d-89b6-78a4f8435043 34401 3 2023-09-04 15:45:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 507eaa73-8264-4bc3-9f86-90595c0c6aaf 0xc0054f4457 0xc0054f4458}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"507eaa73-8264-4bc3-9f86-90595c0c6aaf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054f44f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  4 15:45:50.113: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep  4 15:45:50.113: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-1487  8e17ab1a-9ace-4870-a93b-203e3070494b 34453 3 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 507eaa73-8264-4bc3-9f86-90595c0c6aaf 0xc0054f4367 0xc0054f4368}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"507eaa73-8264-4bc3-9f86-90595c0c6aaf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054f43f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:13,AvailableReplicas:13,Conditions:[]ReplicaSetCondition{},},}
Sep  4 15:45:50.238: INFO: Pod "webserver-deployment-7f5969cbc7-24tg7" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-24tg7 webserver-deployment-7f5969cbc7- deployment-1487  baa46aba-541d-4e38-8138-99cda4e4043d 34432 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:90af31cd22eba51d1ff7a0ad056fffe1c597a5d3336b1117d183db50cd6fa6cf cni.projectcalico.org/podIP:100.64.0.193/32 cni.projectcalico.org/podIPs:100.64.0.193/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f49f7 0xc0054f49f8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nlbxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nlbxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.239: INFO: Pod "webserver-deployment-7f5969cbc7-5nzsg" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5nzsg webserver-deployment-7f5969cbc7- deployment-1487  d09e9731-7c9f-4bba-a5fc-44a21996c7e7 34429 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1088c4408fe8148427f37988660f9cff628e5a352ec63a9498a59deb5f195e9b cni.projectcalico.org/podIP:100.64.0.192/32 cni.projectcalico.org/podIPs:100.64.0.192/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f4bf7 0xc0054f4bf8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45282,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45282,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.239: INFO: Pod "webserver-deployment-7f5969cbc7-9kpt4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9kpt4 webserver-deployment-7f5969cbc7- deployment-1487  93515376-100a-4dfd-8652-13e7c4742f87 34433 0 2023-09-04 15:45:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cdd56479eed81663ea204f1d1a2917d1a3dc351398e5f641cf5c15d995c18463 cni.projectcalico.org/podIP:100.64.0.194/32 cni.projectcalico.org/podIPs:100.64.0.194/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f4df7 0xc0054f4df8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zx6db,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zx6db,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.239: INFO: Pod "webserver-deployment-7f5969cbc7-c4xh5" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c4xh5 webserver-deployment-7f5969cbc7- deployment-1487  443bac01-fc83-4bed-89ad-18eb2ceeb986 34248 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7d359c7c4d7c3f85ca34f6f41b92276d0e04d74d3bce01d4a5a037fc7e8a81cb cni.projectcalico.org/podIP:100.64.0.179/32 cni.projectcalico.org/podIPs:100.64.0.179/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f4ff7 0xc0054f4ff8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lznwv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lznwv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:100.64.0.179,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7c9e5e49c7b88f91e5e517e9ec8fb60a3ee8fed8bf1c0db4586f619756cbf9be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.239: INFO: Pod "webserver-deployment-7f5969cbc7-fgfqc" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fgfqc webserver-deployment-7f5969cbc7- deployment-1487  f1cb62db-4655-4404-91a0-bebbaeb97535 34261 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:24d60209e6ae89a416f533dc776b9f9136ed3c140d93ad26df639ccefeee64f8 cni.projectcalico.org/podIP:100.64.1.90/32 cni.projectcalico.org/podIPs:100.64.1.90/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f5217 0xc0054f5218}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jjlrs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjlrs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.90,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a9c2255edeb786d82861d183aeb14fa1953856645e0e64a32f2f5b3f81172987,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.239: INFO: Pod "webserver-deployment-7f5969cbc7-fsrjp" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fsrjp webserver-deployment-7f5969cbc7- deployment-1487  f242c933-21fc-4fa1-99a5-50d479f485cc 34255 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8964d9348500af431ac035a834b7ffa76c525ef88466282418ce6fdb5d8358ab cni.projectcalico.org/podIP:100.64.1.92/32 cni.projectcalico.org/podIPs:100.64.1.92/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f5430 0xc0054f5431}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c7knd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c7knd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.92,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1b1a5b9a1ce37ac275cb3ef60d10974823e58d4df481525431af4ea26c926e84,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.239: INFO: Pod "webserver-deployment-7f5969cbc7-htqdg" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-htqdg webserver-deployment-7f5969cbc7- deployment-1487  efd5dfd7-c4a2-42a8-96dc-fb4ceb7c0c5c 34251 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c4c81e7943b1fb5627dbea272f57e48b7d4ed8c2a2d1b4d3681f2175f4a1e74a cni.projectcalico.org/podIP:100.64.1.93/32 cni.projectcalico.org/podIPs:100.64.1.93/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f5640 0xc0054f5641}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67rk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67rk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.93,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ae73b32b2f2252ae75c1d85276319657c79189852c1d42651eb8c47becb06f80,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.239: INFO: Pod "webserver-deployment-7f5969cbc7-kf58b" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kf58b webserver-deployment-7f5969cbc7- deployment-1487  f2bc215e-a474-43dc-9dd9-cccbbb6b2140 34440 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7c38f81cdf5a8bbe095cae5c058100660e7775a834c913713440827caf259fbe cni.projectcalico.org/podIP:100.64.1.99/32 cni.projectcalico.org/podIPs:100.64.1.99/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f5850 0xc0054f5851}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sl2wj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sl2wj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.99,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e550f078f1bac57c776128ec1970216281173bb975b466a59b180edb89159beb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.240: INFO: Pod "webserver-deployment-7f5969cbc7-ktd46" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ktd46 webserver-deployment-7f5969cbc7- deployment-1487  bf4d8b7b-7c2f-498e-a3da-4381f57f24e0 34239 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:47158c7da308ee7dbe678571618b7eb0e3518e0c02d426947b39b54454c2b0fa cni.projectcalico.org/podIP:100.64.0.181/32 cni.projectcalico.org/podIPs:100.64.0.181/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f5a70 0xc0054f5a71}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zhd4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zhd4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:100.64.0.181,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://52240602811f440149827b76d5e8b533947040510ecbc6e4d563cbcef4ace2fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.240: INFO: Pod "webserver-deployment-7f5969cbc7-mrcqk" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mrcqk webserver-deployment-7f5969cbc7- deployment-1487  37748712-5c89-47f0-b769-7b8a558f9750 34443 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b233a7b50ae41e2602a28f9e9dd9236918d3b63b29bff4ba7641a7463f5dc4e0 cni.projectcalico.org/podIP:100.64.1.97/32 cni.projectcalico.org/podIPs:100.64.1.97/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f5c97 0xc0054f5c98}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qsv6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qsv6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.97,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://124396240681a9c057ff156e77ff6c9ae3523617ddf16ac5f520a003ee3fe5d3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.240: INFO: Pod "webserver-deployment-7f5969cbc7-pvr5m" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pvr5m webserver-deployment-7f5969cbc7- deployment-1487  13092321-7c04-4be9-86b2-ecb4da61bc91 34446 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:43b90ac3ac89122e7f52249e1d0faab488bfab30b78c4c634a515ddeca8fc079 cni.projectcalico.org/podIP:100.64.1.100/32 cni.projectcalico.org/podIPs:100.64.1.100/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f5ec0 0xc0054f5ec1}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q4gmc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q4gmc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.100,StartTime:2023-09-04 15:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b3a20a56a1dc110dd239ddcfff3e69d8b00db58ea991e187a864bdc65e6e8485,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.240: INFO: Pod "webserver-deployment-7f5969cbc7-qmhmw" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qmhmw webserver-deployment-7f5969cbc7- deployment-1487  226896b4-db34-4dda-8e06-2fa9a26dae0e 34416 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a44b58915d6eb0743db226d17541d734a1ea0c049130ff93a007b92f7224d2ad cni.projectcalico.org/podIP:100.64.0.186/32 cni.projectcalico.org/podIPs:100.64.0.186/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043ba587 0xc0043ba588}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njfk4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njfk4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.240: INFO: Pod "webserver-deployment-7f5969cbc7-sf4s5" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sf4s5 webserver-deployment-7f5969cbc7- deployment-1487  2067e679-763e-4d00-a52d-fcc31f022f23 34245 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:088858f0257ac6fcf4d5a885cf78c3e871119f49d619d3d4aab5b87c21f6e075 cni.projectcalico.org/podIP:100.64.1.89/32 cni.projectcalico.org/podIPs:100.64.1.89/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043ba967 0xc0043ba968}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k4jqh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k4jqh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.89,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8f8444f22747f407a6cd5e345779bc051fc9e985741a25bc35b6dae2b7dac7fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.240: INFO: Pod "webserver-deployment-7f5969cbc7-vczjv" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vczjv webserver-deployment-7f5969cbc7- deployment-1487  0d8000aa-93bc-4fdc-89e6-4956038ac8b0 34418 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:399071ec0599f41decc71ea9e53b559296f35b847cc248454d7f14a2b3bab1a8 cni.projectcalico.org/podIP:100.64.0.187/32 cni.projectcalico.org/podIPs:100.64.0.187/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043bab80 0xc0043bab81}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xh6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xh6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.240: INFO: Pod "webserver-deployment-7f5969cbc7-w8kh7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-w8kh7 webserver-deployment-7f5969cbc7- deployment-1487  194e390e-db23-4537-bc59-ac708e50244a 34452 0 2023-09-04 15:45:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3c697d6a837987676ced4ff384072a17ab4e2e69218bb00d31ff6e634bf530ca cni.projectcalico.org/podIP:100.64.1.102/32 cni.projectcalico.org/podIPs:100.64.1.102/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043bad77 0xc0043bad78}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbgcz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbgcz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.102,StartTime:2023-09-04 15:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b0d163c2609177ddfbf4a720fed6632f05aa4db62ed2d507d389cd9114051c4c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.241: INFO: Pod "webserver-deployment-7f5969cbc7-wtvq9" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wtvq9 webserver-deployment-7f5969cbc7- deployment-1487  43a15da0-b8ab-48fe-b68c-6a6f44931a1a 34427 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:833491b89954254180956c3e75442a5bed7b078e03a305ef8b8a44c0779de2db cni.projectcalico.org/podIP:100.64.0.191/32 cni.projectcalico.org/podIPs:100.64.0.191/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043baf97 0xc0043baf98}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wjgrr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wjgrr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.241: INFO: Pod "webserver-deployment-7f5969cbc7-x4s27" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-x4s27 webserver-deployment-7f5969cbc7- deployment-1487  034a5f75-671e-4c6a-940c-2581892f58db 34236 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a94e5c7f955f829e29d58ca1ac065e7ee0601be7ecf77617eb6351a1e7b69f82 cni.projectcalico.org/podIP:100.64.0.182/32 cni.projectcalico.org/podIPs:100.64.0.182/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043bb197 0xc0043bb198}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6mnzh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6mnzh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:100.64.0.182,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://157b61e06ea9e70a1d36f521d8dc4ea83d045435f4eefbbc0efc4ebb9cf42de1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.241: INFO: Pod "webserver-deployment-7f5969cbc7-xb2jb" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xb2jb webserver-deployment-7f5969cbc7- deployment-1487  4a98f70b-fb3c-444f-95fb-dc50ca2b852c 34449 0 2023-09-04 15:45:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:943f5625ac164c0c28058044e8c9f3d9d33ef561aad0ad17f7927fe8878d6f1f cni.projectcalico.org/podIP:100.64.1.104/32 cni.projectcalico.org/podIPs:100.64.1.104/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043bb3b7 0xc0043bb3b8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b92bc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b92bc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.104,StartTime:2023-09-04 15:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://89a5c11e80571ea519c6812e1fa66da9d9e54174ddfee2fce329fdf567977c5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.241: INFO: Pod "webserver-deployment-7f5969cbc7-xhmxv" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xhmxv webserver-deployment-7f5969cbc7- deployment-1487  2f39e996-592c-4d62-9698-70f9faf6c269 34434 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4962ff005419def734f796ea59ed37ff45f0ed35d4a83c68eb2c7184439c230f cni.projectcalico.org/podIP:100.64.0.195/32 cni.projectcalico.org/podIPs:100.64.0.195/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043bb5d7 0xc0043bb5d8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bg7qr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bg7qr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.241: INFO: Pod "webserver-deployment-7f5969cbc7-z4ppj" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-z4ppj webserver-deployment-7f5969cbc7- deployment-1487  ce2372dc-2129-4b22-b325-5346034875e2 34258 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:387f5a293638b5adb658a986ecbe694cd08e3b8bf48cc77819921d11d502090e cni.projectcalico.org/podIP:100.64.1.91/32 cni.projectcalico.org/podIPs:100.64.1.91/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043bb7d7 0xc0043bb7d8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jl7wc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jl7wc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.91,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://660117b9651185621c143a02e08680f1470edbb861ee29e607c476e6dd942d86,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.241: INFO: Pod "webserver-deployment-d9f79cb5-7865p" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7865p webserver-deployment-d9f79cb5- deployment-1487  0eee2a42-7890-4cd9-a512-f8b46a0512d0 34421 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:37a43326e5d6c3a67a3eaffada8e7a234c46ccade07ce2ed2ce0da1a243a28fd cni.projectcalico.org/podIP:100.64.0.189/32 cni.projectcalico.org/podIPs:100.64.0.189/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc0043bb9bf 0xc0043bb9f0}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mxtqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mxtqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.241: INFO: Pod "webserver-deployment-d9f79cb5-7m6js" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7m6js webserver-deployment-d9f79cb5- deployment-1487  314db683-3a3c-46dd-b7fc-c153565bc92f 34419 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:06cdde8715049020ad6e51fb08a3efb06d798bdf7a50056c231f1a5bc055d5c7 cni.projectcalico.org/podIP:100.64.0.188/32 cni.projectcalico.org/podIPs:100.64.0.188/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc0043bbc07 0xc0043bbc08}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2jz8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2jz8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-9dzp2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9dzp2 webserver-deployment-d9f79cb5- deployment-1487  0c35df35-339f-4b9e-bb1d-a777ebd672f4 34414 0 2023-09-04 15:45:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:edfe168e185196a1f9fe44dc01d501ca18e714648bfb1355e3a9adf6a599fb28 cni.projectcalico.org/podIP:100.64.0.185/32 cni.projectcalico.org/podIPs:100.64.0.185/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc0043bbe27 0xc0043bbe28}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqzc7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqzc7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:100.64.0.185,StartTime:2023-09-04 15:45:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-b9lnc" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b9lnc webserver-deployment-d9f79cb5- deployment-1487  02dba83e-7b74-46dc-a4ed-8d5be84527d9 34423 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2e4d928218749fd43512605962255f84e991dad1fc5cd5acb442659ac9899ea5 cni.projectcalico.org/podIP:100.64.1.101/32 cni.projectcalico.org/podIPs:100.64.1.101/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc007380077 0xc007380078}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b8r5x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b8r5x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-c6wsh" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-c6wsh webserver-deployment-d9f79cb5- deployment-1487  ded3e208-786e-4713-a27d-a092a0e1f957 34417 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ffa7d37be6edb5897bc397a2f85c2bb4ed671dad1a8fbbe034c9153a4b48ee5f cni.projectcalico.org/podIP:100.64.1.98/32 cni.projectcalico.org/podIPs:100.64.1.98/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc007380297 0xc007380298}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvspn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvspn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-d5wq6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d5wq6 webserver-deployment-d9f79cb5- deployment-1487  115edfec-ef2b-4c0d-9d9a-7e3f98fdbe98 34413 0 2023-09-04 15:45:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d79b3904384793b1a1251b1dab6c165f5b0e3579b8220c93bfdd40ee7f441d3c cni.projectcalico.org/podIP:100.64.1.96/32 cni.projectcalico.org/podIPs:100.64.1.96/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc0073804b7 0xc0073804b8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-trzsv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-trzsv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.96,StartTime:2023-09-04 15:45:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-ddr6g" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ddr6g webserver-deployment-d9f79cb5- deployment-1487  5b03da95-6a93-46bf-a0c5-cfac5c6d0e35 34455 0 2023-09-04 15:45:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ea2b97a49a4f87aa034d6610322a42512e55b61007afc0bb85904c05f0cafd4a cni.projectcalico.org/podIP:100.64.1.95/32 cni.projectcalico.org/podIPs:100.64.1.95/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc0073806cf 0xc007380700}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-26xx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-26xx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.95,StartTime:2023-09-04 15:45:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-g2frt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-g2frt webserver-deployment-d9f79cb5- deployment-1487  df4f0196-b018-4817-b344-3e3cb96798c0 34424 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3223b1da79396e8d68801b638f214935d50f6fc22ac5976c8e29506cd1b27c34 cni.projectcalico.org/podIP:100.64.0.190/32 cni.projectcalico.org/podIPs:100.64.0.190/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc00738090f 0xc007380940}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lpvl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lpvl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-hrmwt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hrmwt webserver-deployment-d9f79cb5- deployment-1487  1ecd9b56-5677-4bf9-b01a-6b0d40ee59dc 34412 0 2023-09-04 15:45:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6e3dd86ba959770cdb50865967f63be01db36cf2f7f2574b0299ff08a93d1ebe cni.projectcalico.org/podIP:100.64.0.184/32 cni.projectcalico.org/podIPs:100.64.0.184/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc007380b57 0xc007380b58}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wlnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wlnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:100.64.0.184,StartTime:2023-09-04 15:45:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-krlpd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-krlpd webserver-deployment-d9f79cb5- deployment-1487  ce820572-6b0a-4912-af00-b048e91dbb38 34436 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:db6eb612fc2745e6f4a9783bd16b825a7ab44420f1927579963410a3da8a2eee cni.projectcalico.org/podIP:100.64.0.197/32 cni.projectcalico.org/podIPs:100.64.0.197/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc0073814e7 0xc0073814e8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vfzlk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vfzlk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.243: INFO: Pod "webserver-deployment-d9f79cb5-lfm2r" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lfm2r webserver-deployment-d9f79cb5- deployment-1487  a70b8764-3283-46db-84c9-3499aa061157 34431 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:167b779d2da9d3ee8c3b173977da2d045ae6da480f23180a6c36d2b6c60323c6 cni.projectcalico.org/podIP:100.64.1.103/32 cni.projectcalico.org/podIPs:100.64.1.103/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc007381707 0xc007381708}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9kpq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9kpq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:,StartTime:2023-09-04 15:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.243: INFO: Pod "webserver-deployment-d9f79cb5-m9vfj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-m9vfj webserver-deployment-d9f79cb5- deployment-1487  67b44657-559a-4a14-826e-c3b879b0baad 34435 0 2023-09-04 15:45:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:e9027c98bec547db93e730ce76faab1120d8c663dd8aea19f38350c3925290dd cni.projectcalico.org/podIP:100.64.0.196/32 cni.projectcalico.org/podIPs:100.64.0.196/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc007381927 0xc007381928}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n4rn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n4rn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  4 15:45:50.243: INFO: Pod "webserver-deployment-d9f79cb5-zsqft" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zsqft webserver-deployment-d9f79cb5- deployment-1487  ccd04fe3-1f88-4fcf-9af7-4482aca9c5e5 34410 0 2023-09-04 15:45:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:94167f2295bd6502a85a01c3ccc2fa3850127eb7ef03d8ac5abcd75acac1e329 cni.projectcalico.org/podIP:100.64.1.94/32 cni.projectcalico.org/podIPs:100.64.1.94/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc007381b47 0xc007381b48}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qkbb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qkbb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.94,StartTime:2023-09-04 15:45:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 15:45:50.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1487" for this suite. 09/04/23 15:45:50.271
------------------------------
• [9.747 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:45:40.539
    Sep  4 15:45:40.539: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 09/04/23 15:45:40.54
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:40.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:40.612
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Sep  4 15:45:40.639: INFO: Creating deployment "webserver-deployment"
    Sep  4 15:45:40.654: INFO: Waiting for observed generation 1
    Sep  4 15:45:40.669: INFO: Waiting for all required pods to come up
    Sep  4 15:45:40.684: INFO: Pod name httpd: Found 4 pods out of 10
    Sep  4 15:45:45.716: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 09/04/23 15:45:45.716
    Sep  4 15:45:45.716: INFO: Waiting for deployment "webserver-deployment" to complete
    Sep  4 15:45:45.746: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Sep  4 15:45:45.777: INFO: Updating deployment webserver-deployment
    Sep  4 15:45:45.777: INFO: Waiting for observed generation 2
    Sep  4 15:45:47.808: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Sep  4 15:45:47.823: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Sep  4 15:45:47.837: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep  4 15:45:47.881: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Sep  4 15:45:47.881: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Sep  4 15:45:47.895: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep  4 15:45:47.930: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Sep  4 15:45:47.930: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Sep  4 15:45:47.961: INFO: Updating deployment webserver-deployment
    Sep  4 15:45:47.961: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Sep  4 15:45:47.994: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Sep  4 15:45:50.068: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 15:45:50.099: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-1487  507eaa73-8264-4bc3-9f86-90595c0c6aaf 34454 3 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ddb418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:13,UnavailableReplicas:20,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-04 15:45:47 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-09-04 15:45:49 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,},},ReadyReplicas:13,CollisionCount:nil,},}

    Sep  4 15:45:50.113: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-1487  4b9a070b-6622-480d-89b6-78a4f8435043 34401 3 2023-09-04 15:45:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 507eaa73-8264-4bc3-9f86-90595c0c6aaf 0xc0054f4457 0xc0054f4458}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"507eaa73-8264-4bc3-9f86-90595c0c6aaf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054f44f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 15:45:50.113: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Sep  4 15:45:50.113: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-1487  8e17ab1a-9ace-4870-a93b-203e3070494b 34453 3 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 507eaa73-8264-4bc3-9f86-90595c0c6aaf 0xc0054f4367 0xc0054f4368}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"507eaa73-8264-4bc3-9f86-90595c0c6aaf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054f43f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:13,AvailableReplicas:13,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 15:45:50.238: INFO: Pod "webserver-deployment-7f5969cbc7-24tg7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-24tg7 webserver-deployment-7f5969cbc7- deployment-1487  baa46aba-541d-4e38-8138-99cda4e4043d 34432 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:90af31cd22eba51d1ff7a0ad056fffe1c597a5d3336b1117d183db50cd6fa6cf cni.projectcalico.org/podIP:100.64.0.193/32 cni.projectcalico.org/podIPs:100.64.0.193/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f49f7 0xc0054f49f8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nlbxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nlbxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.239: INFO: Pod "webserver-deployment-7f5969cbc7-5nzsg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5nzsg webserver-deployment-7f5969cbc7- deployment-1487  d09e9731-7c9f-4bba-a5fc-44a21996c7e7 34429 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1088c4408fe8148427f37988660f9cff628e5a352ec63a9498a59deb5f195e9b cni.projectcalico.org/podIP:100.64.0.192/32 cni.projectcalico.org/podIPs:100.64.0.192/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f4bf7 0xc0054f4bf8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45282,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45282,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.239: INFO: Pod "webserver-deployment-7f5969cbc7-9kpt4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9kpt4 webserver-deployment-7f5969cbc7- deployment-1487  93515376-100a-4dfd-8652-13e7c4742f87 34433 0 2023-09-04 15:45:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cdd56479eed81663ea204f1d1a2917d1a3dc351398e5f641cf5c15d995c18463 cni.projectcalico.org/podIP:100.64.0.194/32 cni.projectcalico.org/podIPs:100.64.0.194/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f4df7 0xc0054f4df8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zx6db,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zx6db,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.239: INFO: Pod "webserver-deployment-7f5969cbc7-c4xh5" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c4xh5 webserver-deployment-7f5969cbc7- deployment-1487  443bac01-fc83-4bed-89ad-18eb2ceeb986 34248 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7d359c7c4d7c3f85ca34f6f41b92276d0e04d74d3bce01d4a5a037fc7e8a81cb cni.projectcalico.org/podIP:100.64.0.179/32 cni.projectcalico.org/podIPs:100.64.0.179/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f4ff7 0xc0054f4ff8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lznwv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lznwv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:100.64.0.179,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7c9e5e49c7b88f91e5e517e9ec8fb60a3ee8fed8bf1c0db4586f619756cbf9be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.239: INFO: Pod "webserver-deployment-7f5969cbc7-fgfqc" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fgfqc webserver-deployment-7f5969cbc7- deployment-1487  f1cb62db-4655-4404-91a0-bebbaeb97535 34261 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:24d60209e6ae89a416f533dc776b9f9136ed3c140d93ad26df639ccefeee64f8 cni.projectcalico.org/podIP:100.64.1.90/32 cni.projectcalico.org/podIPs:100.64.1.90/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f5217 0xc0054f5218}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jjlrs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjlrs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.90,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a9c2255edeb786d82861d183aeb14fa1953856645e0e64a32f2f5b3f81172987,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.239: INFO: Pod "webserver-deployment-7f5969cbc7-fsrjp" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fsrjp webserver-deployment-7f5969cbc7- deployment-1487  f242c933-21fc-4fa1-99a5-50d479f485cc 34255 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8964d9348500af431ac035a834b7ffa76c525ef88466282418ce6fdb5d8358ab cni.projectcalico.org/podIP:100.64.1.92/32 cni.projectcalico.org/podIPs:100.64.1.92/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f5430 0xc0054f5431}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c7knd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c7knd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.92,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1b1a5b9a1ce37ac275cb3ef60d10974823e58d4df481525431af4ea26c926e84,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.239: INFO: Pod "webserver-deployment-7f5969cbc7-htqdg" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-htqdg webserver-deployment-7f5969cbc7- deployment-1487  efd5dfd7-c4a2-42a8-96dc-fb4ceb7c0c5c 34251 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c4c81e7943b1fb5627dbea272f57e48b7d4ed8c2a2d1b4d3681f2175f4a1e74a cni.projectcalico.org/podIP:100.64.1.93/32 cni.projectcalico.org/podIPs:100.64.1.93/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f5640 0xc0054f5641}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67rk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67rk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.93,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ae73b32b2f2252ae75c1d85276319657c79189852c1d42651eb8c47becb06f80,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.239: INFO: Pod "webserver-deployment-7f5969cbc7-kf58b" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kf58b webserver-deployment-7f5969cbc7- deployment-1487  f2bc215e-a474-43dc-9dd9-cccbbb6b2140 34440 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7c38f81cdf5a8bbe095cae5c058100660e7775a834c913713440827caf259fbe cni.projectcalico.org/podIP:100.64.1.99/32 cni.projectcalico.org/podIPs:100.64.1.99/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f5850 0xc0054f5851}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sl2wj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sl2wj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.99,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e550f078f1bac57c776128ec1970216281173bb975b466a59b180edb89159beb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.240: INFO: Pod "webserver-deployment-7f5969cbc7-ktd46" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ktd46 webserver-deployment-7f5969cbc7- deployment-1487  bf4d8b7b-7c2f-498e-a3da-4381f57f24e0 34239 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:47158c7da308ee7dbe678571618b7eb0e3518e0c02d426947b39b54454c2b0fa cni.projectcalico.org/podIP:100.64.0.181/32 cni.projectcalico.org/podIPs:100.64.0.181/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f5a70 0xc0054f5a71}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zhd4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zhd4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:100.64.0.181,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://52240602811f440149827b76d5e8b533947040510ecbc6e4d563cbcef4ace2fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.240: INFO: Pod "webserver-deployment-7f5969cbc7-mrcqk" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mrcqk webserver-deployment-7f5969cbc7- deployment-1487  37748712-5c89-47f0-b769-7b8a558f9750 34443 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b233a7b50ae41e2602a28f9e9dd9236918d3b63b29bff4ba7641a7463f5dc4e0 cni.projectcalico.org/podIP:100.64.1.97/32 cni.projectcalico.org/podIPs:100.64.1.97/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f5c97 0xc0054f5c98}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qsv6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qsv6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.97,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://124396240681a9c057ff156e77ff6c9ae3523617ddf16ac5f520a003ee3fe5d3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.240: INFO: Pod "webserver-deployment-7f5969cbc7-pvr5m" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pvr5m webserver-deployment-7f5969cbc7- deployment-1487  13092321-7c04-4be9-86b2-ecb4da61bc91 34446 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:43b90ac3ac89122e7f52249e1d0faab488bfab30b78c4c634a515ddeca8fc079 cni.projectcalico.org/podIP:100.64.1.100/32 cni.projectcalico.org/podIPs:100.64.1.100/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0054f5ec0 0xc0054f5ec1}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q4gmc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q4gmc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.100,StartTime:2023-09-04 15:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b3a20a56a1dc110dd239ddcfff3e69d8b00db58ea991e187a864bdc65e6e8485,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.240: INFO: Pod "webserver-deployment-7f5969cbc7-qmhmw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qmhmw webserver-deployment-7f5969cbc7- deployment-1487  226896b4-db34-4dda-8e06-2fa9a26dae0e 34416 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a44b58915d6eb0743db226d17541d734a1ea0c049130ff93a007b92f7224d2ad cni.projectcalico.org/podIP:100.64.0.186/32 cni.projectcalico.org/podIPs:100.64.0.186/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043ba587 0xc0043ba588}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njfk4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njfk4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.240: INFO: Pod "webserver-deployment-7f5969cbc7-sf4s5" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sf4s5 webserver-deployment-7f5969cbc7- deployment-1487  2067e679-763e-4d00-a52d-fcc31f022f23 34245 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:088858f0257ac6fcf4d5a885cf78c3e871119f49d619d3d4aab5b87c21f6e075 cni.projectcalico.org/podIP:100.64.1.89/32 cni.projectcalico.org/podIPs:100.64.1.89/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043ba967 0xc0043ba968}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k4jqh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k4jqh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.89,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8f8444f22747f407a6cd5e345779bc051fc9e985741a25bc35b6dae2b7dac7fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.240: INFO: Pod "webserver-deployment-7f5969cbc7-vczjv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vczjv webserver-deployment-7f5969cbc7- deployment-1487  0d8000aa-93bc-4fdc-89e6-4956038ac8b0 34418 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:399071ec0599f41decc71ea9e53b559296f35b847cc248454d7f14a2b3bab1a8 cni.projectcalico.org/podIP:100.64.0.187/32 cni.projectcalico.org/podIPs:100.64.0.187/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043bab80 0xc0043bab81}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xh6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xh6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.240: INFO: Pod "webserver-deployment-7f5969cbc7-w8kh7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-w8kh7 webserver-deployment-7f5969cbc7- deployment-1487  194e390e-db23-4537-bc59-ac708e50244a 34452 0 2023-09-04 15:45:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3c697d6a837987676ced4ff384072a17ab4e2e69218bb00d31ff6e634bf530ca cni.projectcalico.org/podIP:100.64.1.102/32 cni.projectcalico.org/podIPs:100.64.1.102/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043bad77 0xc0043bad78}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbgcz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbgcz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.102,StartTime:2023-09-04 15:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b0d163c2609177ddfbf4a720fed6632f05aa4db62ed2d507d389cd9114051c4c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.241: INFO: Pod "webserver-deployment-7f5969cbc7-wtvq9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wtvq9 webserver-deployment-7f5969cbc7- deployment-1487  43a15da0-b8ab-48fe-b68c-6a6f44931a1a 34427 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:833491b89954254180956c3e75442a5bed7b078e03a305ef8b8a44c0779de2db cni.projectcalico.org/podIP:100.64.0.191/32 cni.projectcalico.org/podIPs:100.64.0.191/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043baf97 0xc0043baf98}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wjgrr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wjgrr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.241: INFO: Pod "webserver-deployment-7f5969cbc7-x4s27" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-x4s27 webserver-deployment-7f5969cbc7- deployment-1487  034a5f75-671e-4c6a-940c-2581892f58db 34236 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a94e5c7f955f829e29d58ca1ac065e7ee0601be7ecf77617eb6351a1e7b69f82 cni.projectcalico.org/podIP:100.64.0.182/32 cni.projectcalico.org/podIPs:100.64.0.182/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043bb197 0xc0043bb198}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6mnzh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6mnzh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:100.64.0.182,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://157b61e06ea9e70a1d36f521d8dc4ea83d045435f4eefbbc0efc4ebb9cf42de1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.241: INFO: Pod "webserver-deployment-7f5969cbc7-xb2jb" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xb2jb webserver-deployment-7f5969cbc7- deployment-1487  4a98f70b-fb3c-444f-95fb-dc50ca2b852c 34449 0 2023-09-04 15:45:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:943f5625ac164c0c28058044e8c9f3d9d33ef561aad0ad17f7927fe8878d6f1f cni.projectcalico.org/podIP:100.64.1.104/32 cni.projectcalico.org/podIPs:100.64.1.104/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043bb3b7 0xc0043bb3b8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b92bc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b92bc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.104,StartTime:2023-09-04 15:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://89a5c11e80571ea519c6812e1fa66da9d9e54174ddfee2fce329fdf567977c5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.241: INFO: Pod "webserver-deployment-7f5969cbc7-xhmxv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xhmxv webserver-deployment-7f5969cbc7- deployment-1487  2f39e996-592c-4d62-9698-70f9faf6c269 34434 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4962ff005419def734f796ea59ed37ff45f0ed35d4a83c68eb2c7184439c230f cni.projectcalico.org/podIP:100.64.0.195/32 cni.projectcalico.org/podIPs:100.64.0.195/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043bb5d7 0xc0043bb5d8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bg7qr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bg7qr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.241: INFO: Pod "webserver-deployment-7f5969cbc7-z4ppj" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-z4ppj webserver-deployment-7f5969cbc7- deployment-1487  ce2372dc-2129-4b22-b325-5346034875e2 34258 0 2023-09-04 15:45:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:387f5a293638b5adb658a986ecbe694cd08e3b8bf48cc77819921d11d502090e cni.projectcalico.org/podIP:100.64.1.91/32 cni.projectcalico.org/podIPs:100.64.1.91/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8e17ab1a-9ace-4870-a93b-203e3070494b 0xc0043bb7d7 0xc0043bb7d8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e17ab1a-9ace-4870-a93b-203e3070494b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jl7wc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jl7wc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.91,StartTime:2023-09-04 15:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://660117b9651185621c143a02e08680f1470edbb861ee29e607c476e6dd942d86,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.241: INFO: Pod "webserver-deployment-d9f79cb5-7865p" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7865p webserver-deployment-d9f79cb5- deployment-1487  0eee2a42-7890-4cd9-a512-f8b46a0512d0 34421 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:37a43326e5d6c3a67a3eaffada8e7a234c46ccade07ce2ed2ce0da1a243a28fd cni.projectcalico.org/podIP:100.64.0.189/32 cni.projectcalico.org/podIPs:100.64.0.189/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc0043bb9bf 0xc0043bb9f0}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mxtqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mxtqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.241: INFO: Pod "webserver-deployment-d9f79cb5-7m6js" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7m6js webserver-deployment-d9f79cb5- deployment-1487  314db683-3a3c-46dd-b7fc-c153565bc92f 34419 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:06cdde8715049020ad6e51fb08a3efb06d798bdf7a50056c231f1a5bc055d5c7 cni.projectcalico.org/podIP:100.64.0.188/32 cni.projectcalico.org/podIPs:100.64.0.188/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc0043bbc07 0xc0043bbc08}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2jz8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2jz8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-9dzp2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9dzp2 webserver-deployment-d9f79cb5- deployment-1487  0c35df35-339f-4b9e-bb1d-a777ebd672f4 34414 0 2023-09-04 15:45:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:edfe168e185196a1f9fe44dc01d501ca18e714648bfb1355e3a9adf6a599fb28 cni.projectcalico.org/podIP:100.64.0.185/32 cni.projectcalico.org/podIPs:100.64.0.185/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc0043bbe27 0xc0043bbe28}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqzc7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqzc7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:100.64.0.185,StartTime:2023-09-04 15:45:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-b9lnc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b9lnc webserver-deployment-d9f79cb5- deployment-1487  02dba83e-7b74-46dc-a4ed-8d5be84527d9 34423 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2e4d928218749fd43512605962255f84e991dad1fc5cd5acb442659ac9899ea5 cni.projectcalico.org/podIP:100.64.1.101/32 cni.projectcalico.org/podIPs:100.64.1.101/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc007380077 0xc007380078}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b8r5x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b8r5x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-c6wsh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-c6wsh webserver-deployment-d9f79cb5- deployment-1487  ded3e208-786e-4713-a27d-a092a0e1f957 34417 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ffa7d37be6edb5897bc397a2f85c2bb4ed671dad1a8fbbe034c9153a4b48ee5f cni.projectcalico.org/podIP:100.64.1.98/32 cni.projectcalico.org/podIPs:100.64.1.98/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc007380297 0xc007380298}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvspn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvspn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-d5wq6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d5wq6 webserver-deployment-d9f79cb5- deployment-1487  115edfec-ef2b-4c0d-9d9a-7e3f98fdbe98 34413 0 2023-09-04 15:45:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d79b3904384793b1a1251b1dab6c165f5b0e3579b8220c93bfdd40ee7f441d3c cni.projectcalico.org/podIP:100.64.1.96/32 cni.projectcalico.org/podIPs:100.64.1.96/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc0073804b7 0xc0073804b8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-trzsv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-trzsv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.96,StartTime:2023-09-04 15:45:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-ddr6g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ddr6g webserver-deployment-d9f79cb5- deployment-1487  5b03da95-6a93-46bf-a0c5-cfac5c6d0e35 34455 0 2023-09-04 15:45:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ea2b97a49a4f87aa034d6610322a42512e55b61007afc0bb85904c05f0cafd4a cni.projectcalico.org/podIP:100.64.1.95/32 cni.projectcalico.org/podIPs:100.64.1.95/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc0073806cf 0xc007380700}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-26xx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-26xx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.95,StartTime:2023-09-04 15:45:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-g2frt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-g2frt webserver-deployment-d9f79cb5- deployment-1487  df4f0196-b018-4817-b344-3e3cb96798c0 34424 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3223b1da79396e8d68801b638f214935d50f6fc22ac5976c8e29506cd1b27c34 cni.projectcalico.org/podIP:100.64.0.190/32 cni.projectcalico.org/podIPs:100.64.0.190/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc00738090f 0xc007380940}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lpvl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lpvl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-hrmwt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hrmwt webserver-deployment-d9f79cb5- deployment-1487  1ecd9b56-5677-4bf9-b01a-6b0d40ee59dc 34412 0 2023-09-04 15:45:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6e3dd86ba959770cdb50865967f63be01db36cf2f7f2574b0299ff08a93d1ebe cni.projectcalico.org/podIP:100.64.0.184/32 cni.projectcalico.org/podIPs:100.64.0.184/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc007380b57 0xc007380b58}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wlnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wlnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:100.64.0.184,StartTime:2023-09-04 15:45:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.242: INFO: Pod "webserver-deployment-d9f79cb5-krlpd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-krlpd webserver-deployment-d9f79cb5- deployment-1487  ce820572-6b0a-4912-af00-b048e91dbb38 34436 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:db6eb612fc2745e6f4a9783bd16b825a7ab44420f1927579963410a3da8a2eee cni.projectcalico.org/podIP:100.64.0.197/32 cni.projectcalico.org/podIPs:100.64.0.197/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc0073814e7 0xc0073814e8}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vfzlk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vfzlk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.243: INFO: Pod "webserver-deployment-d9f79cb5-lfm2r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lfm2r webserver-deployment-d9f79cb5- deployment-1487  a70b8764-3283-46db-84c9-3499aa061157 34431 0 2023-09-04 15:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:167b779d2da9d3ee8c3b173977da2d045ae6da480f23180a6c36d2b6c60323c6 cni.projectcalico.org/podIP:100.64.1.103/32 cni.projectcalico.org/podIPs:100.64.1.103/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc007381707 0xc007381708}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9kpq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9kpq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:,StartTime:2023-09-04 15:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.243: INFO: Pod "webserver-deployment-d9f79cb5-m9vfj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-m9vfj webserver-deployment-d9f79cb5- deployment-1487  67b44657-559a-4a14-826e-c3b879b0baad 34435 0 2023-09-04 15:45:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:e9027c98bec547db93e730ce76faab1120d8c663dd8aea19f38350c3925290dd cni.projectcalico.org/podIP:100.64.0.196/32 cni.projectcalico.org/podIPs:100.64.0.196/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc007381927 0xc007381928}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-04 15:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n4rn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n4rn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.105,PodIP:,StartTime:2023-09-04 15:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  4 15:45:50.243: INFO: Pod "webserver-deployment-d9f79cb5-zsqft" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zsqft webserver-deployment-d9f79cb5- deployment-1487  ccd04fe3-1f88-4fcf-9af7-4482aca9c5e5 34410 0 2023-09-04 15:45:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:94167f2295bd6502a85a01c3ccc2fa3850127eb7ef03d8ac5abcd75acac1e329 cni.projectcalico.org/podIP:100.64.1.94/32 cni.projectcalico.org/podIPs:100.64.1.94/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 4b9a070b-6622-480d-89b6-78a4f8435043 0xc007381b47 0xc007381b48}] [] [{kube-controller-manager Update v1 2023-09-04 15:45:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9a070b-6622-480d-89b6-78a4f8435043\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qkbb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qkbb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:45:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.94,StartTime:2023-09-04 15:45:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:45:50.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1487" for this suite. 09/04/23 15:45:50.271
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:45:50.287
Sep  4 15:45:50.288: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate 09/04/23 15:45:50.288
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:50.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:50.359
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 09/04/23 15:45:50.386
Sep  4 15:45:50.401: INFO: created test-podtemplate-1
Sep  4 15:45:50.416: INFO: created test-podtemplate-2
Sep  4 15:45:50.430: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 09/04/23 15:45:50.43
STEP: delete collection of pod templates 09/04/23 15:45:50.445
Sep  4 15:45:50.445: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 09/04/23 15:45:50.463
Sep  4 15:45:50.464: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  4 15:45:50.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9856" for this suite. 09/04/23 15:45:50.504
------------------------------
• [0.233 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:45:50.287
    Sep  4 15:45:50.288: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename podtemplate 09/04/23 15:45:50.288
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:50.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:50.359
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 09/04/23 15:45:50.386
    Sep  4 15:45:50.401: INFO: created test-podtemplate-1
    Sep  4 15:45:50.416: INFO: created test-podtemplate-2
    Sep  4 15:45:50.430: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 09/04/23 15:45:50.43
    STEP: delete collection of pod templates 09/04/23 15:45:50.445
    Sep  4 15:45:50.445: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 09/04/23 15:45:50.463
    Sep  4 15:45:50.464: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:45:50.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9856" for this suite. 09/04/23 15:45:50.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:45:50.523
Sep  4 15:45:50.523: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 09/04/23 15:45:50.524
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:50.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:50.594
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 09/04/23 15:45:50.685
Sep  4 15:45:50.707: INFO: created test-pod-1
Sep  4 15:45:50.725: INFO: created test-pod-2
Sep  4 15:45:50.744: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 09/04/23 15:45:50.744
Sep  4 15:45:50.744: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7940' to be running and ready
Sep  4 15:45:50.787: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  4 15:45:50.787: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  4 15:45:50.787: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  4 15:45:50.787: INFO: 0 / 3 pods in namespace 'pods-7940' are running and ready (0 seconds elapsed)
Sep  4 15:45:50.787: INFO: expected 0 pod replicas in namespace 'pods-7940', 0 are Running and Ready.
Sep  4 15:45:50.787: INFO: POD         NODE                                          PHASE    GRACE  CONDITIONS
Sep  4 15:45:50.787: INFO: test-pod-1  shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC  }]
Sep  4 15:45:50.787: INFO: test-pod-2  shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC  }]
Sep  4 15:45:50.787: INFO: test-pod-3  shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC  }]
Sep  4 15:45:50.787: INFO: 
Sep  4 15:45:52.842: INFO: 3 / 3 pods in namespace 'pods-7940' are running and ready (2 seconds elapsed)
Sep  4 15:45:52.842: INFO: expected 0 pod replicas in namespace 'pods-7940', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 09/04/23 15:45:52.864
Sep  4 15:45:52.878: INFO: Pod quantity 3 is different from expected quantity 0
Sep  4 15:45:53.893: INFO: Pod quantity 3 is different from expected quantity 0
Sep  4 15:45:54.894: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 15:45:55.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7940" for this suite. 09/04/23 15:45:55.932
------------------------------
• [5.425 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:45:50.523
    Sep  4 15:45:50.523: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 09/04/23 15:45:50.524
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:50.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:50.594
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 09/04/23 15:45:50.685
    Sep  4 15:45:50.707: INFO: created test-pod-1
    Sep  4 15:45:50.725: INFO: created test-pod-2
    Sep  4 15:45:50.744: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 09/04/23 15:45:50.744
    Sep  4 15:45:50.744: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7940' to be running and ready
    Sep  4 15:45:50.787: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  4 15:45:50.787: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  4 15:45:50.787: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  4 15:45:50.787: INFO: 0 / 3 pods in namespace 'pods-7940' are running and ready (0 seconds elapsed)
    Sep  4 15:45:50.787: INFO: expected 0 pod replicas in namespace 'pods-7940', 0 are Running and Ready.
    Sep  4 15:45:50.787: INFO: POD         NODE                                          PHASE    GRACE  CONDITIONS
    Sep  4 15:45:50.787: INFO: test-pod-1  shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC  }]
    Sep  4 15:45:50.787: INFO: test-pod-2  shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC  }]
    Sep  4 15:45:50.787: INFO: test-pod-3  shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-04 15:45:50 +0000 UTC  }]
    Sep  4 15:45:50.787: INFO: 
    Sep  4 15:45:52.842: INFO: 3 / 3 pods in namespace 'pods-7940' are running and ready (2 seconds elapsed)
    Sep  4 15:45:52.842: INFO: expected 0 pod replicas in namespace 'pods-7940', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 09/04/23 15:45:52.864
    Sep  4 15:45:52.878: INFO: Pod quantity 3 is different from expected quantity 0
    Sep  4 15:45:53.893: INFO: Pod quantity 3 is different from expected quantity 0
    Sep  4 15:45:54.894: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:45:55.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7940" for this suite. 09/04/23 15:45:55.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:45:55.948
Sep  4 15:45:55.948: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 15:45:55.949
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:55.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:56.02
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 09/04/23 15:45:56.047
Sep  4 15:45:56.069: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3" in namespace "downward-api-3950" to be "Succeeded or Failed"
Sep  4 15:45:56.085: INFO: Pod "downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.091753ms
Sep  4 15:45:58.102: INFO: Pod "downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033350214s
Sep  4 15:46:00.101: INFO: Pod "downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032266072s
STEP: Saw pod success 09/04/23 15:46:00.101
Sep  4 15:46:00.101: INFO: Pod "downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3" satisfied condition "Succeeded or Failed"
Sep  4 15:46:00.116: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3 container client-container: <nil>
STEP: delete the pod 09/04/23 15:46:00.153
Sep  4 15:46:00.172: INFO: Waiting for pod downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3 to disappear
Sep  4 15:46:00.186: INFO: Pod downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 15:46:00.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3950" for this suite. 09/04/23 15:46:00.226
------------------------------
• [4.295 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:45:55.948
    Sep  4 15:45:55.948: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 15:45:55.949
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:45:55.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:45:56.02
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 09/04/23 15:45:56.047
    Sep  4 15:45:56.069: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3" in namespace "downward-api-3950" to be "Succeeded or Failed"
    Sep  4 15:45:56.085: INFO: Pod "downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.091753ms
    Sep  4 15:45:58.102: INFO: Pod "downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033350214s
    Sep  4 15:46:00.101: INFO: Pod "downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032266072s
    STEP: Saw pod success 09/04/23 15:46:00.101
    Sep  4 15:46:00.101: INFO: Pod "downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3" satisfied condition "Succeeded or Failed"
    Sep  4 15:46:00.116: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3 container client-container: <nil>
    STEP: delete the pod 09/04/23 15:46:00.153
    Sep  4 15:46:00.172: INFO: Waiting for pod downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3 to disappear
    Sep  4 15:46:00.186: INFO: Pod downwardapi-volume-4f11e7dc-4fbc-4527-ad32-d4010428f0e3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:46:00.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3950" for this suite. 09/04/23 15:46:00.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:46:00.243
Sep  4 15:46:00.243: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 15:46:00.244
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:46:00.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:46:00.319
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 15:46:00.384
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:46:00.689
STEP: Deploying the webhook pod 09/04/23 15:46:00.707
STEP: Wait for the deployment to be ready 09/04/23 15:46:00.743
Sep  4 15:46:00.786: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 46, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 46, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 46, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 46, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 15:46:02.802
STEP: Verifying the service has paired with the endpoint 09/04/23 15:46:02.822
Sep  4 15:46:03.822: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 09/04/23 15:46:03.838
STEP: create a pod that should be denied by the webhook 09/04/23 15:46:03.98
STEP: create a pod that causes the webhook to hang 09/04/23 15:46:04.166
STEP: create a configmap that should be denied by the webhook 09/04/23 15:46:14.202
STEP: create a configmap that should be admitted by the webhook 09/04/23 15:46:14.304
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/04/23 15:46:14.369
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/04/23 15:46:14.435
STEP: create a namespace that bypass the webhook 09/04/23 15:46:14.568
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/04/23 15:46:14.585
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:46:14.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6506" for this suite. 09/04/23 15:46:14.783
STEP: Destroying namespace "webhook-6506-markers" for this suite. 09/04/23 15:46:14.799
------------------------------
• [14.570 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:46:00.243
    Sep  4 15:46:00.243: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 15:46:00.244
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:46:00.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:46:00.319
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 15:46:00.384
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:46:00.689
    STEP: Deploying the webhook pod 09/04/23 15:46:00.707
    STEP: Wait for the deployment to be ready 09/04/23 15:46:00.743
    Sep  4 15:46:00.786: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 46, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 46, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 46, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 46, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 15:46:02.802
    STEP: Verifying the service has paired with the endpoint 09/04/23 15:46:02.822
    Sep  4 15:46:03.822: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 09/04/23 15:46:03.838
    STEP: create a pod that should be denied by the webhook 09/04/23 15:46:03.98
    STEP: create a pod that causes the webhook to hang 09/04/23 15:46:04.166
    STEP: create a configmap that should be denied by the webhook 09/04/23 15:46:14.202
    STEP: create a configmap that should be admitted by the webhook 09/04/23 15:46:14.304
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/04/23 15:46:14.369
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/04/23 15:46:14.435
    STEP: create a namespace that bypass the webhook 09/04/23 15:46:14.568
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/04/23 15:46:14.585
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:46:14.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6506" for this suite. 09/04/23 15:46:14.783
    STEP: Destroying namespace "webhook-6506-markers" for this suite. 09/04/23 15:46:14.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:46:14.814
Sep  4 15:46:14.814: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 15:46:14.821
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:46:14.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:46:14.897
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Sep  4 15:46:14.924: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/04/23 15:46:16.719
Sep  4 15:46:16.719: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4367 --namespace=crd-publish-openapi-4367 create -f -'
Sep  4 15:46:17.603: INFO: stderr: ""
Sep  4 15:46:17.603: INFO: stdout: "e2e-test-crd-publish-openapi-9782-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  4 15:46:17.603: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4367 --namespace=crd-publish-openapi-4367 delete e2e-test-crd-publish-openapi-9782-crds test-cr'
Sep  4 15:46:17.791: INFO: stderr: ""
Sep  4 15:46:17.791: INFO: stdout: "e2e-test-crd-publish-openapi-9782-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep  4 15:46:17.791: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4367 --namespace=crd-publish-openapi-4367 apply -f -'
Sep  4 15:46:18.525: INFO: stderr: ""
Sep  4 15:46:18.525: INFO: stdout: "e2e-test-crd-publish-openapi-9782-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  4 15:46:18.525: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4367 --namespace=crd-publish-openapi-4367 delete e2e-test-crd-publish-openapi-9782-crds test-cr'
Sep  4 15:46:18.667: INFO: stderr: ""
Sep  4 15:46:18.667: INFO: stdout: "e2e-test-crd-publish-openapi-9782-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/04/23 15:46:18.667
Sep  4 15:46:18.667: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4367 explain e2e-test-crd-publish-openapi-9782-crds'
Sep  4 15:46:18.949: INFO: stderr: ""
Sep  4 15:46:18.949: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9782-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:46:20.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4367" for this suite. 09/04/23 15:46:20.684
------------------------------
• [5.883 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:46:14.814
    Sep  4 15:46:14.814: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 15:46:14.821
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:46:14.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:46:14.897
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Sep  4 15:46:14.924: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/04/23 15:46:16.719
    Sep  4 15:46:16.719: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4367 --namespace=crd-publish-openapi-4367 create -f -'
    Sep  4 15:46:17.603: INFO: stderr: ""
    Sep  4 15:46:17.603: INFO: stdout: "e2e-test-crd-publish-openapi-9782-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep  4 15:46:17.603: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4367 --namespace=crd-publish-openapi-4367 delete e2e-test-crd-publish-openapi-9782-crds test-cr'
    Sep  4 15:46:17.791: INFO: stderr: ""
    Sep  4 15:46:17.791: INFO: stdout: "e2e-test-crd-publish-openapi-9782-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Sep  4 15:46:17.791: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4367 --namespace=crd-publish-openapi-4367 apply -f -'
    Sep  4 15:46:18.525: INFO: stderr: ""
    Sep  4 15:46:18.525: INFO: stdout: "e2e-test-crd-publish-openapi-9782-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep  4 15:46:18.525: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4367 --namespace=crd-publish-openapi-4367 delete e2e-test-crd-publish-openapi-9782-crds test-cr'
    Sep  4 15:46:18.667: INFO: stderr: ""
    Sep  4 15:46:18.667: INFO: stdout: "e2e-test-crd-publish-openapi-9782-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/04/23 15:46:18.667
    Sep  4 15:46:18.667: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4367 explain e2e-test-crd-publish-openapi-9782-crds'
    Sep  4 15:46:18.949: INFO: stderr: ""
    Sep  4 15:46:18.949: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9782-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:46:20.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4367" for this suite. 09/04/23 15:46:20.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:46:20.697
Sep  4 15:46:20.697: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 09/04/23 15:46:20.698
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:46:20.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:46:20.758
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/04/23 15:46:20.779
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-c4nn 09/04/23 15:46:20.805
STEP: Creating a pod to test atomic-volume-subpath 09/04/23 15:46:20.805
Sep  4 15:46:20.843: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c4nn" in namespace "subpath-3975" to be "Succeeded or Failed"
Sep  4 15:46:20.854: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Pending", Reason="", readiness=false. Elapsed: 11.400659ms
Sep  4 15:46:22.868: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 2.025061147s
Sep  4 15:46:24.871: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 4.028040663s
Sep  4 15:46:26.867: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 6.024422827s
Sep  4 15:46:28.866: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 8.023372035s
Sep  4 15:46:30.866: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 10.023528605s
Sep  4 15:46:32.867: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 12.024187618s
Sep  4 15:46:34.868: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 14.025111613s
Sep  4 15:46:36.866: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 16.023580785s
Sep  4 15:46:38.866: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 18.023181505s
Sep  4 15:46:40.866: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 20.023435652s
Sep  4 15:46:42.867: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=false. Elapsed: 22.024365058s
Sep  4 15:46:44.869: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.026128177s
STEP: Saw pod success 09/04/23 15:46:44.869
Sep  4 15:46:44.869: INFO: Pod "pod-subpath-test-configmap-c4nn" satisfied condition "Succeeded or Failed"
Sep  4 15:46:44.880: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-subpath-test-configmap-c4nn container test-container-subpath-configmap-c4nn: <nil>
STEP: delete the pod 09/04/23 15:46:44.912
Sep  4 15:46:44.927: INFO: Waiting for pod pod-subpath-test-configmap-c4nn to disappear
Sep  4 15:46:44.938: INFO: Pod pod-subpath-test-configmap-c4nn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-c4nn 09/04/23 15:46:44.938
Sep  4 15:46:44.938: INFO: Deleting pod "pod-subpath-test-configmap-c4nn" in namespace "subpath-3975"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  4 15:46:44.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3975" for this suite. 09/04/23 15:46:44.971
------------------------------
• [24.287 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:46:20.697
    Sep  4 15:46:20.697: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 09/04/23 15:46:20.698
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:46:20.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:46:20.758
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/04/23 15:46:20.779
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-c4nn 09/04/23 15:46:20.805
    STEP: Creating a pod to test atomic-volume-subpath 09/04/23 15:46:20.805
    Sep  4 15:46:20.843: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c4nn" in namespace "subpath-3975" to be "Succeeded or Failed"
    Sep  4 15:46:20.854: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Pending", Reason="", readiness=false. Elapsed: 11.400659ms
    Sep  4 15:46:22.868: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 2.025061147s
    Sep  4 15:46:24.871: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 4.028040663s
    Sep  4 15:46:26.867: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 6.024422827s
    Sep  4 15:46:28.866: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 8.023372035s
    Sep  4 15:46:30.866: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 10.023528605s
    Sep  4 15:46:32.867: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 12.024187618s
    Sep  4 15:46:34.868: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 14.025111613s
    Sep  4 15:46:36.866: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 16.023580785s
    Sep  4 15:46:38.866: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 18.023181505s
    Sep  4 15:46:40.866: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=true. Elapsed: 20.023435652s
    Sep  4 15:46:42.867: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Running", Reason="", readiness=false. Elapsed: 22.024365058s
    Sep  4 15:46:44.869: INFO: Pod "pod-subpath-test-configmap-c4nn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.026128177s
    STEP: Saw pod success 09/04/23 15:46:44.869
    Sep  4 15:46:44.869: INFO: Pod "pod-subpath-test-configmap-c4nn" satisfied condition "Succeeded or Failed"
    Sep  4 15:46:44.880: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-subpath-test-configmap-c4nn container test-container-subpath-configmap-c4nn: <nil>
    STEP: delete the pod 09/04/23 15:46:44.912
    Sep  4 15:46:44.927: INFO: Waiting for pod pod-subpath-test-configmap-c4nn to disappear
    Sep  4 15:46:44.938: INFO: Pod pod-subpath-test-configmap-c4nn no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-c4nn 09/04/23 15:46:44.938
    Sep  4 15:46:44.938: INFO: Deleting pod "pod-subpath-test-configmap-c4nn" in namespace "subpath-3975"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:46:44.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3975" for this suite. 09/04/23 15:46:44.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:46:44.985
Sep  4 15:46:44.985: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename certificates 09/04/23 15:46:44.985
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:46:45.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:46:45.042
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 09/04/23 15:46:45.708
STEP: getting /apis/certificates.k8s.io 09/04/23 15:46:45.73
STEP: getting /apis/certificates.k8s.io/v1 09/04/23 15:46:45.741
STEP: creating 09/04/23 15:46:45.751
STEP: getting 09/04/23 15:46:45.789
STEP: listing 09/04/23 15:46:45.801
STEP: watching 09/04/23 15:46:45.812
Sep  4 15:46:45.812: INFO: starting watch
STEP: patching 09/04/23 15:46:45.823
STEP: updating 09/04/23 15:46:45.836
Sep  4 15:46:45.849: INFO: waiting for watch events with expected annotations
Sep  4 15:46:45.849: INFO: saw patched and updated annotations
STEP: getting /approval 09/04/23 15:46:45.849
STEP: patching /approval 09/04/23 15:46:45.861
STEP: updating /approval 09/04/23 15:46:45.874
STEP: getting /status 09/04/23 15:46:45.887
STEP: patching /status 09/04/23 15:46:45.9
STEP: updating /status 09/04/23 15:46:45.914
STEP: deleting 09/04/23 15:46:45.928
STEP: deleting a collection 09/04/23 15:46:45.964
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:46:45.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-3619" for this suite. 09/04/23 15:46:46.015
------------------------------
• [1.043 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:46:44.985
    Sep  4 15:46:44.985: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename certificates 09/04/23 15:46:44.985
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:46:45.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:46:45.042
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 09/04/23 15:46:45.708
    STEP: getting /apis/certificates.k8s.io 09/04/23 15:46:45.73
    STEP: getting /apis/certificates.k8s.io/v1 09/04/23 15:46:45.741
    STEP: creating 09/04/23 15:46:45.751
    STEP: getting 09/04/23 15:46:45.789
    STEP: listing 09/04/23 15:46:45.801
    STEP: watching 09/04/23 15:46:45.812
    Sep  4 15:46:45.812: INFO: starting watch
    STEP: patching 09/04/23 15:46:45.823
    STEP: updating 09/04/23 15:46:45.836
    Sep  4 15:46:45.849: INFO: waiting for watch events with expected annotations
    Sep  4 15:46:45.849: INFO: saw patched and updated annotations
    STEP: getting /approval 09/04/23 15:46:45.849
    STEP: patching /approval 09/04/23 15:46:45.861
    STEP: updating /approval 09/04/23 15:46:45.874
    STEP: getting /status 09/04/23 15:46:45.887
    STEP: patching /status 09/04/23 15:46:45.9
    STEP: updating /status 09/04/23 15:46:45.914
    STEP: deleting 09/04/23 15:46:45.928
    STEP: deleting a collection 09/04/23 15:46:45.964
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:46:45.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-3619" for this suite. 09/04/23 15:46:46.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:46:46.029
Sep  4 15:46:46.029: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 15:46:46.03
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:46:46.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:46:46.087
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-20abce52-ccc0-4b43-91d5-45138ae66926 09/04/23 15:46:46.109
STEP: Creating a pod to test consume configMaps 09/04/23 15:46:46.121
Sep  4 15:46:46.139: INFO: Waiting up to 5m0s for pod "pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9" in namespace "configmap-7886" to be "Succeeded or Failed"
Sep  4 15:46:46.151: INFO: Pod "pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.382547ms
Sep  4 15:46:48.164: INFO: Pod "pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024974054s
Sep  4 15:46:50.163: INFO: Pod "pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023804841s
STEP: Saw pod success 09/04/23 15:46:50.163
Sep  4 15:46:50.163: INFO: Pod "pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9" satisfied condition "Succeeded or Failed"
Sep  4 15:46:50.175: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 15:46:50.208
Sep  4 15:46:50.222: INFO: Waiting for pod pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9 to disappear
Sep  4 15:46:50.234: INFO: Pod pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:46:50.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7886" for this suite. 09/04/23 15:46:50.255
------------------------------
• [4.240 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:46:46.029
    Sep  4 15:46:46.029: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 15:46:46.03
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:46:46.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:46:46.087
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-20abce52-ccc0-4b43-91d5-45138ae66926 09/04/23 15:46:46.109
    STEP: Creating a pod to test consume configMaps 09/04/23 15:46:46.121
    Sep  4 15:46:46.139: INFO: Waiting up to 5m0s for pod "pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9" in namespace "configmap-7886" to be "Succeeded or Failed"
    Sep  4 15:46:46.151: INFO: Pod "pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.382547ms
    Sep  4 15:46:48.164: INFO: Pod "pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024974054s
    Sep  4 15:46:50.163: INFO: Pod "pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023804841s
    STEP: Saw pod success 09/04/23 15:46:50.163
    Sep  4 15:46:50.163: INFO: Pod "pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9" satisfied condition "Succeeded or Failed"
    Sep  4 15:46:50.175: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 15:46:50.208
    Sep  4 15:46:50.222: INFO: Waiting for pod pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9 to disappear
    Sep  4 15:46:50.234: INFO: Pod pod-configmaps-05b93b27-1f4c-4cc0-a819-187962aea9e9 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:46:50.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7886" for this suite. 09/04/23 15:46:50.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:46:50.27
Sep  4 15:46:50.270: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 15:46:50.271
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:46:50.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:46:50.326
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/04/23 15:46:50.348
Sep  4 15:46:50.349: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/04/23 15:46:58.451
Sep  4 15:46:58.451: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:47:00.573: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:47:09.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1808" for this suite. 09/04/23 15:47:09.09
------------------------------
• [18.835 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:46:50.27
    Sep  4 15:46:50.270: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 15:46:50.271
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:46:50.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:46:50.326
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/04/23 15:46:50.348
    Sep  4 15:46:50.349: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/04/23 15:46:58.451
    Sep  4 15:46:58.451: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:47:00.573: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:47:09.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1808" for this suite. 09/04/23 15:47:09.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:47:09.106
Sep  4 15:47:09.106: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslicemirroring 09/04/23 15:47:09.107
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:47:09.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:47:09.174
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 09/04/23 15:47:09.217
Sep  4 15:47:09.245: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 09/04/23 15:47:11.26
STEP: mirroring deletion of a custom Endpoint 09/04/23 15:47:11.289
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Sep  4 15:47:11.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-67" for this suite. 09/04/23 15:47:11.343
------------------------------
• [2.252 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:47:09.106
    Sep  4 15:47:09.106: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslicemirroring 09/04/23 15:47:09.107
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:47:09.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:47:09.174
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 09/04/23 15:47:09.217
    Sep  4 15:47:09.245: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 09/04/23 15:47:11.26
    STEP: mirroring deletion of a custom Endpoint 09/04/23 15:47:11.289
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:47:11.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-67" for this suite. 09/04/23 15:47:11.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:47:11.359
Sep  4 15:47:11.359: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 15:47:11.36
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:47:11.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:47:11.427
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 09/04/23 15:47:11.453
Sep  4 15:47:11.473: INFO: Waiting up to 5m0s for pod "labelsupdatec5b38e78-b945-4403-8a38-fb941db0c6c5" in namespace "downward-api-8508" to be "running and ready"
Sep  4 15:47:11.487: INFO: Pod "labelsupdatec5b38e78-b945-4403-8a38-fb941db0c6c5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.066983ms
Sep  4 15:47:11.487: INFO: The phase of Pod labelsupdatec5b38e78-b945-4403-8a38-fb941db0c6c5 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:47:13.503: INFO: Pod "labelsupdatec5b38e78-b945-4403-8a38-fb941db0c6c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.029181794s
Sep  4 15:47:13.503: INFO: The phase of Pod labelsupdatec5b38e78-b945-4403-8a38-fb941db0c6c5 is Running (Ready = true)
Sep  4 15:47:13.503: INFO: Pod "labelsupdatec5b38e78-b945-4403-8a38-fb941db0c6c5" satisfied condition "running and ready"
Sep  4 15:47:14.153: INFO: Successfully updated pod "labelsupdatec5b38e78-b945-4403-8a38-fb941db0c6c5"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 15:47:16.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8508" for this suite. 09/04/23 15:47:16.275
------------------------------
• [4.931 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:47:11.359
    Sep  4 15:47:11.359: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 15:47:11.36
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:47:11.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:47:11.427
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 09/04/23 15:47:11.453
    Sep  4 15:47:11.473: INFO: Waiting up to 5m0s for pod "labelsupdatec5b38e78-b945-4403-8a38-fb941db0c6c5" in namespace "downward-api-8508" to be "running and ready"
    Sep  4 15:47:11.487: INFO: Pod "labelsupdatec5b38e78-b945-4403-8a38-fb941db0c6c5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.066983ms
    Sep  4 15:47:11.487: INFO: The phase of Pod labelsupdatec5b38e78-b945-4403-8a38-fb941db0c6c5 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:47:13.503: INFO: Pod "labelsupdatec5b38e78-b945-4403-8a38-fb941db0c6c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.029181794s
    Sep  4 15:47:13.503: INFO: The phase of Pod labelsupdatec5b38e78-b945-4403-8a38-fb941db0c6c5 is Running (Ready = true)
    Sep  4 15:47:13.503: INFO: Pod "labelsupdatec5b38e78-b945-4403-8a38-fb941db0c6c5" satisfied condition "running and ready"
    Sep  4 15:47:14.153: INFO: Successfully updated pod "labelsupdatec5b38e78-b945-4403-8a38-fb941db0c6c5"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:47:16.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8508" for this suite. 09/04/23 15:47:16.275
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:47:16.29
Sep  4 15:47:16.290: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 09/04/23 15:47:16.291
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:47:16.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:47:16.358
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 15:47:16.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7876" for this suite. 09/04/23 15:47:16.525
------------------------------
• [0.248 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:47:16.29
    Sep  4 15:47:16.290: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 09/04/23 15:47:16.291
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:47:16.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:47:16.358
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:47:16.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7876" for this suite. 09/04/23 15:47:16.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:47:16.539
Sep  4 15:47:16.539: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename lease-test 09/04/23 15:47:16.54
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:47:16.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:47:16.61
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Sep  4 15:47:16.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-9944" for this suite. 09/04/23 15:47:16.831
------------------------------
• [0.306 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:47:16.539
    Sep  4 15:47:16.539: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename lease-test 09/04/23 15:47:16.54
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:47:16.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:47:16.61
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:47:16.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-9944" for this suite. 09/04/23 15:47:16.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:47:16.845
Sep  4 15:47:16.845: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 09/04/23 15:47:16.846
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:47:16.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:47:16.914
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 09/04/23 15:47:16.939
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-170.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-170.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 09/04/23 15:47:16.953
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-170.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-170.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 09/04/23 15:47:16.953
STEP: creating a pod to probe DNS 09/04/23 15:47:16.953
STEP: submitting the pod to kubernetes 09/04/23 15:47:16.953
Sep  4 15:47:16.973: INFO: Waiting up to 15m0s for pod "dns-test-2d8a5db0-21d0-4348-8ffe-e0bb27fcd43a" in namespace "dns-170" to be "running"
Sep  4 15:47:16.987: INFO: Pod "dns-test-2d8a5db0-21d0-4348-8ffe-e0bb27fcd43a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.403348ms
Sep  4 15:47:19.002: INFO: Pod "dns-test-2d8a5db0-21d0-4348-8ffe-e0bb27fcd43a": Phase="Running", Reason="", readiness=true. Elapsed: 2.028970007s
Sep  4 15:47:19.002: INFO: Pod "dns-test-2d8a5db0-21d0-4348-8ffe-e0bb27fcd43a" satisfied condition "running"
STEP: retrieving the pod 09/04/23 15:47:19.002
STEP: looking for the results for each expected name from probers 09/04/23 15:47:19.016
Sep  4 15:47:19.264: INFO: DNS probes using dns-170/dns-test-2d8a5db0-21d0-4348-8ffe-e0bb27fcd43a succeeded

STEP: deleting the pod 09/04/23 15:47:19.264
STEP: deleting the test headless service 09/04/23 15:47:19.281
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 15:47:19.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-170" for this suite. 09/04/23 15:47:19.323
------------------------------
• [2.492 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:47:16.845
    Sep  4 15:47:16.845: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 09/04/23 15:47:16.846
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:47:16.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:47:16.914
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 09/04/23 15:47:16.939
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-170.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-170.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     09/04/23 15:47:16.953
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-170.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-170.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     09/04/23 15:47:16.953
    STEP: creating a pod to probe DNS 09/04/23 15:47:16.953
    STEP: submitting the pod to kubernetes 09/04/23 15:47:16.953
    Sep  4 15:47:16.973: INFO: Waiting up to 15m0s for pod "dns-test-2d8a5db0-21d0-4348-8ffe-e0bb27fcd43a" in namespace "dns-170" to be "running"
    Sep  4 15:47:16.987: INFO: Pod "dns-test-2d8a5db0-21d0-4348-8ffe-e0bb27fcd43a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.403348ms
    Sep  4 15:47:19.002: INFO: Pod "dns-test-2d8a5db0-21d0-4348-8ffe-e0bb27fcd43a": Phase="Running", Reason="", readiness=true. Elapsed: 2.028970007s
    Sep  4 15:47:19.002: INFO: Pod "dns-test-2d8a5db0-21d0-4348-8ffe-e0bb27fcd43a" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 15:47:19.002
    STEP: looking for the results for each expected name from probers 09/04/23 15:47:19.016
    Sep  4 15:47:19.264: INFO: DNS probes using dns-170/dns-test-2d8a5db0-21d0-4348-8ffe-e0bb27fcd43a succeeded

    STEP: deleting the pod 09/04/23 15:47:19.264
    STEP: deleting the test headless service 09/04/23 15:47:19.281
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:47:19.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-170" for this suite. 09/04/23 15:47:19.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:47:19.338
Sep  4 15:47:19.338: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ephemeral-containers-test 09/04/23 15:47:19.339
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:47:19.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:47:19.408
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 09/04/23 15:47:19.433
Sep  4 15:47:19.452: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4994" to be "running and ready"
Sep  4 15:47:19.465: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.204231ms
Sep  4 15:47:19.465: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:47:21.479: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.027142981s
Sep  4 15:47:21.479: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Sep  4 15:47:21.479: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 09/04/23 15:47:21.492
Sep  4 15:47:21.512: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4994" to be "container debugger running"
Sep  4 15:47:21.525: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 13.223268ms
Sep  4 15:47:23.540: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.028751979s
Sep  4 15:47:25.540: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.028209751s
Sep  4 15:47:25.540: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 09/04/23 15:47:25.54
Sep  4 15:47:25.540: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4994 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:47:25.540: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:47:25.541: INFO: ExecWithOptions: Clientset creation
Sep  4 15:47:25.541: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/ephemeral-containers-test-4994/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Sep  4 15:47:26.013: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:47:26.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-4994" for this suite. 09/04/23 15:47:26.144
------------------------------
• [6.821 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:47:19.338
    Sep  4 15:47:19.338: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename ephemeral-containers-test 09/04/23 15:47:19.339
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:47:19.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:47:19.408
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 09/04/23 15:47:19.433
    Sep  4 15:47:19.452: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4994" to be "running and ready"
    Sep  4 15:47:19.465: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.204231ms
    Sep  4 15:47:19.465: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:47:21.479: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.027142981s
    Sep  4 15:47:21.479: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Sep  4 15:47:21.479: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 09/04/23 15:47:21.492
    Sep  4 15:47:21.512: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4994" to be "container debugger running"
    Sep  4 15:47:21.525: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 13.223268ms
    Sep  4 15:47:23.540: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.028751979s
    Sep  4 15:47:25.540: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.028209751s
    Sep  4 15:47:25.540: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 09/04/23 15:47:25.54
    Sep  4 15:47:25.540: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4994 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:47:25.540: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:47:25.541: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:47:25.541: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/ephemeral-containers-test-4994/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Sep  4 15:47:26.013: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:47:26.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-4994" for this suite. 09/04/23 15:47:26.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:47:26.159
Sep  4 15:47:26.159: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 09/04/23 15:47:26.16
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:47:26.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:47:26.226
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  4 15:47:26.291: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  4 15:48:26.424: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 09/04/23 15:48:26.438
Sep  4 15:48:26.519: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep  4 15:48:26.537: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep  4 15:48:26.574: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep  4 15:48:26.647: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/04/23 15:48:26.647
Sep  4 15:48:26.647: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1821" to be "running"
Sep  4 15:48:26.661: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.520051ms
Sep  4 15:48:28.677: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.029901508s
Sep  4 15:48:28.677: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep  4 15:48:28.677: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1821" to be "running"
Sep  4 15:48:28.691: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.997029ms
Sep  4 15:48:28.691: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  4 15:48:28.691: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1821" to be "running"
Sep  4 15:48:28.705: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.207271ms
Sep  4 15:48:28.705: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  4 15:48:28.705: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1821" to be "running"
Sep  4 15:48:28.719: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.865667ms
Sep  4 15:48:28.719: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/04/23 15:48:28.719
Sep  4 15:48:28.737: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-1821" to be "running"
Sep  4 15:48:28.759: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.968022ms
Sep  4 15:48:30.775: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03768348s
Sep  4 15:48:32.774: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036870044s
Sep  4 15:48:34.774: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.036969738s
Sep  4 15:48:34.774: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:48:34.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1821" for this suite. 09/04/23 15:48:34.955
------------------------------
• [68.812 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:47:26.159
    Sep  4 15:47:26.159: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 09/04/23 15:47:26.16
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:47:26.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:47:26.226
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  4 15:47:26.291: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  4 15:48:26.424: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 09/04/23 15:48:26.438
    Sep  4 15:48:26.519: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep  4 15:48:26.537: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep  4 15:48:26.574: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep  4 15:48:26.647: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/04/23 15:48:26.647
    Sep  4 15:48:26.647: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1821" to be "running"
    Sep  4 15:48:26.661: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.520051ms
    Sep  4 15:48:28.677: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.029901508s
    Sep  4 15:48:28.677: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep  4 15:48:28.677: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1821" to be "running"
    Sep  4 15:48:28.691: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.997029ms
    Sep  4 15:48:28.691: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  4 15:48:28.691: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1821" to be "running"
    Sep  4 15:48:28.705: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.207271ms
    Sep  4 15:48:28.705: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  4 15:48:28.705: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1821" to be "running"
    Sep  4 15:48:28.719: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.865667ms
    Sep  4 15:48:28.719: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/04/23 15:48:28.719
    Sep  4 15:48:28.737: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-1821" to be "running"
    Sep  4 15:48:28.759: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.968022ms
    Sep  4 15:48:30.775: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03768348s
    Sep  4 15:48:32.774: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036870044s
    Sep  4 15:48:34.774: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.036969738s
    Sep  4 15:48:34.774: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:48:34.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1821" for this suite. 09/04/23 15:48:34.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:48:34.972
Sep  4 15:48:34.972: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers 09/04/23 15:48:34.973
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:48:35.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:48:35.041
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 09/04/23 15:48:35.067
Sep  4 15:48:35.086: INFO: Waiting up to 5m0s for pod "client-containers-9646568f-526b-47f1-b10b-515414e9c6ee" in namespace "containers-2483" to be "Succeeded or Failed"
Sep  4 15:48:35.100: INFO: Pod "client-containers-9646568f-526b-47f1-b10b-515414e9c6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 13.753943ms
Sep  4 15:48:37.115: INFO: Pod "client-containers-9646568f-526b-47f1-b10b-515414e9c6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028612478s
Sep  4 15:48:39.115: INFO: Pod "client-containers-9646568f-526b-47f1-b10b-515414e9c6ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028942979s
STEP: Saw pod success 09/04/23 15:48:39.115
Sep  4 15:48:39.115: INFO: Pod "client-containers-9646568f-526b-47f1-b10b-515414e9c6ee" satisfied condition "Succeeded or Failed"
Sep  4 15:48:39.129: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod client-containers-9646568f-526b-47f1-b10b-515414e9c6ee container agnhost-container: <nil>
STEP: delete the pod 09/04/23 15:48:39.165
Sep  4 15:48:39.183: INFO: Waiting for pod client-containers-9646568f-526b-47f1-b10b-515414e9c6ee to disappear
Sep  4 15:48:39.197: INFO: Pod client-containers-9646568f-526b-47f1-b10b-515414e9c6ee no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  4 15:48:39.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2483" for this suite. 09/04/23 15:48:39.224
------------------------------
• [4.267 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:48:34.972
    Sep  4 15:48:34.972: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename containers 09/04/23 15:48:34.973
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:48:35.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:48:35.041
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 09/04/23 15:48:35.067
    Sep  4 15:48:35.086: INFO: Waiting up to 5m0s for pod "client-containers-9646568f-526b-47f1-b10b-515414e9c6ee" in namespace "containers-2483" to be "Succeeded or Failed"
    Sep  4 15:48:35.100: INFO: Pod "client-containers-9646568f-526b-47f1-b10b-515414e9c6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 13.753943ms
    Sep  4 15:48:37.115: INFO: Pod "client-containers-9646568f-526b-47f1-b10b-515414e9c6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028612478s
    Sep  4 15:48:39.115: INFO: Pod "client-containers-9646568f-526b-47f1-b10b-515414e9c6ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028942979s
    STEP: Saw pod success 09/04/23 15:48:39.115
    Sep  4 15:48:39.115: INFO: Pod "client-containers-9646568f-526b-47f1-b10b-515414e9c6ee" satisfied condition "Succeeded or Failed"
    Sep  4 15:48:39.129: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod client-containers-9646568f-526b-47f1-b10b-515414e9c6ee container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 15:48:39.165
    Sep  4 15:48:39.183: INFO: Waiting for pod client-containers-9646568f-526b-47f1-b10b-515414e9c6ee to disappear
    Sep  4 15:48:39.197: INFO: Pod client-containers-9646568f-526b-47f1-b10b-515414e9c6ee no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:48:39.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2483" for this suite. 09/04/23 15:48:39.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:48:39.239
Sep  4 15:48:39.239: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events 09/04/23 15:48:39.24
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:48:39.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:48:39.308
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 09/04/23 15:48:39.334
STEP: listing all events in all namespaces 09/04/23 15:48:39.349
STEP: patching the test event 09/04/23 15:48:39.366
STEP: fetching the test event 09/04/23 15:48:39.381
STEP: updating the test event 09/04/23 15:48:39.395
STEP: getting the test event 09/04/23 15:48:39.425
STEP: deleting the test event 09/04/23 15:48:39.439
STEP: listing all events in all namespaces 09/04/23 15:48:39.455
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Sep  4 15:48:39.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2806" for this suite. 09/04/23 15:48:39.485
------------------------------
• [0.260 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:48:39.239
    Sep  4 15:48:39.239: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename events 09/04/23 15:48:39.24
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:48:39.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:48:39.308
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 09/04/23 15:48:39.334
    STEP: listing all events in all namespaces 09/04/23 15:48:39.349
    STEP: patching the test event 09/04/23 15:48:39.366
    STEP: fetching the test event 09/04/23 15:48:39.381
    STEP: updating the test event 09/04/23 15:48:39.395
    STEP: getting the test event 09/04/23 15:48:39.425
    STEP: deleting the test event 09/04/23 15:48:39.439
    STEP: listing all events in all namespaces 09/04/23 15:48:39.455
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:48:39.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2806" for this suite. 09/04/23 15:48:39.485
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:48:39.5
Sep  4 15:48:39.500: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:48:39.501
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:48:39.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:48:39.57
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-ba73af31-1c8e-43a7-9b97-ee9a4e4d0f9d 09/04/23 15:48:39.596
STEP: Creating a pod to test consume secrets 09/04/23 15:48:39.611
Sep  4 15:48:39.632: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74" in namespace "projected-6666" to be "Succeeded or Failed"
Sep  4 15:48:39.645: INFO: Pod "pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74": Phase="Pending", Reason="", readiness=false. Elapsed: 13.600705ms
Sep  4 15:48:41.660: INFO: Pod "pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028553544s
Sep  4 15:48:43.662: INFO: Pod "pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029980215s
STEP: Saw pod success 09/04/23 15:48:43.662
Sep  4 15:48:43.662: INFO: Pod "pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74" satisfied condition "Succeeded or Failed"
Sep  4 15:48:43.676: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74 container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 15:48:43.713
Sep  4 15:48:43.730: INFO: Waiting for pod pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74 to disappear
Sep  4 15:48:43.745: INFO: Pod pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  4 15:48:43.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6666" for this suite. 09/04/23 15:48:43.772
------------------------------
• [4.288 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:48:39.5
    Sep  4 15:48:39.500: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:48:39.501
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:48:39.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:48:39.57
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-ba73af31-1c8e-43a7-9b97-ee9a4e4d0f9d 09/04/23 15:48:39.596
    STEP: Creating a pod to test consume secrets 09/04/23 15:48:39.611
    Sep  4 15:48:39.632: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74" in namespace "projected-6666" to be "Succeeded or Failed"
    Sep  4 15:48:39.645: INFO: Pod "pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74": Phase="Pending", Reason="", readiness=false. Elapsed: 13.600705ms
    Sep  4 15:48:41.660: INFO: Pod "pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028553544s
    Sep  4 15:48:43.662: INFO: Pod "pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029980215s
    STEP: Saw pod success 09/04/23 15:48:43.662
    Sep  4 15:48:43.662: INFO: Pod "pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74" satisfied condition "Succeeded or Failed"
    Sep  4 15:48:43.676: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74 container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 15:48:43.713
    Sep  4 15:48:43.730: INFO: Waiting for pod pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74 to disappear
    Sep  4 15:48:43.745: INFO: Pod pod-projected-secrets-13fbed73-fc8f-441c-8ca9-6c1566707a74 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:48:43.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6666" for this suite. 09/04/23 15:48:43.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:48:43.788
Sep  4 15:48:43.788: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 15:48:43.789
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:48:43.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:48:43.859
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Sep  4 15:48:43.888: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1028 version'
Sep  4 15:48:44.023: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Sep  4 15:48:44.023: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:50:44Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:43:07Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 15:48:44.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1028" for this suite. 09/04/23 15:48:44.039
------------------------------
• [0.268 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:48:43.788
    Sep  4 15:48:43.788: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 15:48:43.789
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:48:43.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:48:43.859
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Sep  4 15:48:43.888: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1028 version'
    Sep  4 15:48:44.023: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Sep  4 15:48:44.023: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:50:44Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:43:07Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:48:44.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1028" for this suite. 09/04/23 15:48:44.039
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:48:44.056
Sep  4 15:48:44.056: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers 09/04/23 15:48:44.057
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:48:44.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:48:44.127
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 09/04/23 15:48:44.154
Sep  4 15:48:44.176: INFO: Waiting up to 5m0s for pod "client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b" in namespace "containers-374" to be "Succeeded or Failed"
Sep  4 15:48:44.190: INFO: Pod "client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.349274ms
Sep  4 15:48:46.207: INFO: Pod "client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b": Phase="Running", Reason="", readiness=false. Elapsed: 2.030640652s
Sep  4 15:48:48.206: INFO: Pod "client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030397346s
STEP: Saw pod success 09/04/23 15:48:48.207
Sep  4 15:48:48.207: INFO: Pod "client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b" satisfied condition "Succeeded or Failed"
Sep  4 15:48:48.221: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b container agnhost-container: <nil>
STEP: delete the pod 09/04/23 15:48:48.258
Sep  4 15:48:48.275: INFO: Waiting for pod client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b to disappear
Sep  4 15:48:48.289: INFO: Pod client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  4 15:48:48.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-374" for this suite. 09/04/23 15:48:48.317
------------------------------
• [4.277 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:48:44.056
    Sep  4 15:48:44.056: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename containers 09/04/23 15:48:44.057
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:48:44.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:48:44.127
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 09/04/23 15:48:44.154
    Sep  4 15:48:44.176: INFO: Waiting up to 5m0s for pod "client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b" in namespace "containers-374" to be "Succeeded or Failed"
    Sep  4 15:48:44.190: INFO: Pod "client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.349274ms
    Sep  4 15:48:46.207: INFO: Pod "client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b": Phase="Running", Reason="", readiness=false. Elapsed: 2.030640652s
    Sep  4 15:48:48.206: INFO: Pod "client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030397346s
    STEP: Saw pod success 09/04/23 15:48:48.207
    Sep  4 15:48:48.207: INFO: Pod "client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b" satisfied condition "Succeeded or Failed"
    Sep  4 15:48:48.221: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 15:48:48.258
    Sep  4 15:48:48.275: INFO: Waiting for pod client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b to disappear
    Sep  4 15:48:48.289: INFO: Pod client-containers-2170dffb-8e38-4c72-9f71-9b2a2e147c7b no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:48:48.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-374" for this suite. 09/04/23 15:48:48.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:48:48.333
Sep  4 15:48:48.333: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 09/04/23 15:48:48.334
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:48:48.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:48:48.405
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 09/04/23 15:48:48.431
STEP: Ensuring more than one job is running at a time 09/04/23 15:48:48.448
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/04/23 15:50:00.464
STEP: Removing cronjob 09/04/23 15:50:00.48
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  4 15:50:00.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6922" for this suite. 09/04/23 15:50:00.528
------------------------------
• [72.212 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:48:48.333
    Sep  4 15:48:48.333: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 09/04/23 15:48:48.334
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:48:48.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:48:48.405
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 09/04/23 15:48:48.431
    STEP: Ensuring more than one job is running at a time 09/04/23 15:48:48.448
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/04/23 15:50:00.464
    STEP: Removing cronjob 09/04/23 15:50:00.48
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:50:00.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6922" for this suite. 09/04/23 15:50:00.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:50:00.545
Sep  4 15:50:00.545: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:50:00.547
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:00.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:00.623
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-d81d02f1-6261-4d49-b1e4-1d4b8872b6b0 09/04/23 15:50:00.651
STEP: Creating a pod to test consume secrets 09/04/23 15:50:00.668
Sep  4 15:50:00.695: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933" in namespace "projected-7846" to be "Succeeded or Failed"
Sep  4 15:50:00.709: INFO: Pod "pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933": Phase="Pending", Reason="", readiness=false. Elapsed: 14.443307ms
Sep  4 15:50:02.726: INFO: Pod "pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03117801s
Sep  4 15:50:04.725: INFO: Pod "pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03041583s
STEP: Saw pod success 09/04/23 15:50:04.725
Sep  4 15:50:04.725: INFO: Pod "pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933" satisfied condition "Succeeded or Failed"
Sep  4 15:50:04.741: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/04/23 15:50:04.78
Sep  4 15:50:04.799: INFO: Waiting for pod pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933 to disappear
Sep  4 15:50:04.814: INFO: Pod pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  4 15:50:04.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7846" for this suite. 09/04/23 15:50:04.843
------------------------------
• [4.314 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:50:00.545
    Sep  4 15:50:00.545: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:50:00.547
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:00.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:00.623
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-d81d02f1-6261-4d49-b1e4-1d4b8872b6b0 09/04/23 15:50:00.651
    STEP: Creating a pod to test consume secrets 09/04/23 15:50:00.668
    Sep  4 15:50:00.695: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933" in namespace "projected-7846" to be "Succeeded or Failed"
    Sep  4 15:50:00.709: INFO: Pod "pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933": Phase="Pending", Reason="", readiness=false. Elapsed: 14.443307ms
    Sep  4 15:50:02.726: INFO: Pod "pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03117801s
    Sep  4 15:50:04.725: INFO: Pod "pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03041583s
    STEP: Saw pod success 09/04/23 15:50:04.725
    Sep  4 15:50:04.725: INFO: Pod "pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933" satisfied condition "Succeeded or Failed"
    Sep  4 15:50:04.741: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 15:50:04.78
    Sep  4 15:50:04.799: INFO: Waiting for pod pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933 to disappear
    Sep  4 15:50:04.814: INFO: Pod pod-projected-secrets-8833503d-e928-44e8-a32d-b8d78dcd7933 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:50:04.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7846" for this suite. 09/04/23 15:50:04.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:50:04.86
Sep  4 15:50:04.860: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 15:50:04.861
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:04.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:04.935
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-7097 09/04/23 15:50:04.963
STEP: creating service affinity-nodeport-transition in namespace services-7097 09/04/23 15:50:04.963
STEP: creating replication controller affinity-nodeport-transition in namespace services-7097 09/04/23 15:50:04.983
I0904 15:50:05.000338    7754 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7097, replica count: 3
I0904 15:50:08.052389    7754 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 15:50:08.114: INFO: Creating new exec pod
Sep  4 15:50:08.136: INFO: Waiting up to 5m0s for pod "execpod-affinitywvmrk" in namespace "services-7097" to be "running"
Sep  4 15:50:08.150: INFO: Pod "execpod-affinitywvmrk": Phase="Pending", Reason="", readiness=false. Elapsed: 14.446489ms
Sep  4 15:50:10.167: INFO: Pod "execpod-affinitywvmrk": Phase="Running", Reason="", readiness=true. Elapsed: 2.031162518s
Sep  4 15:50:10.167: INFO: Pod "execpod-affinitywvmrk" satisfied condition "running"
Sep  4 15:50:11.197: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7097 exec execpod-affinitywvmrk -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Sep  4 15:50:11.815: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep  4 15:50:11.815: INFO: stdout: ""
Sep  4 15:50:11.815: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7097 exec execpod-affinitywvmrk -- /bin/sh -x -c nc -v -z -w 2 100.104.252.15 80'
Sep  4 15:50:12.478: INFO: stderr: "+ nc -v -z -w 2 100.104.252.15 80\nConnection to 100.104.252.15 80 port [tcp/http] succeeded!\n"
Sep  4 15:50:12.478: INFO: stdout: ""
Sep  4 15:50:12.478: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7097 exec execpod-affinitywvmrk -- /bin/sh -x -c nc -v -z -w 2 10.250.1.105 30545'
Sep  4 15:50:12.896: INFO: stderr: "+ nc -v -z -w 2 10.250.1.105 30545\nConnection to 10.250.1.105 30545 port [tcp/*] succeeded!\n"
Sep  4 15:50:12.896: INFO: stdout: ""
Sep  4 15:50:12.896: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7097 exec execpod-affinitywvmrk -- /bin/sh -x -c nc -v -z -w 2 10.250.1.231 30545'
Sep  4 15:50:13.483: INFO: stderr: "+ nc -v -z -w 2 10.250.1.231 30545\nConnection to 10.250.1.231 30545 port [tcp/*] succeeded!\n"
Sep  4 15:50:13.483: INFO: stdout: ""
Sep  4 15:50:13.515: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7097 exec execpod-affinitywvmrk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.1.105:30545/ ; done'
Sep  4 15:50:14.045: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n"
Sep  4 15:50:14.046: INFO: stdout: "\naffinity-nodeport-transition-46kpm\naffinity-nodeport-transition-d8ll5\naffinity-nodeport-transition-d8ll5\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-46kpm\naffinity-nodeport-transition-46kpm\naffinity-nodeport-transition-d8ll5\naffinity-nodeport-transition-46kpm\naffinity-nodeport-transition-d8ll5\naffinity-nodeport-transition-46kpm\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-d8ll5\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-d8ll5\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw"
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-46kpm
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-d8ll5
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-d8ll5
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-46kpm
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-46kpm
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-d8ll5
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-46kpm
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-d8ll5
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-46kpm
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-d8ll5
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-d8ll5
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.078: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7097 exec execpod-affinitywvmrk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.1.105:30545/ ; done'
Sep  4 15:50:14.809: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n"
Sep  4 15:50:14.809: INFO: stdout: "\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw"
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
Sep  4 15:50:14.809: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7097, will wait for the garbage collector to delete the pods 09/04/23 15:50:14.83
Sep  4 15:50:14.911: INFO: Deleting ReplicationController affinity-nodeport-transition took: 16.802697ms
Sep  4 15:50:15.012: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.626992ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 15:50:18.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7097" for this suite. 09/04/23 15:50:18.167
------------------------------
• [13.323 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:50:04.86
    Sep  4 15:50:04.860: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 15:50:04.861
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:04.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:04.935
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-7097 09/04/23 15:50:04.963
    STEP: creating service affinity-nodeport-transition in namespace services-7097 09/04/23 15:50:04.963
    STEP: creating replication controller affinity-nodeport-transition in namespace services-7097 09/04/23 15:50:04.983
    I0904 15:50:05.000338    7754 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7097, replica count: 3
    I0904 15:50:08.052389    7754 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 15:50:08.114: INFO: Creating new exec pod
    Sep  4 15:50:08.136: INFO: Waiting up to 5m0s for pod "execpod-affinitywvmrk" in namespace "services-7097" to be "running"
    Sep  4 15:50:08.150: INFO: Pod "execpod-affinitywvmrk": Phase="Pending", Reason="", readiness=false. Elapsed: 14.446489ms
    Sep  4 15:50:10.167: INFO: Pod "execpod-affinitywvmrk": Phase="Running", Reason="", readiness=true. Elapsed: 2.031162518s
    Sep  4 15:50:10.167: INFO: Pod "execpod-affinitywvmrk" satisfied condition "running"
    Sep  4 15:50:11.197: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7097 exec execpod-affinitywvmrk -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Sep  4 15:50:11.815: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Sep  4 15:50:11.815: INFO: stdout: ""
    Sep  4 15:50:11.815: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7097 exec execpod-affinitywvmrk -- /bin/sh -x -c nc -v -z -w 2 100.104.252.15 80'
    Sep  4 15:50:12.478: INFO: stderr: "+ nc -v -z -w 2 100.104.252.15 80\nConnection to 100.104.252.15 80 port [tcp/http] succeeded!\n"
    Sep  4 15:50:12.478: INFO: stdout: ""
    Sep  4 15:50:12.478: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7097 exec execpod-affinitywvmrk -- /bin/sh -x -c nc -v -z -w 2 10.250.1.105 30545'
    Sep  4 15:50:12.896: INFO: stderr: "+ nc -v -z -w 2 10.250.1.105 30545\nConnection to 10.250.1.105 30545 port [tcp/*] succeeded!\n"
    Sep  4 15:50:12.896: INFO: stdout: ""
    Sep  4 15:50:12.896: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7097 exec execpod-affinitywvmrk -- /bin/sh -x -c nc -v -z -w 2 10.250.1.231 30545'
    Sep  4 15:50:13.483: INFO: stderr: "+ nc -v -z -w 2 10.250.1.231 30545\nConnection to 10.250.1.231 30545 port [tcp/*] succeeded!\n"
    Sep  4 15:50:13.483: INFO: stdout: ""
    Sep  4 15:50:13.515: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7097 exec execpod-affinitywvmrk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.1.105:30545/ ; done'
    Sep  4 15:50:14.045: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n"
    Sep  4 15:50:14.046: INFO: stdout: "\naffinity-nodeport-transition-46kpm\naffinity-nodeport-transition-d8ll5\naffinity-nodeport-transition-d8ll5\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-46kpm\naffinity-nodeport-transition-46kpm\naffinity-nodeport-transition-d8ll5\naffinity-nodeport-transition-46kpm\naffinity-nodeport-transition-d8ll5\naffinity-nodeport-transition-46kpm\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-d8ll5\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-d8ll5\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw"
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-46kpm
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-d8ll5
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-d8ll5
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-46kpm
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-46kpm
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-d8ll5
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-46kpm
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-d8ll5
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-46kpm
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-d8ll5
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-d8ll5
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.046: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.078: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7097 exec execpod-affinitywvmrk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.1.105:30545/ ; done'
    Sep  4 15:50:14.809: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.105:30545/\n"
    Sep  4 15:50:14.809: INFO: stdout: "\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw\naffinity-nodeport-transition-tbqcw"
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Received response from host: affinity-nodeport-transition-tbqcw
    Sep  4 15:50:14.809: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7097, will wait for the garbage collector to delete the pods 09/04/23 15:50:14.83
    Sep  4 15:50:14.911: INFO: Deleting ReplicationController affinity-nodeport-transition took: 16.802697ms
    Sep  4 15:50:15.012: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.626992ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:50:18.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7097" for this suite. 09/04/23 15:50:18.167
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:50:18.183
Sep  4 15:50:18.184: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 15:50:18.185
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:18.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:18.258
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 09/04/23 15:50:18.286
Sep  4 15:50:18.310: INFO: Waiting up to 5m0s for pod "pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18" in namespace "emptydir-6239" to be "Succeeded or Failed"
Sep  4 15:50:18.325: INFO: Pod "pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18": Phase="Pending", Reason="", readiness=false. Elapsed: 14.479444ms
Sep  4 15:50:20.341: INFO: Pod "pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030966499s
Sep  4 15:50:22.341: INFO: Pod "pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030939095s
STEP: Saw pod success 09/04/23 15:50:22.341
Sep  4 15:50:22.341: INFO: Pod "pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18" satisfied condition "Succeeded or Failed"
Sep  4 15:50:22.371: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18 container test-container: <nil>
STEP: delete the pod 09/04/23 15:50:22.409
Sep  4 15:50:22.428: INFO: Waiting for pod pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18 to disappear
Sep  4 15:50:22.443: INFO: Pod pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 15:50:22.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6239" for this suite. 09/04/23 15:50:22.471
------------------------------
• [4.304 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:50:18.183
    Sep  4 15:50:18.184: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 15:50:18.185
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:18.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:18.258
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 09/04/23 15:50:18.286
    Sep  4 15:50:18.310: INFO: Waiting up to 5m0s for pod "pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18" in namespace "emptydir-6239" to be "Succeeded or Failed"
    Sep  4 15:50:18.325: INFO: Pod "pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18": Phase="Pending", Reason="", readiness=false. Elapsed: 14.479444ms
    Sep  4 15:50:20.341: INFO: Pod "pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030966499s
    Sep  4 15:50:22.341: INFO: Pod "pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030939095s
    STEP: Saw pod success 09/04/23 15:50:22.341
    Sep  4 15:50:22.341: INFO: Pod "pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18" satisfied condition "Succeeded or Failed"
    Sep  4 15:50:22.371: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18 container test-container: <nil>
    STEP: delete the pod 09/04/23 15:50:22.409
    Sep  4 15:50:22.428: INFO: Waiting for pod pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18 to disappear
    Sep  4 15:50:22.443: INFO: Pod pod-ce3414a5-2775-4bee-bc6e-aca5c6e12a18 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:50:22.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6239" for this suite. 09/04/23 15:50:22.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:50:22.489
Sep  4 15:50:22.489: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 15:50:22.489
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:22.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:22.564
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 15:50:22.627
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:50:22.8
STEP: Deploying the webhook pod 09/04/23 15:50:22.818
STEP: Wait for the deployment to be ready 09/04/23 15:50:22.85
Sep  4 15:50:22.893: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 50, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 50, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 15:50:24.909
STEP: Verifying the service has paired with the endpoint 09/04/23 15:50:24.929
Sep  4 15:50:25.929: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/04/23 15:50:25.944
STEP: create a namespace for the webhook 09/04/23 15:50:26.086
STEP: create a configmap should be unconditionally rejected by the webhook 09/04/23 15:50:26.103
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:50:26.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9844" for this suite. 09/04/23 15:50:26.342
STEP: Destroying namespace "webhook-9844-markers" for this suite. 09/04/23 15:50:26.359
------------------------------
• [3.886 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:50:22.489
    Sep  4 15:50:22.489: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 15:50:22.489
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:22.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:22.564
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 15:50:22.627
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:50:22.8
    STEP: Deploying the webhook pod 09/04/23 15:50:22.818
    STEP: Wait for the deployment to be ready 09/04/23 15:50:22.85
    Sep  4 15:50:22.893: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 50, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 50, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 15:50:24.909
    STEP: Verifying the service has paired with the endpoint 09/04/23 15:50:24.929
    Sep  4 15:50:25.929: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/04/23 15:50:25.944
    STEP: create a namespace for the webhook 09/04/23 15:50:26.086
    STEP: create a configmap should be unconditionally rejected by the webhook 09/04/23 15:50:26.103
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:50:26.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9844" for this suite. 09/04/23 15:50:26.342
    STEP: Destroying namespace "webhook-9844-markers" for this suite. 09/04/23 15:50:26.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:50:26.375
Sep  4 15:50:26.375: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 09/04/23 15:50:26.376
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:26.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:26.446
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-85596a87-c12e-4617-b9fe-4e9a358fd2a6 09/04/23 15:50:26.473
STEP: Creating a pod to test consume secrets 09/04/23 15:50:26.488
Sep  4 15:50:26.508: INFO: Waiting up to 5m0s for pod "pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334" in namespace "secrets-4813" to be "Succeeded or Failed"
Sep  4 15:50:26.522: INFO: Pod "pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334": Phase="Pending", Reason="", readiness=false. Elapsed: 14.499428ms
Sep  4 15:50:28.540: INFO: Pod "pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031785189s
Sep  4 15:50:30.539: INFO: Pod "pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031255151s
STEP: Saw pod success 09/04/23 15:50:30.539
Sep  4 15:50:30.539: INFO: Pod "pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334" satisfied condition "Succeeded or Failed"
Sep  4 15:50:30.554: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334 container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 15:50:30.591
Sep  4 15:50:30.610: INFO: Waiting for pod pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334 to disappear
Sep  4 15:50:30.625: INFO: Pod pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 15:50:30.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4813" for this suite. 09/04/23 15:50:30.653
------------------------------
• [4.294 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:50:26.375
    Sep  4 15:50:26.375: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 09/04/23 15:50:26.376
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:26.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:26.446
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-85596a87-c12e-4617-b9fe-4e9a358fd2a6 09/04/23 15:50:26.473
    STEP: Creating a pod to test consume secrets 09/04/23 15:50:26.488
    Sep  4 15:50:26.508: INFO: Waiting up to 5m0s for pod "pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334" in namespace "secrets-4813" to be "Succeeded or Failed"
    Sep  4 15:50:26.522: INFO: Pod "pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334": Phase="Pending", Reason="", readiness=false. Elapsed: 14.499428ms
    Sep  4 15:50:28.540: INFO: Pod "pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031785189s
    Sep  4 15:50:30.539: INFO: Pod "pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031255151s
    STEP: Saw pod success 09/04/23 15:50:30.539
    Sep  4 15:50:30.539: INFO: Pod "pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334" satisfied condition "Succeeded or Failed"
    Sep  4 15:50:30.554: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334 container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 15:50:30.591
    Sep  4 15:50:30.610: INFO: Waiting for pod pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334 to disappear
    Sep  4 15:50:30.625: INFO: Pod pod-secrets-f52860e8-fa33-4bea-b24b-2dbe0a955334 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:50:30.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4813" for this suite. 09/04/23 15:50:30.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:50:30.67
Sep  4 15:50:30.670: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:50:30.67
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:30.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:30.748
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 09/04/23 15:50:30.776
Sep  4 15:50:30.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c" in namespace "projected-895" to be "Succeeded or Failed"
Sep  4 15:50:30.812: INFO: Pod "downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.172779ms
Sep  4 15:50:32.828: INFO: Pod "downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02967353s
Sep  4 15:50:34.828: INFO: Pod "downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029631815s
STEP: Saw pod success 09/04/23 15:50:34.828
Sep  4 15:50:34.828: INFO: Pod "downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c" satisfied condition "Succeeded or Failed"
Sep  4 15:50:34.843: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c container client-container: <nil>
STEP: delete the pod 09/04/23 15:50:34.879
Sep  4 15:50:34.897: INFO: Waiting for pod downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c to disappear
Sep  4 15:50:34.912: INFO: Pod downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 15:50:34.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-895" for this suite. 09/04/23 15:50:34.939
------------------------------
• [4.284 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:50:30.67
    Sep  4 15:50:30.670: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:50:30.67
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:30.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:30.748
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 09/04/23 15:50:30.776
    Sep  4 15:50:30.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c" in namespace "projected-895" to be "Succeeded or Failed"
    Sep  4 15:50:30.812: INFO: Pod "downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.172779ms
    Sep  4 15:50:32.828: INFO: Pod "downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02967353s
    Sep  4 15:50:34.828: INFO: Pod "downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029631815s
    STEP: Saw pod success 09/04/23 15:50:34.828
    Sep  4 15:50:34.828: INFO: Pod "downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c" satisfied condition "Succeeded or Failed"
    Sep  4 15:50:34.843: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c container client-container: <nil>
    STEP: delete the pod 09/04/23 15:50:34.879
    Sep  4 15:50:34.897: INFO: Waiting for pod downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c to disappear
    Sep  4 15:50:34.912: INFO: Pod downwardapi-volume-4f797c77-e426-4080-a7c5-ca17ca3ce28c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:50:34.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-895" for this suite. 09/04/23 15:50:34.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:50:34.955
Sep  4 15:50:34.955: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 09/04/23 15:50:34.956
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:34.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:35.026
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 09/04/23 15:50:35.053
STEP: Waiting for the pdb to be processed 09/04/23 15:50:35.069
STEP: First trying to evict a pod which shouldn't be evictable 09/04/23 15:50:35.099
STEP: Waiting for all pods to be running 09/04/23 15:50:35.099
Sep  4 15:50:35.113: INFO: pods: 1 < 3
STEP: locating a running pod 09/04/23 15:50:37.13
STEP: Updating the pdb to allow a pod to be evicted 09/04/23 15:50:37.164
STEP: Waiting for the pdb to be processed 09/04/23 15:50:37.195
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/04/23 15:50:37.209
STEP: Waiting for all pods to be running 09/04/23 15:50:37.209
STEP: Waiting for the pdb to observed all healthy pods 09/04/23 15:50:37.225
STEP: Patching the pdb to disallow a pod to be evicted 09/04/23 15:50:37.263
STEP: Waiting for the pdb to be processed 09/04/23 15:50:37.294
STEP: Waiting for all pods to be running 09/04/23 15:50:37.309
Sep  4 15:50:37.325: INFO: running pods: 2 < 3
STEP: locating a running pod 09/04/23 15:50:39.342
STEP: Deleting the pdb to allow a pod to be evicted 09/04/23 15:50:39.391
STEP: Waiting for the pdb to be deleted 09/04/23 15:50:39.407
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/04/23 15:50:39.422
STEP: Waiting for all pods to be running 09/04/23 15:50:39.422
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  4 15:50:39.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1267" for this suite. 09/04/23 15:50:39.487
------------------------------
• [4.547 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:50:34.955
    Sep  4 15:50:34.955: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 09/04/23 15:50:34.956
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:34.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:35.026
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 09/04/23 15:50:35.053
    STEP: Waiting for the pdb to be processed 09/04/23 15:50:35.069
    STEP: First trying to evict a pod which shouldn't be evictable 09/04/23 15:50:35.099
    STEP: Waiting for all pods to be running 09/04/23 15:50:35.099
    Sep  4 15:50:35.113: INFO: pods: 1 < 3
    STEP: locating a running pod 09/04/23 15:50:37.13
    STEP: Updating the pdb to allow a pod to be evicted 09/04/23 15:50:37.164
    STEP: Waiting for the pdb to be processed 09/04/23 15:50:37.195
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/04/23 15:50:37.209
    STEP: Waiting for all pods to be running 09/04/23 15:50:37.209
    STEP: Waiting for the pdb to observed all healthy pods 09/04/23 15:50:37.225
    STEP: Patching the pdb to disallow a pod to be evicted 09/04/23 15:50:37.263
    STEP: Waiting for the pdb to be processed 09/04/23 15:50:37.294
    STEP: Waiting for all pods to be running 09/04/23 15:50:37.309
    Sep  4 15:50:37.325: INFO: running pods: 2 < 3
    STEP: locating a running pod 09/04/23 15:50:39.342
    STEP: Deleting the pdb to allow a pod to be evicted 09/04/23 15:50:39.391
    STEP: Waiting for the pdb to be deleted 09/04/23 15:50:39.407
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/04/23 15:50:39.422
    STEP: Waiting for all pods to be running 09/04/23 15:50:39.422
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:50:39.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1267" for this suite. 09/04/23 15:50:39.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:50:39.502
Sep  4 15:50:39.502: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test 09/04/23 15:50:39.503
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:39.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:39.577
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-1761 09/04/23 15:50:39.605
STEP: creating a selector 09/04/23 15:50:39.605
STEP: Creating the service pods in kubernetes 09/04/23 15:50:39.605
Sep  4 15:50:39.605: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  4 15:50:39.678: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1761" to be "running and ready"
Sep  4 15:50:39.692: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.369448ms
Sep  4 15:50:39.692: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:50:41.707: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.029536222s
Sep  4 15:50:41.707: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:50:43.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.030779224s
Sep  4 15:50:43.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:50:45.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.03012686s
Sep  4 15:50:45.708: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:50:47.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.030198264s
Sep  4 15:50:47.708: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:50:49.710: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.032160242s
Sep  4 15:50:49.710: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:50:51.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.030571697s
Sep  4 15:50:51.708: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:50:53.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.030700517s
Sep  4 15:50:53.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:50:55.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.030819042s
Sep  4 15:50:55.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:50:57.710: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.031763589s
Sep  4 15:50:57.710: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:50:59.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.03058298s
Sep  4 15:50:59.708: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 15:51:01.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.030648225s
Sep  4 15:51:01.708: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  4 15:51:01.708: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  4 15:51:01.724: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1761" to be "running and ready"
Sep  4 15:51:01.739: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.922609ms
Sep  4 15:51:01.739: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  4 15:51:01.739: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/04/23 15:51:01.755
Sep  4 15:51:01.794: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1761" to be "running"
Sep  4 15:51:01.808: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.062221ms
Sep  4 15:51:03.824: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.030503412s
Sep  4 15:51:03.824: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  4 15:51:03.840: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1761" to be "running"
Sep  4 15:51:03.855: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 14.817948ms
Sep  4 15:51:03.855: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep  4 15:51:03.870: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  4 15:51:03.870: INFO: Going to poll 100.64.0.206 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep  4 15:51:03.884: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.0.206 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1761 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:51:03.884: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:51:03.885: INFO: ExecWithOptions: Clientset creation
Sep  4 15:51:03.885: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1761/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.64.0.206+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  4 15:51:05.389: INFO: Found all 1 expected endpoints: [netserver-0]
Sep  4 15:51:05.389: INFO: Going to poll 100.64.1.130 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep  4 15:51:05.405: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.1.130 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1761 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:51:05.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:51:05.405: INFO: ExecWithOptions: Clientset creation
Sep  4 15:51:05.405: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1761/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.64.1.130+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  4 15:51:06.932: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  4 15:51:06.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1761" for this suite. 09/04/23 15:51:06.961
------------------------------
• [27.475 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:50:39.502
    Sep  4 15:50:39.502: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pod-network-test 09/04/23 15:50:39.503
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:50:39.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:50:39.577
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-1761 09/04/23 15:50:39.605
    STEP: creating a selector 09/04/23 15:50:39.605
    STEP: Creating the service pods in kubernetes 09/04/23 15:50:39.605
    Sep  4 15:50:39.605: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  4 15:50:39.678: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1761" to be "running and ready"
    Sep  4 15:50:39.692: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.369448ms
    Sep  4 15:50:39.692: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:50:41.707: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.029536222s
    Sep  4 15:50:41.707: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:50:43.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.030779224s
    Sep  4 15:50:43.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:50:45.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.03012686s
    Sep  4 15:50:45.708: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:50:47.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.030198264s
    Sep  4 15:50:47.708: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:50:49.710: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.032160242s
    Sep  4 15:50:49.710: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:50:51.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.030571697s
    Sep  4 15:50:51.708: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:50:53.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.030700517s
    Sep  4 15:50:53.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:50:55.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.030819042s
    Sep  4 15:50:55.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:50:57.710: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.031763589s
    Sep  4 15:50:57.710: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:50:59.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.03058298s
    Sep  4 15:50:59.708: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 15:51:01.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.030648225s
    Sep  4 15:51:01.708: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  4 15:51:01.708: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  4 15:51:01.724: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1761" to be "running and ready"
    Sep  4 15:51:01.739: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.922609ms
    Sep  4 15:51:01.739: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  4 15:51:01.739: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/04/23 15:51:01.755
    Sep  4 15:51:01.794: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1761" to be "running"
    Sep  4 15:51:01.808: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.062221ms
    Sep  4 15:51:03.824: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.030503412s
    Sep  4 15:51:03.824: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  4 15:51:03.840: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1761" to be "running"
    Sep  4 15:51:03.855: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 14.817948ms
    Sep  4 15:51:03.855: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep  4 15:51:03.870: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  4 15:51:03.870: INFO: Going to poll 100.64.0.206 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Sep  4 15:51:03.884: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.0.206 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1761 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:51:03.884: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:51:03.885: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:51:03.885: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1761/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.64.0.206+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  4 15:51:05.389: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep  4 15:51:05.389: INFO: Going to poll 100.64.1.130 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Sep  4 15:51:05.405: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.1.130 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1761 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:51:05.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:51:05.405: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:51:05.405: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1761/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.64.1.130+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  4 15:51:06.932: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:51:06.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1761" for this suite. 09/04/23 15:51:06.961
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:51:06.978
Sep  4 15:51:06.978: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 09/04/23 15:51:06.979
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:07.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:07.054
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 09/04/23 15:51:07.082
STEP: wait for the container to reach Succeeded 09/04/23 15:51:07.106
STEP: get the container status 09/04/23 15:51:11.187
STEP: the container should be terminated 09/04/23 15:51:11.202
STEP: the termination message should be set 09/04/23 15:51:11.202
Sep  4 15:51:11.202: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 09/04/23 15:51:11.202
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  4 15:51:11.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7637" for this suite. 09/04/23 15:51:11.266
------------------------------
• [4.308 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:51:06.978
    Sep  4 15:51:06.978: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 09/04/23 15:51:06.979
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:07.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:07.054
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 09/04/23 15:51:07.082
    STEP: wait for the container to reach Succeeded 09/04/23 15:51:07.106
    STEP: get the container status 09/04/23 15:51:11.187
    STEP: the container should be terminated 09/04/23 15:51:11.202
    STEP: the termination message should be set 09/04/23 15:51:11.202
    Sep  4 15:51:11.202: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 09/04/23 15:51:11.202
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:51:11.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7637" for this suite. 09/04/23 15:51:11.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:51:11.286
Sep  4 15:51:11.286: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 09/04/23 15:51:11.287
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:11.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:11.378
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/04/23 15:51:11.406
Sep  4 15:51:11.429: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-546" to be "running and ready"
Sep  4 15:51:11.444: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 14.772534ms
Sep  4 15:51:11.444: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:51:13.460: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.03053811s
Sep  4 15:51:13.460: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Sep  4 15:51:13.460: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 09/04/23 15:51:13.474
STEP: Then the orphan pod is adopted 09/04/23 15:51:13.491
STEP: When the matched label of one of its pods change 09/04/23 15:51:13.506
Sep  4 15:51:13.522: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 09/04/23 15:51:13.555
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  4 15:51:13.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-546" for this suite. 09/04/23 15:51:13.598
------------------------------
• [2.329 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:51:11.286
    Sep  4 15:51:11.286: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 09/04/23 15:51:11.287
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:11.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:11.378
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/04/23 15:51:11.406
    Sep  4 15:51:11.429: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-546" to be "running and ready"
    Sep  4 15:51:11.444: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 14.772534ms
    Sep  4 15:51:11.444: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:51:13.460: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.03053811s
    Sep  4 15:51:13.460: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Sep  4 15:51:13.460: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 09/04/23 15:51:13.474
    STEP: Then the orphan pod is adopted 09/04/23 15:51:13.491
    STEP: When the matched label of one of its pods change 09/04/23 15:51:13.506
    Sep  4 15:51:13.522: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/04/23 15:51:13.555
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:51:13.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-546" for this suite. 09/04/23 15:51:13.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:51:13.615
Sep  4 15:51:13.615: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 15:51:13.616
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:13.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:13.688
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 09/04/23 15:51:13.716
Sep  4 15:51:13.716: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: rename a version 09/04/23 15:51:19.166
STEP: check the new version name is served 09/04/23 15:51:19.207
STEP: check the old version name is removed 09/04/23 15:51:21.303
STEP: check the other version is not changed 09/04/23 15:51:22.381
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:51:26.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4828" for this suite. 09/04/23 15:51:26.732
------------------------------
• [13.130 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:51:13.615
    Sep  4 15:51:13.615: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 15:51:13.616
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:13.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:13.688
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 09/04/23 15:51:13.716
    Sep  4 15:51:13.716: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: rename a version 09/04/23 15:51:19.166
    STEP: check the new version name is served 09/04/23 15:51:19.207
    STEP: check the old version name is removed 09/04/23 15:51:21.303
    STEP: check the other version is not changed 09/04/23 15:51:22.381
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:51:26.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4828" for this suite. 09/04/23 15:51:26.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:51:26.745
Sep  4 15:51:26.745: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/04/23 15:51:26.746
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:26.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:26.799
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 09/04/23 15:51:26.82
STEP: Creating hostNetwork=false pod 09/04/23 15:51:26.82
Sep  4 15:51:26.840: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4294" to be "running and ready"
Sep  4 15:51:26.851: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.062756ms
Sep  4 15:51:26.851: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:51:28.864: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02410709s
Sep  4 15:51:28.864: INFO: The phase of Pod test-pod is Running (Ready = true)
Sep  4 15:51:28.864: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 09/04/23 15:51:28.876
Sep  4 15:51:28.893: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4294" to be "running and ready"
Sep  4 15:51:28.904: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.875635ms
Sep  4 15:51:28.904: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:51:30.917: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023728985s
Sep  4 15:51:30.917: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Sep  4 15:51:30.917: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 09/04/23 15:51:30.928
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/04/23 15:51:30.929
Sep  4 15:51:30.929: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:51:30.929: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:51:30.929: INFO: ExecWithOptions: Clientset creation
Sep  4 15:51:30.929: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  4 15:51:31.386: INFO: Exec stderr: ""
Sep  4 15:51:31.386: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:51:31.386: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:51:31.387: INFO: ExecWithOptions: Clientset creation
Sep  4 15:51:31.387: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  4 15:51:31.829: INFO: Exec stderr: ""
Sep  4 15:51:31.829: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:51:31.829: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:51:31.830: INFO: ExecWithOptions: Clientset creation
Sep  4 15:51:31.830: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  4 15:51:32.391: INFO: Exec stderr: ""
Sep  4 15:51:32.391: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:51:32.391: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:51:32.392: INFO: ExecWithOptions: Clientset creation
Sep  4 15:51:32.392: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  4 15:51:32.738: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/04/23 15:51:32.738
Sep  4 15:51:32.738: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:51:32.738: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:51:32.739: INFO: ExecWithOptions: Clientset creation
Sep  4 15:51:32.739: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep  4 15:51:33.228: INFO: Exec stderr: ""
Sep  4 15:51:33.228: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:51:33.228: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:51:33.229: INFO: ExecWithOptions: Clientset creation
Sep  4 15:51:33.229: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep  4 15:51:33.673: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/04/23 15:51:33.673
Sep  4 15:51:33.673: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:51:33.673: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:51:33.674: INFO: ExecWithOptions: Clientset creation
Sep  4 15:51:33.674: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  4 15:51:34.166: INFO: Exec stderr: ""
Sep  4 15:51:34.166: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:51:34.167: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:51:34.167: INFO: ExecWithOptions: Clientset creation
Sep  4 15:51:34.167: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  4 15:51:34.758: INFO: Exec stderr: ""
Sep  4 15:51:34.758: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:51:34.758: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:51:34.759: INFO: ExecWithOptions: Clientset creation
Sep  4 15:51:34.759: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  4 15:51:35.142: INFO: Exec stderr: ""
Sep  4 15:51:35.142: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 15:51:35.142: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 15:51:35.143: INFO: ExecWithOptions: Clientset creation
Sep  4 15:51:35.143: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  4 15:51:35.734: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Sep  4 15:51:35.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4294" for this suite. 09/04/23 15:51:35.755
------------------------------
• [9.023 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:51:26.745
    Sep  4 15:51:26.745: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/04/23 15:51:26.746
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:26.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:26.799
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 09/04/23 15:51:26.82
    STEP: Creating hostNetwork=false pod 09/04/23 15:51:26.82
    Sep  4 15:51:26.840: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4294" to be "running and ready"
    Sep  4 15:51:26.851: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.062756ms
    Sep  4 15:51:26.851: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:51:28.864: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02410709s
    Sep  4 15:51:28.864: INFO: The phase of Pod test-pod is Running (Ready = true)
    Sep  4 15:51:28.864: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 09/04/23 15:51:28.876
    Sep  4 15:51:28.893: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4294" to be "running and ready"
    Sep  4 15:51:28.904: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.875635ms
    Sep  4 15:51:28.904: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:51:30.917: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023728985s
    Sep  4 15:51:30.917: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Sep  4 15:51:30.917: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 09/04/23 15:51:30.928
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/04/23 15:51:30.929
    Sep  4 15:51:30.929: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:51:30.929: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:51:30.929: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:51:30.929: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  4 15:51:31.386: INFO: Exec stderr: ""
    Sep  4 15:51:31.386: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:51:31.386: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:51:31.387: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:51:31.387: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  4 15:51:31.829: INFO: Exec stderr: ""
    Sep  4 15:51:31.829: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:51:31.829: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:51:31.830: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:51:31.830: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  4 15:51:32.391: INFO: Exec stderr: ""
    Sep  4 15:51:32.391: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:51:32.391: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:51:32.392: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:51:32.392: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  4 15:51:32.738: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/04/23 15:51:32.738
    Sep  4 15:51:32.738: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:51:32.738: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:51:32.739: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:51:32.739: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep  4 15:51:33.228: INFO: Exec stderr: ""
    Sep  4 15:51:33.228: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:51:33.228: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:51:33.229: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:51:33.229: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep  4 15:51:33.673: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/04/23 15:51:33.673
    Sep  4 15:51:33.673: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:51:33.673: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:51:33.674: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:51:33.674: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  4 15:51:34.166: INFO: Exec stderr: ""
    Sep  4 15:51:34.166: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:51:34.167: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:51:34.167: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:51:34.167: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  4 15:51:34.758: INFO: Exec stderr: ""
    Sep  4 15:51:34.758: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:51:34.758: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:51:34.759: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:51:34.759: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  4 15:51:35.142: INFO: Exec stderr: ""
    Sep  4 15:51:35.142: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4294 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 15:51:35.142: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 15:51:35.143: INFO: ExecWithOptions: Clientset creation
    Sep  4 15:51:35.143: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-4294/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  4 15:51:35.734: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:51:35.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-4294" for this suite. 09/04/23 15:51:35.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:51:35.769
Sep  4 15:51:35.769: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 15:51:35.77
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:35.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:35.827
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 09/04/23 15:51:35.847
Sep  4 15:51:35.866: INFO: Waiting up to 5m0s for pod "downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686" in namespace "downward-api-9420" to be "Succeeded or Failed"
Sep  4 15:51:35.878: INFO: Pod "downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686": Phase="Pending", Reason="", readiness=false. Elapsed: 11.479141ms
Sep  4 15:51:37.891: INFO: Pod "downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024339575s
Sep  4 15:51:39.892: INFO: Pod "downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025419519s
STEP: Saw pod success 09/04/23 15:51:39.892
Sep  4 15:51:39.892: INFO: Pod "downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686" satisfied condition "Succeeded or Failed"
Sep  4 15:51:39.904: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx pod downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686 container dapi-container: <nil>
STEP: delete the pod 09/04/23 15:51:39.938
Sep  4 15:51:39.954: INFO: Waiting for pod downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686 to disappear
Sep  4 15:51:39.966: INFO: Pod downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  4 15:51:39.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9420" for this suite. 09/04/23 15:51:39.988
------------------------------
• [4.233 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:51:35.769
    Sep  4 15:51:35.769: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 15:51:35.77
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:35.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:35.827
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 09/04/23 15:51:35.847
    Sep  4 15:51:35.866: INFO: Waiting up to 5m0s for pod "downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686" in namespace "downward-api-9420" to be "Succeeded or Failed"
    Sep  4 15:51:35.878: INFO: Pod "downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686": Phase="Pending", Reason="", readiness=false. Elapsed: 11.479141ms
    Sep  4 15:51:37.891: INFO: Pod "downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024339575s
    Sep  4 15:51:39.892: INFO: Pod "downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025419519s
    STEP: Saw pod success 09/04/23 15:51:39.892
    Sep  4 15:51:39.892: INFO: Pod "downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686" satisfied condition "Succeeded or Failed"
    Sep  4 15:51:39.904: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx pod downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686 container dapi-container: <nil>
    STEP: delete the pod 09/04/23 15:51:39.938
    Sep  4 15:51:39.954: INFO: Waiting for pod downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686 to disappear
    Sep  4 15:51:39.966: INFO: Pod downward-api-441419fa-8686-4884-8afc-5f2ffc0cc686 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:51:39.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9420" for this suite. 09/04/23 15:51:39.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:51:40.002
Sep  4 15:51:40.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 15:51:40.003
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:40.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:40.061
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Sep  4 15:51:40.082: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/04/23 15:51:42.277
Sep  4 15:51:42.277: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 create -f -'
Sep  4 15:51:43.171: INFO: stderr: ""
Sep  4 15:51:43.171: INFO: stdout: "e2e-test-crd-publish-openapi-1746-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  4 15:51:43.171: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 delete e2e-test-crd-publish-openapi-1746-crds test-foo'
Sep  4 15:51:43.275: INFO: stderr: ""
Sep  4 15:51:43.275: INFO: stdout: "e2e-test-crd-publish-openapi-1746-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep  4 15:51:43.275: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 apply -f -'
Sep  4 15:51:43.881: INFO: stderr: ""
Sep  4 15:51:43.881: INFO: stdout: "e2e-test-crd-publish-openapi-1746-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  4 15:51:43.881: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 delete e2e-test-crd-publish-openapi-1746-crds test-foo'
Sep  4 15:51:44.009: INFO: stderr: ""
Sep  4 15:51:44.009: INFO: stdout: "e2e-test-crd-publish-openapi-1746-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/04/23 15:51:44.009
Sep  4 15:51:44.009: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 create -f -'
Sep  4 15:51:44.259: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/04/23 15:51:44.259
Sep  4 15:51:44.259: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 create -f -'
Sep  4 15:51:44.488: INFO: rc: 1
Sep  4 15:51:44.488: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 apply -f -'
Sep  4 15:51:44.754: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/04/23 15:51:44.754
Sep  4 15:51:44.754: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 create -f -'
Sep  4 15:51:44.985: INFO: rc: 1
Sep  4 15:51:44.986: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 apply -f -'
Sep  4 15:51:45.235: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 09/04/23 15:51:45.235
Sep  4 15:51:45.235: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 explain e2e-test-crd-publish-openapi-1746-crds'
Sep  4 15:51:45.441: INFO: stderr: ""
Sep  4 15:51:45.441: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1746-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 09/04/23 15:51:45.441
Sep  4 15:51:45.441: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 explain e2e-test-crd-publish-openapi-1746-crds.metadata'
Sep  4 15:51:45.647: INFO: stderr: ""
Sep  4 15:51:45.647: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1746-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep  4 15:51:45.647: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 explain e2e-test-crd-publish-openapi-1746-crds.spec'
Sep  4 15:51:45.847: INFO: stderr: ""
Sep  4 15:51:45.847: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1746-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep  4 15:51:45.847: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 explain e2e-test-crd-publish-openapi-1746-crds.spec.bars'
Sep  4 15:51:46.043: INFO: stderr: ""
Sep  4 15:51:46.043: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1746-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/04/23 15:51:46.043
Sep  4 15:51:46.043: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 explain e2e-test-crd-publish-openapi-1746-crds.spec.bars2'
Sep  4 15:51:46.239: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:51:48.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5878" for this suite. 09/04/23 15:51:48.484
------------------------------
• [8.498 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:51:40.002
    Sep  4 15:51:40.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 15:51:40.003
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:40.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:40.061
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Sep  4 15:51:40.082: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/04/23 15:51:42.277
    Sep  4 15:51:42.277: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 create -f -'
    Sep  4 15:51:43.171: INFO: stderr: ""
    Sep  4 15:51:43.171: INFO: stdout: "e2e-test-crd-publish-openapi-1746-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep  4 15:51:43.171: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 delete e2e-test-crd-publish-openapi-1746-crds test-foo'
    Sep  4 15:51:43.275: INFO: stderr: ""
    Sep  4 15:51:43.275: INFO: stdout: "e2e-test-crd-publish-openapi-1746-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Sep  4 15:51:43.275: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 apply -f -'
    Sep  4 15:51:43.881: INFO: stderr: ""
    Sep  4 15:51:43.881: INFO: stdout: "e2e-test-crd-publish-openapi-1746-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep  4 15:51:43.881: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 delete e2e-test-crd-publish-openapi-1746-crds test-foo'
    Sep  4 15:51:44.009: INFO: stderr: ""
    Sep  4 15:51:44.009: INFO: stdout: "e2e-test-crd-publish-openapi-1746-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/04/23 15:51:44.009
    Sep  4 15:51:44.009: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 create -f -'
    Sep  4 15:51:44.259: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/04/23 15:51:44.259
    Sep  4 15:51:44.259: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 create -f -'
    Sep  4 15:51:44.488: INFO: rc: 1
    Sep  4 15:51:44.488: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 apply -f -'
    Sep  4 15:51:44.754: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/04/23 15:51:44.754
    Sep  4 15:51:44.754: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 create -f -'
    Sep  4 15:51:44.985: INFO: rc: 1
    Sep  4 15:51:44.986: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 --namespace=crd-publish-openapi-5878 apply -f -'
    Sep  4 15:51:45.235: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 09/04/23 15:51:45.235
    Sep  4 15:51:45.235: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 explain e2e-test-crd-publish-openapi-1746-crds'
    Sep  4 15:51:45.441: INFO: stderr: ""
    Sep  4 15:51:45.441: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1746-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 09/04/23 15:51:45.441
    Sep  4 15:51:45.441: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 explain e2e-test-crd-publish-openapi-1746-crds.metadata'
    Sep  4 15:51:45.647: INFO: stderr: ""
    Sep  4 15:51:45.647: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1746-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Sep  4 15:51:45.647: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 explain e2e-test-crd-publish-openapi-1746-crds.spec'
    Sep  4 15:51:45.847: INFO: stderr: ""
    Sep  4 15:51:45.847: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1746-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Sep  4 15:51:45.847: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 explain e2e-test-crd-publish-openapi-1746-crds.spec.bars'
    Sep  4 15:51:46.043: INFO: stderr: ""
    Sep  4 15:51:46.043: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1746-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/04/23 15:51:46.043
    Sep  4 15:51:46.043: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5878 explain e2e-test-crd-publish-openapi-1746-crds.spec.bars2'
    Sep  4 15:51:46.239: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:51:48.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5878" for this suite. 09/04/23 15:51:48.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:51:48.5
Sep  4 15:51:48.500: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 15:51:48.501
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:48.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:48.572
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 09/04/23 15:51:48.598
Sep  4 15:51:48.620: INFO: Waiting up to 5m0s for pod "downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64" in namespace "downward-api-695" to be "Succeeded or Failed"
Sep  4 15:51:48.634: INFO: Pod "downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64": Phase="Pending", Reason="", readiness=false. Elapsed: 13.623973ms
Sep  4 15:51:50.651: INFO: Pod "downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64": Phase="Running", Reason="", readiness=false. Elapsed: 2.030289117s
Sep  4 15:51:52.649: INFO: Pod "downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028948472s
STEP: Saw pod success 09/04/23 15:51:52.649
Sep  4 15:51:52.649: INFO: Pod "downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64" satisfied condition "Succeeded or Failed"
Sep  4 15:51:52.664: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64 container client-container: <nil>
STEP: delete the pod 09/04/23 15:51:52.701
Sep  4 15:51:52.719: INFO: Waiting for pod downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64 to disappear
Sep  4 15:51:52.736: INFO: Pod downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 15:51:52.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-695" for this suite. 09/04/23 15:51:52.763
------------------------------
• [4.278 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:51:48.5
    Sep  4 15:51:48.500: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 15:51:48.501
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:48.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:48.572
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 09/04/23 15:51:48.598
    Sep  4 15:51:48.620: INFO: Waiting up to 5m0s for pod "downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64" in namespace "downward-api-695" to be "Succeeded or Failed"
    Sep  4 15:51:48.634: INFO: Pod "downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64": Phase="Pending", Reason="", readiness=false. Elapsed: 13.623973ms
    Sep  4 15:51:50.651: INFO: Pod "downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64": Phase="Running", Reason="", readiness=false. Elapsed: 2.030289117s
    Sep  4 15:51:52.649: INFO: Pod "downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028948472s
    STEP: Saw pod success 09/04/23 15:51:52.649
    Sep  4 15:51:52.649: INFO: Pod "downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64" satisfied condition "Succeeded or Failed"
    Sep  4 15:51:52.664: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64 container client-container: <nil>
    STEP: delete the pod 09/04/23 15:51:52.701
    Sep  4 15:51:52.719: INFO: Waiting for pod downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64 to disappear
    Sep  4 15:51:52.736: INFO: Pod downwardapi-volume-62f8f709-ea30-406d-a9a5-a26262559c64 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:51:52.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-695" for this suite. 09/04/23 15:51:52.763
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:51:52.779
Sep  4 15:51:52.779: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 09/04/23 15:51:52.78
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:52.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:52.848
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/04/23 15:51:52.874
Sep  4 15:51:52.904: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/04/23 15:51:52.904
Sep  4 15:51:52.904: INFO: Waiting up to 5m0s for pod "test-rs-9dfbv" in namespace "replicaset-4869" to be "running"
Sep  4 15:51:52.918: INFO: Pod "test-rs-9dfbv": Phase="Pending", Reason="", readiness=false. Elapsed: 13.643022ms
Sep  4 15:51:54.934: INFO: Pod "test-rs-9dfbv": Phase="Running", Reason="", readiness=true. Elapsed: 2.029789956s
Sep  4 15:51:54.934: INFO: Pod "test-rs-9dfbv" satisfied condition "running"
STEP: getting scale subresource 09/04/23 15:51:54.934
STEP: updating a scale subresource 09/04/23 15:51:54.948
STEP: verifying the replicaset Spec.Replicas was modified 09/04/23 15:51:54.964
STEP: Patch a scale subresource 09/04/23 15:51:54.977
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  4 15:51:55.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4869" for this suite. 09/04/23 15:51:55.037
------------------------------
• [2.274 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:51:52.779
    Sep  4 15:51:52.779: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 09/04/23 15:51:52.78
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:52.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:52.848
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/04/23 15:51:52.874
    Sep  4 15:51:52.904: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/04/23 15:51:52.904
    Sep  4 15:51:52.904: INFO: Waiting up to 5m0s for pod "test-rs-9dfbv" in namespace "replicaset-4869" to be "running"
    Sep  4 15:51:52.918: INFO: Pod "test-rs-9dfbv": Phase="Pending", Reason="", readiness=false. Elapsed: 13.643022ms
    Sep  4 15:51:54.934: INFO: Pod "test-rs-9dfbv": Phase="Running", Reason="", readiness=true. Elapsed: 2.029789956s
    Sep  4 15:51:54.934: INFO: Pod "test-rs-9dfbv" satisfied condition "running"
    STEP: getting scale subresource 09/04/23 15:51:54.934
    STEP: updating a scale subresource 09/04/23 15:51:54.948
    STEP: verifying the replicaset Spec.Replicas was modified 09/04/23 15:51:54.964
    STEP: Patch a scale subresource 09/04/23 15:51:54.977
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:51:55.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4869" for this suite. 09/04/23 15:51:55.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:51:55.053
Sep  4 15:51:55.053: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 09/04/23 15:51:55.055
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:55.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:55.125
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 09/04/23 15:51:55.152
STEP: Creating a ResourceQuota 09/04/23 15:52:00.167
STEP: Ensuring resource quota status is calculated 09/04/23 15:52:00.182
STEP: Creating a ReplicationController 09/04/23 15:52:02.199
STEP: Ensuring resource quota status captures replication controller creation 09/04/23 15:52:02.219
STEP: Deleting a ReplicationController 09/04/23 15:52:04.234
STEP: Ensuring resource quota status released usage 09/04/23 15:52:04.249
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 15:52:06.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8855" for this suite. 09/04/23 15:52:06.292
------------------------------
• [11.256 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:51:55.053
    Sep  4 15:51:55.053: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 09/04/23 15:51:55.055
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:51:55.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:51:55.125
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 09/04/23 15:51:55.152
    STEP: Creating a ResourceQuota 09/04/23 15:52:00.167
    STEP: Ensuring resource quota status is calculated 09/04/23 15:52:00.182
    STEP: Creating a ReplicationController 09/04/23 15:52:02.199
    STEP: Ensuring resource quota status captures replication controller creation 09/04/23 15:52:02.219
    STEP: Deleting a ReplicationController 09/04/23 15:52:04.234
    STEP: Ensuring resource quota status released usage 09/04/23 15:52:04.249
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:52:06.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8855" for this suite. 09/04/23 15:52:06.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:52:06.31
Sep  4 15:52:06.310: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 09/04/23 15:52:06.311
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:06.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:06.389
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 09/04/23 15:52:06.418
STEP: Getting a ResourceQuota 09/04/23 15:52:06.432
STEP: Updating a ResourceQuota 09/04/23 15:52:06.447
STEP: Verifying a ResourceQuota was modified 09/04/23 15:52:06.463
STEP: Deleting a ResourceQuota 09/04/23 15:52:06.476
STEP: Verifying the deleted ResourceQuota 09/04/23 15:52:06.491
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 15:52:06.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8617" for this suite. 09/04/23 15:52:06.52
------------------------------
• [0.226 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:52:06.31
    Sep  4 15:52:06.310: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 09/04/23 15:52:06.311
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:06.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:06.389
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 09/04/23 15:52:06.418
    STEP: Getting a ResourceQuota 09/04/23 15:52:06.432
    STEP: Updating a ResourceQuota 09/04/23 15:52:06.447
    STEP: Verifying a ResourceQuota was modified 09/04/23 15:52:06.463
    STEP: Deleting a ResourceQuota 09/04/23 15:52:06.476
    STEP: Verifying the deleted ResourceQuota 09/04/23 15:52:06.491
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:52:06.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8617" for this suite. 09/04/23 15:52:06.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:52:06.536
Sep  4 15:52:06.536: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 09/04/23 15:52:06.537
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:06.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:06.606
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 09/04/23 15:52:06.632
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 140.159.106.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.106.159.140_udp@PTR;check="$$(dig +tcp +noall +answer +search 140.159.106.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.106.159.140_tcp@PTR;sleep 1; done
 09/04/23 15:52:06.666
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 140.159.106.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.106.159.140_udp@PTR;check="$$(dig +tcp +noall +answer +search 140.159.106.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.106.159.140_tcp@PTR;sleep 1; done
 09/04/23 15:52:06.666
STEP: creating a pod to probe DNS 09/04/23 15:52:06.666
STEP: submitting the pod to kubernetes 09/04/23 15:52:06.666
Sep  4 15:52:06.690: INFO: Waiting up to 15m0s for pod "dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7" in namespace "dns-9223" to be "running"
Sep  4 15:52:06.703: INFO: Pod "dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.714675ms
Sep  4 15:52:08.719: INFO: Pod "dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.029320996s
Sep  4 15:52:08.719: INFO: Pod "dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7" satisfied condition "running"
STEP: retrieving the pod 09/04/23 15:52:08.719
STEP: looking for the results for each expected name from probers 09/04/23 15:52:08.734
Sep  4 15:52:08.861: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:08.904: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:08.946: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:09.021: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:09.153: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:09.180: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:09.207: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:09.250: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:09.385: INFO: Lookups using dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

Sep  4 15:52:14.414: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:14.467: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:14.493: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:14.518: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:14.656: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:14.682: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:14.707: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:14.733: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:14.877: INFO: Lookups using dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

Sep  4 15:52:19.411: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:19.463: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:19.489: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:19.515: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:19.643: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:19.671: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:19.697: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:19.722: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
Sep  4 15:52:19.829: INFO: Lookups using dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

Sep  4 15:52:24.944: INFO: DNS probes using dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7 succeeded

STEP: deleting the pod 09/04/23 15:52:24.944
STEP: deleting the test service 09/04/23 15:52:24.968
STEP: deleting the test headless service 09/04/23 15:52:24.989
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 15:52:25.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9223" for this suite. 09/04/23 15:52:25.045
------------------------------
• [18.525 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:52:06.536
    Sep  4 15:52:06.536: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 09/04/23 15:52:06.537
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:06.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:06.606
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 09/04/23 15:52:06.632
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 140.159.106.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.106.159.140_udp@PTR;check="$$(dig +tcp +noall +answer +search 140.159.106.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.106.159.140_tcp@PTR;sleep 1; done
     09/04/23 15:52:06.666
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 140.159.106.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.106.159.140_udp@PTR;check="$$(dig +tcp +noall +answer +search 140.159.106.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.106.159.140_tcp@PTR;sleep 1; done
     09/04/23 15:52:06.666
    STEP: creating a pod to probe DNS 09/04/23 15:52:06.666
    STEP: submitting the pod to kubernetes 09/04/23 15:52:06.666
    Sep  4 15:52:06.690: INFO: Waiting up to 15m0s for pod "dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7" in namespace "dns-9223" to be "running"
    Sep  4 15:52:06.703: INFO: Pod "dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.714675ms
    Sep  4 15:52:08.719: INFO: Pod "dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.029320996s
    Sep  4 15:52:08.719: INFO: Pod "dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 15:52:08.719
    STEP: looking for the results for each expected name from probers 09/04/23 15:52:08.734
    Sep  4 15:52:08.861: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:08.904: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:08.946: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:09.021: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:09.153: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:09.180: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:09.207: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:09.250: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:09.385: INFO: Lookups using dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

    Sep  4 15:52:14.414: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:14.467: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:14.493: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:14.518: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:14.656: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:14.682: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:14.707: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:14.733: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:14.877: INFO: Lookups using dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

    Sep  4 15:52:19.411: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:19.463: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:19.489: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:19.515: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:19.643: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:19.671: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:19.697: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:19.722: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7: the server could not find the requested resource (get pods dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7)
    Sep  4 15:52:19.829: INFO: Lookups using dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

    Sep  4 15:52:24.944: INFO: DNS probes using dns-9223/dns-test-9c3dbc78-78fc-4020-a5b4-8cbb538591e7 succeeded

    STEP: deleting the pod 09/04/23 15:52:24.944
    STEP: deleting the test service 09/04/23 15:52:24.968
    STEP: deleting the test headless service 09/04/23 15:52:24.989
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:52:25.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9223" for this suite. 09/04/23 15:52:25.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:52:25.061
Sep  4 15:52:25.061: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 09/04/23 15:52:25.062
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:25.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:25.133
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 09/04/23 15:52:25.161
Sep  4 15:52:25.182: INFO: Waiting up to 5m0s for pod "var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f" in namespace "var-expansion-6146" to be "Succeeded or Failed"
Sep  4 15:52:25.196: INFO: Pod "var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.930928ms
Sep  4 15:52:27.211: INFO: Pod "var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029203391s
Sep  4 15:52:29.215: INFO: Pod "var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032623814s
STEP: Saw pod success 09/04/23 15:52:29.215
Sep  4 15:52:29.215: INFO: Pod "var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f" satisfied condition "Succeeded or Failed"
Sep  4 15:52:29.229: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f container dapi-container: <nil>
STEP: delete the pod 09/04/23 15:52:29.277
Sep  4 15:52:29.296: INFO: Waiting for pod var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f to disappear
Sep  4 15:52:29.310: INFO: Pod var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 15:52:29.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6146" for this suite. 09/04/23 15:52:29.336
------------------------------
• [4.291 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:52:25.061
    Sep  4 15:52:25.061: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 09/04/23 15:52:25.062
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:25.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:25.133
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 09/04/23 15:52:25.161
    Sep  4 15:52:25.182: INFO: Waiting up to 5m0s for pod "var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f" in namespace "var-expansion-6146" to be "Succeeded or Failed"
    Sep  4 15:52:25.196: INFO: Pod "var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.930928ms
    Sep  4 15:52:27.211: INFO: Pod "var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029203391s
    Sep  4 15:52:29.215: INFO: Pod "var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032623814s
    STEP: Saw pod success 09/04/23 15:52:29.215
    Sep  4 15:52:29.215: INFO: Pod "var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f" satisfied condition "Succeeded or Failed"
    Sep  4 15:52:29.229: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f container dapi-container: <nil>
    STEP: delete the pod 09/04/23 15:52:29.277
    Sep  4 15:52:29.296: INFO: Waiting for pod var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f to disappear
    Sep  4 15:52:29.310: INFO: Pod var-expansion-3ba47243-1f1d-475e-8aef-087a1a4f862f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:52:29.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6146" for this suite. 09/04/23 15:52:29.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:52:29.353
Sep  4 15:52:29.353: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 09/04/23 15:52:29.354
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:29.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:29.432
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Sep  4 15:52:29.534: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9d21c2ec-3b5d-4d4d-a883-d212404a3953", Controller:(*bool)(0xc005278346), BlockOwnerDeletion:(*bool)(0xc005278347)}}
Sep  4 15:52:29.550: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"23894903-a834-4dd2-976a-bc0f1596d273", Controller:(*bool)(0xc005dacb3e), BlockOwnerDeletion:(*bool)(0xc005dacb3f)}}
Sep  4 15:52:29.567: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4616db96-872a-4ba5-b020-3f9437319541", Controller:(*bool)(0xc00309653e), BlockOwnerDeletion:(*bool)(0xc00309653f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  4 15:52:34.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9411" for this suite. 09/04/23 15:52:34.626
------------------------------
• [5.288 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:52:29.353
    Sep  4 15:52:29.353: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 09/04/23 15:52:29.354
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:29.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:29.432
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Sep  4 15:52:29.534: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9d21c2ec-3b5d-4d4d-a883-d212404a3953", Controller:(*bool)(0xc005278346), BlockOwnerDeletion:(*bool)(0xc005278347)}}
    Sep  4 15:52:29.550: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"23894903-a834-4dd2-976a-bc0f1596d273", Controller:(*bool)(0xc005dacb3e), BlockOwnerDeletion:(*bool)(0xc005dacb3f)}}
    Sep  4 15:52:29.567: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4616db96-872a-4ba5-b020-3f9437319541", Controller:(*bool)(0xc00309653e), BlockOwnerDeletion:(*bool)(0xc00309653f)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:52:34.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9411" for this suite. 09/04/23 15:52:34.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:52:34.642
Sep  4 15:52:34.642: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 15:52:34.643
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:34.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:34.713
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 09/04/23 15:52:34.739
Sep  4 15:52:34.762: INFO: Waiting up to 5m0s for pod "annotationupdate8f7318fd-c4cd-4323-83d8-c1d314350df1" in namespace "downward-api-2483" to be "running and ready"
Sep  4 15:52:34.776: INFO: Pod "annotationupdate8f7318fd-c4cd-4323-83d8-c1d314350df1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.961468ms
Sep  4 15:52:34.776: INFO: The phase of Pod annotationupdate8f7318fd-c4cd-4323-83d8-c1d314350df1 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 15:52:36.792: INFO: Pod "annotationupdate8f7318fd-c4cd-4323-83d8-c1d314350df1": Phase="Running", Reason="", readiness=true. Elapsed: 2.030128901s
Sep  4 15:52:36.792: INFO: The phase of Pod annotationupdate8f7318fd-c4cd-4323-83d8-c1d314350df1 is Running (Ready = true)
Sep  4 15:52:36.792: INFO: Pod "annotationupdate8f7318fd-c4cd-4323-83d8-c1d314350df1" satisfied condition "running and ready"
Sep  4 15:52:37.380: INFO: Successfully updated pod "annotationupdate8f7318fd-c4cd-4323-83d8-c1d314350df1"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 15:52:39.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2483" for this suite. 09/04/23 15:52:39.479
------------------------------
• [4.853 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:52:34.642
    Sep  4 15:52:34.642: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 15:52:34.643
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:34.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:34.713
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 09/04/23 15:52:34.739
    Sep  4 15:52:34.762: INFO: Waiting up to 5m0s for pod "annotationupdate8f7318fd-c4cd-4323-83d8-c1d314350df1" in namespace "downward-api-2483" to be "running and ready"
    Sep  4 15:52:34.776: INFO: Pod "annotationupdate8f7318fd-c4cd-4323-83d8-c1d314350df1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.961468ms
    Sep  4 15:52:34.776: INFO: The phase of Pod annotationupdate8f7318fd-c4cd-4323-83d8-c1d314350df1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 15:52:36.792: INFO: Pod "annotationupdate8f7318fd-c4cd-4323-83d8-c1d314350df1": Phase="Running", Reason="", readiness=true. Elapsed: 2.030128901s
    Sep  4 15:52:36.792: INFO: The phase of Pod annotationupdate8f7318fd-c4cd-4323-83d8-c1d314350df1 is Running (Ready = true)
    Sep  4 15:52:36.792: INFO: Pod "annotationupdate8f7318fd-c4cd-4323-83d8-c1d314350df1" satisfied condition "running and ready"
    Sep  4 15:52:37.380: INFO: Successfully updated pod "annotationupdate8f7318fd-c4cd-4323-83d8-c1d314350df1"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:52:39.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2483" for this suite. 09/04/23 15:52:39.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:52:39.496
Sep  4 15:52:39.496: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 15:52:39.497
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:39.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:39.568
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 09/04/23 15:52:39.594
Sep  4 15:52:39.617: INFO: Waiting up to 5m0s for pod "downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1" in namespace "downward-api-3203" to be "Succeeded or Failed"
Sep  4 15:52:39.631: INFO: Pod "downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.843068ms
Sep  4 15:52:41.647: INFO: Pod "downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030139174s
Sep  4 15:52:43.646: INFO: Pod "downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029506408s
STEP: Saw pod success 09/04/23 15:52:43.646
Sep  4 15:52:43.647: INFO: Pod "downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1" satisfied condition "Succeeded or Failed"
Sep  4 15:52:43.661: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1 container dapi-container: <nil>
STEP: delete the pod 09/04/23 15:52:43.698
Sep  4 15:52:43.715: INFO: Waiting for pod downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1 to disappear
Sep  4 15:52:43.729: INFO: Pod downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  4 15:52:43.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3203" for this suite. 09/04/23 15:52:43.764
------------------------------
• [4.283 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:52:39.496
    Sep  4 15:52:39.496: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 15:52:39.497
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:39.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:39.568
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 09/04/23 15:52:39.594
    Sep  4 15:52:39.617: INFO: Waiting up to 5m0s for pod "downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1" in namespace "downward-api-3203" to be "Succeeded or Failed"
    Sep  4 15:52:39.631: INFO: Pod "downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.843068ms
    Sep  4 15:52:41.647: INFO: Pod "downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030139174s
    Sep  4 15:52:43.646: INFO: Pod "downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029506408s
    STEP: Saw pod success 09/04/23 15:52:43.646
    Sep  4 15:52:43.647: INFO: Pod "downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1" satisfied condition "Succeeded or Failed"
    Sep  4 15:52:43.661: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1 container dapi-container: <nil>
    STEP: delete the pod 09/04/23 15:52:43.698
    Sep  4 15:52:43.715: INFO: Waiting for pod downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1 to disappear
    Sep  4 15:52:43.729: INFO: Pod downward-api-74ab005d-d68e-4341-ba6c-1940b04646a1 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:52:43.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3203" for this suite. 09/04/23 15:52:43.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:52:43.78
Sep  4 15:52:43.780: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 09/04/23 15:52:43.781
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:43.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:43.852
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Sep  4 15:52:43.879: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  4 15:52:43.911: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/04/23 15:52:43.911
Sep  4 15:52:43.911: INFO: Waiting up to 5m0s for pod "test-rolling-update-controller-qwgzz" in namespace "deployment-7451" to be "running"
Sep  4 15:52:43.925: INFO: Pod "test-rolling-update-controller-qwgzz": Phase="Pending", Reason="", readiness=false. Elapsed: 13.515543ms
Sep  4 15:52:45.941: INFO: Pod "test-rolling-update-controller-qwgzz": Phase="Running", Reason="", readiness=true. Elapsed: 2.029900873s
Sep  4 15:52:45.941: INFO: Pod "test-rolling-update-controller-qwgzz" satisfied condition "running"
Sep  4 15:52:45.941: INFO: Creating deployment "test-rolling-update-deployment"
Sep  4 15:52:45.957: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  4 15:52:45.986: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Sep  4 15:52:48.016: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  4 15:52:48.030: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 15:52:48.075: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7451  7f845197-303e-4cb3-b9e6-472233bd62b3 38102 1 2023-09-04 15:52:45 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-09-04 15:52:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:52:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043ee868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-04 15:52:45 +0000 UTC,LastTransitionTime:2023-09-04 15:52:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-09-04 15:52:47 +0000 UTC,LastTransitionTime:2023-09-04 15:52:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  4 15:52:48.090: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-7451  2ff6b96e-e3fa-4451-9042-1ee68904fa08 38094 1 2023-09-04 15:52:45 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7f845197-303e-4cb3-b9e6-472233bd62b3 0xc0043eed57 0xc0043eed58}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:52:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f845197-303e-4cb3-b9e6-472233bd62b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:52:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043eee08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  4 15:52:48.090: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  4 15:52:48.090: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7451  7fc3517b-cb26-4d13-a101-65f5bcfe3a60 38101 2 2023-09-04 15:52:43 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7f845197-303e-4cb3-b9e6-472233bd62b3 0xc0043eec17 0xc0043eec18}] [] [{e2e.test Update apps/v1 2023-09-04 15:52:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:52:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f845197-303e-4cb3-b9e6-472233bd62b3\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:52:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0043eece8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  4 15:52:48.105: INFO: Pod "test-rolling-update-deployment-7549d9f46d-wntdk" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-wntdk test-rolling-update-deployment-7549d9f46d- deployment-7451  309ca739-6e8a-4ba7-b92c-210fe8f50460 38093 0 2023-09-04 15:52:45 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:eba59e5275b396ebbb31d871bc2f4f9d2154b47bcdabda2091f77bcdb945f8ae cni.projectcalico.org/podIP:100.64.1.142/32 cni.projectcalico.org/podIPs:100.64.1.142/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 2ff6b96e-e3fa-4451-9042-1ee68904fa08 0xc0043ef277 0xc0043ef278}] [] [{kube-controller-manager Update v1 2023-09-04 15:52:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ff6b96e-e3fa-4451-9042-1ee68904fa08\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:52:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:52:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7slmq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7slmq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.142,StartTime:2023-09-04 15:52:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:52:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://8cc9f08bb336c4c309f471ffc59f51a5a7fbe4594f6e5db79c8170c79db44eae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.142,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 15:52:48.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7451" for this suite. 09/04/23 15:52:48.132
------------------------------
• [4.367 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:52:43.78
    Sep  4 15:52:43.780: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 09/04/23 15:52:43.781
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:43.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:43.852
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Sep  4 15:52:43.879: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Sep  4 15:52:43.911: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/04/23 15:52:43.911
    Sep  4 15:52:43.911: INFO: Waiting up to 5m0s for pod "test-rolling-update-controller-qwgzz" in namespace "deployment-7451" to be "running"
    Sep  4 15:52:43.925: INFO: Pod "test-rolling-update-controller-qwgzz": Phase="Pending", Reason="", readiness=false. Elapsed: 13.515543ms
    Sep  4 15:52:45.941: INFO: Pod "test-rolling-update-controller-qwgzz": Phase="Running", Reason="", readiness=true. Elapsed: 2.029900873s
    Sep  4 15:52:45.941: INFO: Pod "test-rolling-update-controller-qwgzz" satisfied condition "running"
    Sep  4 15:52:45.941: INFO: Creating deployment "test-rolling-update-deployment"
    Sep  4 15:52:45.957: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Sep  4 15:52:45.986: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
    Sep  4 15:52:48.016: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Sep  4 15:52:48.030: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 15:52:48.075: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7451  7f845197-303e-4cb3-b9e6-472233bd62b3 38102 1 2023-09-04 15:52:45 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-09-04 15:52:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:52:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043ee868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-04 15:52:45 +0000 UTC,LastTransitionTime:2023-09-04 15:52:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-09-04 15:52:47 +0000 UTC,LastTransitionTime:2023-09-04 15:52:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  4 15:52:48.090: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-7451  2ff6b96e-e3fa-4451-9042-1ee68904fa08 38094 1 2023-09-04 15:52:45 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7f845197-303e-4cb3-b9e6-472233bd62b3 0xc0043eed57 0xc0043eed58}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:52:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f845197-303e-4cb3-b9e6-472233bd62b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:52:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043eee08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 15:52:48.090: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Sep  4 15:52:48.090: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7451  7fc3517b-cb26-4d13-a101-65f5bcfe3a60 38101 2 2023-09-04 15:52:43 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7f845197-303e-4cb3-b9e6-472233bd62b3 0xc0043eec17 0xc0043eec18}] [] [{e2e.test Update apps/v1 2023-09-04 15:52:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:52:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f845197-303e-4cb3-b9e6-472233bd62b3\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:52:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0043eece8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 15:52:48.105: INFO: Pod "test-rolling-update-deployment-7549d9f46d-wntdk" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-wntdk test-rolling-update-deployment-7549d9f46d- deployment-7451  309ca739-6e8a-4ba7-b92c-210fe8f50460 38093 0 2023-09-04 15:52:45 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:eba59e5275b396ebbb31d871bc2f4f9d2154b47bcdabda2091f77bcdb945f8ae cni.projectcalico.org/podIP:100.64.1.142/32 cni.projectcalico.org/podIPs:100.64.1.142/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 2ff6b96e-e3fa-4451-9042-1ee68904fa08 0xc0043ef277 0xc0043ef278}] [] [{kube-controller-manager Update v1 2023-09-04 15:52:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ff6b96e-e3fa-4451-9042-1ee68904fa08\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:52:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:52:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7slmq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7slmq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.142,StartTime:2023-09-04 15:52:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:52:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://8cc9f08bb336c4c309f471ffc59f51a5a7fbe4594f6e5db79c8170c79db44eae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.142,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:52:48.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7451" for this suite. 09/04/23 15:52:48.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:52:48.148
Sep  4 15:52:48.148: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 09/04/23 15:52:48.149
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:48.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:48.218
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Sep  4 15:52:48.273: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/04/23 15:52:48.273
Sep  4 15:52:48.274: INFO: Waiting up to 5m0s for pod "test-rollover-controller-tqg2k" in namespace "deployment-4698" to be "running"
Sep  4 15:52:48.287: INFO: Pod "test-rollover-controller-tqg2k": Phase="Pending", Reason="", readiness=false. Elapsed: 13.809248ms
Sep  4 15:52:50.303: INFO: Pod "test-rollover-controller-tqg2k": Phase="Running", Reason="", readiness=true. Elapsed: 2.029690774s
Sep  4 15:52:50.303: INFO: Pod "test-rollover-controller-tqg2k" satisfied condition "running"
Sep  4 15:52:50.303: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  4 15:52:52.319: INFO: Creating deployment "test-rollover-deployment"
Sep  4 15:52:52.355: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  4 15:52:54.395: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  4 15:52:54.424: INFO: Ensure that both replica sets have 1 created replica
Sep  4 15:52:54.452: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  4 15:52:54.481: INFO: Updating deployment test-rollover-deployment
Sep  4 15:52:54.481: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  4 15:52:56.512: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  4 15:52:56.542: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  4 15:52:56.570: INFO: all replica sets need to contain the pod-template-hash label
Sep  4 15:52:56.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 15:52:58.599: INFO: all replica sets need to contain the pod-template-hash label
Sep  4 15:52:58.599: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 15:53:00.602: INFO: all replica sets need to contain the pod-template-hash label
Sep  4 15:53:00.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 15:53:02.599: INFO: all replica sets need to contain the pod-template-hash label
Sep  4 15:53:02.599: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 15:53:04.602: INFO: all replica sets need to contain the pod-template-hash label
Sep  4 15:53:04.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  4 15:53:06.600: INFO: 
Sep  4 15:53:06.600: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  4 15:53:06.643: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4698  4e6017db-2d4a-489f-887b-555624d7f4f7 38265 2 2023-09-04 15:52:52 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-04 15:52:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00372c1a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-04 15:52:52 +0000 UTC,LastTransitionTime:2023-09-04 15:52:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-09-04 15:53:05 +0000 UTC,LastTransitionTime:2023-09-04 15:52:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  4 15:53:06.658: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-4698  2fa640b4-9a90-4137-9088-3a0fc05f0fb5 38258 2 2023-09-04 15:52:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4e6017db-2d4a-489f-887b-555624d7f4f7 0xc006a2c367 0xc006a2c368}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:52:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e6017db-2d4a-489f-887b-555624d7f4f7\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:53:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a2c418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  4 15:53:06.658: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  4 15:53:06.658: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4698  e5c697e7-6132-4fc0-98b5-54e921ef2f75 38264 2 2023-09-04 15:52:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4e6017db-2d4a-489f-887b-555624d7f4f7 0xc006a2c237 0xc006a2c238}] [] [{e2e.test Update apps/v1 2023-09-04 15:52:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e6017db-2d4a-489f-887b-555624d7f4f7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:53:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006a2c2f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  4 15:53:06.658: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-4698  593caf83-198c-48de-b668-f3e7097af703 38190 2 2023-09-04 15:52:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4e6017db-2d4a-489f-887b-555624d7f4f7 0xc006a2c487 0xc006a2c488}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:52:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e6017db-2d4a-489f-887b-555624d7f4f7\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:52:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a2c538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  4 15:53:06.673: INFO: Pod "test-rollover-deployment-6c6df9974f-qzgp6" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-qzgp6 test-rollover-deployment-6c6df9974f- deployment-4698  81a844fb-0a28-4f9f-b6c8-df81e9f30d00 38205 0 2023-09-04 15:52:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:bde6a87a83df77296a4e93b6827b8c4f7004df09f0a20cbe14c17801519c9d78 cni.projectcalico.org/podIP:100.64.1.145/32 cni.projectcalico.org/podIPs:100.64.1.145/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 2fa640b4-9a90-4137-9088-3a0fc05f0fb5 0xc00372c637 0xc00372c638}] [] [{kube-controller-manager Update v1 2023-09-04 15:52:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2fa640b4-9a90-4137-9088-3a0fc05f0fb5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:52:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:52:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k7czm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7czm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.145,StartTime:2023-09-04 15:52:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:52:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://98750c359e9da36955ff60c0efa783abb3cd24bd4c19f6c2088de4cbf7a3596b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  4 15:53:06.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4698" for this suite. 09/04/23 15:53:06.7
------------------------------
• [18.568 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:52:48.148
    Sep  4 15:52:48.148: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 09/04/23 15:52:48.149
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:52:48.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:52:48.218
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Sep  4 15:52:48.273: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/04/23 15:52:48.273
    Sep  4 15:52:48.274: INFO: Waiting up to 5m0s for pod "test-rollover-controller-tqg2k" in namespace "deployment-4698" to be "running"
    Sep  4 15:52:48.287: INFO: Pod "test-rollover-controller-tqg2k": Phase="Pending", Reason="", readiness=false. Elapsed: 13.809248ms
    Sep  4 15:52:50.303: INFO: Pod "test-rollover-controller-tqg2k": Phase="Running", Reason="", readiness=true. Elapsed: 2.029690774s
    Sep  4 15:52:50.303: INFO: Pod "test-rollover-controller-tqg2k" satisfied condition "running"
    Sep  4 15:52:50.303: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Sep  4 15:52:52.319: INFO: Creating deployment "test-rollover-deployment"
    Sep  4 15:52:52.355: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Sep  4 15:52:54.395: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Sep  4 15:52:54.424: INFO: Ensure that both replica sets have 1 created replica
    Sep  4 15:52:54.452: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Sep  4 15:52:54.481: INFO: Updating deployment test-rollover-deployment
    Sep  4 15:52:54.481: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Sep  4 15:52:56.512: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Sep  4 15:52:56.542: INFO: Make sure deployment "test-rollover-deployment" is complete
    Sep  4 15:52:56.570: INFO: all replica sets need to contain the pod-template-hash label
    Sep  4 15:52:56.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 15:52:58.599: INFO: all replica sets need to contain the pod-template-hash label
    Sep  4 15:52:58.599: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 15:53:00.602: INFO: all replica sets need to contain the pod-template-hash label
    Sep  4 15:53:00.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 15:53:02.599: INFO: all replica sets need to contain the pod-template-hash label
    Sep  4 15:53:02.599: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 15:53:04.602: INFO: all replica sets need to contain the pod-template-hash label
    Sep  4 15:53:04.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 52, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 52, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  4 15:53:06.600: INFO: 
    Sep  4 15:53:06.600: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  4 15:53:06.643: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-4698  4e6017db-2d4a-489f-887b-555624d7f4f7 38265 2 2023-09-04 15:52:52 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-04 15:52:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00372c1a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-04 15:52:52 +0000 UTC,LastTransitionTime:2023-09-04 15:52:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-09-04 15:53:05 +0000 UTC,LastTransitionTime:2023-09-04 15:52:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  4 15:53:06.658: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-4698  2fa640b4-9a90-4137-9088-3a0fc05f0fb5 38258 2 2023-09-04 15:52:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4e6017db-2d4a-489f-887b-555624d7f4f7 0xc006a2c367 0xc006a2c368}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:52:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e6017db-2d4a-489f-887b-555624d7f4f7\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:53:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a2c418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 15:53:06.658: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Sep  4 15:53:06.658: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4698  e5c697e7-6132-4fc0-98b5-54e921ef2f75 38264 2 2023-09-04 15:52:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4e6017db-2d4a-489f-887b-555624d7f4f7 0xc006a2c237 0xc006a2c238}] [] [{e2e.test Update apps/v1 2023-09-04 15:52:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e6017db-2d4a-489f-887b-555624d7f4f7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:53:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006a2c2f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 15:53:06.658: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-4698  593caf83-198c-48de-b668-f3e7097af703 38190 2 2023-09-04 15:52:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4e6017db-2d4a-489f-887b-555624d7f4f7 0xc006a2c487 0xc006a2c488}] [] [{kube-controller-manager Update apps/v1 2023-09-04 15:52:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e6017db-2d4a-489f-887b-555624d7f4f7\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-04 15:52:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a2c538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  4 15:53:06.673: INFO: Pod "test-rollover-deployment-6c6df9974f-qzgp6" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-qzgp6 test-rollover-deployment-6c6df9974f- deployment-4698  81a844fb-0a28-4f9f-b6c8-df81e9f30d00 38205 0 2023-09-04 15:52:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:bde6a87a83df77296a4e93b6827b8c4f7004df09f0a20cbe14c17801519c9d78 cni.projectcalico.org/podIP:100.64.1.145/32 cni.projectcalico.org/podIPs:100.64.1.145/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 2fa640b4-9a90-4137-9088-3a0fc05f0fb5 0xc00372c637 0xc00372c638}] [] [{kube-controller-manager Update v1 2023-09-04 15:52:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2fa640b4-9a90-4137-9088-3a0fc05f0fb5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-04 15:52:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-04 15:52:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k7czm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmud5-dd2.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7czm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-04 15:52:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.231,PodIP:100.64.1.145,StartTime:2023-09-04 15:52:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-04 15:52:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://98750c359e9da36955ff60c0efa783abb3cd24bd4c19f6c2088de4cbf7a3596b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:53:06.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4698" for this suite. 09/04/23 15:53:06.7
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:53:06.722
Sep  4 15:53:06.722: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 09/04/23 15:53:06.723
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:53:06.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:53:06.793
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-6e1b76ff-7ec5-47a3-913b-cff95128ca77 09/04/23 15:53:06.82
STEP: Creating a pod to test consume configMaps 09/04/23 15:53:06.835
Sep  4 15:53:06.855: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24" in namespace "configmap-9866" to be "Succeeded or Failed"
Sep  4 15:53:06.869: INFO: Pod "pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24": Phase="Pending", Reason="", readiness=false. Elapsed: 13.919792ms
Sep  4 15:53:08.884: INFO: Pod "pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029252218s
Sep  4 15:53:10.885: INFO: Pod "pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030021055s
STEP: Saw pod success 09/04/23 15:53:10.885
Sep  4 15:53:10.885: INFO: Pod "pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24" satisfied condition "Succeeded or Failed"
Sep  4 15:53:10.900: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 15:53:10.937
Sep  4 15:53:10.956: INFO: Waiting for pod pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24 to disappear
Sep  4 15:53:10.971: INFO: Pod pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  4 15:53:10.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9866" for this suite. 09/04/23 15:53:10.998
------------------------------
• [4.293 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:53:06.722
    Sep  4 15:53:06.722: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 09/04/23 15:53:06.723
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:53:06.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:53:06.793
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-6e1b76ff-7ec5-47a3-913b-cff95128ca77 09/04/23 15:53:06.82
    STEP: Creating a pod to test consume configMaps 09/04/23 15:53:06.835
    Sep  4 15:53:06.855: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24" in namespace "configmap-9866" to be "Succeeded or Failed"
    Sep  4 15:53:06.869: INFO: Pod "pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24": Phase="Pending", Reason="", readiness=false. Elapsed: 13.919792ms
    Sep  4 15:53:08.884: INFO: Pod "pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029252218s
    Sep  4 15:53:10.885: INFO: Pod "pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030021055s
    STEP: Saw pod success 09/04/23 15:53:10.885
    Sep  4 15:53:10.885: INFO: Pod "pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24" satisfied condition "Succeeded or Failed"
    Sep  4 15:53:10.900: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 15:53:10.937
    Sep  4 15:53:10.956: INFO: Waiting for pod pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24 to disappear
    Sep  4 15:53:10.971: INFO: Pod pod-configmaps-4e6a8df2-d9d8-4119-a161-51efde8f6f24 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:53:10.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9866" for this suite. 09/04/23 15:53:10.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:53:11.016
Sep  4 15:53:11.016: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 09/04/23 15:53:11.017
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:53:11.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:53:11.087
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-vdzj2" 09/04/23 15:53:11.115
Sep  4 15:53:11.160: INFO: Namespace "e2e-ns-vdzj2-8820" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-vdzj2-8820" 09/04/23 15:53:11.16
Sep  4 15:53:11.191: INFO: Namespace "e2e-ns-vdzj2-8820" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-vdzj2-8820" 09/04/23 15:53:11.191
Sep  4 15:53:11.220: INFO: Namespace "e2e-ns-vdzj2-8820" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:53:11.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9150" for this suite. 09/04/23 15:53:11.236
STEP: Destroying namespace "e2e-ns-vdzj2-8820" for this suite. 09/04/23 15:53:11.252
------------------------------
• [0.253 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:53:11.016
    Sep  4 15:53:11.016: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 09/04/23 15:53:11.017
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:53:11.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:53:11.087
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-vdzj2" 09/04/23 15:53:11.115
    Sep  4 15:53:11.160: INFO: Namespace "e2e-ns-vdzj2-8820" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-vdzj2-8820" 09/04/23 15:53:11.16
    Sep  4 15:53:11.191: INFO: Namespace "e2e-ns-vdzj2-8820" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-vdzj2-8820" 09/04/23 15:53:11.191
    Sep  4 15:53:11.220: INFO: Namespace "e2e-ns-vdzj2-8820" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:53:11.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9150" for this suite. 09/04/23 15:53:11.236
    STEP: Destroying namespace "e2e-ns-vdzj2-8820" for this suite. 09/04/23 15:53:11.252
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:53:11.27
Sep  4 15:53:11.270: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 09/04/23 15:53:11.271
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:53:11.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:53:11.341
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
Sep  4 15:53:11.434: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 09/04/23 15:53:11.45
Sep  4 15:53:11.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:53:11.464: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 09/04/23 15:53:11.464
Sep  4 15:53:11.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:53:11.530: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:53:12.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 15:53:12.546: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 09/04/23 15:53:12.561
Sep  4 15:53:12.633: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:53:12.633: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/04/23 15:53:12.633
Sep  4 15:53:12.666: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:53:12.666: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:53:13.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:53:13.684: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:53:14.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:53:14.681: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:53:15.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:53:15.682: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:53:16.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 15:53:16.681: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/04/23 15:53:16.711
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9854, will wait for the garbage collector to delete the pods 09/04/23 15:53:16.711
Sep  4 15:53:16.792: INFO: Deleting DaemonSet.extensions daemon-set took: 15.935942ms
Sep  4 15:53:16.892: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.52427ms
Sep  4 15:53:19.608: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:53:19.608: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  4 15:53:19.623: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38453"},"items":null}

Sep  4 15:53:19.637: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38453"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:53:19.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9854" for this suite. 09/04/23 15:53:19.732
------------------------------
• [8.494 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:53:11.27
    Sep  4 15:53:11.270: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 09/04/23 15:53:11.271
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:53:11.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:53:11.341
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:205
    Sep  4 15:53:11.434: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 09/04/23 15:53:11.45
    Sep  4 15:53:11.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:53:11.464: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 09/04/23 15:53:11.464
    Sep  4 15:53:11.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:53:11.530: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:53:12.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 15:53:12.546: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 09/04/23 15:53:12.561
    Sep  4 15:53:12.633: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:53:12.633: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/04/23 15:53:12.633
    Sep  4 15:53:12.666: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:53:12.666: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:53:13.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:53:13.684: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:53:14.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:53:14.681: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:53:15.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:53:15.682: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:53:16.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 15:53:16.681: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/04/23 15:53:16.711
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9854, will wait for the garbage collector to delete the pods 09/04/23 15:53:16.711
    Sep  4 15:53:16.792: INFO: Deleting DaemonSet.extensions daemon-set took: 15.935942ms
    Sep  4 15:53:16.892: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.52427ms
    Sep  4 15:53:19.608: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:53:19.608: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  4 15:53:19.623: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38453"},"items":null}

    Sep  4 15:53:19.637: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38453"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:53:19.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9854" for this suite. 09/04/23 15:53:19.732
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:53:19.764
Sep  4 15:53:19.764: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 09/04/23 15:53:19.765
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:53:19.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:53:19.836
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  4 15:53:19.863: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  4 15:53:19.894: INFO: Waiting for terminating namespaces to be deleted...
Sep  4 15:53:19.909: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx before test
Sep  4 15:53:19.932: INFO: addons-nginx-ingress-controller-56b5dc8f6c-tr9n6 from kube-system started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep  4 15:53:19.932: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-58685978db-gpxdq from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Sep  4 15:53:19.932: INFO: apiserver-proxy-bc5jh from kube-system started at 2023-09-04 14:34:44 +0000 UTC (2 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container proxy ready: true, restart count 0
Sep  4 15:53:19.932: INFO: 	Container sidecar ready: true, restart count 0
Sep  4 15:53:19.932: INFO: blackbox-exporter-585854d657-fvtbg from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container blackbox-exporter ready: true, restart count 0
Sep  4 15:53:19.932: INFO: blackbox-exporter-585854d657-vz8rw from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container blackbox-exporter ready: true, restart count 0
Sep  4 15:53:19.932: INFO: calico-kube-controllers-684b9f4889-f24wb from kube-system started at 2023-09-04 14:34:42 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  4 15:53:19.932: INFO: calico-node-959qs from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 15:53:19.932: INFO: calico-node-vertical-autoscaler-7bbd54698f-zb6st from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container autoscaler ready: true, restart count 0
Sep  4 15:53:19.932: INFO: calico-typha-deploy-6f4475c6d5-z7t2m from kube-system started at 2023-09-04 14:35:50 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container calico-typha ready: true, restart count 0
Sep  4 15:53:19.932: INFO: calico-typha-horizontal-autoscaler-7b89d5ff97-gdvkd from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container autoscaler ready: true, restart count 0
Sep  4 15:53:19.932: INFO: calico-typha-vertical-autoscaler-656479b7b5-khd8g from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container autoscaler ready: true, restart count 0
Sep  4 15:53:19.932: INFO: coredns-89679867b-2fr9s from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container coredns ready: true, restart count 0
Sep  4 15:53:19.932: INFO: coredns-89679867b-5qtl7 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container coredns ready: true, restart count 0
Sep  4 15:53:19.932: INFO: csi-driver-node-5kfkx from kube-system started at 2023-09-04 14:34:46 +0000 UTC (3 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container csi-driver ready: true, restart count 0
Sep  4 15:53:19.932: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Sep  4 15:53:19.932: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Sep  4 15:53:19.932: INFO: egress-filter-applier-n4kfc from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container egress-filter-applier ready: true, restart count 0
Sep  4 15:53:19.932: INFO: kube-proxy-worker-1-v1.26.8-ch6px from kube-system started at 2023-09-04 15:01:11 +0000 UTC (2 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container conntrack-fix ready: true, restart count 0
Sep  4 15:53:19.932: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 15:53:19.932: INFO: metrics-server-78947f8d7c-9cdcs from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container metrics-server ready: true, restart count 0
Sep  4 15:53:19.932: INFO: metrics-server-78947f8d7c-w8gr6 from kube-system started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container metrics-server ready: true, restart count 0
Sep  4 15:53:19.932: INFO: network-problem-detector-host-xf9l5 from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Sep  4 15:53:19.932: INFO: network-problem-detector-pod-lbp4l from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Sep  4 15:53:19.932: INFO: node-exporter-ktjnf from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container node-exporter ready: true, restart count 0
Sep  4 15:53:19.932: INFO: node-local-dns-zhl7r from kube-system started at 2023-09-04 15:35:11 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container node-cache ready: true, restart count 0
Sep  4 15:53:19.932: INFO: node-problem-detector-rqmbk from kube-system started at 2023-09-04 14:41:10 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep  4 15:53:19.932: INFO: vpn-shoot-5d596dbb88-5vx52 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container vpn-shoot ready: true, restart count 0
Sep  4 15:53:19.932: INFO: dashboard-metrics-scraper-6c889fdd54-555v8 from kubernetes-dashboard started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Sep  4 15:53:19.932: INFO: kubernetes-dashboard-b9859c4d7-bq9mj from kubernetes-dashboard started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.932: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep  4 15:53:19.932: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh before test
Sep  4 15:53:19.964: INFO: apiserver-proxy-4xjvl from kube-system started at 2023-09-04 14:35:03 +0000 UTC (2 container statuses recorded)
Sep  4 15:53:19.964: INFO: 	Container proxy ready: true, restart count 0
Sep  4 15:53:19.964: INFO: 	Container sidecar ready: true, restart count 0
Sep  4 15:53:19.964: INFO: calico-node-dqrx8 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.964: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 15:53:19.964: INFO: calico-typha-deploy-6f4475c6d5-p54fg from kube-system started at 2023-09-04 14:51:41 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.964: INFO: 	Container calico-typha ready: true, restart count 0
Sep  4 15:53:19.964: INFO: csi-driver-node-fcbsh from kube-system started at 2023-09-04 14:35:03 +0000 UTC (3 container statuses recorded)
Sep  4 15:53:19.964: INFO: 	Container csi-driver ready: true, restart count 0
Sep  4 15:53:19.964: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Sep  4 15:53:19.964: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Sep  4 15:53:19.964: INFO: egress-filter-applier-42dhz from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.964: INFO: 	Container egress-filter-applier ready: true, restart count 0
Sep  4 15:53:19.964: INFO: kube-proxy-worker-1-v1.26.8-fsg4b from kube-system started at 2023-09-04 15:45:12 +0000 UTC (2 container statuses recorded)
Sep  4 15:53:19.964: INFO: 	Container conntrack-fix ready: true, restart count 0
Sep  4 15:53:19.964: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 15:53:19.964: INFO: network-problem-detector-host-4k5k5 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.964: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Sep  4 15:53:19.964: INFO: network-problem-detector-pod-sv5cs from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.964: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Sep  4 15:53:19.964: INFO: node-exporter-mp64n from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.964: INFO: 	Container node-exporter ready: true, restart count 0
Sep  4 15:53:19.964: INFO: node-local-dns-xnc4s from kube-system started at 2023-09-04 15:36:10 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.964: INFO: 	Container node-cache ready: true, restart count 0
Sep  4 15:53:19.964: INFO: node-problem-detector-gqcc9 from kube-system started at 2023-09-04 14:42:11 +0000 UTC (1 container statuses recorded)
Sep  4 15:53:19.964: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/04/23 15:53:19.964
Sep  4 15:53:19.985: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9292" to be "running"
Sep  4 15:53:20.006: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 20.601901ms
Sep  4 15:53:22.023: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.03727397s
Sep  4 15:53:22.023: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/04/23 15:53:22.038
STEP: Trying to apply a random label on the found node. 09/04/23 15:53:22.057
STEP: verifying the node has the label kubernetes.io/e2e-f1632809-62bc-4679-8d9a-868872d06adc 42 09/04/23 15:53:22.095
STEP: Trying to relaunch the pod, now with labels. 09/04/23 15:53:22.11
Sep  4 15:53:22.131: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-9292" to be "not pending"
Sep  4 15:53:22.146: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 14.668399ms
Sep  4 15:53:24.162: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.030550133s
Sep  4 15:53:24.162: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-f1632809-62bc-4679-8d9a-868872d06adc off the node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh 09/04/23 15:53:24.177
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f1632809-62bc-4679-8d9a-868872d06adc 09/04/23 15:53:24.24
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:53:24.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9292" for this suite. 09/04/23 15:53:24.271
------------------------------
• [4.522 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:53:19.764
    Sep  4 15:53:19.764: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 09/04/23 15:53:19.765
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:53:19.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:53:19.836
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  4 15:53:19.863: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  4 15:53:19.894: INFO: Waiting for terminating namespaces to be deleted...
    Sep  4 15:53:19.909: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx before test
    Sep  4 15:53:19.932: INFO: addons-nginx-ingress-controller-56b5dc8f6c-tr9n6 from kube-system started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-58685978db-gpxdq from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: apiserver-proxy-bc5jh from kube-system started at 2023-09-04 14:34:44 +0000 UTC (2 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container proxy ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: 	Container sidecar ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: blackbox-exporter-585854d657-fvtbg from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: blackbox-exporter-585854d657-vz8rw from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: calico-kube-controllers-684b9f4889-f24wb from kube-system started at 2023-09-04 14:34:42 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: calico-node-959qs from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: calico-node-vertical-autoscaler-7bbd54698f-zb6st from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: calico-typha-deploy-6f4475c6d5-z7t2m from kube-system started at 2023-09-04 14:35:50 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container calico-typha ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: calico-typha-horizontal-autoscaler-7b89d5ff97-gdvkd from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: calico-typha-vertical-autoscaler-656479b7b5-khd8g from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: coredns-89679867b-2fr9s from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: coredns-89679867b-5qtl7 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: csi-driver-node-5kfkx from kube-system started at 2023-09-04 14:34:46 +0000 UTC (3 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container csi-driver ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: egress-filter-applier-n4kfc from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: kube-proxy-worker-1-v1.26.8-ch6px from kube-system started at 2023-09-04 15:01:11 +0000 UTC (2 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container conntrack-fix ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: metrics-server-78947f8d7c-9cdcs from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container metrics-server ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: metrics-server-78947f8d7c-w8gr6 from kube-system started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container metrics-server ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: network-problem-detector-host-xf9l5 from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: network-problem-detector-pod-lbp4l from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: node-exporter-ktjnf from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: node-local-dns-zhl7r from kube-system started at 2023-09-04 15:35:11 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container node-cache ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: node-problem-detector-rqmbk from kube-system started at 2023-09-04 14:41:10 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: vpn-shoot-5d596dbb88-5vx52 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container vpn-shoot ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: dashboard-metrics-scraper-6c889fdd54-555v8 from kubernetes-dashboard started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: kubernetes-dashboard-b9859c4d7-bq9mj from kubernetes-dashboard started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.932: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Sep  4 15:53:19.932: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh before test
    Sep  4 15:53:19.964: INFO: apiserver-proxy-4xjvl from kube-system started at 2023-09-04 14:35:03 +0000 UTC (2 container statuses recorded)
    Sep  4 15:53:19.964: INFO: 	Container proxy ready: true, restart count 0
    Sep  4 15:53:19.964: INFO: 	Container sidecar ready: true, restart count 0
    Sep  4 15:53:19.964: INFO: calico-node-dqrx8 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.964: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 15:53:19.964: INFO: calico-typha-deploy-6f4475c6d5-p54fg from kube-system started at 2023-09-04 14:51:41 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.964: INFO: 	Container calico-typha ready: true, restart count 0
    Sep  4 15:53:19.964: INFO: csi-driver-node-fcbsh from kube-system started at 2023-09-04 14:35:03 +0000 UTC (3 container statuses recorded)
    Sep  4 15:53:19.964: INFO: 	Container csi-driver ready: true, restart count 0
    Sep  4 15:53:19.964: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Sep  4 15:53:19.964: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Sep  4 15:53:19.964: INFO: egress-filter-applier-42dhz from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.964: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Sep  4 15:53:19.964: INFO: kube-proxy-worker-1-v1.26.8-fsg4b from kube-system started at 2023-09-04 15:45:12 +0000 UTC (2 container statuses recorded)
    Sep  4 15:53:19.964: INFO: 	Container conntrack-fix ready: true, restart count 0
    Sep  4 15:53:19.964: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 15:53:19.964: INFO: network-problem-detector-host-4k5k5 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.964: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Sep  4 15:53:19.964: INFO: network-problem-detector-pod-sv5cs from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.964: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Sep  4 15:53:19.964: INFO: node-exporter-mp64n from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.964: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  4 15:53:19.964: INFO: node-local-dns-xnc4s from kube-system started at 2023-09-04 15:36:10 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.964: INFO: 	Container node-cache ready: true, restart count 0
    Sep  4 15:53:19.964: INFO: node-problem-detector-gqcc9 from kube-system started at 2023-09-04 14:42:11 +0000 UTC (1 container statuses recorded)
    Sep  4 15:53:19.964: INFO: 	Container node-problem-detector ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/04/23 15:53:19.964
    Sep  4 15:53:19.985: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9292" to be "running"
    Sep  4 15:53:20.006: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 20.601901ms
    Sep  4 15:53:22.023: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.03727397s
    Sep  4 15:53:22.023: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/04/23 15:53:22.038
    STEP: Trying to apply a random label on the found node. 09/04/23 15:53:22.057
    STEP: verifying the node has the label kubernetes.io/e2e-f1632809-62bc-4679-8d9a-868872d06adc 42 09/04/23 15:53:22.095
    STEP: Trying to relaunch the pod, now with labels. 09/04/23 15:53:22.11
    Sep  4 15:53:22.131: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-9292" to be "not pending"
    Sep  4 15:53:22.146: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 14.668399ms
    Sep  4 15:53:24.162: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.030550133s
    Sep  4 15:53:24.162: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-f1632809-62bc-4679-8d9a-868872d06adc off the node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh 09/04/23 15:53:24.177
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-f1632809-62bc-4679-8d9a-868872d06adc 09/04/23 15:53:24.24
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:53:24.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9292" for this suite. 09/04/23 15:53:24.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:53:24.287
Sep  4 15:53:24.287: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 09/04/23 15:53:24.288
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:53:24.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:53:24.359
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  09/04/23 15:53:24.388
Sep  4 15:53:24.409: INFO: Waiting up to 5m0s for pod "test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c" in namespace "svcaccounts-7860" to be "Succeeded or Failed"
Sep  4 15:53:24.423: INFO: Pod "test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.617928ms
Sep  4 15:53:26.438: INFO: Pod "test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029237656s
Sep  4 15:53:28.438: INFO: Pod "test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029246571s
STEP: Saw pod success 09/04/23 15:53:28.438
Sep  4 15:53:28.439: INFO: Pod "test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c" satisfied condition "Succeeded or Failed"
Sep  4 15:53:28.453: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c container agnhost-container: <nil>
STEP: delete the pod 09/04/23 15:53:28.489
Sep  4 15:53:28.508: INFO: Waiting for pod test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c to disappear
Sep  4 15:53:28.522: INFO: Pod test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  4 15:53:28.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7860" for this suite. 09/04/23 15:53:28.548
------------------------------
• [4.277 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:53:24.287
    Sep  4 15:53:24.287: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 09/04/23 15:53:24.288
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:53:24.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:53:24.359
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  09/04/23 15:53:24.388
    Sep  4 15:53:24.409: INFO: Waiting up to 5m0s for pod "test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c" in namespace "svcaccounts-7860" to be "Succeeded or Failed"
    Sep  4 15:53:24.423: INFO: Pod "test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.617928ms
    Sep  4 15:53:26.438: INFO: Pod "test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029237656s
    Sep  4 15:53:28.438: INFO: Pod "test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029246571s
    STEP: Saw pod success 09/04/23 15:53:28.438
    Sep  4 15:53:28.439: INFO: Pod "test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c" satisfied condition "Succeeded or Failed"
    Sep  4 15:53:28.453: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 15:53:28.489
    Sep  4 15:53:28.508: INFO: Waiting for pod test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c to disappear
    Sep  4 15:53:28.522: INFO: Pod test-pod-ac5ba28a-2a2e-47e1-aae2-ecdb4609660c no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:53:28.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7860" for this suite. 09/04/23 15:53:28.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:53:28.564
Sep  4 15:53:28.564: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 09/04/23 15:53:28.565
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:53:28.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:53:28.634
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Sep  4 15:53:28.695: INFO: created pod
Sep  4 15:53:28.695: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-3230" to be "Succeeded or Failed"
Sep  4 15:53:28.709: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.804459ms
Sep  4 15:53:30.725: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 2.029708963s
Sep  4 15:53:32.724: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029219632s
STEP: Saw pod success 09/04/23 15:53:32.724
Sep  4 15:53:32.724: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Sep  4 15:54:02.726: INFO: polling logs
Sep  4 15:54:02.763: INFO: Pod logs: 
I0904 15:53:29.406319       1 log.go:198] OK: Got token
I0904 15:53:29.406351       1 log.go:198] validating with in-cluster discovery
I0904 15:53:29.406574       1 log.go:198] OK: got issuer https://api.tmud5-dd2.it.internal.staging.k8s.ondemand.com
I0904 15:53:29.406591       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tmud5-dd2.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-3230:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693843408, NotBefore:1693842808, IssuedAt:1693842808, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3230", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"68479f5a-57b9-4920-8e75-1a2153313e02"}}}
I0904 15:53:29.443987       1 log.go:198] OK: Constructed OIDC provider for issuer https://api.tmud5-dd2.it.internal.staging.k8s.ondemand.com
I0904 15:53:29.454181       1 log.go:198] OK: Validated signature on JWT
I0904 15:53:29.454252       1 log.go:198] OK: Got valid claims from token!
I0904 15:53:29.454270       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tmud5-dd2.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-3230:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693843408, NotBefore:1693842808, IssuedAt:1693842808, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3230", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"68479f5a-57b9-4920-8e75-1a2153313e02"}}}

Sep  4 15:54:02.763: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  4 15:54:02.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3230" for this suite. 09/04/23 15:54:02.807
------------------------------
• [34.260 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:53:28.564
    Sep  4 15:53:28.564: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 09/04/23 15:53:28.565
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:53:28.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:53:28.634
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Sep  4 15:53:28.695: INFO: created pod
    Sep  4 15:53:28.695: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-3230" to be "Succeeded or Failed"
    Sep  4 15:53:28.709: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.804459ms
    Sep  4 15:53:30.725: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 2.029708963s
    Sep  4 15:53:32.724: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029219632s
    STEP: Saw pod success 09/04/23 15:53:32.724
    Sep  4 15:53:32.724: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Sep  4 15:54:02.726: INFO: polling logs
    Sep  4 15:54:02.763: INFO: Pod logs: 
    I0904 15:53:29.406319       1 log.go:198] OK: Got token
    I0904 15:53:29.406351       1 log.go:198] validating with in-cluster discovery
    I0904 15:53:29.406574       1 log.go:198] OK: got issuer https://api.tmud5-dd2.it.internal.staging.k8s.ondemand.com
    I0904 15:53:29.406591       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tmud5-dd2.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-3230:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693843408, NotBefore:1693842808, IssuedAt:1693842808, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3230", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"68479f5a-57b9-4920-8e75-1a2153313e02"}}}
    I0904 15:53:29.443987       1 log.go:198] OK: Constructed OIDC provider for issuer https://api.tmud5-dd2.it.internal.staging.k8s.ondemand.com
    I0904 15:53:29.454181       1 log.go:198] OK: Validated signature on JWT
    I0904 15:53:29.454252       1 log.go:198] OK: Got valid claims from token!
    I0904 15:53:29.454270       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tmud5-dd2.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-3230:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693843408, NotBefore:1693842808, IssuedAt:1693842808, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3230", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"68479f5a-57b9-4920-8e75-1a2153313e02"}}}

    Sep  4 15:54:02.763: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:54:02.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3230" for this suite. 09/04/23 15:54:02.807
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:54:02.824
Sep  4 15:54:02.824: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 15:54:02.824
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:54:02.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:54:02.897
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 15:54:02.96
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:54:03.395
STEP: Deploying the webhook pod 09/04/23 15:54:03.413
STEP: Wait for the deployment to be ready 09/04/23 15:54:03.446
Sep  4 15:54:03.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 54, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 54, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 54, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 54, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 15:54:05.505
STEP: Verifying the service has paired with the endpoint 09/04/23 15:54:05.525
Sep  4 15:54:06.525: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/04/23 15:54:06.54
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/04/23 15:54:06.687
STEP: Creating a dummy validating-webhook-configuration object 09/04/23 15:54:06.823
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/04/23 15:54:06.918
STEP: Creating a dummy mutating-webhook-configuration object 09/04/23 15:54:06.934
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/04/23 15:54:07.07
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:54:07.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5052" for this suite. 09/04/23 15:54:07.213
STEP: Destroying namespace "webhook-5052-markers" for this suite. 09/04/23 15:54:07.229
------------------------------
• [4.421 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:54:02.824
    Sep  4 15:54:02.824: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 15:54:02.824
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:54:02.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:54:02.897
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 15:54:02.96
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:54:03.395
    STEP: Deploying the webhook pod 09/04/23 15:54:03.413
    STEP: Wait for the deployment to be ready 09/04/23 15:54:03.446
    Sep  4 15:54:03.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 54, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 54, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 54, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 54, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 15:54:05.505
    STEP: Verifying the service has paired with the endpoint 09/04/23 15:54:05.525
    Sep  4 15:54:06.525: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/04/23 15:54:06.54
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/04/23 15:54:06.687
    STEP: Creating a dummy validating-webhook-configuration object 09/04/23 15:54:06.823
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/04/23 15:54:06.918
    STEP: Creating a dummy mutating-webhook-configuration object 09/04/23 15:54:06.934
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/04/23 15:54:07.07
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:54:07.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5052" for this suite. 09/04/23 15:54:07.213
    STEP: Destroying namespace "webhook-5052-markers" for this suite. 09/04/23 15:54:07.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:54:07.246
Sep  4 15:54:07.246: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 09/04/23 15:54:07.246
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:54:07.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:54:07.319
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
STEP: Creating simple DaemonSet "daemon-set" 09/04/23 15:54:07.44
STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 15:54:07.457
Sep  4 15:54:07.488: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:54:07.488: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:54:08.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:54:08.532: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 15:54:09.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 15:54:09.532: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 09/04/23 15:54:09.547
Sep  4 15:54:09.562: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 09/04/23 15:54:09.562
Sep  4 15:54:09.594: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 09/04/23 15:54:09.595
Sep  4 15:54:09.609: INFO: Observed &DaemonSet event: ADDED
Sep  4 15:54:09.609: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 15:54:09.609: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 15:54:09.609: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 15:54:09.609: INFO: Found daemon set daemon-set in namespace daemonsets-2868 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  4 15:54:09.609: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 09/04/23 15:54:09.609
STEP: watching for the daemon set status to be patched 09/04/23 15:54:09.627
Sep  4 15:54:09.641: INFO: Observed &DaemonSet event: ADDED
Sep  4 15:54:09.641: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 15:54:09.641: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 15:54:09.641: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 15:54:09.641: INFO: Observed daemon set daemon-set in namespace daemonsets-2868 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  4 15:54:09.641: INFO: Observed &DaemonSet event: MODIFIED
Sep  4 15:54:09.641: INFO: Found daemon set daemon-set in namespace daemonsets-2868 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Sep  4 15:54:09.641: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/04/23 15:54:09.656
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2868, will wait for the garbage collector to delete the pods 09/04/23 15:54:09.656
Sep  4 15:54:09.737: INFO: Deleting DaemonSet.extensions daemon-set took: 16.524246ms
Sep  4 15:54:09.838: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.106804ms
Sep  4 15:54:12.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 15:54:12.652: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  4 15:54:12.667: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38910"},"items":null}

Sep  4 15:54:12.681: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38910"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:54:12.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2868" for this suite. 09/04/23 15:54:12.756
------------------------------
• [5.526 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:54:07.246
    Sep  4 15:54:07.246: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 09/04/23 15:54:07.246
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:54:07.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:54:07.319
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:873
    STEP: Creating simple DaemonSet "daemon-set" 09/04/23 15:54:07.44
    STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 15:54:07.457
    Sep  4 15:54:07.488: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:54:07.488: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:54:08.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:54:08.532: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 15:54:09.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 15:54:09.532: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 09/04/23 15:54:09.547
    Sep  4 15:54:09.562: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 09/04/23 15:54:09.562
    Sep  4 15:54:09.594: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 09/04/23 15:54:09.595
    Sep  4 15:54:09.609: INFO: Observed &DaemonSet event: ADDED
    Sep  4 15:54:09.609: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 15:54:09.609: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 15:54:09.609: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 15:54:09.609: INFO: Found daemon set daemon-set in namespace daemonsets-2868 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  4 15:54:09.609: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 09/04/23 15:54:09.609
    STEP: watching for the daemon set status to be patched 09/04/23 15:54:09.627
    Sep  4 15:54:09.641: INFO: Observed &DaemonSet event: ADDED
    Sep  4 15:54:09.641: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 15:54:09.641: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 15:54:09.641: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 15:54:09.641: INFO: Observed daemon set daemon-set in namespace daemonsets-2868 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  4 15:54:09.641: INFO: Observed &DaemonSet event: MODIFIED
    Sep  4 15:54:09.641: INFO: Found daemon set daemon-set in namespace daemonsets-2868 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Sep  4 15:54:09.641: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/04/23 15:54:09.656
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2868, will wait for the garbage collector to delete the pods 09/04/23 15:54:09.656
    Sep  4 15:54:09.737: INFO: Deleting DaemonSet.extensions daemon-set took: 16.524246ms
    Sep  4 15:54:09.838: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.106804ms
    Sep  4 15:54:12.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 15:54:12.652: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  4 15:54:12.667: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38910"},"items":null}

    Sep  4 15:54:12.681: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38910"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:54:12.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2868" for this suite. 09/04/23 15:54:12.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:54:12.772
Sep  4 15:54:12.772: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 15:54:12.772
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:54:12.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:54:12.845
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 09/04/23 15:54:12.873
Sep  4 15:54:12.895: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64" in namespace "projected-1001" to be "Succeeded or Failed"
Sep  4 15:54:12.911: INFO: Pod "downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64": Phase="Pending", Reason="", readiness=false. Elapsed: 16.027791ms
Sep  4 15:54:14.928: INFO: Pod "downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032383705s
Sep  4 15:54:16.928: INFO: Pod "downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032836874s
STEP: Saw pod success 09/04/23 15:54:16.928
Sep  4 15:54:16.928: INFO: Pod "downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64" satisfied condition "Succeeded or Failed"
Sep  4 15:54:16.943: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64 container client-container: <nil>
STEP: delete the pod 09/04/23 15:54:17.021
Sep  4 15:54:17.038: INFO: Waiting for pod downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64 to disappear
Sep  4 15:54:17.053: INFO: Pod downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 15:54:17.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1001" for this suite. 09/04/23 15:54:17.08
------------------------------
• [4.324 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:54:12.772
    Sep  4 15:54:12.772: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 15:54:12.772
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:54:12.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:54:12.845
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 09/04/23 15:54:12.873
    Sep  4 15:54:12.895: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64" in namespace "projected-1001" to be "Succeeded or Failed"
    Sep  4 15:54:12.911: INFO: Pod "downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64": Phase="Pending", Reason="", readiness=false. Elapsed: 16.027791ms
    Sep  4 15:54:14.928: INFO: Pod "downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032383705s
    Sep  4 15:54:16.928: INFO: Pod "downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032836874s
    STEP: Saw pod success 09/04/23 15:54:16.928
    Sep  4 15:54:16.928: INFO: Pod "downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64" satisfied condition "Succeeded or Failed"
    Sep  4 15:54:16.943: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64 container client-container: <nil>
    STEP: delete the pod 09/04/23 15:54:17.021
    Sep  4 15:54:17.038: INFO: Waiting for pod downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64 to disappear
    Sep  4 15:54:17.053: INFO: Pod downwardapi-volume-e37a7309-7728-4ce6-8377-44f2c86dfb64 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:54:17.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1001" for this suite. 09/04/23 15:54:17.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:54:17.096
Sep  4 15:54:17.096: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 15:54:17.097
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:54:17.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:54:17.168
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 09/04/23 15:54:17.199
Sep  4 15:54:17.223: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366" in namespace "downward-api-1766" to be "Succeeded or Failed"
Sep  4 15:54:17.237: INFO: Pod "downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366": Phase="Pending", Reason="", readiness=false. Elapsed: 13.990076ms
Sep  4 15:54:19.254: INFO: Pod "downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030193821s
Sep  4 15:54:21.254: INFO: Pod "downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03023753s
STEP: Saw pod success 09/04/23 15:54:21.254
Sep  4 15:54:21.254: INFO: Pod "downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366" satisfied condition "Succeeded or Failed"
Sep  4 15:54:21.269: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366 container client-container: <nil>
STEP: delete the pod 09/04/23 15:54:21.305
Sep  4 15:54:21.324: INFO: Waiting for pod downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366 to disappear
Sep  4 15:54:21.339: INFO: Pod downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 15:54:21.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1766" for this suite. 09/04/23 15:54:21.366
------------------------------
• [4.285 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:54:17.096
    Sep  4 15:54:17.096: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 15:54:17.097
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:54:17.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:54:17.168
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 09/04/23 15:54:17.199
    Sep  4 15:54:17.223: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366" in namespace "downward-api-1766" to be "Succeeded or Failed"
    Sep  4 15:54:17.237: INFO: Pod "downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366": Phase="Pending", Reason="", readiness=false. Elapsed: 13.990076ms
    Sep  4 15:54:19.254: INFO: Pod "downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030193821s
    Sep  4 15:54:21.254: INFO: Pod "downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03023753s
    STEP: Saw pod success 09/04/23 15:54:21.254
    Sep  4 15:54:21.254: INFO: Pod "downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366" satisfied condition "Succeeded or Failed"
    Sep  4 15:54:21.269: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366 container client-container: <nil>
    STEP: delete the pod 09/04/23 15:54:21.305
    Sep  4 15:54:21.324: INFO: Waiting for pod downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366 to disappear
    Sep  4 15:54:21.339: INFO: Pod downwardapi-volume-0e2f180c-b037-4b8f-a0c3-b57333365366 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:54:21.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1766" for this suite. 09/04/23 15:54:21.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:54:21.382
Sep  4 15:54:21.382: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 09/04/23 15:54:21.383
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:54:21.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:54:21.455
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 09/04/23 15:54:21.483
STEP: wait for the container to reach Succeeded 09/04/23 15:54:21.505
STEP: get the container status 09/04/23 15:54:25.584
STEP: the container should be terminated 09/04/23 15:54:25.598
STEP: the termination message should be set 09/04/23 15:54:25.598
Sep  4 15:54:25.599: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 09/04/23 15:54:25.599
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  4 15:54:25.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2559" for this suite. 09/04/23 15:54:25.659
------------------------------
• [4.292 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:54:21.382
    Sep  4 15:54:21.382: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 09/04/23 15:54:21.383
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:54:21.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:54:21.455
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 09/04/23 15:54:21.483
    STEP: wait for the container to reach Succeeded 09/04/23 15:54:21.505
    STEP: get the container status 09/04/23 15:54:25.584
    STEP: the container should be terminated 09/04/23 15:54:25.598
    STEP: the termination message should be set 09/04/23 15:54:25.598
    Sep  4 15:54:25.599: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 09/04/23 15:54:25.599
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:54:25.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2559" for this suite. 09/04/23 15:54:25.659
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:54:25.675
Sep  4 15:54:25.675: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 15:54:25.676
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:54:25.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:54:25.746
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 15:54:25.805
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:54:25.988
STEP: Deploying the webhook pod 09/04/23 15:54:26.004
STEP: Wait for the deployment to be ready 09/04/23 15:54:26.036
Sep  4 15:54:26.078: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 54, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 54, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 54, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 54, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 15:54:28.095
STEP: Verifying the service has paired with the endpoint 09/04/23 15:54:28.114
Sep  4 15:54:29.114: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/04/23 15:54:29.13
STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 15:54:29.13
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/04/23 15:54:29.264
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/04/23 15:54:30.296
STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 15:54:30.296
STEP: Having no error when timeout is longer than webhook latency 09/04/23 15:54:31.433
STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 15:54:31.433
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/04/23 15:54:36.668
STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 15:54:36.668
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 15:54:41.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3108" for this suite. 09/04/23 15:54:41.952
STEP: Destroying namespace "webhook-3108-markers" for this suite. 09/04/23 15:54:41.968
------------------------------
• [16.309 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:54:25.675
    Sep  4 15:54:25.675: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 15:54:25.676
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:54:25.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:54:25.746
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 15:54:25.805
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 15:54:25.988
    STEP: Deploying the webhook pod 09/04/23 15:54:26.004
    STEP: Wait for the deployment to be ready 09/04/23 15:54:26.036
    Sep  4 15:54:26.078: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 15, 54, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 54, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 15, 54, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 15, 54, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 15:54:28.095
    STEP: Verifying the service has paired with the endpoint 09/04/23 15:54:28.114
    Sep  4 15:54:29.114: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/04/23 15:54:29.13
    STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 15:54:29.13
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/04/23 15:54:29.264
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/04/23 15:54:30.296
    STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 15:54:30.296
    STEP: Having no error when timeout is longer than webhook latency 09/04/23 15:54:31.433
    STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 15:54:31.433
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/04/23 15:54:36.668
    STEP: Registering slow webhook via the AdmissionRegistration API 09/04/23 15:54:36.668
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:54:41.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3108" for this suite. 09/04/23 15:54:41.952
    STEP: Destroying namespace "webhook-3108-markers" for this suite. 09/04/23 15:54:41.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:54:41.984
Sep  4 15:54:41.984: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 09/04/23 15:54:41.985
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:54:42.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:54:42.056
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-ed92c652-acf3-4dfd-99eb-6191866c4dcf in namespace container-probe-3284 09/04/23 15:54:42.083
Sep  4 15:54:42.106: INFO: Waiting up to 5m0s for pod "test-webserver-ed92c652-acf3-4dfd-99eb-6191866c4dcf" in namespace "container-probe-3284" to be "not pending"
Sep  4 15:54:42.120: INFO: Pod "test-webserver-ed92c652-acf3-4dfd-99eb-6191866c4dcf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.505249ms
Sep  4 15:54:44.137: INFO: Pod "test-webserver-ed92c652-acf3-4dfd-99eb-6191866c4dcf": Phase="Running", Reason="", readiness=true. Elapsed: 2.031646449s
Sep  4 15:54:44.137: INFO: Pod "test-webserver-ed92c652-acf3-4dfd-99eb-6191866c4dcf" satisfied condition "not pending"
Sep  4 15:54:44.137: INFO: Started pod test-webserver-ed92c652-acf3-4dfd-99eb-6191866c4dcf in namespace container-probe-3284
STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 15:54:44.137
Sep  4 15:54:44.152: INFO: Initial restart count of pod test-webserver-ed92c652-acf3-4dfd-99eb-6191866c4dcf is 0
STEP: deleting the pod 09/04/23 15:58:46.147
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 15:58:46.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3284" for this suite. 09/04/23 15:58:46.194
------------------------------
• [244.226 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:54:41.984
    Sep  4 15:54:41.984: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 09/04/23 15:54:41.985
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:54:42.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:54:42.056
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-ed92c652-acf3-4dfd-99eb-6191866c4dcf in namespace container-probe-3284 09/04/23 15:54:42.083
    Sep  4 15:54:42.106: INFO: Waiting up to 5m0s for pod "test-webserver-ed92c652-acf3-4dfd-99eb-6191866c4dcf" in namespace "container-probe-3284" to be "not pending"
    Sep  4 15:54:42.120: INFO: Pod "test-webserver-ed92c652-acf3-4dfd-99eb-6191866c4dcf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.505249ms
    Sep  4 15:54:44.137: INFO: Pod "test-webserver-ed92c652-acf3-4dfd-99eb-6191866c4dcf": Phase="Running", Reason="", readiness=true. Elapsed: 2.031646449s
    Sep  4 15:54:44.137: INFO: Pod "test-webserver-ed92c652-acf3-4dfd-99eb-6191866c4dcf" satisfied condition "not pending"
    Sep  4 15:54:44.137: INFO: Started pod test-webserver-ed92c652-acf3-4dfd-99eb-6191866c4dcf in namespace container-probe-3284
    STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 15:54:44.137
    Sep  4 15:54:44.152: INFO: Initial restart count of pod test-webserver-ed92c652-acf3-4dfd-99eb-6191866c4dcf is 0
    STEP: deleting the pod 09/04/23 15:58:46.147
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 15:58:46.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3284" for this suite. 09/04/23 15:58:46.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 15:58:46.212
Sep  4 15:58:46.212: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 09/04/23 15:58:46.213
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:58:46.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:58:46.286
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-6ef03e2f-7bae-4fba-8888-c482f043c5d9 in namespace container-probe-30 09/04/23 15:58:46.314
Sep  4 15:58:46.348: INFO: Waiting up to 5m0s for pod "busybox-6ef03e2f-7bae-4fba-8888-c482f043c5d9" in namespace "container-probe-30" to be "not pending"
Sep  4 15:58:46.376: INFO: Pod "busybox-6ef03e2f-7bae-4fba-8888-c482f043c5d9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.309233ms
Sep  4 15:58:48.392: INFO: Pod "busybox-6ef03e2f-7bae-4fba-8888-c482f043c5d9": Phase="Running", Reason="", readiness=true. Elapsed: 2.044571022s
Sep  4 15:58:48.392: INFO: Pod "busybox-6ef03e2f-7bae-4fba-8888-c482f043c5d9" satisfied condition "not pending"
Sep  4 15:58:48.392: INFO: Started pod busybox-6ef03e2f-7bae-4fba-8888-c482f043c5d9 in namespace container-probe-30
STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 15:58:48.392
Sep  4 15:58:48.407: INFO: Initial restart count of pod busybox-6ef03e2f-7bae-4fba-8888-c482f043c5d9 is 0
STEP: deleting the pod 09/04/23 16:02:50.412
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 16:02:50.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-30" for this suite. 09/04/23 16:02:50.471
------------------------------
• [244.275 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 15:58:46.212
    Sep  4 15:58:46.212: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 09/04/23 15:58:46.213
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 15:58:46.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 15:58:46.286
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-6ef03e2f-7bae-4fba-8888-c482f043c5d9 in namespace container-probe-30 09/04/23 15:58:46.314
    Sep  4 15:58:46.348: INFO: Waiting up to 5m0s for pod "busybox-6ef03e2f-7bae-4fba-8888-c482f043c5d9" in namespace "container-probe-30" to be "not pending"
    Sep  4 15:58:46.376: INFO: Pod "busybox-6ef03e2f-7bae-4fba-8888-c482f043c5d9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.309233ms
    Sep  4 15:58:48.392: INFO: Pod "busybox-6ef03e2f-7bae-4fba-8888-c482f043c5d9": Phase="Running", Reason="", readiness=true. Elapsed: 2.044571022s
    Sep  4 15:58:48.392: INFO: Pod "busybox-6ef03e2f-7bae-4fba-8888-c482f043c5d9" satisfied condition "not pending"
    Sep  4 15:58:48.392: INFO: Started pod busybox-6ef03e2f-7bae-4fba-8888-c482f043c5d9 in namespace container-probe-30
    STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 15:58:48.392
    Sep  4 15:58:48.407: INFO: Initial restart count of pod busybox-6ef03e2f-7bae-4fba-8888-c482f043c5d9 is 0
    STEP: deleting the pod 09/04/23 16:02:50.412
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:02:50.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-30" for this suite. 09/04/23 16:02:50.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:02:50.488
Sep  4 16:02:50.488: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 16:02:50.488
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:02:50.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:02:50.559
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 16:02:50.62
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 16:02:51.213
STEP: Deploying the webhook pod 09/04/23 16:02:51.231
STEP: Wait for the deployment to be ready 09/04/23 16:02:51.264
Sep  4 16:02:51.309: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 16, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 2, 51, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 16, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 16:02:53.326
STEP: Verifying the service has paired with the endpoint 09/04/23 16:02:53.346
Sep  4 16:02:54.346: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Sep  4 16:02:54.362: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/04/23 16:02:54.392
STEP: Creating a custom resource that should be denied by the webhook 09/04/23 16:02:54.528
STEP: Creating a custom resource whose deletion would be denied by the webhook 09/04/23 16:02:56.639
STEP: Updating the custom resource with disallowed data should be denied 09/04/23 16:02:56.72
STEP: Deleting the custom resource should be denied 09/04/23 16:02:56.817
STEP: Remove the offending key and value from the custom resource data 09/04/23 16:02:56.87
STEP: Deleting the updated custom resource should be successful 09/04/23 16:02:56.925
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:02:57.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3826" for this suite. 09/04/23 16:02:57.117
STEP: Destroying namespace "webhook-3826-markers" for this suite. 09/04/23 16:02:57.133
------------------------------
• [6.662 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:02:50.488
    Sep  4 16:02:50.488: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 16:02:50.488
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:02:50.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:02:50.559
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 16:02:50.62
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 16:02:51.213
    STEP: Deploying the webhook pod 09/04/23 16:02:51.231
    STEP: Wait for the deployment to be ready 09/04/23 16:02:51.264
    Sep  4 16:02:51.309: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 16, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 2, 51, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 16, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 16:02:53.326
    STEP: Verifying the service has paired with the endpoint 09/04/23 16:02:53.346
    Sep  4 16:02:54.346: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Sep  4 16:02:54.362: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/04/23 16:02:54.392
    STEP: Creating a custom resource that should be denied by the webhook 09/04/23 16:02:54.528
    STEP: Creating a custom resource whose deletion would be denied by the webhook 09/04/23 16:02:56.639
    STEP: Updating the custom resource with disallowed data should be denied 09/04/23 16:02:56.72
    STEP: Deleting the custom resource should be denied 09/04/23 16:02:56.817
    STEP: Remove the offending key and value from the custom resource data 09/04/23 16:02:56.87
    STEP: Deleting the updated custom resource should be successful 09/04/23 16:02:56.925
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:02:57.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3826" for this suite. 09/04/23 16:02:57.117
    STEP: Destroying namespace "webhook-3826-markers" for this suite. 09/04/23 16:02:57.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:02:57.15
Sep  4 16:02:57.150: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 09/04/23 16:02:57.151
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:02:57.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:02:57.222
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-fr4c2"  09/04/23 16:02:57.25
Sep  4 16:02:57.266: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-fr4c2"  09/04/23 16:02:57.266
Sep  4 16:02:57.296: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  4 16:02:57.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4803" for this suite. 09/04/23 16:02:57.335
------------------------------
• [0.201 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:02:57.15
    Sep  4 16:02:57.150: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 09/04/23 16:02:57.151
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:02:57.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:02:57.222
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-fr4c2"  09/04/23 16:02:57.25
    Sep  4 16:02:57.266: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-fr4c2"  09/04/23 16:02:57.266
    Sep  4 16:02:57.296: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:02:57.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4803" for this suite. 09/04/23 16:02:57.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:02:57.352
Sep  4 16:02:57.352: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 09/04/23 16:02:57.353
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:02:57.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:02:57.425
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-2dab1a35-6f28-46d4-b941-8c56b2d3da80 09/04/23 16:02:57.452
STEP: Creating a pod to test consume secrets 09/04/23 16:02:57.467
Sep  4 16:02:57.500: INFO: Waiting up to 5m0s for pod "pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7" in namespace "secrets-7239" to be "Succeeded or Failed"
Sep  4 16:02:57.514: INFO: Pod "pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.860788ms
Sep  4 16:02:59.530: INFO: Pod "pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029344292s
Sep  4 16:03:01.530: INFO: Pod "pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02976927s
STEP: Saw pod success 09/04/23 16:03:01.53
Sep  4 16:03:01.530: INFO: Pod "pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7" satisfied condition "Succeeded or Failed"
Sep  4 16:03:01.545: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7 container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 16:03:01.758
Sep  4 16:03:01.777: INFO: Waiting for pod pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7 to disappear
Sep  4 16:03:01.791: INFO: Pod pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 16:03:01.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7239" for this suite. 09/04/23 16:03:01.83
------------------------------
• [4.494 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:02:57.352
    Sep  4 16:02:57.352: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 09/04/23 16:02:57.353
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:02:57.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:02:57.425
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-2dab1a35-6f28-46d4-b941-8c56b2d3da80 09/04/23 16:02:57.452
    STEP: Creating a pod to test consume secrets 09/04/23 16:02:57.467
    Sep  4 16:02:57.500: INFO: Waiting up to 5m0s for pod "pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7" in namespace "secrets-7239" to be "Succeeded or Failed"
    Sep  4 16:02:57.514: INFO: Pod "pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.860788ms
    Sep  4 16:02:59.530: INFO: Pod "pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029344292s
    Sep  4 16:03:01.530: INFO: Pod "pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02976927s
    STEP: Saw pod success 09/04/23 16:03:01.53
    Sep  4 16:03:01.530: INFO: Pod "pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7" satisfied condition "Succeeded or Failed"
    Sep  4 16:03:01.545: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7 container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 16:03:01.758
    Sep  4 16:03:01.777: INFO: Waiting for pod pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7 to disappear
    Sep  4 16:03:01.791: INFO: Pod pod-secrets-8e8f958b-7c3d-4b42-b323-b08ce6fa10d7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:03:01.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7239" for this suite. 09/04/23 16:03:01.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:03:01.847
Sep  4 16:03:01.847: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 09/04/23 16:03:01.848
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:03:01.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:03:01.92
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 09/04/23 16:03:01.947
STEP: Ensuring ResourceQuota status is calculated 09/04/23 16:03:01.963
STEP: Creating a ResourceQuota with not best effort scope 09/04/23 16:03:03.979
STEP: Ensuring ResourceQuota status is calculated 09/04/23 16:03:03.994
STEP: Creating a best-effort pod 09/04/23 16:03:06.012
STEP: Ensuring resource quota with best effort scope captures the pod usage 09/04/23 16:03:06.035
STEP: Ensuring resource quota with not best effort ignored the pod usage 09/04/23 16:03:08.051
STEP: Deleting the pod 09/04/23 16:03:10.068
STEP: Ensuring resource quota status released the pod usage 09/04/23 16:03:10.086
STEP: Creating a not best-effort pod 09/04/23 16:03:12.102
STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/04/23 16:03:12.126
STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/04/23 16:03:14.141
STEP: Deleting the pod 09/04/23 16:03:16.157
STEP: Ensuring resource quota status released the pod usage 09/04/23 16:03:16.176
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 16:03:18.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5671" for this suite. 09/04/23 16:03:18.222
------------------------------
• [16.392 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:03:01.847
    Sep  4 16:03:01.847: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 09/04/23 16:03:01.848
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:03:01.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:03:01.92
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 09/04/23 16:03:01.947
    STEP: Ensuring ResourceQuota status is calculated 09/04/23 16:03:01.963
    STEP: Creating a ResourceQuota with not best effort scope 09/04/23 16:03:03.979
    STEP: Ensuring ResourceQuota status is calculated 09/04/23 16:03:03.994
    STEP: Creating a best-effort pod 09/04/23 16:03:06.012
    STEP: Ensuring resource quota with best effort scope captures the pod usage 09/04/23 16:03:06.035
    STEP: Ensuring resource quota with not best effort ignored the pod usage 09/04/23 16:03:08.051
    STEP: Deleting the pod 09/04/23 16:03:10.068
    STEP: Ensuring resource quota status released the pod usage 09/04/23 16:03:10.086
    STEP: Creating a not best-effort pod 09/04/23 16:03:12.102
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/04/23 16:03:12.126
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/04/23 16:03:14.141
    STEP: Deleting the pod 09/04/23 16:03:16.157
    STEP: Ensuring resource quota status released the pod usage 09/04/23 16:03:16.176
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:03:18.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5671" for this suite. 09/04/23 16:03:18.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:03:18.239
Sep  4 16:03:18.239: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 16:03:18.24
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:03:18.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:03:18.311
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-65c5cb45-573a-40a3-b340-53e9e05b618f 09/04/23 16:03:18.338
STEP: Creating a pod to test consume configMaps 09/04/23 16:03:18.353
Sep  4 16:03:18.382: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949" in namespace "projected-9100" to be "Succeeded or Failed"
Sep  4 16:03:18.397: INFO: Pod "pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949": Phase="Pending", Reason="", readiness=false. Elapsed: 14.575919ms
Sep  4 16:03:20.413: INFO: Pod "pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030446232s
Sep  4 16:03:22.413: INFO: Pod "pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030742389s
STEP: Saw pod success 09/04/23 16:03:22.413
Sep  4 16:03:22.413: INFO: Pod "pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949" satisfied condition "Succeeded or Failed"
Sep  4 16:03:22.428: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 16:03:22.465
Sep  4 16:03:22.484: INFO: Waiting for pod pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949 to disappear
Sep  4 16:03:22.498: INFO: Pod pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 16:03:22.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9100" for this suite. 09/04/23 16:03:22.525
------------------------------
• [4.301 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:03:18.239
    Sep  4 16:03:18.239: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 16:03:18.24
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:03:18.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:03:18.311
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-65c5cb45-573a-40a3-b340-53e9e05b618f 09/04/23 16:03:18.338
    STEP: Creating a pod to test consume configMaps 09/04/23 16:03:18.353
    Sep  4 16:03:18.382: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949" in namespace "projected-9100" to be "Succeeded or Failed"
    Sep  4 16:03:18.397: INFO: Pod "pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949": Phase="Pending", Reason="", readiness=false. Elapsed: 14.575919ms
    Sep  4 16:03:20.413: INFO: Pod "pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030446232s
    Sep  4 16:03:22.413: INFO: Pod "pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030742389s
    STEP: Saw pod success 09/04/23 16:03:22.413
    Sep  4 16:03:22.413: INFO: Pod "pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949" satisfied condition "Succeeded or Failed"
    Sep  4 16:03:22.428: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 16:03:22.465
    Sep  4 16:03:22.484: INFO: Waiting for pod pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949 to disappear
    Sep  4 16:03:22.498: INFO: Pod pod-projected-configmaps-a606c505-4cc9-4f6e-afc0-f65cb1b46949 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:03:22.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9100" for this suite. 09/04/23 16:03:22.525
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:03:22.541
Sep  4 16:03:22.541: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy 09/04/23 16:03:22.541
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:03:22.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:03:22.613
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 09/04/23 16:03:22.66
STEP: creating replication controller proxy-service-dppnl in namespace proxy-1146 09/04/23 16:03:22.66
I0904 16:03:22.676747    7754 runners.go:193] Created replication controller with name: proxy-service-dppnl, namespace: proxy-1146, replica count: 1
I0904 16:03:23.727847    7754 runners.go:193] proxy-service-dppnl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0904 16:03:24.728982    7754 runners.go:193] proxy-service-dppnl Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 16:03:24.743: INFO: setup took 2.102339576s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/04/23 16:03:24.743
Sep  4 16:03:24.797: INFO: (0) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 53.302089ms)
Sep  4 16:03:24.797: INFO: (0) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 53.202028ms)
Sep  4 16:03:24.797: INFO: (0) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 53.21866ms)
Sep  4 16:03:24.797: INFO: (0) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 53.271186ms)
Sep  4 16:03:24.797: INFO: (0) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 53.260406ms)
Sep  4 16:03:24.797: INFO: (0) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 53.275462ms)
Sep  4 16:03:24.806: INFO: (0) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 62.487828ms)
Sep  4 16:03:24.806: INFO: (0) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 62.412732ms)
Sep  4 16:03:24.808: INFO: (0) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 64.574903ms)
Sep  4 16:03:24.808: INFO: (0) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 64.578108ms)
Sep  4 16:03:24.808: INFO: (0) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 64.57176ms)
Sep  4 16:03:24.818: INFO: (0) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 74.362805ms)
Sep  4 16:03:24.818: INFO: (0) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 74.531661ms)
Sep  4 16:03:24.818: INFO: (0) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 74.535429ms)
Sep  4 16:03:24.818: INFO: (0) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 74.486227ms)
Sep  4 16:03:24.830: INFO: (0) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 86.851277ms)
Sep  4 16:03:24.857: INFO: (1) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 26.214568ms)
Sep  4 16:03:24.857: INFO: (1) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.391777ms)
Sep  4 16:03:24.857: INFO: (1) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.409239ms)
Sep  4 16:03:24.857: INFO: (1) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 26.449279ms)
Sep  4 16:03:24.857: INFO: (1) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 26.509217ms)
Sep  4 16:03:24.858: INFO: (1) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 26.948348ms)
Sep  4 16:03:24.866: INFO: (1) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 35.476636ms)
Sep  4 16:03:24.866: INFO: (1) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 35.583267ms)
Sep  4 16:03:24.866: INFO: (1) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 35.480402ms)
Sep  4 16:03:24.866: INFO: (1) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 35.533221ms)
Sep  4 16:03:24.866: INFO: (1) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 35.518343ms)
Sep  4 16:03:24.876: INFO: (1) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 45.078738ms)
Sep  4 16:03:24.876: INFO: (1) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 45.278419ms)
Sep  4 16:03:24.876: INFO: (1) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 45.338581ms)
Sep  4 16:03:24.886: INFO: (1) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 55.691784ms)
Sep  4 16:03:24.886: INFO: (1) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 55.631408ms)
Sep  4 16:03:24.913: INFO: (2) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.503687ms)
Sep  4 16:03:24.913: INFO: (2) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.531547ms)
Sep  4 16:03:24.913: INFO: (2) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 26.421188ms)
Sep  4 16:03:24.913: INFO: (2) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.467464ms)
Sep  4 16:03:24.914: INFO: (2) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 27.717491ms)
Sep  4 16:03:24.914: INFO: (2) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 27.692424ms)
Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 36.104226ms)
Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 36.173657ms)
Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 36.137795ms)
Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 36.141899ms)
Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 36.175566ms)
Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 36.657713ms)
Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 36.818365ms)
Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 36.854306ms)
Sep  4 16:03:24.933: INFO: (2) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 46.265091ms)
Sep  4 16:03:24.933: INFO: (2) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 46.263973ms)
Sep  4 16:03:24.958: INFO: (3) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 25.589568ms)
Sep  4 16:03:24.958: INFO: (3) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 25.55504ms)
Sep  4 16:03:24.958: INFO: (3) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 25.735064ms)
Sep  4 16:03:24.958: INFO: (3) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.62169ms)
Sep  4 16:03:24.959: INFO: (3) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.007294ms)
Sep  4 16:03:24.959: INFO: (3) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.388002ms)
Sep  4 16:03:24.959: INFO: (3) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.347085ms)
Sep  4 16:03:24.959: INFO: (3) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 26.304076ms)
Sep  4 16:03:24.959: INFO: (3) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.34144ms)
Sep  4 16:03:24.959: INFO: (3) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 26.32349ms)
Sep  4 16:03:24.961: INFO: (3) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 28.052412ms)
Sep  4 16:03:24.961: INFO: (3) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 28.036123ms)
Sep  4 16:03:24.970: INFO: (3) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 37.348875ms)
Sep  4 16:03:24.970: INFO: (3) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 37.30717ms)
Sep  4 16:03:24.970: INFO: (3) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 37.277753ms)
Sep  4 16:03:24.979: INFO: (3) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 46.64939ms)
Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 39.713307ms)
Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 39.710135ms)
Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 39.79372ms)
Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 39.635194ms)
Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 39.695308ms)
Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 39.71236ms)
Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 39.812822ms)
Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 39.827947ms)
Sep  4 16:03:25.020: INFO: (4) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 40.05808ms)
Sep  4 16:03:25.020: INFO: (4) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 39.833641ms)
Sep  4 16:03:25.020: INFO: (4) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 40.048252ms)
Sep  4 16:03:25.020: INFO: (4) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 40.081612ms)
Sep  4 16:03:25.039: INFO: (4) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 59.622108ms)
Sep  4 16:03:25.039: INFO: (4) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 59.74283ms)
Sep  4 16:03:25.039: INFO: (4) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 59.589268ms)
Sep  4 16:03:25.039: INFO: (4) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 59.941309ms)
Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.90171ms)
Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.021728ms)
Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 26.196598ms)
Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 26.170424ms)
Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 26.290125ms)
Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.209224ms)
Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 26.717934ms)
Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.759361ms)
Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.898427ms)
Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.940472ms)
Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 26.845631ms)
Sep  4 16:03:25.075: INFO: (5) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 35.259598ms)
Sep  4 16:03:25.084: INFO: (5) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 44.806632ms)
Sep  4 16:03:25.084: INFO: (5) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 44.905231ms)
Sep  4 16:03:25.093: INFO: (5) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 53.351146ms)
Sep  4 16:03:25.093: INFO: (5) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 53.364595ms)
Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.037521ms)
Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 26.019914ms)
Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 26.134753ms)
Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.045226ms)
Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 26.337905ms)
Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.28969ms)
Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 26.331391ms)
Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 26.319082ms)
Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.222683ms)
Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 26.254078ms)
Sep  4 16:03:25.121: INFO: (6) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 27.827556ms)
Sep  4 16:03:25.121: INFO: (6) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 27.89229ms)
Sep  4 16:03:25.131: INFO: (6) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 38.123002ms)
Sep  4 16:03:25.131: INFO: (6) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 38.15319ms)
Sep  4 16:03:25.140: INFO: (6) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 46.4952ms)
Sep  4 16:03:25.140: INFO: (6) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 46.497628ms)
Sep  4 16:03:25.166: INFO: (7) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.312104ms)
Sep  4 16:03:25.166: INFO: (7) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.143879ms)
Sep  4 16:03:25.166: INFO: (7) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 26.269394ms)
Sep  4 16:03:25.167: INFO: (7) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 27.010257ms)
Sep  4 16:03:25.167: INFO: (7) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.938666ms)
Sep  4 16:03:25.167: INFO: (7) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 27.274335ms)
Sep  4 16:03:25.167: INFO: (7) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 27.279879ms)
Sep  4 16:03:25.167: INFO: (7) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 27.42943ms)
Sep  4 16:03:25.176: INFO: (7) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 36.478382ms)
Sep  4 16:03:25.176: INFO: (7) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 36.549533ms)
Sep  4 16:03:25.176: INFO: (7) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 36.515718ms)
Sep  4 16:03:25.176: INFO: (7) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 36.54816ms)
Sep  4 16:03:25.185: INFO: (7) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 45.275563ms)
Sep  4 16:03:25.185: INFO: (7) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 45.272696ms)
Sep  4 16:03:25.194: INFO: (7) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 54.28069ms)
Sep  4 16:03:25.194: INFO: (7) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 54.339138ms)
Sep  4 16:03:25.220: INFO: (8) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 25.252021ms)
Sep  4 16:03:25.220: INFO: (8) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.296796ms)
Sep  4 16:03:25.220: INFO: (8) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.317188ms)
Sep  4 16:03:25.220: INFO: (8) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 25.250554ms)
Sep  4 16:03:25.222: INFO: (8) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 27.760498ms)
Sep  4 16:03:25.222: INFO: (8) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 27.768126ms)
Sep  4 16:03:25.222: INFO: (8) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 27.816925ms)
Sep  4 16:03:25.222: INFO: (8) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 27.879061ms)
Sep  4 16:03:25.222: INFO: (8) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 27.738446ms)
Sep  4 16:03:25.229: INFO: (8) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 35.185732ms)
Sep  4 16:03:25.230: INFO: (8) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 35.357792ms)
Sep  4 16:03:25.230: INFO: (8) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 35.391899ms)
Sep  4 16:03:25.230: INFO: (8) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 35.368528ms)
Sep  4 16:03:25.231: INFO: (8) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 37.003928ms)
Sep  4 16:03:25.240: INFO: (8) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 46.076854ms)
Sep  4 16:03:25.250: INFO: (8) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 56.003807ms)
Sep  4 16:03:25.276: INFO: (9) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.382726ms)
Sep  4 16:03:25.276: INFO: (9) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 25.264258ms)
Sep  4 16:03:25.276: INFO: (9) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.289429ms)
Sep  4 16:03:25.276: INFO: (9) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 25.445474ms)
Sep  4 16:03:25.277: INFO: (9) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.249812ms)
Sep  4 16:03:25.277: INFO: (9) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 26.556475ms)
Sep  4 16:03:25.277: INFO: (9) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 26.591816ms)
Sep  4 16:03:25.277: INFO: (9) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 26.386795ms)
Sep  4 16:03:25.277: INFO: (9) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.482937ms)
Sep  4 16:03:25.277: INFO: (9) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.328108ms)
Sep  4 16:03:25.277: INFO: (9) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 26.921185ms)
Sep  4 16:03:25.287: INFO: (9) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 36.256705ms)
Sep  4 16:03:25.287: INFO: (9) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 36.380256ms)
Sep  4 16:03:25.287: INFO: (9) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 36.245206ms)
Sep  4 16:03:25.287: INFO: (9) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 36.368629ms)
Sep  4 16:03:25.296: INFO: (9) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 45.586709ms)
Sep  4 16:03:25.322: INFO: (10) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.028758ms)
Sep  4 16:03:25.322: INFO: (10) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 25.869495ms)
Sep  4 16:03:25.322: INFO: (10) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 25.918266ms)
Sep  4 16:03:25.323: INFO: (10) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.603773ms)
Sep  4 16:03:25.323: INFO: (10) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 27.007918ms)
Sep  4 16:03:25.323: INFO: (10) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.456486ms)
Sep  4 16:03:25.333: INFO: (10) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 36.168668ms)
Sep  4 16:03:25.333: INFO: (10) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 36.262603ms)
Sep  4 16:03:25.333: INFO: (10) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 36.490808ms)
Sep  4 16:03:25.333: INFO: (10) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 36.578963ms)
Sep  4 16:03:25.333: INFO: (10) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 36.443176ms)
Sep  4 16:03:25.333: INFO: (10) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 36.149079ms)
Sep  4 16:03:25.342: INFO: (10) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 45.726573ms)
Sep  4 16:03:25.342: INFO: (10) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 45.321911ms)
Sep  4 16:03:25.351: INFO: (10) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 54.551192ms)
Sep  4 16:03:25.361: INFO: (10) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 64.157945ms)
Sep  4 16:03:25.388: INFO: (11) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 27.367635ms)
Sep  4 16:03:25.388: INFO: (11) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 27.293945ms)
Sep  4 16:03:25.388: INFO: (11) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 27.37732ms)
Sep  4 16:03:25.388: INFO: (11) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 27.440265ms)
Sep  4 16:03:25.388: INFO: (11) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 27.289343ms)
Sep  4 16:03:25.388: INFO: (11) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 27.396182ms)
Sep  4 16:03:25.398: INFO: (11) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 36.781779ms)
Sep  4 16:03:25.398: INFO: (11) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 36.771656ms)
Sep  4 16:03:25.398: INFO: (11) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 36.799145ms)
Sep  4 16:03:25.398: INFO: (11) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 36.846564ms)
Sep  4 16:03:25.398: INFO: (11) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 36.819278ms)
Sep  4 16:03:25.398: INFO: (11) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 36.901941ms)
Sep  4 16:03:25.407: INFO: (11) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 45.977317ms)
Sep  4 16:03:25.407: INFO: (11) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 46.064379ms)
Sep  4 16:03:25.416: INFO: (11) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 55.519048ms)
Sep  4 16:03:25.426: INFO: (11) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 65.24069ms)
Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.470825ms)
Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 25.396488ms)
Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.427301ms)
Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.461968ms)
Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 25.352071ms)
Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 25.524911ms)
Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 25.270312ms)
Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.420542ms)
Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 25.283035ms)
Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 25.335827ms)
Sep  4 16:03:25.453: INFO: (12) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 27.164283ms)
Sep  4 16:03:25.453: INFO: (12) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 27.224692ms)
Sep  4 16:03:25.463: INFO: (12) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 36.771507ms)
Sep  4 16:03:25.463: INFO: (12) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 36.894438ms)
Sep  4 16:03:25.472: INFO: (12) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 45.851585ms)
Sep  4 16:03:25.472: INFO: (12) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 45.986084ms)
Sep  4 16:03:25.498: INFO: (13) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.064054ms)
Sep  4 16:03:25.498: INFO: (13) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.127794ms)
Sep  4 16:03:25.498: INFO: (13) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.084203ms)
Sep  4 16:03:25.501: INFO: (13) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 28.522415ms)
Sep  4 16:03:25.501: INFO: (13) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 28.620552ms)
Sep  4 16:03:25.501: INFO: (13) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 28.564734ms)
Sep  4 16:03:25.501: INFO: (13) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 28.599075ms)
Sep  4 16:03:25.501: INFO: (13) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 28.675878ms)
Sep  4 16:03:25.508: INFO: (13) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 35.944589ms)
Sep  4 16:03:25.508: INFO: (13) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 36.03881ms)
Sep  4 16:03:25.508: INFO: (13) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 35.97275ms)
Sep  4 16:03:25.508: INFO: (13) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 36.079861ms)
Sep  4 16:03:25.510: INFO: (13) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 37.737903ms)
Sep  4 16:03:25.510: INFO: (13) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 37.780954ms)
Sep  4 16:03:25.519: INFO: (13) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 46.846581ms)
Sep  4 16:03:25.519: INFO: (13) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 46.789228ms)
Sep  4 16:03:25.545: INFO: (14) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.664089ms)
Sep  4 16:03:25.545: INFO: (14) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.738887ms)
Sep  4 16:03:25.545: INFO: (14) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.821741ms)
Sep  4 16:03:25.547: INFO: (14) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 28.098651ms)
Sep  4 16:03:25.547: INFO: (14) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 28.182696ms)
Sep  4 16:03:25.547: INFO: (14) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 28.114896ms)
Sep  4 16:03:25.547: INFO: (14) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 28.263441ms)
Sep  4 16:03:25.555: INFO: (14) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 35.650411ms)
Sep  4 16:03:25.555: INFO: (14) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 35.666298ms)
Sep  4 16:03:25.555: INFO: (14) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 35.780585ms)
Sep  4 16:03:25.556: INFO: (14) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 37.135675ms)
Sep  4 16:03:25.556: INFO: (14) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 37.114427ms)
Sep  4 16:03:25.557: INFO: (14) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 37.373518ms)
Sep  4 16:03:25.557: INFO: (14) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 37.332987ms)
Sep  4 16:03:25.566: INFO: (14) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 47.05468ms)
Sep  4 16:03:25.575: INFO: (14) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 55.815594ms)
Sep  4 16:03:25.602: INFO: (15) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.985371ms)
Sep  4 16:03:25.602: INFO: (15) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 27.023588ms)
Sep  4 16:03:25.602: INFO: (15) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 27.051464ms)
Sep  4 16:03:25.602: INFO: (15) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 27.047819ms)
Sep  4 16:03:25.604: INFO: (15) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 28.597056ms)
Sep  4 16:03:25.604: INFO: (15) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 28.648411ms)
Sep  4 16:03:25.604: INFO: (15) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 28.707659ms)
Sep  4 16:03:25.604: INFO: (15) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 28.609111ms)
Sep  4 16:03:25.612: INFO: (15) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 36.872993ms)
Sep  4 16:03:25.612: INFO: (15) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 36.876881ms)
Sep  4 16:03:25.612: INFO: (15) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 36.841596ms)
Sep  4 16:03:25.612: INFO: (15) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 36.848636ms)
Sep  4 16:03:25.613: INFO: (15) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 37.849847ms)
Sep  4 16:03:25.613: INFO: (15) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 37.892838ms)
Sep  4 16:03:25.622: INFO: (15) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 46.760096ms)
Sep  4 16:03:25.631: INFO: (15) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 56.054697ms)
Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 25.820335ms)
Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.923723ms)
Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.854474ms)
Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 25.935335ms)
Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 25.869658ms)
Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 25.903207ms)
Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 26.018366ms)
Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 25.905488ms)
Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.023857ms)
Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 25.989441ms)
Sep  4 16:03:25.660: INFO: (16) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 28.752399ms)
Sep  4 16:03:25.660: INFO: (16) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 28.806619ms)
Sep  4 16:03:25.669: INFO: (16) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 37.875582ms)
Sep  4 16:03:25.669: INFO: (16) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 37.882287ms)
Sep  4 16:03:25.679: INFO: (16) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 47.347965ms)
Sep  4 16:03:25.688: INFO: (16) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 57.191042ms)
Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.426909ms)
Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 26.380992ms)
Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 26.434318ms)
Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 26.620098ms)
Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.422367ms)
Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 26.549452ms)
Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 26.480181ms)
Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 26.465185ms)
Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.478459ms)
Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 26.596556ms)
Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 26.535873ms)
Sep  4 16:03:25.716: INFO: (17) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 27.668299ms)
Sep  4 16:03:25.726: INFO: (17) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 36.968094ms)
Sep  4 16:03:25.726: INFO: (17) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 37.000626ms)
Sep  4 16:03:25.735: INFO: (17) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 46.143139ms)
Sep  4 16:03:25.735: INFO: (17) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 45.916864ms)
Sep  4 16:03:25.760: INFO: (18) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.493776ms)
Sep  4 16:03:25.760: INFO: (18) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 25.595367ms)
Sep  4 16:03:25.760: INFO: (18) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 25.623048ms)
Sep  4 16:03:25.760: INFO: (18) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.552053ms)
Sep  4 16:03:25.763: INFO: (18) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 27.729738ms)
Sep  4 16:03:25.763: INFO: (18) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 27.835072ms)
Sep  4 16:03:25.763: INFO: (18) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 27.811921ms)
Sep  4 16:03:25.763: INFO: (18) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 27.941511ms)
Sep  4 16:03:25.770: INFO: (18) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 35.428414ms)
Sep  4 16:03:25.770: INFO: (18) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 35.412277ms)
Sep  4 16:03:25.770: INFO: (18) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 35.399856ms)
Sep  4 16:03:25.770: INFO: (18) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 35.389578ms)
Sep  4 16:03:25.779: INFO: (18) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 44.598884ms)
Sep  4 16:03:25.779: INFO: (18) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 44.561055ms)
Sep  4 16:03:25.789: INFO: (18) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 54.350009ms)
Sep  4 16:03:25.789: INFO: (18) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 54.358822ms)
Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.20126ms)
Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 25.911847ms)
Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.125942ms)
Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 26.199551ms)
Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.000349ms)
Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 26.04393ms)
Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.361352ms)
Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 26.080529ms)
Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 26.125781ms)
Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 26.297829ms)
Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 26.945025ms)
Sep  4 16:03:25.825: INFO: (19) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 35.992092ms)
Sep  4 16:03:25.835: INFO: (19) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 45.529901ms)
Sep  4 16:03:25.835: INFO: (19) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 45.279611ms)
Sep  4 16:03:25.835: INFO: (19) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 45.428568ms)
Sep  4 16:03:25.835: INFO: (19) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 45.198919ms)
STEP: deleting ReplicationController proxy-service-dppnl in namespace proxy-1146, will wait for the garbage collector to delete the pods 09/04/23 16:03:25.835
Sep  4 16:03:25.916: INFO: Deleting ReplicationController proxy-service-dppnl took: 16.293384ms
Sep  4 16:03:26.017: INFO: Terminating ReplicationController proxy-service-dppnl pods took: 101.147143ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  4 16:03:28.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-1146" for this suite. 09/04/23 16:03:28.746
------------------------------
• [6.222 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:03:22.541
    Sep  4 16:03:22.541: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename proxy 09/04/23 16:03:22.541
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:03:22.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:03:22.613
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 09/04/23 16:03:22.66
    STEP: creating replication controller proxy-service-dppnl in namespace proxy-1146 09/04/23 16:03:22.66
    I0904 16:03:22.676747    7754 runners.go:193] Created replication controller with name: proxy-service-dppnl, namespace: proxy-1146, replica count: 1
    I0904 16:03:23.727847    7754 runners.go:193] proxy-service-dppnl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0904 16:03:24.728982    7754 runners.go:193] proxy-service-dppnl Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 16:03:24.743: INFO: setup took 2.102339576s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/04/23 16:03:24.743
    Sep  4 16:03:24.797: INFO: (0) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 53.302089ms)
    Sep  4 16:03:24.797: INFO: (0) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 53.202028ms)
    Sep  4 16:03:24.797: INFO: (0) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 53.21866ms)
    Sep  4 16:03:24.797: INFO: (0) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 53.271186ms)
    Sep  4 16:03:24.797: INFO: (0) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 53.260406ms)
    Sep  4 16:03:24.797: INFO: (0) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 53.275462ms)
    Sep  4 16:03:24.806: INFO: (0) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 62.487828ms)
    Sep  4 16:03:24.806: INFO: (0) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 62.412732ms)
    Sep  4 16:03:24.808: INFO: (0) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 64.574903ms)
    Sep  4 16:03:24.808: INFO: (0) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 64.578108ms)
    Sep  4 16:03:24.808: INFO: (0) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 64.57176ms)
    Sep  4 16:03:24.818: INFO: (0) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 74.362805ms)
    Sep  4 16:03:24.818: INFO: (0) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 74.531661ms)
    Sep  4 16:03:24.818: INFO: (0) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 74.535429ms)
    Sep  4 16:03:24.818: INFO: (0) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 74.486227ms)
    Sep  4 16:03:24.830: INFO: (0) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 86.851277ms)
    Sep  4 16:03:24.857: INFO: (1) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 26.214568ms)
    Sep  4 16:03:24.857: INFO: (1) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.391777ms)
    Sep  4 16:03:24.857: INFO: (1) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.409239ms)
    Sep  4 16:03:24.857: INFO: (1) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 26.449279ms)
    Sep  4 16:03:24.857: INFO: (1) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 26.509217ms)
    Sep  4 16:03:24.858: INFO: (1) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 26.948348ms)
    Sep  4 16:03:24.866: INFO: (1) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 35.476636ms)
    Sep  4 16:03:24.866: INFO: (1) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 35.583267ms)
    Sep  4 16:03:24.866: INFO: (1) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 35.480402ms)
    Sep  4 16:03:24.866: INFO: (1) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 35.533221ms)
    Sep  4 16:03:24.866: INFO: (1) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 35.518343ms)
    Sep  4 16:03:24.876: INFO: (1) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 45.078738ms)
    Sep  4 16:03:24.876: INFO: (1) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 45.278419ms)
    Sep  4 16:03:24.876: INFO: (1) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 45.338581ms)
    Sep  4 16:03:24.886: INFO: (1) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 55.691784ms)
    Sep  4 16:03:24.886: INFO: (1) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 55.631408ms)
    Sep  4 16:03:24.913: INFO: (2) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.503687ms)
    Sep  4 16:03:24.913: INFO: (2) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.531547ms)
    Sep  4 16:03:24.913: INFO: (2) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 26.421188ms)
    Sep  4 16:03:24.913: INFO: (2) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.467464ms)
    Sep  4 16:03:24.914: INFO: (2) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 27.717491ms)
    Sep  4 16:03:24.914: INFO: (2) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 27.692424ms)
    Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 36.104226ms)
    Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 36.173657ms)
    Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 36.137795ms)
    Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 36.141899ms)
    Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 36.175566ms)
    Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 36.657713ms)
    Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 36.818365ms)
    Sep  4 16:03:24.923: INFO: (2) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 36.854306ms)
    Sep  4 16:03:24.933: INFO: (2) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 46.265091ms)
    Sep  4 16:03:24.933: INFO: (2) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 46.263973ms)
    Sep  4 16:03:24.958: INFO: (3) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 25.589568ms)
    Sep  4 16:03:24.958: INFO: (3) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 25.55504ms)
    Sep  4 16:03:24.958: INFO: (3) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 25.735064ms)
    Sep  4 16:03:24.958: INFO: (3) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.62169ms)
    Sep  4 16:03:24.959: INFO: (3) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.007294ms)
    Sep  4 16:03:24.959: INFO: (3) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.388002ms)
    Sep  4 16:03:24.959: INFO: (3) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.347085ms)
    Sep  4 16:03:24.959: INFO: (3) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 26.304076ms)
    Sep  4 16:03:24.959: INFO: (3) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.34144ms)
    Sep  4 16:03:24.959: INFO: (3) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 26.32349ms)
    Sep  4 16:03:24.961: INFO: (3) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 28.052412ms)
    Sep  4 16:03:24.961: INFO: (3) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 28.036123ms)
    Sep  4 16:03:24.970: INFO: (3) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 37.348875ms)
    Sep  4 16:03:24.970: INFO: (3) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 37.30717ms)
    Sep  4 16:03:24.970: INFO: (3) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 37.277753ms)
    Sep  4 16:03:24.979: INFO: (3) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 46.64939ms)
    Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 39.713307ms)
    Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 39.710135ms)
    Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 39.79372ms)
    Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 39.635194ms)
    Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 39.695308ms)
    Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 39.71236ms)
    Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 39.812822ms)
    Sep  4 16:03:25.019: INFO: (4) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 39.827947ms)
    Sep  4 16:03:25.020: INFO: (4) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 40.05808ms)
    Sep  4 16:03:25.020: INFO: (4) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 39.833641ms)
    Sep  4 16:03:25.020: INFO: (4) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 40.048252ms)
    Sep  4 16:03:25.020: INFO: (4) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 40.081612ms)
    Sep  4 16:03:25.039: INFO: (4) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 59.622108ms)
    Sep  4 16:03:25.039: INFO: (4) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 59.74283ms)
    Sep  4 16:03:25.039: INFO: (4) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 59.589268ms)
    Sep  4 16:03:25.039: INFO: (4) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 59.941309ms)
    Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.90171ms)
    Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.021728ms)
    Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 26.196598ms)
    Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 26.170424ms)
    Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 26.290125ms)
    Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.209224ms)
    Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 26.717934ms)
    Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.759361ms)
    Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.898427ms)
    Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.940472ms)
    Sep  4 16:03:25.066: INFO: (5) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 26.845631ms)
    Sep  4 16:03:25.075: INFO: (5) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 35.259598ms)
    Sep  4 16:03:25.084: INFO: (5) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 44.806632ms)
    Sep  4 16:03:25.084: INFO: (5) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 44.905231ms)
    Sep  4 16:03:25.093: INFO: (5) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 53.351146ms)
    Sep  4 16:03:25.093: INFO: (5) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 53.364595ms)
    Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.037521ms)
    Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 26.019914ms)
    Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 26.134753ms)
    Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.045226ms)
    Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 26.337905ms)
    Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.28969ms)
    Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 26.331391ms)
    Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 26.319082ms)
    Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.222683ms)
    Sep  4 16:03:25.119: INFO: (6) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 26.254078ms)
    Sep  4 16:03:25.121: INFO: (6) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 27.827556ms)
    Sep  4 16:03:25.121: INFO: (6) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 27.89229ms)
    Sep  4 16:03:25.131: INFO: (6) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 38.123002ms)
    Sep  4 16:03:25.131: INFO: (6) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 38.15319ms)
    Sep  4 16:03:25.140: INFO: (6) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 46.4952ms)
    Sep  4 16:03:25.140: INFO: (6) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 46.497628ms)
    Sep  4 16:03:25.166: INFO: (7) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.312104ms)
    Sep  4 16:03:25.166: INFO: (7) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.143879ms)
    Sep  4 16:03:25.166: INFO: (7) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 26.269394ms)
    Sep  4 16:03:25.167: INFO: (7) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 27.010257ms)
    Sep  4 16:03:25.167: INFO: (7) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.938666ms)
    Sep  4 16:03:25.167: INFO: (7) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 27.274335ms)
    Sep  4 16:03:25.167: INFO: (7) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 27.279879ms)
    Sep  4 16:03:25.167: INFO: (7) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 27.42943ms)
    Sep  4 16:03:25.176: INFO: (7) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 36.478382ms)
    Sep  4 16:03:25.176: INFO: (7) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 36.549533ms)
    Sep  4 16:03:25.176: INFO: (7) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 36.515718ms)
    Sep  4 16:03:25.176: INFO: (7) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 36.54816ms)
    Sep  4 16:03:25.185: INFO: (7) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 45.275563ms)
    Sep  4 16:03:25.185: INFO: (7) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 45.272696ms)
    Sep  4 16:03:25.194: INFO: (7) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 54.28069ms)
    Sep  4 16:03:25.194: INFO: (7) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 54.339138ms)
    Sep  4 16:03:25.220: INFO: (8) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 25.252021ms)
    Sep  4 16:03:25.220: INFO: (8) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.296796ms)
    Sep  4 16:03:25.220: INFO: (8) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.317188ms)
    Sep  4 16:03:25.220: INFO: (8) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 25.250554ms)
    Sep  4 16:03:25.222: INFO: (8) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 27.760498ms)
    Sep  4 16:03:25.222: INFO: (8) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 27.768126ms)
    Sep  4 16:03:25.222: INFO: (8) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 27.816925ms)
    Sep  4 16:03:25.222: INFO: (8) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 27.879061ms)
    Sep  4 16:03:25.222: INFO: (8) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 27.738446ms)
    Sep  4 16:03:25.229: INFO: (8) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 35.185732ms)
    Sep  4 16:03:25.230: INFO: (8) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 35.357792ms)
    Sep  4 16:03:25.230: INFO: (8) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 35.391899ms)
    Sep  4 16:03:25.230: INFO: (8) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 35.368528ms)
    Sep  4 16:03:25.231: INFO: (8) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 37.003928ms)
    Sep  4 16:03:25.240: INFO: (8) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 46.076854ms)
    Sep  4 16:03:25.250: INFO: (8) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 56.003807ms)
    Sep  4 16:03:25.276: INFO: (9) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.382726ms)
    Sep  4 16:03:25.276: INFO: (9) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 25.264258ms)
    Sep  4 16:03:25.276: INFO: (9) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.289429ms)
    Sep  4 16:03:25.276: INFO: (9) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 25.445474ms)
    Sep  4 16:03:25.277: INFO: (9) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.249812ms)
    Sep  4 16:03:25.277: INFO: (9) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 26.556475ms)
    Sep  4 16:03:25.277: INFO: (9) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 26.591816ms)
    Sep  4 16:03:25.277: INFO: (9) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 26.386795ms)
    Sep  4 16:03:25.277: INFO: (9) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.482937ms)
    Sep  4 16:03:25.277: INFO: (9) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.328108ms)
    Sep  4 16:03:25.277: INFO: (9) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 26.921185ms)
    Sep  4 16:03:25.287: INFO: (9) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 36.256705ms)
    Sep  4 16:03:25.287: INFO: (9) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 36.380256ms)
    Sep  4 16:03:25.287: INFO: (9) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 36.245206ms)
    Sep  4 16:03:25.287: INFO: (9) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 36.368629ms)
    Sep  4 16:03:25.296: INFO: (9) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 45.586709ms)
    Sep  4 16:03:25.322: INFO: (10) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.028758ms)
    Sep  4 16:03:25.322: INFO: (10) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 25.869495ms)
    Sep  4 16:03:25.322: INFO: (10) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 25.918266ms)
    Sep  4 16:03:25.323: INFO: (10) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.603773ms)
    Sep  4 16:03:25.323: INFO: (10) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 27.007918ms)
    Sep  4 16:03:25.323: INFO: (10) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.456486ms)
    Sep  4 16:03:25.333: INFO: (10) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 36.168668ms)
    Sep  4 16:03:25.333: INFO: (10) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 36.262603ms)
    Sep  4 16:03:25.333: INFO: (10) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 36.490808ms)
    Sep  4 16:03:25.333: INFO: (10) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 36.578963ms)
    Sep  4 16:03:25.333: INFO: (10) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 36.443176ms)
    Sep  4 16:03:25.333: INFO: (10) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 36.149079ms)
    Sep  4 16:03:25.342: INFO: (10) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 45.726573ms)
    Sep  4 16:03:25.342: INFO: (10) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 45.321911ms)
    Sep  4 16:03:25.351: INFO: (10) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 54.551192ms)
    Sep  4 16:03:25.361: INFO: (10) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 64.157945ms)
    Sep  4 16:03:25.388: INFO: (11) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 27.367635ms)
    Sep  4 16:03:25.388: INFO: (11) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 27.293945ms)
    Sep  4 16:03:25.388: INFO: (11) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 27.37732ms)
    Sep  4 16:03:25.388: INFO: (11) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 27.440265ms)
    Sep  4 16:03:25.388: INFO: (11) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 27.289343ms)
    Sep  4 16:03:25.388: INFO: (11) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 27.396182ms)
    Sep  4 16:03:25.398: INFO: (11) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 36.781779ms)
    Sep  4 16:03:25.398: INFO: (11) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 36.771656ms)
    Sep  4 16:03:25.398: INFO: (11) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 36.799145ms)
    Sep  4 16:03:25.398: INFO: (11) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 36.846564ms)
    Sep  4 16:03:25.398: INFO: (11) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 36.819278ms)
    Sep  4 16:03:25.398: INFO: (11) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 36.901941ms)
    Sep  4 16:03:25.407: INFO: (11) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 45.977317ms)
    Sep  4 16:03:25.407: INFO: (11) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 46.064379ms)
    Sep  4 16:03:25.416: INFO: (11) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 55.519048ms)
    Sep  4 16:03:25.426: INFO: (11) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 65.24069ms)
    Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.470825ms)
    Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 25.396488ms)
    Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.427301ms)
    Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.461968ms)
    Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 25.352071ms)
    Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 25.524911ms)
    Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 25.270312ms)
    Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.420542ms)
    Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 25.283035ms)
    Sep  4 16:03:25.452: INFO: (12) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 25.335827ms)
    Sep  4 16:03:25.453: INFO: (12) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 27.164283ms)
    Sep  4 16:03:25.453: INFO: (12) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 27.224692ms)
    Sep  4 16:03:25.463: INFO: (12) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 36.771507ms)
    Sep  4 16:03:25.463: INFO: (12) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 36.894438ms)
    Sep  4 16:03:25.472: INFO: (12) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 45.851585ms)
    Sep  4 16:03:25.472: INFO: (12) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 45.986084ms)
    Sep  4 16:03:25.498: INFO: (13) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.064054ms)
    Sep  4 16:03:25.498: INFO: (13) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.127794ms)
    Sep  4 16:03:25.498: INFO: (13) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.084203ms)
    Sep  4 16:03:25.501: INFO: (13) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 28.522415ms)
    Sep  4 16:03:25.501: INFO: (13) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 28.620552ms)
    Sep  4 16:03:25.501: INFO: (13) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 28.564734ms)
    Sep  4 16:03:25.501: INFO: (13) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 28.599075ms)
    Sep  4 16:03:25.501: INFO: (13) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 28.675878ms)
    Sep  4 16:03:25.508: INFO: (13) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 35.944589ms)
    Sep  4 16:03:25.508: INFO: (13) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 36.03881ms)
    Sep  4 16:03:25.508: INFO: (13) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 35.97275ms)
    Sep  4 16:03:25.508: INFO: (13) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 36.079861ms)
    Sep  4 16:03:25.510: INFO: (13) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 37.737903ms)
    Sep  4 16:03:25.510: INFO: (13) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 37.780954ms)
    Sep  4 16:03:25.519: INFO: (13) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 46.846581ms)
    Sep  4 16:03:25.519: INFO: (13) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 46.789228ms)
    Sep  4 16:03:25.545: INFO: (14) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.664089ms)
    Sep  4 16:03:25.545: INFO: (14) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.738887ms)
    Sep  4 16:03:25.545: INFO: (14) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.821741ms)
    Sep  4 16:03:25.547: INFO: (14) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 28.098651ms)
    Sep  4 16:03:25.547: INFO: (14) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 28.182696ms)
    Sep  4 16:03:25.547: INFO: (14) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 28.114896ms)
    Sep  4 16:03:25.547: INFO: (14) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 28.263441ms)
    Sep  4 16:03:25.555: INFO: (14) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 35.650411ms)
    Sep  4 16:03:25.555: INFO: (14) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 35.666298ms)
    Sep  4 16:03:25.555: INFO: (14) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 35.780585ms)
    Sep  4 16:03:25.556: INFO: (14) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 37.135675ms)
    Sep  4 16:03:25.556: INFO: (14) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 37.114427ms)
    Sep  4 16:03:25.557: INFO: (14) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 37.373518ms)
    Sep  4 16:03:25.557: INFO: (14) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 37.332987ms)
    Sep  4 16:03:25.566: INFO: (14) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 47.05468ms)
    Sep  4 16:03:25.575: INFO: (14) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 55.815594ms)
    Sep  4 16:03:25.602: INFO: (15) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.985371ms)
    Sep  4 16:03:25.602: INFO: (15) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 27.023588ms)
    Sep  4 16:03:25.602: INFO: (15) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 27.051464ms)
    Sep  4 16:03:25.602: INFO: (15) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 27.047819ms)
    Sep  4 16:03:25.604: INFO: (15) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 28.597056ms)
    Sep  4 16:03:25.604: INFO: (15) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 28.648411ms)
    Sep  4 16:03:25.604: INFO: (15) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 28.707659ms)
    Sep  4 16:03:25.604: INFO: (15) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 28.609111ms)
    Sep  4 16:03:25.612: INFO: (15) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 36.872993ms)
    Sep  4 16:03:25.612: INFO: (15) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 36.876881ms)
    Sep  4 16:03:25.612: INFO: (15) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 36.841596ms)
    Sep  4 16:03:25.612: INFO: (15) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 36.848636ms)
    Sep  4 16:03:25.613: INFO: (15) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 37.849847ms)
    Sep  4 16:03:25.613: INFO: (15) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 37.892838ms)
    Sep  4 16:03:25.622: INFO: (15) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 46.760096ms)
    Sep  4 16:03:25.631: INFO: (15) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 56.054697ms)
    Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 25.820335ms)
    Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.923723ms)
    Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.854474ms)
    Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 25.935335ms)
    Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 25.869658ms)
    Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 25.903207ms)
    Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 26.018366ms)
    Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 25.905488ms)
    Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.023857ms)
    Sep  4 16:03:25.657: INFO: (16) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 25.989441ms)
    Sep  4 16:03:25.660: INFO: (16) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 28.752399ms)
    Sep  4 16:03:25.660: INFO: (16) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 28.806619ms)
    Sep  4 16:03:25.669: INFO: (16) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 37.875582ms)
    Sep  4 16:03:25.669: INFO: (16) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 37.882287ms)
    Sep  4 16:03:25.679: INFO: (16) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 47.347965ms)
    Sep  4 16:03:25.688: INFO: (16) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 57.191042ms)
    Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.426909ms)
    Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 26.380992ms)
    Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 26.434318ms)
    Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 26.620098ms)
    Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.422367ms)
    Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 26.549452ms)
    Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 26.480181ms)
    Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 26.465185ms)
    Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.478459ms)
    Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 26.596556ms)
    Sep  4 16:03:25.715: INFO: (17) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 26.535873ms)
    Sep  4 16:03:25.716: INFO: (17) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 27.668299ms)
    Sep  4 16:03:25.726: INFO: (17) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 36.968094ms)
    Sep  4 16:03:25.726: INFO: (17) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 37.000626ms)
    Sep  4 16:03:25.735: INFO: (17) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 46.143139ms)
    Sep  4 16:03:25.735: INFO: (17) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 45.916864ms)
    Sep  4 16:03:25.760: INFO: (18) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 25.493776ms)
    Sep  4 16:03:25.760: INFO: (18) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 25.595367ms)
    Sep  4 16:03:25.760: INFO: (18) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 25.623048ms)
    Sep  4 16:03:25.760: INFO: (18) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 25.552053ms)
    Sep  4 16:03:25.763: INFO: (18) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 27.729738ms)
    Sep  4 16:03:25.763: INFO: (18) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 27.835072ms)
    Sep  4 16:03:25.763: INFO: (18) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 27.811921ms)
    Sep  4 16:03:25.763: INFO: (18) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 27.941511ms)
    Sep  4 16:03:25.770: INFO: (18) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 35.428414ms)
    Sep  4 16:03:25.770: INFO: (18) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 35.412277ms)
    Sep  4 16:03:25.770: INFO: (18) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 35.399856ms)
    Sep  4 16:03:25.770: INFO: (18) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 35.389578ms)
    Sep  4 16:03:25.779: INFO: (18) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 44.598884ms)
    Sep  4 16:03:25.779: INFO: (18) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 44.561055ms)
    Sep  4 16:03:25.789: INFO: (18) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 54.350009ms)
    Sep  4 16:03:25.789: INFO: (18) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 54.358822ms)
    Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 26.20126ms)
    Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:460/proxy/: tls baz (200; 25.911847ms)
    Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">... (200; 26.125942ms)
    Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname1/proxy/: foo (200; 26.199551ms)
    Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:1080/proxy/rewriteme">test<... (200; 26.000349ms)
    Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx/proxy/rewriteme">test</a> (200; 26.04393ms)
    Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/http:proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 26.361352ms)
    Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname2/proxy/: tls qux (200; 26.080529ms)
    Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/services/http:proxy-service-dppnl:portname2/proxy/: bar (200; 26.125781ms)
    Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:462/proxy/: tls qux (200; 26.297829ms)
    Sep  4 16:03:25.816: INFO: (19) /api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/: <a href="/api/v1/namespaces/proxy-1146/pods/https:proxy-service-dppnl-pbjxx:443/proxy/tlsrewritem... (200; 26.945025ms)
    Sep  4 16:03:25.825: INFO: (19) /api/v1/namespaces/proxy-1146/services/https:proxy-service-dppnl:tlsportname1/proxy/: tls baz (200; 35.992092ms)
    Sep  4 16:03:25.835: INFO: (19) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname1/proxy/: foo (200; 45.529901ms)
    Sep  4 16:03:25.835: INFO: (19) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:160/proxy/: foo (200; 45.279611ms)
    Sep  4 16:03:25.835: INFO: (19) /api/v1/namespaces/proxy-1146/pods/proxy-service-dppnl-pbjxx:162/proxy/: bar (200; 45.428568ms)
    Sep  4 16:03:25.835: INFO: (19) /api/v1/namespaces/proxy-1146/services/proxy-service-dppnl:portname2/proxy/: bar (200; 45.198919ms)
    STEP: deleting ReplicationController proxy-service-dppnl in namespace proxy-1146, will wait for the garbage collector to delete the pods 09/04/23 16:03:25.835
    Sep  4 16:03:25.916: INFO: Deleting ReplicationController proxy-service-dppnl took: 16.293384ms
    Sep  4 16:03:26.017: INFO: Terminating ReplicationController proxy-service-dppnl pods took: 101.147143ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:03:28.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-1146" for this suite. 09/04/23 16:03:28.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:03:28.763
Sep  4 16:03:28.763: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 16:03:28.764
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:03:28.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:03:28.833
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 16:03:28.894
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 16:03:29.101
STEP: Deploying the webhook pod 09/04/23 16:03:29.119
STEP: Wait for the deployment to be ready 09/04/23 16:03:29.151
Sep  4 16:03:29.194: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 16, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 3, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 16, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 3, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 16:03:31.21
STEP: Verifying the service has paired with the endpoint 09/04/23 16:03:31.23
Sep  4 16:03:32.231: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/04/23 16:03:32.247
STEP: create a pod that should be updated by the webhook 09/04/23 16:03:32.384
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:03:32.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4964" for this suite. 09/04/23 16:03:32.674
STEP: Destroying namespace "webhook-4964-markers" for this suite. 09/04/23 16:03:32.69
------------------------------
• [3.943 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:03:28.763
    Sep  4 16:03:28.763: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 16:03:28.764
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:03:28.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:03:28.833
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 16:03:28.894
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 16:03:29.101
    STEP: Deploying the webhook pod 09/04/23 16:03:29.119
    STEP: Wait for the deployment to be ready 09/04/23 16:03:29.151
    Sep  4 16:03:29.194: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 16, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 3, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 16, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 3, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 16:03:31.21
    STEP: Verifying the service has paired with the endpoint 09/04/23 16:03:31.23
    Sep  4 16:03:32.231: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/04/23 16:03:32.247
    STEP: create a pod that should be updated by the webhook 09/04/23 16:03:32.384
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:03:32.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4964" for this suite. 09/04/23 16:03:32.674
    STEP: Destroying namespace "webhook-4964-markers" for this suite. 09/04/23 16:03:32.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:03:32.707
Sep  4 16:03:32.707: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 09/04/23 16:03:32.708
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:03:32.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:03:32.78
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 09/04/23 16:03:32.807
STEP: Creating a ResourceQuota 09/04/23 16:03:37.822
STEP: Ensuring resource quota status is calculated 09/04/23 16:03:37.836
STEP: Creating a ReplicaSet 09/04/23 16:03:39.852
STEP: Ensuring resource quota status captures replicaset creation 09/04/23 16:03:39.872
STEP: Deleting a ReplicaSet 09/04/23 16:03:41.889
STEP: Ensuring resource quota status released usage 09/04/23 16:03:41.905
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 16:03:43.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1503" for this suite. 09/04/23 16:03:43.95
------------------------------
• [11.260 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:03:32.707
    Sep  4 16:03:32.707: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 09/04/23 16:03:32.708
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:03:32.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:03:32.78
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 09/04/23 16:03:32.807
    STEP: Creating a ResourceQuota 09/04/23 16:03:37.822
    STEP: Ensuring resource quota status is calculated 09/04/23 16:03:37.836
    STEP: Creating a ReplicaSet 09/04/23 16:03:39.852
    STEP: Ensuring resource quota status captures replicaset creation 09/04/23 16:03:39.872
    STEP: Deleting a ReplicaSet 09/04/23 16:03:41.889
    STEP: Ensuring resource quota status released usage 09/04/23 16:03:41.905
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:03:43.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1503" for this suite. 09/04/23 16:03:43.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:03:43.967
Sep  4 16:03:43.967: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 16:03:43.968
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:03:44.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:03:44.047
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-a4ab5254-cf0d-47cd-8d3f-581fa25b84d2 09/04/23 16:03:44.074
STEP: Creating a pod to test consume secrets 09/04/23 16:03:44.09
Sep  4 16:03:44.113: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07" in namespace "projected-5984" to be "Succeeded or Failed"
Sep  4 16:03:44.128: INFO: Pod "pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07": Phase="Pending", Reason="", readiness=false. Elapsed: 14.904462ms
Sep  4 16:03:46.144: INFO: Pod "pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031439568s
Sep  4 16:03:48.145: INFO: Pod "pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032456313s
STEP: Saw pod success 09/04/23 16:03:48.145
Sep  4 16:03:48.146: INFO: Pod "pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07" satisfied condition "Succeeded or Failed"
Sep  4 16:03:48.160: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/04/23 16:03:48.197
Sep  4 16:03:48.216: INFO: Waiting for pod pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07 to disappear
Sep  4 16:03:48.230: INFO: Pod pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  4 16:03:48.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5984" for this suite. 09/04/23 16:03:48.257
------------------------------
• [4.307 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:03:43.967
    Sep  4 16:03:43.967: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 16:03:43.968
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:03:44.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:03:44.047
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-a4ab5254-cf0d-47cd-8d3f-581fa25b84d2 09/04/23 16:03:44.074
    STEP: Creating a pod to test consume secrets 09/04/23 16:03:44.09
    Sep  4 16:03:44.113: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07" in namespace "projected-5984" to be "Succeeded or Failed"
    Sep  4 16:03:44.128: INFO: Pod "pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07": Phase="Pending", Reason="", readiness=false. Elapsed: 14.904462ms
    Sep  4 16:03:46.144: INFO: Pod "pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031439568s
    Sep  4 16:03:48.145: INFO: Pod "pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032456313s
    STEP: Saw pod success 09/04/23 16:03:48.145
    Sep  4 16:03:48.146: INFO: Pod "pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07" satisfied condition "Succeeded or Failed"
    Sep  4 16:03:48.160: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 16:03:48.197
    Sep  4 16:03:48.216: INFO: Waiting for pod pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07 to disappear
    Sep  4 16:03:48.230: INFO: Pod pod-projected-secrets-0b2e69e5-06e3-4f75-bedb-59e795d35d07 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:03:48.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5984" for this suite. 09/04/23 16:03:48.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:03:48.275
Sep  4 16:03:48.275: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 16:03:48.276
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:03:48.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:03:48.348
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 09/04/23 16:03:48.375
Sep  4 16:03:48.375: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 create -f -'
Sep  4 16:03:49.292: INFO: stderr: ""
Sep  4 16:03:49.292: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 16:03:49.292
Sep  4 16:03:49.292: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  4 16:03:49.386: INFO: stderr: ""
Sep  4 16:03:49.386: INFO: stdout: "update-demo-nautilus-g5h4k update-demo-nautilus-lqspd "
Sep  4 16:03:49.387: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-g5h4k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 16:03:49.482: INFO: stderr: ""
Sep  4 16:03:49.482: INFO: stdout: ""
Sep  4 16:03:49.482: INFO: update-demo-nautilus-g5h4k is created but not running
Sep  4 16:03:54.483: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  4 16:03:54.583: INFO: stderr: ""
Sep  4 16:03:54.583: INFO: stdout: "update-demo-nautilus-g5h4k update-demo-nautilus-lqspd "
Sep  4 16:03:54.583: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-g5h4k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 16:03:54.681: INFO: stderr: ""
Sep  4 16:03:54.681: INFO: stdout: "true"
Sep  4 16:03:54.681: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-g5h4k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  4 16:03:54.786: INFO: stderr: ""
Sep  4 16:03:54.786: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  4 16:03:54.786: INFO: validating pod update-demo-nautilus-g5h4k
Sep  4 16:03:54.923: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  4 16:03:54.923: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  4 16:03:54.923: INFO: update-demo-nautilus-g5h4k is verified up and running
Sep  4 16:03:54.923: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-lqspd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 16:03:55.034: INFO: stderr: ""
Sep  4 16:03:55.034: INFO: stdout: "true"
Sep  4 16:03:55.034: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-lqspd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  4 16:03:55.132: INFO: stderr: ""
Sep  4 16:03:55.132: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  4 16:03:55.132: INFO: validating pod update-demo-nautilus-lqspd
Sep  4 16:03:55.266: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  4 16:03:55.266: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  4 16:03:55.266: INFO: update-demo-nautilus-lqspd is verified up and running
STEP: scaling down the replication controller 09/04/23 16:03:55.266
Sep  4 16:03:55.267: INFO: scanned /root for discovery docs: <nil>
Sep  4 16:03:55.267: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Sep  4 16:03:56.398: INFO: stderr: ""
Sep  4 16:03:56.398: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 16:03:56.398
Sep  4 16:03:56.398: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  4 16:03:56.505: INFO: stderr: ""
Sep  4 16:03:56.505: INFO: stdout: "update-demo-nautilus-g5h4k update-demo-nautilus-lqspd "
STEP: Replicas for name=update-demo: expected=1 actual=2 09/04/23 16:03:56.505
Sep  4 16:04:01.506: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  4 16:04:01.606: INFO: stderr: ""
Sep  4 16:04:01.606: INFO: stdout: "update-demo-nautilus-g5h4k "
Sep  4 16:04:01.606: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-g5h4k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 16:04:01.703: INFO: stderr: ""
Sep  4 16:04:01.703: INFO: stdout: "true"
Sep  4 16:04:01.703: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-g5h4k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  4 16:04:01.794: INFO: stderr: ""
Sep  4 16:04:01.794: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  4 16:04:01.794: INFO: validating pod update-demo-nautilus-g5h4k
Sep  4 16:04:01.821: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  4 16:04:01.821: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  4 16:04:01.821: INFO: update-demo-nautilus-g5h4k is verified up and running
STEP: scaling up the replication controller 09/04/23 16:04:01.821
Sep  4 16:04:01.822: INFO: scanned /root for discovery docs: <nil>
Sep  4 16:04:01.822: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Sep  4 16:04:02.951: INFO: stderr: ""
Sep  4 16:04:02.951: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 16:04:02.951
Sep  4 16:04:02.951: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  4 16:04:03.053: INFO: stderr: ""
Sep  4 16:04:03.053: INFO: stdout: "update-demo-nautilus-g5h4k update-demo-nautilus-knkcj "
Sep  4 16:04:03.053: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-g5h4k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 16:04:03.148: INFO: stderr: ""
Sep  4 16:04:03.148: INFO: stdout: "true"
Sep  4 16:04:03.148: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-g5h4k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  4 16:04:03.245: INFO: stderr: ""
Sep  4 16:04:03.245: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  4 16:04:03.245: INFO: validating pod update-demo-nautilus-g5h4k
Sep  4 16:04:03.271: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  4 16:04:03.271: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  4 16:04:03.271: INFO: update-demo-nautilus-g5h4k is verified up and running
Sep  4 16:04:03.271: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-knkcj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 16:04:03.369: INFO: stderr: ""
Sep  4 16:04:03.369: INFO: stdout: "true"
Sep  4 16:04:03.369: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-knkcj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  4 16:04:03.468: INFO: stderr: ""
Sep  4 16:04:03.468: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  4 16:04:03.468: INFO: validating pod update-demo-nautilus-knkcj
Sep  4 16:04:03.610: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  4 16:04:03.610: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  4 16:04:03.611: INFO: update-demo-nautilus-knkcj is verified up and running
STEP: using delete to clean up resources 09/04/23 16:04:03.611
Sep  4 16:04:03.611: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 delete --grace-period=0 --force -f -'
Sep  4 16:04:03.724: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 16:04:03.724: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  4 16:04:03.724: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get rc,svc -l name=update-demo --no-headers'
Sep  4 16:04:03.827: INFO: stderr: "No resources found in kubectl-7886 namespace.\n"
Sep  4 16:04:03.827: INFO: stdout: ""
Sep  4 16:04:03.827: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  4 16:04:03.950: INFO: stderr: ""
Sep  4 16:04:03.950: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 16:04:03.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7886" for this suite. 09/04/23 16:04:03.978
------------------------------
• [15.719 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:03:48.275
    Sep  4 16:03:48.275: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 16:03:48.276
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:03:48.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:03:48.348
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 09/04/23 16:03:48.375
    Sep  4 16:03:48.375: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 create -f -'
    Sep  4 16:03:49.292: INFO: stderr: ""
    Sep  4 16:03:49.292: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 16:03:49.292
    Sep  4 16:03:49.292: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  4 16:03:49.386: INFO: stderr: ""
    Sep  4 16:03:49.386: INFO: stdout: "update-demo-nautilus-g5h4k update-demo-nautilus-lqspd "
    Sep  4 16:03:49.387: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-g5h4k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 16:03:49.482: INFO: stderr: ""
    Sep  4 16:03:49.482: INFO: stdout: ""
    Sep  4 16:03:49.482: INFO: update-demo-nautilus-g5h4k is created but not running
    Sep  4 16:03:54.483: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  4 16:03:54.583: INFO: stderr: ""
    Sep  4 16:03:54.583: INFO: stdout: "update-demo-nautilus-g5h4k update-demo-nautilus-lqspd "
    Sep  4 16:03:54.583: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-g5h4k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 16:03:54.681: INFO: stderr: ""
    Sep  4 16:03:54.681: INFO: stdout: "true"
    Sep  4 16:03:54.681: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-g5h4k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  4 16:03:54.786: INFO: stderr: ""
    Sep  4 16:03:54.786: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  4 16:03:54.786: INFO: validating pod update-demo-nautilus-g5h4k
    Sep  4 16:03:54.923: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  4 16:03:54.923: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  4 16:03:54.923: INFO: update-demo-nautilus-g5h4k is verified up and running
    Sep  4 16:03:54.923: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-lqspd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 16:03:55.034: INFO: stderr: ""
    Sep  4 16:03:55.034: INFO: stdout: "true"
    Sep  4 16:03:55.034: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-lqspd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  4 16:03:55.132: INFO: stderr: ""
    Sep  4 16:03:55.132: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  4 16:03:55.132: INFO: validating pod update-demo-nautilus-lqspd
    Sep  4 16:03:55.266: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  4 16:03:55.266: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  4 16:03:55.266: INFO: update-demo-nautilus-lqspd is verified up and running
    STEP: scaling down the replication controller 09/04/23 16:03:55.266
    Sep  4 16:03:55.267: INFO: scanned /root for discovery docs: <nil>
    Sep  4 16:03:55.267: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Sep  4 16:03:56.398: INFO: stderr: ""
    Sep  4 16:03:56.398: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 16:03:56.398
    Sep  4 16:03:56.398: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  4 16:03:56.505: INFO: stderr: ""
    Sep  4 16:03:56.505: INFO: stdout: "update-demo-nautilus-g5h4k update-demo-nautilus-lqspd "
    STEP: Replicas for name=update-demo: expected=1 actual=2 09/04/23 16:03:56.505
    Sep  4 16:04:01.506: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  4 16:04:01.606: INFO: stderr: ""
    Sep  4 16:04:01.606: INFO: stdout: "update-demo-nautilus-g5h4k "
    Sep  4 16:04:01.606: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-g5h4k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 16:04:01.703: INFO: stderr: ""
    Sep  4 16:04:01.703: INFO: stdout: "true"
    Sep  4 16:04:01.703: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-g5h4k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  4 16:04:01.794: INFO: stderr: ""
    Sep  4 16:04:01.794: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  4 16:04:01.794: INFO: validating pod update-demo-nautilus-g5h4k
    Sep  4 16:04:01.821: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  4 16:04:01.821: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  4 16:04:01.821: INFO: update-demo-nautilus-g5h4k is verified up and running
    STEP: scaling up the replication controller 09/04/23 16:04:01.821
    Sep  4 16:04:01.822: INFO: scanned /root for discovery docs: <nil>
    Sep  4 16:04:01.822: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Sep  4 16:04:02.951: INFO: stderr: ""
    Sep  4 16:04:02.951: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 16:04:02.951
    Sep  4 16:04:02.951: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  4 16:04:03.053: INFO: stderr: ""
    Sep  4 16:04:03.053: INFO: stdout: "update-demo-nautilus-g5h4k update-demo-nautilus-knkcj "
    Sep  4 16:04:03.053: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-g5h4k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 16:04:03.148: INFO: stderr: ""
    Sep  4 16:04:03.148: INFO: stdout: "true"
    Sep  4 16:04:03.148: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-g5h4k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  4 16:04:03.245: INFO: stderr: ""
    Sep  4 16:04:03.245: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  4 16:04:03.245: INFO: validating pod update-demo-nautilus-g5h4k
    Sep  4 16:04:03.271: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  4 16:04:03.271: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  4 16:04:03.271: INFO: update-demo-nautilus-g5h4k is verified up and running
    Sep  4 16:04:03.271: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-knkcj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 16:04:03.369: INFO: stderr: ""
    Sep  4 16:04:03.369: INFO: stdout: "true"
    Sep  4 16:04:03.369: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods update-demo-nautilus-knkcj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  4 16:04:03.468: INFO: stderr: ""
    Sep  4 16:04:03.468: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  4 16:04:03.468: INFO: validating pod update-demo-nautilus-knkcj
    Sep  4 16:04:03.610: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  4 16:04:03.610: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  4 16:04:03.611: INFO: update-demo-nautilus-knkcj is verified up and running
    STEP: using delete to clean up resources 09/04/23 16:04:03.611
    Sep  4 16:04:03.611: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 delete --grace-period=0 --force -f -'
    Sep  4 16:04:03.724: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 16:04:03.724: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep  4 16:04:03.724: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get rc,svc -l name=update-demo --no-headers'
    Sep  4 16:04:03.827: INFO: stderr: "No resources found in kubectl-7886 namespace.\n"
    Sep  4 16:04:03.827: INFO: stdout: ""
    Sep  4 16:04:03.827: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7886 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  4 16:04:03.950: INFO: stderr: ""
    Sep  4 16:04:03.950: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:04:03.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7886" for this suite. 09/04/23 16:04:03.978
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:04:03.994
Sep  4 16:04:03.994: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 09/04/23 16:04:03.995
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:04.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:04.069
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 09/04/23 16:04:04.111
STEP: waiting for RC to be added 09/04/23 16:04:04.127
STEP: waiting for available Replicas 09/04/23 16:04:04.127
STEP: patching ReplicationController 09/04/23 16:04:05.757
STEP: waiting for RC to be modified 09/04/23 16:04:05.776
STEP: patching ReplicationController status 09/04/23 16:04:05.776
STEP: waiting for RC to be modified 09/04/23 16:04:05.792
STEP: waiting for available Replicas 09/04/23 16:04:05.793
STEP: fetching ReplicationController status 09/04/23 16:04:05.797
STEP: patching ReplicationController scale 09/04/23 16:04:05.812
STEP: waiting for RC to be modified 09/04/23 16:04:05.828
STEP: waiting for ReplicationController's scale to be the max amount 09/04/23 16:04:05.828
STEP: fetching ReplicationController; ensuring that it's patched 09/04/23 16:04:07.042
STEP: updating ReplicationController status 09/04/23 16:04:07.057
STEP: waiting for RC to be modified 09/04/23 16:04:07.073
STEP: listing all ReplicationControllers 09/04/23 16:04:07.073
STEP: checking that ReplicationController has expected values 09/04/23 16:04:07.088
STEP: deleting ReplicationControllers by collection 09/04/23 16:04:07.088
STEP: waiting for ReplicationController to have a DELETED watchEvent 09/04/23 16:04:07.105
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  4 16:04:07.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9701" for this suite. 09/04/23 16:04:07.182
------------------------------
• [3.203 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:04:03.994
    Sep  4 16:04:03.994: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 09/04/23 16:04:03.995
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:04.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:04.069
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 09/04/23 16:04:04.111
    STEP: waiting for RC to be added 09/04/23 16:04:04.127
    STEP: waiting for available Replicas 09/04/23 16:04:04.127
    STEP: patching ReplicationController 09/04/23 16:04:05.757
    STEP: waiting for RC to be modified 09/04/23 16:04:05.776
    STEP: patching ReplicationController status 09/04/23 16:04:05.776
    STEP: waiting for RC to be modified 09/04/23 16:04:05.792
    STEP: waiting for available Replicas 09/04/23 16:04:05.793
    STEP: fetching ReplicationController status 09/04/23 16:04:05.797
    STEP: patching ReplicationController scale 09/04/23 16:04:05.812
    STEP: waiting for RC to be modified 09/04/23 16:04:05.828
    STEP: waiting for ReplicationController's scale to be the max amount 09/04/23 16:04:05.828
    STEP: fetching ReplicationController; ensuring that it's patched 09/04/23 16:04:07.042
    STEP: updating ReplicationController status 09/04/23 16:04:07.057
    STEP: waiting for RC to be modified 09/04/23 16:04:07.073
    STEP: listing all ReplicationControllers 09/04/23 16:04:07.073
    STEP: checking that ReplicationController has expected values 09/04/23 16:04:07.088
    STEP: deleting ReplicationControllers by collection 09/04/23 16:04:07.088
    STEP: waiting for ReplicationController to have a DELETED watchEvent 09/04/23 16:04:07.105
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:04:07.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9701" for this suite. 09/04/23 16:04:07.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:04:07.197
Sep  4 16:04:07.197: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 16:04:07.198
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:07.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:07.268
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-1752 09/04/23 16:04:07.295
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1752 to expose endpoints map[] 09/04/23 16:04:07.315
Sep  4 16:04:07.358: INFO: successfully validated that service multi-endpoint-test in namespace services-1752 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1752 09/04/23 16:04:07.358
Sep  4 16:04:07.380: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1752" to be "running and ready"
Sep  4 16:04:07.394: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.502016ms
Sep  4 16:04:07.394: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 16:04:09.409: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.029723389s
Sep  4 16:04:09.409: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  4 16:04:09.409: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1752 to expose endpoints map[pod1:[100]] 09/04/23 16:04:09.424
Sep  4 16:04:09.480: INFO: successfully validated that service multi-endpoint-test in namespace services-1752 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1752 09/04/23 16:04:09.481
Sep  4 16:04:09.500: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1752" to be "running and ready"
Sep  4 16:04:09.514: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009622ms
Sep  4 16:04:09.514: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 16:04:11.530: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.029762841s
Sep  4 16:04:11.530: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  4 16:04:11.530: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1752 to expose endpoints map[pod1:[100] pod2:[101]] 09/04/23 16:04:11.544
Sep  4 16:04:11.616: INFO: successfully validated that service multi-endpoint-test in namespace services-1752 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 09/04/23 16:04:11.616
Sep  4 16:04:11.616: INFO: Creating new exec pod
Sep  4 16:04:11.635: INFO: Waiting up to 5m0s for pod "execpodtnlgr" in namespace "services-1752" to be "running"
Sep  4 16:04:11.650: INFO: Pod "execpodtnlgr": Phase="Pending", Reason="", readiness=false. Elapsed: 14.748813ms
Sep  4 16:04:13.667: INFO: Pod "execpodtnlgr": Phase="Running", Reason="", readiness=true. Elapsed: 2.031586865s
Sep  4 16:04:13.667: INFO: Pod "execpodtnlgr" satisfied condition "running"
Sep  4 16:04:14.667: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1752 exec execpodtnlgr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Sep  4 16:04:15.107: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Sep  4 16:04:15.107: INFO: stdout: ""
Sep  4 16:04:15.107: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1752 exec execpodtnlgr -- /bin/sh -x -c nc -v -z -w 2 100.108.158.226 80'
Sep  4 16:04:15.668: INFO: stderr: "+ nc -v -z -w 2 100.108.158.226 80\nConnection to 100.108.158.226 80 port [tcp/http] succeeded!\n"
Sep  4 16:04:15.668: INFO: stdout: ""
Sep  4 16:04:15.668: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1752 exec execpodtnlgr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Sep  4 16:04:16.201: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Sep  4 16:04:16.201: INFO: stdout: ""
Sep  4 16:04:16.201: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1752 exec execpodtnlgr -- /bin/sh -x -c nc -v -z -w 2 100.108.158.226 81'
Sep  4 16:04:16.679: INFO: stderr: "+ nc -v -z -w 2 100.108.158.226 81\nConnection to 100.108.158.226 81 port [tcp/*] succeeded!\n"
Sep  4 16:04:16.679: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-1752 09/04/23 16:04:16.679
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1752 to expose endpoints map[pod2:[101]] 09/04/23 16:04:16.699
Sep  4 16:04:16.755: INFO: successfully validated that service multi-endpoint-test in namespace services-1752 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1752 09/04/23 16:04:16.755
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1752 to expose endpoints map[] 09/04/23 16:04:16.774
Sep  4 16:04:16.816: INFO: successfully validated that service multi-endpoint-test in namespace services-1752 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 16:04:16.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1752" for this suite. 09/04/23 16:04:16.863
------------------------------
• [9.681 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:04:07.197
    Sep  4 16:04:07.197: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 16:04:07.198
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:07.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:07.268
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-1752 09/04/23 16:04:07.295
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1752 to expose endpoints map[] 09/04/23 16:04:07.315
    Sep  4 16:04:07.358: INFO: successfully validated that service multi-endpoint-test in namespace services-1752 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1752 09/04/23 16:04:07.358
    Sep  4 16:04:07.380: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1752" to be "running and ready"
    Sep  4 16:04:07.394: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.502016ms
    Sep  4 16:04:07.394: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 16:04:09.409: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.029723389s
    Sep  4 16:04:09.409: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  4 16:04:09.409: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1752 to expose endpoints map[pod1:[100]] 09/04/23 16:04:09.424
    Sep  4 16:04:09.480: INFO: successfully validated that service multi-endpoint-test in namespace services-1752 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-1752 09/04/23 16:04:09.481
    Sep  4 16:04:09.500: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1752" to be "running and ready"
    Sep  4 16:04:09.514: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009622ms
    Sep  4 16:04:09.514: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 16:04:11.530: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.029762841s
    Sep  4 16:04:11.530: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  4 16:04:11.530: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1752 to expose endpoints map[pod1:[100] pod2:[101]] 09/04/23 16:04:11.544
    Sep  4 16:04:11.616: INFO: successfully validated that service multi-endpoint-test in namespace services-1752 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 09/04/23 16:04:11.616
    Sep  4 16:04:11.616: INFO: Creating new exec pod
    Sep  4 16:04:11.635: INFO: Waiting up to 5m0s for pod "execpodtnlgr" in namespace "services-1752" to be "running"
    Sep  4 16:04:11.650: INFO: Pod "execpodtnlgr": Phase="Pending", Reason="", readiness=false. Elapsed: 14.748813ms
    Sep  4 16:04:13.667: INFO: Pod "execpodtnlgr": Phase="Running", Reason="", readiness=true. Elapsed: 2.031586865s
    Sep  4 16:04:13.667: INFO: Pod "execpodtnlgr" satisfied condition "running"
    Sep  4 16:04:14.667: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1752 exec execpodtnlgr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Sep  4 16:04:15.107: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Sep  4 16:04:15.107: INFO: stdout: ""
    Sep  4 16:04:15.107: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1752 exec execpodtnlgr -- /bin/sh -x -c nc -v -z -w 2 100.108.158.226 80'
    Sep  4 16:04:15.668: INFO: stderr: "+ nc -v -z -w 2 100.108.158.226 80\nConnection to 100.108.158.226 80 port [tcp/http] succeeded!\n"
    Sep  4 16:04:15.668: INFO: stdout: ""
    Sep  4 16:04:15.668: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1752 exec execpodtnlgr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Sep  4 16:04:16.201: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Sep  4 16:04:16.201: INFO: stdout: ""
    Sep  4 16:04:16.201: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1752 exec execpodtnlgr -- /bin/sh -x -c nc -v -z -w 2 100.108.158.226 81'
    Sep  4 16:04:16.679: INFO: stderr: "+ nc -v -z -w 2 100.108.158.226 81\nConnection to 100.108.158.226 81 port [tcp/*] succeeded!\n"
    Sep  4 16:04:16.679: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-1752 09/04/23 16:04:16.679
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1752 to expose endpoints map[pod2:[101]] 09/04/23 16:04:16.699
    Sep  4 16:04:16.755: INFO: successfully validated that service multi-endpoint-test in namespace services-1752 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-1752 09/04/23 16:04:16.755
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1752 to expose endpoints map[] 09/04/23 16:04:16.774
    Sep  4 16:04:16.816: INFO: successfully validated that service multi-endpoint-test in namespace services-1752 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:04:16.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1752" for this suite. 09/04/23 16:04:16.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:04:16.879
Sep  4 16:04:16.879: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 09/04/23 16:04:16.88
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:16.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:16.981
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 09/04/23 16:04:17.018
STEP: Ensuring job reaches completions 09/04/23 16:04:17.034
STEP: Ensuring pods with index for job exist 09/04/23 16:04:27.05
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  4 16:04:27.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8919" for this suite. 09/04/23 16:04:27.105
------------------------------
• [10.242 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:04:16.879
    Sep  4 16:04:16.879: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 09/04/23 16:04:16.88
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:16.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:16.981
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 09/04/23 16:04:17.018
    STEP: Ensuring job reaches completions 09/04/23 16:04:17.034
    STEP: Ensuring pods with index for job exist 09/04/23 16:04:27.05
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:04:27.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8919" for this suite. 09/04/23 16:04:27.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:04:27.122
Sep  4 16:04:27.122: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 09/04/23 16:04:27.123
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:27.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:27.193
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 09/04/23 16:04:27.219
STEP: wait for the container to reach Failed 09/04/23 16:04:27.24
STEP: get the container status 09/04/23 16:04:31.317
STEP: the container should be terminated 09/04/23 16:04:31.331
STEP: the termination message should be set 09/04/23 16:04:31.331
Sep  4 16:04:31.332: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/04/23 16:04:31.332
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  4 16:04:31.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-665" for this suite. 09/04/23 16:04:31.39
------------------------------
• [4.286 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:04:27.122
    Sep  4 16:04:27.122: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 09/04/23 16:04:27.123
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:27.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:27.193
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 09/04/23 16:04:27.219
    STEP: wait for the container to reach Failed 09/04/23 16:04:27.24
    STEP: get the container status 09/04/23 16:04:31.317
    STEP: the container should be terminated 09/04/23 16:04:31.331
    STEP: the termination message should be set 09/04/23 16:04:31.331
    Sep  4 16:04:31.332: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/04/23 16:04:31.332
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:04:31.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-665" for this suite. 09/04/23 16:04:31.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:04:31.408
Sep  4 16:04:31.408: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 09/04/23 16:04:31.409
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:31.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:31.48
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Sep  4 16:04:31.556: INFO: created pod pod-service-account-defaultsa
Sep  4 16:04:31.556: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  4 16:04:31.573: INFO: created pod pod-service-account-mountsa
Sep  4 16:04:31.573: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  4 16:04:31.590: INFO: created pod pod-service-account-nomountsa
Sep  4 16:04:31.590: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  4 16:04:31.607: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  4 16:04:31.607: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  4 16:04:31.625: INFO: created pod pod-service-account-mountsa-mountspec
Sep  4 16:04:31.625: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  4 16:04:31.642: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  4 16:04:31.642: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  4 16:04:31.661: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  4 16:04:31.661: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  4 16:04:31.678: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  4 16:04:31.678: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  4 16:04:31.695: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  4 16:04:31.695: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  4 16:04:31.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8154" for this suite. 09/04/23 16:04:31.723
------------------------------
• [0.329 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:04:31.408
    Sep  4 16:04:31.408: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 09/04/23 16:04:31.409
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:31.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:31.48
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Sep  4 16:04:31.556: INFO: created pod pod-service-account-defaultsa
    Sep  4 16:04:31.556: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Sep  4 16:04:31.573: INFO: created pod pod-service-account-mountsa
    Sep  4 16:04:31.573: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Sep  4 16:04:31.590: INFO: created pod pod-service-account-nomountsa
    Sep  4 16:04:31.590: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Sep  4 16:04:31.607: INFO: created pod pod-service-account-defaultsa-mountspec
    Sep  4 16:04:31.607: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Sep  4 16:04:31.625: INFO: created pod pod-service-account-mountsa-mountspec
    Sep  4 16:04:31.625: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Sep  4 16:04:31.642: INFO: created pod pod-service-account-nomountsa-mountspec
    Sep  4 16:04:31.642: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Sep  4 16:04:31.661: INFO: created pod pod-service-account-defaultsa-nomountspec
    Sep  4 16:04:31.661: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Sep  4 16:04:31.678: INFO: created pod pod-service-account-mountsa-nomountspec
    Sep  4 16:04:31.678: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Sep  4 16:04:31.695: INFO: created pod pod-service-account-nomountsa-nomountspec
    Sep  4 16:04:31.695: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:04:31.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8154" for this suite. 09/04/23 16:04:31.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:04:31.738
Sep  4 16:04:31.738: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 16:04:31.739
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:31.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:31.809
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/04/23 16:04:31.836
Sep  4 16:04:31.855: INFO: Waiting up to 5m0s for pod "pod-96bfdadb-56fc-4b48-b771-65aa561212b2" in namespace "emptydir-3785" to be "Succeeded or Failed"
Sep  4 16:04:31.869: INFO: Pod "pod-96bfdadb-56fc-4b48-b771-65aa561212b2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.721764ms
Sep  4 16:04:33.885: INFO: Pod "pod-96bfdadb-56fc-4b48-b771-65aa561212b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029941346s
Sep  4 16:04:35.885: INFO: Pod "pod-96bfdadb-56fc-4b48-b771-65aa561212b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029852365s
STEP: Saw pod success 09/04/23 16:04:35.885
Sep  4 16:04:35.885: INFO: Pod "pod-96bfdadb-56fc-4b48-b771-65aa561212b2" satisfied condition "Succeeded or Failed"
Sep  4 16:04:35.900: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-96bfdadb-56fc-4b48-b771-65aa561212b2 container test-container: <nil>
STEP: delete the pod 09/04/23 16:04:35.937
Sep  4 16:04:35.954: INFO: Waiting for pod pod-96bfdadb-56fc-4b48-b771-65aa561212b2 to disappear
Sep  4 16:04:35.967: INFO: Pod pod-96bfdadb-56fc-4b48-b771-65aa561212b2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 16:04:35.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3785" for this suite. 09/04/23 16:04:35.995
------------------------------
• [4.272 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:04:31.738
    Sep  4 16:04:31.738: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 16:04:31.739
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:31.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:31.809
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/04/23 16:04:31.836
    Sep  4 16:04:31.855: INFO: Waiting up to 5m0s for pod "pod-96bfdadb-56fc-4b48-b771-65aa561212b2" in namespace "emptydir-3785" to be "Succeeded or Failed"
    Sep  4 16:04:31.869: INFO: Pod "pod-96bfdadb-56fc-4b48-b771-65aa561212b2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.721764ms
    Sep  4 16:04:33.885: INFO: Pod "pod-96bfdadb-56fc-4b48-b771-65aa561212b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029941346s
    Sep  4 16:04:35.885: INFO: Pod "pod-96bfdadb-56fc-4b48-b771-65aa561212b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029852365s
    STEP: Saw pod success 09/04/23 16:04:35.885
    Sep  4 16:04:35.885: INFO: Pod "pod-96bfdadb-56fc-4b48-b771-65aa561212b2" satisfied condition "Succeeded or Failed"
    Sep  4 16:04:35.900: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-96bfdadb-56fc-4b48-b771-65aa561212b2 container test-container: <nil>
    STEP: delete the pod 09/04/23 16:04:35.937
    Sep  4 16:04:35.954: INFO: Waiting for pod pod-96bfdadb-56fc-4b48-b771-65aa561212b2 to disappear
    Sep  4 16:04:35.967: INFO: Pod pod-96bfdadb-56fc-4b48-b771-65aa561212b2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:04:35.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3785" for this suite. 09/04/23 16:04:35.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:04:36.011
Sep  4 16:04:36.011: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container 09/04/23 16:04:36.012
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:36.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:36.081
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 09/04/23 16:04:36.108
Sep  4 16:04:36.108: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:04:39.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5473" for this suite. 09/04/23 16:04:39.898
------------------------------
• [3.903 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:04:36.011
    Sep  4 16:04:36.011: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename init-container 09/04/23 16:04:36.012
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:36.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:36.081
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 09/04/23 16:04:36.108
    Sep  4 16:04:36.108: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:04:39.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5473" for this suite. 09/04/23 16:04:39.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:04:39.915
Sep  4 16:04:39.916: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test 09/04/23 16:04:39.916
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:39.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:39.986
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-3133 09/04/23 16:04:40.013
STEP: creating a selector 09/04/23 16:04:40.013
STEP: Creating the service pods in kubernetes 09/04/23 16:04:40.013
Sep  4 16:04:40.013: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  4 16:04:40.083: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3133" to be "running and ready"
Sep  4 16:04:40.098: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.149901ms
Sep  4 16:04:40.098: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 16:04:42.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.030182119s
Sep  4 16:04:42.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 16:04:44.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.030658879s
Sep  4 16:04:44.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 16:04:46.115: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.031299321s
Sep  4 16:04:46.115: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 16:04:48.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.030297507s
Sep  4 16:04:48.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 16:04:50.113: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.02945229s
Sep  4 16:04:50.113: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  4 16:04:52.113: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.029434411s
Sep  4 16:04:52.113: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  4 16:04:52.113: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  4 16:04:52.127: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3133" to be "running and ready"
Sep  4 16:04:52.141: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.137143ms
Sep  4 16:04:52.141: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  4 16:04:52.141: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/04/23 16:04:52.156
Sep  4 16:04:52.192: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3133" to be "running"
Sep  4 16:04:52.206: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.226962ms
Sep  4 16:04:54.222: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029849068s
Sep  4 16:04:54.222: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  4 16:04:54.236: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3133" to be "running"
Sep  4 16:04:54.250: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 13.886668ms
Sep  4 16:04:54.250: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep  4 16:04:54.265: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  4 16:04:54.265: INFO: Going to poll 100.64.0.217 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Sep  4 16:04:54.279: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.0.217:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3133 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 16:04:54.279: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 16:04:54.280: INFO: ExecWithOptions: Clientset creation
Sep  4 16:04:54.280: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-3133/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.64.0.217%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  4 16:04:54.749: INFO: Found all 1 expected endpoints: [netserver-0]
Sep  4 16:04:54.749: INFO: Going to poll 100.64.1.178 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Sep  4 16:04:54.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.1.178:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3133 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  4 16:04:54.764: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 16:04:54.764: INFO: ExecWithOptions: Clientset creation
Sep  4 16:04:54.764: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-3133/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.64.1.178%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  4 16:04:55.314: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  4 16:04:55.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3133" for this suite. 09/04/23 16:04:55.342
------------------------------
• [15.443 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:04:39.915
    Sep  4 16:04:39.916: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pod-network-test 09/04/23 16:04:39.916
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:39.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:39.986
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-3133 09/04/23 16:04:40.013
    STEP: creating a selector 09/04/23 16:04:40.013
    STEP: Creating the service pods in kubernetes 09/04/23 16:04:40.013
    Sep  4 16:04:40.013: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  4 16:04:40.083: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3133" to be "running and ready"
    Sep  4 16:04:40.098: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.149901ms
    Sep  4 16:04:40.098: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 16:04:42.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.030182119s
    Sep  4 16:04:42.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 16:04:44.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.030658879s
    Sep  4 16:04:44.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 16:04:46.115: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.031299321s
    Sep  4 16:04:46.115: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 16:04:48.114: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.030297507s
    Sep  4 16:04:48.114: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 16:04:50.113: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.02945229s
    Sep  4 16:04:50.113: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  4 16:04:52.113: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.029434411s
    Sep  4 16:04:52.113: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  4 16:04:52.113: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  4 16:04:52.127: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3133" to be "running and ready"
    Sep  4 16:04:52.141: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.137143ms
    Sep  4 16:04:52.141: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  4 16:04:52.141: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/04/23 16:04:52.156
    Sep  4 16:04:52.192: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3133" to be "running"
    Sep  4 16:04:52.206: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.226962ms
    Sep  4 16:04:54.222: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029849068s
    Sep  4 16:04:54.222: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  4 16:04:54.236: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3133" to be "running"
    Sep  4 16:04:54.250: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 13.886668ms
    Sep  4 16:04:54.250: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep  4 16:04:54.265: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  4 16:04:54.265: INFO: Going to poll 100.64.0.217 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Sep  4 16:04:54.279: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.0.217:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3133 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 16:04:54.279: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 16:04:54.280: INFO: ExecWithOptions: Clientset creation
    Sep  4 16:04:54.280: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-3133/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.64.0.217%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  4 16:04:54.749: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep  4 16:04:54.749: INFO: Going to poll 100.64.1.178 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Sep  4 16:04:54.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.1.178:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3133 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  4 16:04:54.764: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 16:04:54.764: INFO: ExecWithOptions: Clientset creation
    Sep  4 16:04:54.764: INFO: ExecWithOptions: execute(POST https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-3133/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.64.1.178%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  4 16:04:55.314: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:04:55.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3133" for this suite. 09/04/23 16:04:55.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:04:55.359
Sep  4 16:04:55.359: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 09/04/23 16:04:55.36
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:55.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:55.432
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a in namespace container-probe-6433 09/04/23 16:04:55.46
Sep  4 16:04:55.480: INFO: Waiting up to 5m0s for pod "liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a" in namespace "container-probe-6433" to be "not pending"
Sep  4 16:04:55.494: INFO: Pod "liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.975898ms
Sep  4 16:04:57.511: INFO: Pod "liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a": Phase="Running", Reason="", readiness=true. Elapsed: 2.031044091s
Sep  4 16:04:57.511: INFO: Pod "liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a" satisfied condition "not pending"
Sep  4 16:04:57.511: INFO: Started pod liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a in namespace container-probe-6433
STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 16:04:57.511
Sep  4 16:04:57.525: INFO: Initial restart count of pod liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a is 0
Sep  4 16:05:17.700: INFO: Restart count of pod container-probe-6433/liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a is now 1 (20.174702198s elapsed)
STEP: deleting the pod 09/04/23 16:05:17.7
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 16:05:17.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6433" for this suite. 09/04/23 16:05:17.754
------------------------------
• [22.411 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:04:55.359
    Sep  4 16:04:55.359: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 09/04/23 16:04:55.36
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:04:55.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:04:55.432
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a in namespace container-probe-6433 09/04/23 16:04:55.46
    Sep  4 16:04:55.480: INFO: Waiting up to 5m0s for pod "liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a" in namespace "container-probe-6433" to be "not pending"
    Sep  4 16:04:55.494: INFO: Pod "liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.975898ms
    Sep  4 16:04:57.511: INFO: Pod "liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a": Phase="Running", Reason="", readiness=true. Elapsed: 2.031044091s
    Sep  4 16:04:57.511: INFO: Pod "liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a" satisfied condition "not pending"
    Sep  4 16:04:57.511: INFO: Started pod liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a in namespace container-probe-6433
    STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 16:04:57.511
    Sep  4 16:04:57.525: INFO: Initial restart count of pod liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a is 0
    Sep  4 16:05:17.700: INFO: Restart count of pod container-probe-6433/liveness-f3325d1e-871d-406c-9a0f-ad73fa7ec29a is now 1 (20.174702198s elapsed)
    STEP: deleting the pod 09/04/23 16:05:17.7
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:05:17.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6433" for this suite. 09/04/23 16:05:17.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:05:17.771
Sep  4 16:05:17.771: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 16:05:17.772
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:17.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:17.844
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1373 09/04/23 16:05:17.871
STEP: changing the ExternalName service to type=NodePort 09/04/23 16:05:17.886
STEP: creating replication controller externalname-service in namespace services-1373 09/04/23 16:05:17.92
I0904 16:05:17.937755    7754 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1373, replica count: 2
I0904 16:05:20.989347    7754 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 16:05:20.989: INFO: Creating new exec pod
Sep  4 16:05:21.010: INFO: Waiting up to 5m0s for pod "execpodtdr24" in namespace "services-1373" to be "running"
Sep  4 16:05:21.025: INFO: Pod "execpodtdr24": Phase="Pending", Reason="", readiness=false. Elapsed: 14.290501ms
Sep  4 16:05:23.041: INFO: Pod "execpodtdr24": Phase="Running", Reason="", readiness=true. Elapsed: 2.030911015s
Sep  4 16:05:23.041: INFO: Pod "execpodtdr24" satisfied condition "running"
Sep  4 16:05:24.069: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1373 exec execpodtdr24 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Sep  4 16:05:24.737: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  4 16:05:24.737: INFO: stdout: ""
Sep  4 16:05:24.737: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1373 exec execpodtdr24 -- /bin/sh -x -c nc -v -z -w 2 100.108.150.183 80'
Sep  4 16:05:25.108: INFO: stderr: "+ nc -v -z -w 2 100.108.150.183 80\nConnection to 100.108.150.183 80 port [tcp/http] succeeded!\n"
Sep  4 16:05:25.108: INFO: stdout: ""
Sep  4 16:05:25.108: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1373 exec execpodtdr24 -- /bin/sh -x -c nc -v -z -w 2 10.250.1.105 31258'
Sep  4 16:05:25.694: INFO: stderr: "+ nc -v -z -w 2 10.250.1.105 31258\nConnection to 10.250.1.105 31258 port [tcp/*] succeeded!\n"
Sep  4 16:05:25.694: INFO: stdout: ""
Sep  4 16:05:25.694: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1373 exec execpodtdr24 -- /bin/sh -x -c nc -v -z -w 2 10.250.1.231 31258'
Sep  4 16:05:26.147: INFO: stderr: "+ nc -v -z -w 2 10.250.1.231 31258\nConnection to 10.250.1.231 31258 port [tcp/*] succeeded!\n"
Sep  4 16:05:26.147: INFO: stdout: ""
Sep  4 16:05:26.147: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 16:05:26.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1373" for this suite. 09/04/23 16:05:26.198
------------------------------
• [8.443 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:05:17.771
    Sep  4 16:05:17.771: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 16:05:17.772
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:17.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:17.844
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-1373 09/04/23 16:05:17.871
    STEP: changing the ExternalName service to type=NodePort 09/04/23 16:05:17.886
    STEP: creating replication controller externalname-service in namespace services-1373 09/04/23 16:05:17.92
    I0904 16:05:17.937755    7754 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1373, replica count: 2
    I0904 16:05:20.989347    7754 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 16:05:20.989: INFO: Creating new exec pod
    Sep  4 16:05:21.010: INFO: Waiting up to 5m0s for pod "execpodtdr24" in namespace "services-1373" to be "running"
    Sep  4 16:05:21.025: INFO: Pod "execpodtdr24": Phase="Pending", Reason="", readiness=false. Elapsed: 14.290501ms
    Sep  4 16:05:23.041: INFO: Pod "execpodtdr24": Phase="Running", Reason="", readiness=true. Elapsed: 2.030911015s
    Sep  4 16:05:23.041: INFO: Pod "execpodtdr24" satisfied condition "running"
    Sep  4 16:05:24.069: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1373 exec execpodtdr24 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Sep  4 16:05:24.737: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  4 16:05:24.737: INFO: stdout: ""
    Sep  4 16:05:24.737: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1373 exec execpodtdr24 -- /bin/sh -x -c nc -v -z -w 2 100.108.150.183 80'
    Sep  4 16:05:25.108: INFO: stderr: "+ nc -v -z -w 2 100.108.150.183 80\nConnection to 100.108.150.183 80 port [tcp/http] succeeded!\n"
    Sep  4 16:05:25.108: INFO: stdout: ""
    Sep  4 16:05:25.108: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1373 exec execpodtdr24 -- /bin/sh -x -c nc -v -z -w 2 10.250.1.105 31258'
    Sep  4 16:05:25.694: INFO: stderr: "+ nc -v -z -w 2 10.250.1.105 31258\nConnection to 10.250.1.105 31258 port [tcp/*] succeeded!\n"
    Sep  4 16:05:25.694: INFO: stdout: ""
    Sep  4 16:05:25.694: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1373 exec execpodtdr24 -- /bin/sh -x -c nc -v -z -w 2 10.250.1.231 31258'
    Sep  4 16:05:26.147: INFO: stderr: "+ nc -v -z -w 2 10.250.1.231 31258\nConnection to 10.250.1.231 31258 port [tcp/*] succeeded!\n"
    Sep  4 16:05:26.147: INFO: stdout: ""
    Sep  4 16:05:26.147: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:05:26.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1373" for this suite. 09/04/23 16:05:26.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:05:26.215
Sep  4 16:05:26.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 16:05:26.215
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:26.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:26.287
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 16:05:26.347
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 16:05:27.087
STEP: Deploying the webhook pod 09/04/23 16:05:27.105
STEP: Wait for the deployment to be ready 09/04/23 16:05:27.138
Sep  4 16:05:27.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 16, 5, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 5, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 16, 5, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 5, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 16:05:29.199
STEP: Verifying the service has paired with the endpoint 09/04/23 16:05:29.219
Sep  4 16:05:30.219: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Sep  4 16:05:30.234: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9568-crds.webhook.example.com via the AdmissionRegistration API 09/04/23 16:05:30.279
STEP: Creating a custom resource while v1 is storage version 09/04/23 16:05:30.425
STEP: Patching Custom Resource Definition to set v2 as storage 09/04/23 16:05:32.648
STEP: Patching the custom resource while v2 is storage version 09/04/23 16:05:32.673
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:05:32.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8881" for this suite. 09/04/23 16:05:32.948
STEP: Destroying namespace "webhook-8881-markers" for this suite. 09/04/23 16:05:32.964
------------------------------
• [6.765 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:05:26.215
    Sep  4 16:05:26.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 16:05:26.215
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:26.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:26.287
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 16:05:26.347
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 16:05:27.087
    STEP: Deploying the webhook pod 09/04/23 16:05:27.105
    STEP: Wait for the deployment to be ready 09/04/23 16:05:27.138
    Sep  4 16:05:27.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 16, 5, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 5, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 16, 5, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 5, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 16:05:29.199
    STEP: Verifying the service has paired with the endpoint 09/04/23 16:05:29.219
    Sep  4 16:05:30.219: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Sep  4 16:05:30.234: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9568-crds.webhook.example.com via the AdmissionRegistration API 09/04/23 16:05:30.279
    STEP: Creating a custom resource while v1 is storage version 09/04/23 16:05:30.425
    STEP: Patching Custom Resource Definition to set v2 as storage 09/04/23 16:05:32.648
    STEP: Patching the custom resource while v2 is storage version 09/04/23 16:05:32.673
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:05:32.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8881" for this suite. 09/04/23 16:05:32.948
    STEP: Destroying namespace "webhook-8881-markers" for this suite. 09/04/23 16:05:32.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:05:32.981
Sep  4 16:05:32.981: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 09/04/23 16:05:32.982
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:33.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:33.06
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-bkcgn" 09/04/23 16:05:33.087
Sep  4 16:05:33.102: INFO: Get Replication Controller "e2e-rc-bkcgn" to confirm replicas
Sep  4 16:05:34.116: INFO: Get Replication Controller "e2e-rc-bkcgn" to confirm replicas
Sep  4 16:05:34.131: INFO: Found 1 replicas for "e2e-rc-bkcgn" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-bkcgn" 09/04/23 16:05:34.131
STEP: Updating a scale subresource 09/04/23 16:05:34.146
STEP: Verifying replicas where modified for replication controller "e2e-rc-bkcgn" 09/04/23 16:05:34.161
Sep  4 16:05:34.161: INFO: Get Replication Controller "e2e-rc-bkcgn" to confirm replicas
Sep  4 16:05:35.176: INFO: Get Replication Controller "e2e-rc-bkcgn" to confirm replicas
Sep  4 16:05:35.191: INFO: Found 2 replicas for "e2e-rc-bkcgn" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  4 16:05:35.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-101" for this suite. 09/04/23 16:05:35.219
------------------------------
• [2.254 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:05:32.981
    Sep  4 16:05:32.981: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 09/04/23 16:05:32.982
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:33.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:33.06
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-bkcgn" 09/04/23 16:05:33.087
    Sep  4 16:05:33.102: INFO: Get Replication Controller "e2e-rc-bkcgn" to confirm replicas
    Sep  4 16:05:34.116: INFO: Get Replication Controller "e2e-rc-bkcgn" to confirm replicas
    Sep  4 16:05:34.131: INFO: Found 1 replicas for "e2e-rc-bkcgn" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-bkcgn" 09/04/23 16:05:34.131
    STEP: Updating a scale subresource 09/04/23 16:05:34.146
    STEP: Verifying replicas where modified for replication controller "e2e-rc-bkcgn" 09/04/23 16:05:34.161
    Sep  4 16:05:34.161: INFO: Get Replication Controller "e2e-rc-bkcgn" to confirm replicas
    Sep  4 16:05:35.176: INFO: Get Replication Controller "e2e-rc-bkcgn" to confirm replicas
    Sep  4 16:05:35.191: INFO: Found 2 replicas for "e2e-rc-bkcgn" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:05:35.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-101" for this suite. 09/04/23 16:05:35.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:05:35.236
Sep  4 16:05:35.236: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 16:05:35.237
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:35.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:35.309
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 09/04/23 16:05:35.336
Sep  4 16:05:35.336: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 create -f -'
Sep  4 16:05:36.242: INFO: stderr: ""
Sep  4 16:05:36.242: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 16:05:36.242
Sep  4 16:05:36.242: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  4 16:05:36.339: INFO: stderr: ""
Sep  4 16:05:36.339: INFO: stdout: "update-demo-nautilus-b9lwr update-demo-nautilus-cn7f2 "
Sep  4 16:05:36.339: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods update-demo-nautilus-b9lwr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 16:05:36.431: INFO: stderr: ""
Sep  4 16:05:36.431: INFO: stdout: ""
Sep  4 16:05:36.431: INFO: update-demo-nautilus-b9lwr is created but not running
Sep  4 16:05:41.431: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  4 16:05:41.543: INFO: stderr: ""
Sep  4 16:05:41.543: INFO: stdout: "update-demo-nautilus-b9lwr update-demo-nautilus-cn7f2 "
Sep  4 16:05:41.543: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods update-demo-nautilus-b9lwr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 16:05:41.636: INFO: stderr: ""
Sep  4 16:05:41.636: INFO: stdout: "true"
Sep  4 16:05:41.636: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods update-demo-nautilus-b9lwr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  4 16:05:41.727: INFO: stderr: ""
Sep  4 16:05:41.727: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  4 16:05:41.727: INFO: validating pod update-demo-nautilus-b9lwr
Sep  4 16:05:41.879: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  4 16:05:41.879: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  4 16:05:41.879: INFO: update-demo-nautilus-b9lwr is verified up and running
Sep  4 16:05:41.879: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods update-demo-nautilus-cn7f2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  4 16:05:41.975: INFO: stderr: ""
Sep  4 16:05:41.975: INFO: stdout: "true"
Sep  4 16:05:41.976: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods update-demo-nautilus-cn7f2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  4 16:05:42.078: INFO: stderr: ""
Sep  4 16:05:42.078: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  4 16:05:42.078: INFO: validating pod update-demo-nautilus-cn7f2
Sep  4 16:05:42.214: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  4 16:05:42.214: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  4 16:05:42.214: INFO: update-demo-nautilus-cn7f2 is verified up and running
STEP: using delete to clean up resources 09/04/23 16:05:42.214
Sep  4 16:05:42.215: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 delete --grace-period=0 --force -f -'
Sep  4 16:05:42.328: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  4 16:05:42.328: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  4 16:05:42.328: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get rc,svc -l name=update-demo --no-headers'
Sep  4 16:05:42.450: INFO: stderr: "No resources found in kubectl-9807 namespace.\n"
Sep  4 16:05:42.450: INFO: stdout: ""
Sep  4 16:05:42.450: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  4 16:05:42.560: INFO: stderr: ""
Sep  4 16:05:42.560: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 16:05:42.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9807" for this suite. 09/04/23 16:05:42.588
------------------------------
• [7.368 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:05:35.236
    Sep  4 16:05:35.236: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 16:05:35.237
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:35.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:35.309
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 09/04/23 16:05:35.336
    Sep  4 16:05:35.336: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 create -f -'
    Sep  4 16:05:36.242: INFO: stderr: ""
    Sep  4 16:05:36.242: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/04/23 16:05:36.242
    Sep  4 16:05:36.242: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  4 16:05:36.339: INFO: stderr: ""
    Sep  4 16:05:36.339: INFO: stdout: "update-demo-nautilus-b9lwr update-demo-nautilus-cn7f2 "
    Sep  4 16:05:36.339: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods update-demo-nautilus-b9lwr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 16:05:36.431: INFO: stderr: ""
    Sep  4 16:05:36.431: INFO: stdout: ""
    Sep  4 16:05:36.431: INFO: update-demo-nautilus-b9lwr is created but not running
    Sep  4 16:05:41.431: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  4 16:05:41.543: INFO: stderr: ""
    Sep  4 16:05:41.543: INFO: stdout: "update-demo-nautilus-b9lwr update-demo-nautilus-cn7f2 "
    Sep  4 16:05:41.543: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods update-demo-nautilus-b9lwr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 16:05:41.636: INFO: stderr: ""
    Sep  4 16:05:41.636: INFO: stdout: "true"
    Sep  4 16:05:41.636: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods update-demo-nautilus-b9lwr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  4 16:05:41.727: INFO: stderr: ""
    Sep  4 16:05:41.727: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  4 16:05:41.727: INFO: validating pod update-demo-nautilus-b9lwr
    Sep  4 16:05:41.879: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  4 16:05:41.879: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  4 16:05:41.879: INFO: update-demo-nautilus-b9lwr is verified up and running
    Sep  4 16:05:41.879: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods update-demo-nautilus-cn7f2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  4 16:05:41.975: INFO: stderr: ""
    Sep  4 16:05:41.975: INFO: stdout: "true"
    Sep  4 16:05:41.976: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods update-demo-nautilus-cn7f2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  4 16:05:42.078: INFO: stderr: ""
    Sep  4 16:05:42.078: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  4 16:05:42.078: INFO: validating pod update-demo-nautilus-cn7f2
    Sep  4 16:05:42.214: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  4 16:05:42.214: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  4 16:05:42.214: INFO: update-demo-nautilus-cn7f2 is verified up and running
    STEP: using delete to clean up resources 09/04/23 16:05:42.214
    Sep  4 16:05:42.215: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 delete --grace-period=0 --force -f -'
    Sep  4 16:05:42.328: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  4 16:05:42.328: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep  4 16:05:42.328: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get rc,svc -l name=update-demo --no-headers'
    Sep  4 16:05:42.450: INFO: stderr: "No resources found in kubectl-9807 namespace.\n"
    Sep  4 16:05:42.450: INFO: stdout: ""
    Sep  4 16:05:42.450: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9807 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  4 16:05:42.560: INFO: stderr: ""
    Sep  4 16:05:42.560: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:05:42.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9807" for this suite. 09/04/23 16:05:42.588
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:05:42.604
Sep  4 16:05:42.604: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 16:05:42.605
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:42.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:42.675
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/04/23 16:05:42.703
Sep  4 16:05:42.703: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Sep  4 16:05:44.845: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:05:53.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2571" for this suite. 09/04/23 16:05:53.581
------------------------------
• [10.993 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:05:42.604
    Sep  4 16:05:42.604: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 16:05:42.605
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:42.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:42.675
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/04/23 16:05:42.703
    Sep  4 16:05:42.703: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Sep  4 16:05:44.845: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:05:53.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2571" for this suite. 09/04/23 16:05:53.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:05:53.597
Sep  4 16:05:53.597: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container 09/04/23 16:05:53.598
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:53.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:53.664
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 09/04/23 16:05:53.69
Sep  4 16:05:53.690: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:05:58.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1776" for this suite. 09/04/23 16:05:58.076
------------------------------
• [4.493 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:05:53.597
    Sep  4 16:05:53.597: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename init-container 09/04/23 16:05:53.598
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:53.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:53.664
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 09/04/23 16:05:53.69
    Sep  4 16:05:53.690: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:05:58.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1776" for this suite. 09/04/23 16:05:58.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:05:58.091
Sep  4 16:05:58.091: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename tables 09/04/23 16:05:58.091
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:58.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:58.156
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Sep  4 16:05:58.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-3267" for this suite. 09/04/23 16:05:58.22
------------------------------
• [0.144 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:05:58.091
    Sep  4 16:05:58.091: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename tables 09/04/23 16:05:58.091
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:58.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:58.156
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:05:58.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-3267" for this suite. 09/04/23 16:05:58.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:05:58.236
Sep  4 16:05:58.236: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 16:05:58.236
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:58.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:58.303
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 09/04/23 16:05:58.328
Sep  4 16:05:58.328: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: mark a version not serverd 09/04/23 16:06:03.875
STEP: check the unserved version gets removed 09/04/23 16:06:03.945
STEP: check the other version is not changed 09/04/23 16:06:05.585
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:10.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7831" for this suite. 09/04/23 16:06:10.116
------------------------------
• [11.897 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:05:58.236
    Sep  4 16:05:58.236: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 09/04/23 16:05:58.236
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:05:58.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:05:58.303
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 09/04/23 16:05:58.328
    Sep  4 16:05:58.328: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: mark a version not serverd 09/04/23 16:06:03.875
    STEP: check the unserved version gets removed 09/04/23 16:06:03.945
    STEP: check the other version is not changed 09/04/23 16:06:05.585
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:10.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7831" for this suite. 09/04/23 16:06:10.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:06:10.132
Sep  4 16:06:10.133: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 09/04/23 16:06:10.133
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:10.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:10.201
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 09/04/23 16:06:10.227
STEP: Ensuring ResourceQuota status is calculated 09/04/23 16:06:10.242
STEP: Creating a ResourceQuota with not terminating scope 09/04/23 16:06:12.257
STEP: Ensuring ResourceQuota status is calculated 09/04/23 16:06:12.272
STEP: Creating a long running pod 09/04/23 16:06:14.286
STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/04/23 16:06:14.309
STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/04/23 16:06:16.325
STEP: Deleting the pod 09/04/23 16:06:18.34
STEP: Ensuring resource quota status released the pod usage 09/04/23 16:06:18.359
STEP: Creating a terminating pod 09/04/23 16:06:20.375
STEP: Ensuring resource quota with terminating scope captures the pod usage 09/04/23 16:06:20.397
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/04/23 16:06:22.412
STEP: Deleting the pod 09/04/23 16:06:24.428
STEP: Ensuring resource quota status released the pod usage 09/04/23 16:06:24.445
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:26.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8075" for this suite. 09/04/23 16:06:26.488
------------------------------
• [16.372 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:06:10.132
    Sep  4 16:06:10.133: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 09/04/23 16:06:10.133
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:10.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:10.201
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 09/04/23 16:06:10.227
    STEP: Ensuring ResourceQuota status is calculated 09/04/23 16:06:10.242
    STEP: Creating a ResourceQuota with not terminating scope 09/04/23 16:06:12.257
    STEP: Ensuring ResourceQuota status is calculated 09/04/23 16:06:12.272
    STEP: Creating a long running pod 09/04/23 16:06:14.286
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/04/23 16:06:14.309
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/04/23 16:06:16.325
    STEP: Deleting the pod 09/04/23 16:06:18.34
    STEP: Ensuring resource quota status released the pod usage 09/04/23 16:06:18.359
    STEP: Creating a terminating pod 09/04/23 16:06:20.375
    STEP: Ensuring resource quota with terminating scope captures the pod usage 09/04/23 16:06:20.397
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/04/23 16:06:22.412
    STEP: Deleting the pod 09/04/23 16:06:24.428
    STEP: Ensuring resource quota status released the pod usage 09/04/23 16:06:24.445
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:26.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8075" for this suite. 09/04/23 16:06:26.488
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:06:26.504
Sep  4 16:06:26.504: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 09/04/23 16:06:26.505
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:26.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:26.574
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 09/04/23 16:06:26.6
STEP: Ensure pods equal to parallelism count is attached to the job 09/04/23 16:06:26.615
STEP: patching /status 09/04/23 16:06:28.631
STEP: updating /status 09/04/23 16:06:28.647
STEP: get /status 09/04/23 16:06:28.677
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:28.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1803" for this suite. 09/04/23 16:06:28.717
------------------------------
• [2.229 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:06:26.504
    Sep  4 16:06:26.504: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 09/04/23 16:06:26.505
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:26.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:26.574
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 09/04/23 16:06:26.6
    STEP: Ensure pods equal to parallelism count is attached to the job 09/04/23 16:06:26.615
    STEP: patching /status 09/04/23 16:06:28.631
    STEP: updating /status 09/04/23 16:06:28.647
    STEP: get /status 09/04/23 16:06:28.677
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:28.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1803" for this suite. 09/04/23 16:06:28.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:06:28.734
Sep  4 16:06:28.734: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 16:06:28.735
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:28.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:28.802
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 09/04/23 16:06:28.827
Sep  4 16:06:28.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334" in namespace "projected-4495" to be "Succeeded or Failed"
Sep  4 16:06:28.865: INFO: Pod "downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334": Phase="Pending", Reason="", readiness=false. Elapsed: 14.448629ms
Sep  4 16:06:30.881: INFO: Pod "downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030564758s
Sep  4 16:06:32.881: INFO: Pod "downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030639829s
STEP: Saw pod success 09/04/23 16:06:32.881
Sep  4 16:06:32.881: INFO: Pod "downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334" satisfied condition "Succeeded or Failed"
Sep  4 16:06:32.896: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx pod downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334 container client-container: <nil>
STEP: delete the pod 09/04/23 16:06:32.931
Sep  4 16:06:32.949: INFO: Waiting for pod downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334 to disappear
Sep  4 16:06:32.965: INFO: Pod downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:32.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4495" for this suite. 09/04/23 16:06:32.992
------------------------------
• [4.275 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:06:28.734
    Sep  4 16:06:28.734: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 16:06:28.735
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:28.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:28.802
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 09/04/23 16:06:28.827
    Sep  4 16:06:28.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334" in namespace "projected-4495" to be "Succeeded or Failed"
    Sep  4 16:06:28.865: INFO: Pod "downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334": Phase="Pending", Reason="", readiness=false. Elapsed: 14.448629ms
    Sep  4 16:06:30.881: INFO: Pod "downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030564758s
    Sep  4 16:06:32.881: INFO: Pod "downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030639829s
    STEP: Saw pod success 09/04/23 16:06:32.881
    Sep  4 16:06:32.881: INFO: Pod "downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334" satisfied condition "Succeeded or Failed"
    Sep  4 16:06:32.896: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx pod downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334 container client-container: <nil>
    STEP: delete the pod 09/04/23 16:06:32.931
    Sep  4 16:06:32.949: INFO: Waiting for pod downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334 to disappear
    Sep  4 16:06:32.965: INFO: Pod downwardapi-volume-6e20df51-ba89-4011-b9c6-24add46ce334 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:32.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4495" for this suite. 09/04/23 16:06:32.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:06:33.01
Sep  4 16:06:33.010: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 09/04/23 16:06:33.01
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:33.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:33.081
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 09/04/23 16:06:33.106
STEP: getting /apis/node.k8s.io 09/04/23 16:06:33.132
STEP: getting /apis/node.k8s.io/v1 09/04/23 16:06:33.144
STEP: creating 09/04/23 16:06:33.157
STEP: watching 09/04/23 16:06:33.201
Sep  4 16:06:33.201: INFO: starting watch
STEP: getting 09/04/23 16:06:33.228
STEP: listing 09/04/23 16:06:33.242
STEP: patching 09/04/23 16:06:33.256
STEP: updating 09/04/23 16:06:33.27
Sep  4 16:06:33.285: INFO: waiting for watch events with expected annotations
STEP: deleting 09/04/23 16:06:33.285
STEP: deleting a collection 09/04/23 16:06:33.327
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:33.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9278" for this suite. 09/04/23 16:06:33.378
------------------------------
• [0.384 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:06:33.01
    Sep  4 16:06:33.010: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 09/04/23 16:06:33.01
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:33.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:33.081
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 09/04/23 16:06:33.106
    STEP: getting /apis/node.k8s.io 09/04/23 16:06:33.132
    STEP: getting /apis/node.k8s.io/v1 09/04/23 16:06:33.144
    STEP: creating 09/04/23 16:06:33.157
    STEP: watching 09/04/23 16:06:33.201
    Sep  4 16:06:33.201: INFO: starting watch
    STEP: getting 09/04/23 16:06:33.228
    STEP: listing 09/04/23 16:06:33.242
    STEP: patching 09/04/23 16:06:33.256
    STEP: updating 09/04/23 16:06:33.27
    Sep  4 16:06:33.285: INFO: waiting for watch events with expected annotations
    STEP: deleting 09/04/23 16:06:33.285
    STEP: deleting a collection 09/04/23 16:06:33.327
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:33.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9278" for this suite. 09/04/23 16:06:33.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:06:33.394
Sep  4 16:06:33.394: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 16:06:33.395
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:33.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:33.462
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 09/04/23 16:06:33.487
Sep  4 16:06:33.487: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1373 create -f -'
Sep  4 16:06:34.243: INFO: stderr: ""
Sep  4 16:06:34.243: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/04/23 16:06:34.243
Sep  4 16:06:35.259: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 16:06:35.259: INFO: Found 0 / 1
Sep  4 16:06:36.258: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 16:06:36.258: INFO: Found 1 / 1
Sep  4 16:06:36.258: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 09/04/23 16:06:36.258
Sep  4 16:06:36.273: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 16:06:36.273: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  4 16:06:36.273: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1373 patch pod agnhost-primary-f87zr -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  4 16:06:36.437: INFO: stderr: ""
Sep  4 16:06:36.437: INFO: stdout: "pod/agnhost-primary-f87zr patched\n"
STEP: checking annotations 09/04/23 16:06:36.437
Sep  4 16:06:36.451: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  4 16:06:36.451: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:36.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1373" for this suite. 09/04/23 16:06:36.478
------------------------------
• [3.100 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:06:33.394
    Sep  4 16:06:33.394: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 16:06:33.395
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:33.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:33.462
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 09/04/23 16:06:33.487
    Sep  4 16:06:33.487: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1373 create -f -'
    Sep  4 16:06:34.243: INFO: stderr: ""
    Sep  4 16:06:34.243: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/04/23 16:06:34.243
    Sep  4 16:06:35.259: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 16:06:35.259: INFO: Found 0 / 1
    Sep  4 16:06:36.258: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 16:06:36.258: INFO: Found 1 / 1
    Sep  4 16:06:36.258: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 09/04/23 16:06:36.258
    Sep  4 16:06:36.273: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 16:06:36.273: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  4 16:06:36.273: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1373 patch pod agnhost-primary-f87zr -p {"metadata":{"annotations":{"x":"y"}}}'
    Sep  4 16:06:36.437: INFO: stderr: ""
    Sep  4 16:06:36.437: INFO: stdout: "pod/agnhost-primary-f87zr patched\n"
    STEP: checking annotations 09/04/23 16:06:36.437
    Sep  4 16:06:36.451: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  4 16:06:36.451: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:36.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1373" for this suite. 09/04/23 16:06:36.478
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:06:36.494
Sep  4 16:06:36.494: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 16:06:36.494
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:36.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:36.563
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 09/04/23 16:06:36.589
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:36.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7498" for this suite. 09/04/23 16:06:36.619
------------------------------
• [0.141 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:06:36.494
    Sep  4 16:06:36.494: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 16:06:36.494
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:36.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:36.563
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 09/04/23 16:06:36.589
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:36.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7498" for this suite. 09/04/23 16:06:36.619
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:06:36.635
Sep  4 16:06:36.635: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 16:06:36.636
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:36.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:36.703
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/04/23 16:06:36.73
Sep  4 16:06:36.752: INFO: Waiting up to 5m0s for pod "pod-c188479d-a26c-489f-9a03-df7efbbd5e0f" in namespace "emptydir-230" to be "Succeeded or Failed"
Sep  4 16:06:36.765: INFO: Pod "pod-c188479d-a26c-489f-9a03-df7efbbd5e0f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.585434ms
Sep  4 16:06:38.780: INFO: Pod "pod-c188479d-a26c-489f-9a03-df7efbbd5e0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028591753s
Sep  4 16:06:40.782: INFO: Pod "pod-c188479d-a26c-489f-9a03-df7efbbd5e0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030362976s
STEP: Saw pod success 09/04/23 16:06:40.782
Sep  4 16:06:40.782: INFO: Pod "pod-c188479d-a26c-489f-9a03-df7efbbd5e0f" satisfied condition "Succeeded or Failed"
Sep  4 16:06:40.796: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-c188479d-a26c-489f-9a03-df7efbbd5e0f container test-container: <nil>
STEP: delete the pod 09/04/23 16:06:40.874
Sep  4 16:06:40.893: INFO: Waiting for pod pod-c188479d-a26c-489f-9a03-df7efbbd5e0f to disappear
Sep  4 16:06:40.911: INFO: Pod pod-c188479d-a26c-489f-9a03-df7efbbd5e0f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:40.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-230" for this suite. 09/04/23 16:06:40.939
------------------------------
• [4.319 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:06:36.635
    Sep  4 16:06:36.635: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 16:06:36.636
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:36.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:36.703
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/04/23 16:06:36.73
    Sep  4 16:06:36.752: INFO: Waiting up to 5m0s for pod "pod-c188479d-a26c-489f-9a03-df7efbbd5e0f" in namespace "emptydir-230" to be "Succeeded or Failed"
    Sep  4 16:06:36.765: INFO: Pod "pod-c188479d-a26c-489f-9a03-df7efbbd5e0f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.585434ms
    Sep  4 16:06:38.780: INFO: Pod "pod-c188479d-a26c-489f-9a03-df7efbbd5e0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028591753s
    Sep  4 16:06:40.782: INFO: Pod "pod-c188479d-a26c-489f-9a03-df7efbbd5e0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030362976s
    STEP: Saw pod success 09/04/23 16:06:40.782
    Sep  4 16:06:40.782: INFO: Pod "pod-c188479d-a26c-489f-9a03-df7efbbd5e0f" satisfied condition "Succeeded or Failed"
    Sep  4 16:06:40.796: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-c188479d-a26c-489f-9a03-df7efbbd5e0f container test-container: <nil>
    STEP: delete the pod 09/04/23 16:06:40.874
    Sep  4 16:06:40.893: INFO: Waiting for pod pod-c188479d-a26c-489f-9a03-df7efbbd5e0f to disappear
    Sep  4 16:06:40.911: INFO: Pod pod-c188479d-a26c-489f-9a03-df7efbbd5e0f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:40.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-230" for this suite. 09/04/23 16:06:40.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:06:40.954
Sep  4 16:06:40.954: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 09/04/23 16:06:40.955
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:40.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:41.024
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 09/04/23 16:06:41.051
Sep  4 16:06:41.074: INFO: Waiting up to 5m0s for pod "var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344" in namespace "var-expansion-4569" to be "Succeeded or Failed"
Sep  4 16:06:41.089: INFO: Pod "var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344": Phase="Pending", Reason="", readiness=false. Elapsed: 14.858484ms
Sep  4 16:06:43.105: INFO: Pod "var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030789876s
Sep  4 16:06:45.106: INFO: Pod "var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031446753s
STEP: Saw pod success 09/04/23 16:06:45.106
Sep  4 16:06:45.106: INFO: Pod "var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344" satisfied condition "Succeeded or Failed"
Sep  4 16:06:45.120: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344 container dapi-container: <nil>
STEP: delete the pod 09/04/23 16:06:45.155
Sep  4 16:06:45.173: INFO: Waiting for pod var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344 to disappear
Sep  4 16:06:45.186: INFO: Pod var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:45.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4569" for this suite. 09/04/23 16:06:45.213
------------------------------
• [4.274 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:06:40.954
    Sep  4 16:06:40.954: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 09/04/23 16:06:40.955
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:40.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:41.024
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 09/04/23 16:06:41.051
    Sep  4 16:06:41.074: INFO: Waiting up to 5m0s for pod "var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344" in namespace "var-expansion-4569" to be "Succeeded or Failed"
    Sep  4 16:06:41.089: INFO: Pod "var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344": Phase="Pending", Reason="", readiness=false. Elapsed: 14.858484ms
    Sep  4 16:06:43.105: INFO: Pod "var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030789876s
    Sep  4 16:06:45.106: INFO: Pod "var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031446753s
    STEP: Saw pod success 09/04/23 16:06:45.106
    Sep  4 16:06:45.106: INFO: Pod "var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344" satisfied condition "Succeeded or Failed"
    Sep  4 16:06:45.120: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344 container dapi-container: <nil>
    STEP: delete the pod 09/04/23 16:06:45.155
    Sep  4 16:06:45.173: INFO: Waiting for pod var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344 to disappear
    Sep  4 16:06:45.186: INFO: Pod var-expansion-e20fb971-cc4e-4e81-bddf-f7c7d09e6344 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:45.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4569" for this suite. 09/04/23 16:06:45.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:06:45.229
Sep  4 16:06:45.229: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename server-version 09/04/23 16:06:45.23
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:45.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:45.299
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 09/04/23 16:06:45.326
STEP: Confirm major version 09/04/23 16:06:45.338
Sep  4 16:06:45.338: INFO: Major version: 1
STEP: Confirm minor version 09/04/23 16:06:45.338
Sep  4 16:06:45.338: INFO: cleanMinorVersion: 26
Sep  4 16:06:45.338: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:45.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-932" for this suite. 09/04/23 16:06:45.354
------------------------------
• [0.140 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:06:45.229
    Sep  4 16:06:45.229: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename server-version 09/04/23 16:06:45.23
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:45.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:45.299
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 09/04/23 16:06:45.326
    STEP: Confirm major version 09/04/23 16:06:45.338
    Sep  4 16:06:45.338: INFO: Major version: 1
    STEP: Confirm minor version 09/04/23 16:06:45.338
    Sep  4 16:06:45.338: INFO: cleanMinorVersion: 26
    Sep  4 16:06:45.338: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:45.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-932" for this suite. 09/04/23 16:06:45.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:06:45.37
Sep  4 16:06:45.370: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 16:06:45.371
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:45.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:45.44
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:45.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7559" for this suite. 09/04/23 16:06:45.497
------------------------------
• [0.143 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:06:45.37
    Sep  4 16:06:45.370: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 16:06:45.371
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:45.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:45.44
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:45.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7559" for this suite. 09/04/23 16:06:45.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:06:45.513
Sep  4 16:06:45.513: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 16:06:45.514
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:45.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:45.583
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-9ef29720-215e-4c98-8f02-8f33f4404c6f 09/04/23 16:06:45.609
STEP: Creating a pod to test consume configMaps 09/04/23 16:06:45.624
Sep  4 16:06:45.645: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334" in namespace "projected-1714" to be "Succeeded or Failed"
Sep  4 16:06:45.659: INFO: Pod "pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334": Phase="Pending", Reason="", readiness=false. Elapsed: 13.927824ms
Sep  4 16:06:47.675: INFO: Pod "pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029341695s
Sep  4 16:06:49.676: INFO: Pod "pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030816338s
STEP: Saw pod success 09/04/23 16:06:49.676
Sep  4 16:06:49.676: INFO: Pod "pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334" satisfied condition "Succeeded or Failed"
Sep  4 16:06:49.691: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 16:06:49.728
Sep  4 16:06:49.748: INFO: Waiting for pod pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334 to disappear
Sep  4 16:06:49.762: INFO: Pod pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:49.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1714" for this suite. 09/04/23 16:06:49.791
------------------------------
• [4.294 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:06:45.513
    Sep  4 16:06:45.513: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 16:06:45.514
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:45.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:45.583
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-9ef29720-215e-4c98-8f02-8f33f4404c6f 09/04/23 16:06:45.609
    STEP: Creating a pod to test consume configMaps 09/04/23 16:06:45.624
    Sep  4 16:06:45.645: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334" in namespace "projected-1714" to be "Succeeded or Failed"
    Sep  4 16:06:45.659: INFO: Pod "pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334": Phase="Pending", Reason="", readiness=false. Elapsed: 13.927824ms
    Sep  4 16:06:47.675: INFO: Pod "pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029341695s
    Sep  4 16:06:49.676: INFO: Pod "pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030816338s
    STEP: Saw pod success 09/04/23 16:06:49.676
    Sep  4 16:06:49.676: INFO: Pod "pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334" satisfied condition "Succeeded or Failed"
    Sep  4 16:06:49.691: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 16:06:49.728
    Sep  4 16:06:49.748: INFO: Waiting for pod pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334 to disappear
    Sep  4 16:06:49.762: INFO: Pod pod-projected-configmaps-1c07e2e4-69c9-4cd9-bcad-afa02cdfa334 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:49.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1714" for this suite. 09/04/23 16:06:49.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:06:49.808
Sep  4 16:06:49.808: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 16:06:49.808
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:49.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:49.877
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 16:06:49.94
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 16:06:50.276
STEP: Deploying the webhook pod 09/04/23 16:06:50.292
STEP: Wait for the deployment to be ready 09/04/23 16:06:50.321
Sep  4 16:06:50.361: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 16, 6, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 6, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 16, 6, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 6, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 16:06:52.376
STEP: Verifying the service has paired with the endpoint 09/04/23 16:06:52.397
Sep  4 16:06:53.397: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 09/04/23 16:06:53.566
STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 16:06:53.707
STEP: Deleting the collection of validation webhooks 09/04/23 16:06:53.84
STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 16:06:53.875
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:53.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7433" for this suite. 09/04/23 16:06:53.996
STEP: Destroying namespace "webhook-7433-markers" for this suite. 09/04/23 16:06:54.017
------------------------------
• [4.226 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:06:49.808
    Sep  4 16:06:49.808: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 16:06:49.808
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:49.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:49.877
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 16:06:49.94
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 16:06:50.276
    STEP: Deploying the webhook pod 09/04/23 16:06:50.292
    STEP: Wait for the deployment to be ready 09/04/23 16:06:50.321
    Sep  4 16:06:50.361: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 16, 6, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 6, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 16, 6, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 6, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 16:06:52.376
    STEP: Verifying the service has paired with the endpoint 09/04/23 16:06:52.397
    Sep  4 16:06:53.397: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 09/04/23 16:06:53.566
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 16:06:53.707
    STEP: Deleting the collection of validation webhooks 09/04/23 16:06:53.84
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/04/23 16:06:53.875
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:53.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7433" for this suite. 09/04/23 16:06:53.996
    STEP: Destroying namespace "webhook-7433-markers" for this suite. 09/04/23 16:06:54.017
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:06:54.034
Sep  4 16:06:54.034: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 09/04/23 16:06:54.034
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:54.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:54.105
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 09/04/23 16:06:54.131
Sep  4 16:06:54.153: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8538" to be "running and ready"
Sep  4 16:06:54.167: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 14.151954ms
Sep  4 16:06:54.167: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  4 16:06:56.183: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.029544957s
Sep  4 16:06:56.183: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Sep  4 16:06:56.183: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 09/04/23 16:06:56.196
STEP: Then the orphan pod is adopted 09/04/23 16:06:56.211
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:56.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8538" for this suite. 09/04/23 16:06:56.251
------------------------------
• [2.233 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:06:54.034
    Sep  4 16:06:54.034: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 09/04/23 16:06:54.034
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:54.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:54.105
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 09/04/23 16:06:54.131
    Sep  4 16:06:54.153: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8538" to be "running and ready"
    Sep  4 16:06:54.167: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 14.151954ms
    Sep  4 16:06:54.167: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 16:06:56.183: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.029544957s
    Sep  4 16:06:56.183: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Sep  4 16:06:56.183: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 09/04/23 16:06:56.196
    STEP: Then the orphan pod is adopted 09/04/23 16:06:56.211
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:56.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8538" for this suite. 09/04/23 16:06:56.251
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:06:56.267
Sep  4 16:06:56.267: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 16:06:56.268
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:56.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:56.335
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 16:06:56.393
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 16:06:56.743
STEP: Deploying the webhook pod 09/04/23 16:06:56.759
STEP: Wait for the deployment to be ready 09/04/23 16:06:56.79
Sep  4 16:06:56.834: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 16, 6, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 6, 56, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 16, 6, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 6, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 16:06:58.85
STEP: Verifying the service has paired with the endpoint 09/04/23 16:06:58.87
Sep  4 16:06:59.870: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 09/04/23 16:06:59.885
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/04/23 16:06:59.898
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/04/23 16:06:59.898
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/04/23 16:06:59.898
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/04/23 16:06:59.911
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/04/23 16:06:59.911
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/04/23 16:06:59.924
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:06:59.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6225" for this suite. 09/04/23 16:07:00.031
STEP: Destroying namespace "webhook-6225-markers" for this suite. 09/04/23 16:07:00.048
------------------------------
• [3.796 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:06:56.267
    Sep  4 16:06:56.267: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 16:06:56.268
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:06:56.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:06:56.335
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 16:06:56.393
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 16:06:56.743
    STEP: Deploying the webhook pod 09/04/23 16:06:56.759
    STEP: Wait for the deployment to be ready 09/04/23 16:06:56.79
    Sep  4 16:06:56.834: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 16, 6, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 6, 56, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 16, 6, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 6, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 16:06:58.85
    STEP: Verifying the service has paired with the endpoint 09/04/23 16:06:58.87
    Sep  4 16:06:59.870: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 09/04/23 16:06:59.885
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/04/23 16:06:59.898
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/04/23 16:06:59.898
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/04/23 16:06:59.898
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/04/23 16:06:59.911
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/04/23 16:06:59.911
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/04/23 16:06:59.924
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:06:59.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6225" for this suite. 09/04/23 16:07:00.031
    STEP: Destroying namespace "webhook-6225-markers" for this suite. 09/04/23 16:07:00.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:07:00.064
Sep  4 16:07:00.064: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 16:07:00.065
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:07:00.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:07:00.137
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 09/04/23 16:07:00.18
STEP: waiting for available Endpoint 09/04/23 16:07:00.196
STEP: listing all Endpoints 09/04/23 16:07:00.21
STEP: updating the Endpoint 09/04/23 16:07:00.225
STEP: fetching the Endpoint 09/04/23 16:07:00.254
STEP: patching the Endpoint 09/04/23 16:07:00.269
STEP: fetching the Endpoint 09/04/23 16:07:00.301
STEP: deleting the Endpoint by Collection 09/04/23 16:07:00.315
STEP: waiting for Endpoint deletion 09/04/23 16:07:00.332
STEP: fetching the Endpoint 09/04/23 16:07:00.346
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 16:07:00.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8422" for this suite. 09/04/23 16:07:00.378
------------------------------
• [0.330 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:07:00.064
    Sep  4 16:07:00.064: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 16:07:00.065
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:07:00.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:07:00.137
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 09/04/23 16:07:00.18
    STEP: waiting for available Endpoint 09/04/23 16:07:00.196
    STEP: listing all Endpoints 09/04/23 16:07:00.21
    STEP: updating the Endpoint 09/04/23 16:07:00.225
    STEP: fetching the Endpoint 09/04/23 16:07:00.254
    STEP: patching the Endpoint 09/04/23 16:07:00.269
    STEP: fetching the Endpoint 09/04/23 16:07:00.301
    STEP: deleting the Endpoint by Collection 09/04/23 16:07:00.315
    STEP: waiting for Endpoint deletion 09/04/23 16:07:00.332
    STEP: fetching the Endpoint 09/04/23 16:07:00.346
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:07:00.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8422" for this suite. 09/04/23 16:07:00.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:07:00.394
Sep  4 16:07:00.394: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 09/04/23 16:07:00.395
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:07:00.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:07:00.467
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-b5d538a5-f24a-457a-b5eb-675202b3c7d4 in namespace container-probe-5317 09/04/23 16:07:00.494
Sep  4 16:07:00.516: INFO: Waiting up to 5m0s for pod "liveness-b5d538a5-f24a-457a-b5eb-675202b3c7d4" in namespace "container-probe-5317" to be "not pending"
Sep  4 16:07:00.530: INFO: Pod "liveness-b5d538a5-f24a-457a-b5eb-675202b3c7d4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.403259ms
Sep  4 16:07:02.546: INFO: Pod "liveness-b5d538a5-f24a-457a-b5eb-675202b3c7d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.030150874s
Sep  4 16:07:02.546: INFO: Pod "liveness-b5d538a5-f24a-457a-b5eb-675202b3c7d4" satisfied condition "not pending"
Sep  4 16:07:02.546: INFO: Started pod liveness-b5d538a5-f24a-457a-b5eb-675202b3c7d4 in namespace container-probe-5317
STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 16:07:02.546
Sep  4 16:07:02.561: INFO: Initial restart count of pod liveness-b5d538a5-f24a-457a-b5eb-675202b3c7d4 is 0
STEP: deleting the pod 09/04/23 16:11:04.466
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  4 16:11:04.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5317" for this suite. 09/04/23 16:11:04.512
------------------------------
• [244.133 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:07:00.394
    Sep  4 16:07:00.394: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 09/04/23 16:07:00.395
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:07:00.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:07:00.467
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-b5d538a5-f24a-457a-b5eb-675202b3c7d4 in namespace container-probe-5317 09/04/23 16:07:00.494
    Sep  4 16:07:00.516: INFO: Waiting up to 5m0s for pod "liveness-b5d538a5-f24a-457a-b5eb-675202b3c7d4" in namespace "container-probe-5317" to be "not pending"
    Sep  4 16:07:00.530: INFO: Pod "liveness-b5d538a5-f24a-457a-b5eb-675202b3c7d4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.403259ms
    Sep  4 16:07:02.546: INFO: Pod "liveness-b5d538a5-f24a-457a-b5eb-675202b3c7d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.030150874s
    Sep  4 16:07:02.546: INFO: Pod "liveness-b5d538a5-f24a-457a-b5eb-675202b3c7d4" satisfied condition "not pending"
    Sep  4 16:07:02.546: INFO: Started pod liveness-b5d538a5-f24a-457a-b5eb-675202b3c7d4 in namespace container-probe-5317
    STEP: checking the pod's current state and verifying that restartCount is present 09/04/23 16:07:02.546
    Sep  4 16:07:02.561: INFO: Initial restart count of pod liveness-b5d538a5-f24a-457a-b5eb-675202b3c7d4 is 0
    STEP: deleting the pod 09/04/23 16:11:04.466
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:11:04.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5317" for this suite. 09/04/23 16:11:04.512
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:11:04.528
Sep  4 16:11:04.528: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy 09/04/23 16:11:04.528
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:04.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:04.598
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Sep  4 16:11:04.624: INFO: Creating pod...
Sep  4 16:11:04.650: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5654" to be "running"
Sep  4 16:11:04.664: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 13.633632ms
Sep  4 16:11:06.679: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.029328473s
Sep  4 16:11:06.679: INFO: Pod "agnhost" satisfied condition "running"
Sep  4 16:11:06.679: INFO: Creating service...
Sep  4 16:11:06.697: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/pods/agnhost/proxy/some/path/with/DELETE
Sep  4 16:11:06.838: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  4 16:11:06.838: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/pods/agnhost/proxy/some/path/with/GET
Sep  4 16:11:06.899: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep  4 16:11:06.899: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/pods/agnhost/proxy/some/path/with/HEAD
Sep  4 16:11:06.923: INFO: http.Client request:HEAD | StatusCode:200
Sep  4 16:11:06.923: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/pods/agnhost/proxy/some/path/with/OPTIONS
Sep  4 16:11:06.947: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  4 16:11:06.948: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/pods/agnhost/proxy/some/path/with/PATCH
Sep  4 16:11:06.971: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  4 16:11:06.971: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/pods/agnhost/proxy/some/path/with/POST
Sep  4 16:11:06.995: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  4 16:11:06.995: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/pods/agnhost/proxy/some/path/with/PUT
Sep  4 16:11:07.021: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  4 16:11:07.021: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/services/test-service/proxy/some/path/with/DELETE
Sep  4 16:11:07.047: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  4 16:11:07.047: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/services/test-service/proxy/some/path/with/GET
Sep  4 16:11:07.072: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep  4 16:11:07.072: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/services/test-service/proxy/some/path/with/HEAD
Sep  4 16:11:07.106: INFO: http.Client request:HEAD | StatusCode:200
Sep  4 16:11:07.106: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/services/test-service/proxy/some/path/with/OPTIONS
Sep  4 16:11:07.133: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  4 16:11:07.133: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/services/test-service/proxy/some/path/with/PATCH
Sep  4 16:11:07.158: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  4 16:11:07.158: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/services/test-service/proxy/some/path/with/POST
Sep  4 16:11:07.184: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  4 16:11:07.184: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/services/test-service/proxy/some/path/with/PUT
Sep  4 16:11:07.210: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  4 16:11:07.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5654" for this suite. 09/04/23 16:11:07.237
------------------------------
• [2.725 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:11:04.528
    Sep  4 16:11:04.528: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename proxy 09/04/23 16:11:04.528
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:04.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:04.598
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Sep  4 16:11:04.624: INFO: Creating pod...
    Sep  4 16:11:04.650: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5654" to be "running"
    Sep  4 16:11:04.664: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 13.633632ms
    Sep  4 16:11:06.679: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.029328473s
    Sep  4 16:11:06.679: INFO: Pod "agnhost" satisfied condition "running"
    Sep  4 16:11:06.679: INFO: Creating service...
    Sep  4 16:11:06.697: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/pods/agnhost/proxy/some/path/with/DELETE
    Sep  4 16:11:06.838: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  4 16:11:06.838: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/pods/agnhost/proxy/some/path/with/GET
    Sep  4 16:11:06.899: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep  4 16:11:06.899: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/pods/agnhost/proxy/some/path/with/HEAD
    Sep  4 16:11:06.923: INFO: http.Client request:HEAD | StatusCode:200
    Sep  4 16:11:06.923: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/pods/agnhost/proxy/some/path/with/OPTIONS
    Sep  4 16:11:06.947: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  4 16:11:06.948: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/pods/agnhost/proxy/some/path/with/PATCH
    Sep  4 16:11:06.971: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  4 16:11:06.971: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/pods/agnhost/proxy/some/path/with/POST
    Sep  4 16:11:06.995: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  4 16:11:06.995: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/pods/agnhost/proxy/some/path/with/PUT
    Sep  4 16:11:07.021: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  4 16:11:07.021: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/services/test-service/proxy/some/path/with/DELETE
    Sep  4 16:11:07.047: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  4 16:11:07.047: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/services/test-service/proxy/some/path/with/GET
    Sep  4 16:11:07.072: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep  4 16:11:07.072: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/services/test-service/proxy/some/path/with/HEAD
    Sep  4 16:11:07.106: INFO: http.Client request:HEAD | StatusCode:200
    Sep  4 16:11:07.106: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/services/test-service/proxy/some/path/with/OPTIONS
    Sep  4 16:11:07.133: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  4 16:11:07.133: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/services/test-service/proxy/some/path/with/PATCH
    Sep  4 16:11:07.158: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  4 16:11:07.158: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/services/test-service/proxy/some/path/with/POST
    Sep  4 16:11:07.184: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  4 16:11:07.184: INFO: Starting http.Client for https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-5654/services/test-service/proxy/some/path/with/PUT
    Sep  4 16:11:07.210: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:11:07.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5654" for this suite. 09/04/23 16:11:07.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:11:07.253
Sep  4 16:11:07.253: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper 09/04/23 16:11:07.254
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:07.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:07.323
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Sep  4 16:11:07.399: INFO: Waiting up to 5m0s for pod "pod-secrets-36b14b11-c7c7-47f3-adcb-41dd3e3d77e5" in namespace "emptydir-wrapper-4394" to be "running and ready"
Sep  4 16:11:07.412: INFO: Pod "pod-secrets-36b14b11-c7c7-47f3-adcb-41dd3e3d77e5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.545622ms
Sep  4 16:11:07.412: INFO: The phase of Pod pod-secrets-36b14b11-c7c7-47f3-adcb-41dd3e3d77e5 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 16:11:09.427: INFO: Pod "pod-secrets-36b14b11-c7c7-47f3-adcb-41dd3e3d77e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.02881388s
Sep  4 16:11:09.427: INFO: The phase of Pod pod-secrets-36b14b11-c7c7-47f3-adcb-41dd3e3d77e5 is Running (Ready = true)
Sep  4 16:11:09.427: INFO: Pod "pod-secrets-36b14b11-c7c7-47f3-adcb-41dd3e3d77e5" satisfied condition "running and ready"
STEP: Cleaning up the secret 09/04/23 16:11:09.442
STEP: Cleaning up the configmap 09/04/23 16:11:09.458
STEP: Cleaning up the pod 09/04/23 16:11:09.476
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 16:11:09.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-4394" for this suite. 09/04/23 16:11:09.521
------------------------------
• [2.284 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:11:07.253
    Sep  4 16:11:07.253: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir-wrapper 09/04/23 16:11:07.254
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:07.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:07.323
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Sep  4 16:11:07.399: INFO: Waiting up to 5m0s for pod "pod-secrets-36b14b11-c7c7-47f3-adcb-41dd3e3d77e5" in namespace "emptydir-wrapper-4394" to be "running and ready"
    Sep  4 16:11:07.412: INFO: Pod "pod-secrets-36b14b11-c7c7-47f3-adcb-41dd3e3d77e5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.545622ms
    Sep  4 16:11:07.412: INFO: The phase of Pod pod-secrets-36b14b11-c7c7-47f3-adcb-41dd3e3d77e5 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 16:11:09.427: INFO: Pod "pod-secrets-36b14b11-c7c7-47f3-adcb-41dd3e3d77e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.02881388s
    Sep  4 16:11:09.427: INFO: The phase of Pod pod-secrets-36b14b11-c7c7-47f3-adcb-41dd3e3d77e5 is Running (Ready = true)
    Sep  4 16:11:09.427: INFO: Pod "pod-secrets-36b14b11-c7c7-47f3-adcb-41dd3e3d77e5" satisfied condition "running and ready"
    STEP: Cleaning up the secret 09/04/23 16:11:09.442
    STEP: Cleaning up the configmap 09/04/23 16:11:09.458
    STEP: Cleaning up the pod 09/04/23 16:11:09.476
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:11:09.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-4394" for this suite. 09/04/23 16:11:09.521
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:11:09.537
Sep  4 16:11:09.537: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 09/04/23 16:11:09.538
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:09.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:09.607
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7407.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7407.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 09/04/23 16:11:09.633
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7407.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7407.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 09/04/23 16:11:09.633
STEP: creating a pod to probe /etc/hosts 09/04/23 16:11:09.633
STEP: submitting the pod to kubernetes 09/04/23 16:11:09.633
Sep  4 16:11:09.654: INFO: Waiting up to 15m0s for pod "dns-test-f3ec383a-c92e-4f65-ae9d-0436b4922030" in namespace "dns-7407" to be "running"
Sep  4 16:11:09.668: INFO: Pod "dns-test-f3ec383a-c92e-4f65-ae9d-0436b4922030": Phase="Pending", Reason="", readiness=false. Elapsed: 13.91311ms
Sep  4 16:11:11.685: INFO: Pod "dns-test-f3ec383a-c92e-4f65-ae9d-0436b4922030": Phase="Running", Reason="", readiness=true. Elapsed: 2.030662672s
Sep  4 16:11:11.685: INFO: Pod "dns-test-f3ec383a-c92e-4f65-ae9d-0436b4922030" satisfied condition "running"
STEP: retrieving the pod 09/04/23 16:11:11.685
STEP: looking for the results for each expected name from probers 09/04/23 16:11:11.699
Sep  4 16:11:11.939: INFO: DNS probes using dns-7407/dns-test-f3ec383a-c92e-4f65-ae9d-0436b4922030 succeeded

STEP: deleting the pod 09/04/23 16:11:11.939
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  4 16:11:11.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7407" for this suite. 09/04/23 16:11:11.987
------------------------------
• [2.465 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:11:09.537
    Sep  4 16:11:09.537: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 09/04/23 16:11:09.538
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:09.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:09.607
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7407.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7407.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     09/04/23 16:11:09.633
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7407.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7407.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     09/04/23 16:11:09.633
    STEP: creating a pod to probe /etc/hosts 09/04/23 16:11:09.633
    STEP: submitting the pod to kubernetes 09/04/23 16:11:09.633
    Sep  4 16:11:09.654: INFO: Waiting up to 15m0s for pod "dns-test-f3ec383a-c92e-4f65-ae9d-0436b4922030" in namespace "dns-7407" to be "running"
    Sep  4 16:11:09.668: INFO: Pod "dns-test-f3ec383a-c92e-4f65-ae9d-0436b4922030": Phase="Pending", Reason="", readiness=false. Elapsed: 13.91311ms
    Sep  4 16:11:11.685: INFO: Pod "dns-test-f3ec383a-c92e-4f65-ae9d-0436b4922030": Phase="Running", Reason="", readiness=true. Elapsed: 2.030662672s
    Sep  4 16:11:11.685: INFO: Pod "dns-test-f3ec383a-c92e-4f65-ae9d-0436b4922030" satisfied condition "running"
    STEP: retrieving the pod 09/04/23 16:11:11.685
    STEP: looking for the results for each expected name from probers 09/04/23 16:11:11.699
    Sep  4 16:11:11.939: INFO: DNS probes using dns-7407/dns-test-f3ec383a-c92e-4f65-ae9d-0436b4922030 succeeded

    STEP: deleting the pod 09/04/23 16:11:11.939
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:11:11.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7407" for this suite. 09/04/23 16:11:11.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:11:12.002
Sep  4 16:11:12.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 09/04/23 16:11:12.003
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:12.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:12.073
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 09/04/23 16:11:12.116
STEP: create the rc2 09/04/23 16:11:12.131
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/04/23 16:11:17.161
STEP: delete the rc simpletest-rc-to-be-deleted 09/04/23 16:11:17.999
STEP: wait for the rc to be deleted 09/04/23 16:11:18.016
Sep  4 16:11:23.066: INFO: 70 pods remaining
Sep  4 16:11:23.066: INFO: 70 pods has nil DeletionTimestamp
Sep  4 16:11:23.066: INFO: 
STEP: Gathering metrics 09/04/23 16:11:28.065
W0904 16:11:28.100471    7754 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep  4 16:11:28.100: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  4 16:11:28.100: INFO: Deleting pod "simpletest-rc-to-be-deleted-22gkb" in namespace "gc-3754"
Sep  4 16:11:28.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-25h6h" in namespace "gc-3754"
Sep  4 16:11:28.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-2knt9" in namespace "gc-3754"
Sep  4 16:11:28.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-2l79f" in namespace "gc-3754"
Sep  4 16:11:28.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tp9b" in namespace "gc-3754"
Sep  4 16:11:28.194: INFO: Deleting pod "simpletest-rc-to-be-deleted-44qmb" in namespace "gc-3754"
Sep  4 16:11:28.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dw8r" in namespace "gc-3754"
Sep  4 16:11:28.231: INFO: Deleting pod "simpletest-rc-to-be-deleted-4m4dm" in namespace "gc-3754"
Sep  4 16:11:28.251: INFO: Deleting pod "simpletest-rc-to-be-deleted-558mb" in namespace "gc-3754"
Sep  4 16:11:28.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lvsh" in namespace "gc-3754"
Sep  4 16:11:28.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vrlb" in namespace "gc-3754"
Sep  4 16:11:28.306: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lf95" in namespace "gc-3754"
Sep  4 16:11:28.326: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nt6x" in namespace "gc-3754"
Sep  4 16:11:28.345: INFO: Deleting pod "simpletest-rc-to-be-deleted-7z58c" in namespace "gc-3754"
Sep  4 16:11:28.363: INFO: Deleting pod "simpletest-rc-to-be-deleted-82dzw" in namespace "gc-3754"
Sep  4 16:11:28.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cq9s" in namespace "gc-3754"
Sep  4 16:11:28.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-8sdnj" in namespace "gc-3754"
Sep  4 16:11:28.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-9425n" in namespace "gc-3754"
Sep  4 16:11:28.441: INFO: Deleting pod "simpletest-rc-to-be-deleted-98nsc" in namespace "gc-3754"
Sep  4 16:11:28.458: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fn4g" in namespace "gc-3754"
Sep  4 16:11:28.478: INFO: Deleting pod "simpletest-rc-to-be-deleted-9hdg9" in namespace "gc-3754"
Sep  4 16:11:28.497: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lz76" in namespace "gc-3754"
Sep  4 16:11:28.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-9q6b2" in namespace "gc-3754"
Sep  4 16:11:28.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-b52pc" in namespace "gc-3754"
Sep  4 16:11:28.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6b7p" in namespace "gc-3754"
Sep  4 16:11:28.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7ksx" in namespace "gc-3754"
Sep  4 16:11:28.589: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7qgn" in namespace "gc-3754"
Sep  4 16:11:28.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-bc5fm" in namespace "gc-3754"
Sep  4 16:11:28.625: INFO: Deleting pod "simpletest-rc-to-be-deleted-bf98s" in namespace "gc-3754"
Sep  4 16:11:28.644: INFO: Deleting pod "simpletest-rc-to-be-deleted-cl87q" in namespace "gc-3754"
Sep  4 16:11:28.662: INFO: Deleting pod "simpletest-rc-to-be-deleted-csbf2" in namespace "gc-3754"
Sep  4 16:11:28.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctv2l" in namespace "gc-3754"
Sep  4 16:11:28.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8b2n" in namespace "gc-3754"
Sep  4 16:11:28.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl7s9" in namespace "gc-3754"
Sep  4 16:11:28.735: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnzkl" in namespace "gc-3754"
Sep  4 16:11:28.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4897" in namespace "gc-3754"
Sep  4 16:11:28.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbf7h" in namespace "gc-3754"
Sep  4 16:11:28.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdqjc" in namespace "gc-3754"
Sep  4 16:11:28.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-fw8zv" in namespace "gc-3754"
Sep  4 16:11:28.827: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghssr" in namespace "gc-3754"
Sep  4 16:11:28.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-gk6vv" in namespace "gc-3754"
Sep  4 16:11:28.866: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkhkw" in namespace "gc-3754"
Sep  4 16:11:28.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-gx6zc" in namespace "gc-3754"
Sep  4 16:11:28.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8bds" in namespace "gc-3754"
Sep  4 16:11:28.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjsjt" in namespace "gc-3754"
Sep  4 16:11:28.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-htgqp" in namespace "gc-3754"
Sep  4 16:11:28.956: INFO: Deleting pod "simpletest-rc-to-be-deleted-jblrm" in namespace "gc-3754"
Sep  4 16:11:28.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-jflnq" in namespace "gc-3754"
Sep  4 16:11:28.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-jxv2c" in namespace "gc-3754"
Sep  4 16:11:29.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-k7vdv" in namespace "gc-3754"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  4 16:11:29.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3754" for this suite. 09/04/23 16:11:29.063
------------------------------
• [17.077 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:11:12.002
    Sep  4 16:11:12.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 09/04/23 16:11:12.003
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:12.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:12.073
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 09/04/23 16:11:12.116
    STEP: create the rc2 09/04/23 16:11:12.131
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/04/23 16:11:17.161
    STEP: delete the rc simpletest-rc-to-be-deleted 09/04/23 16:11:17.999
    STEP: wait for the rc to be deleted 09/04/23 16:11:18.016
    Sep  4 16:11:23.066: INFO: 70 pods remaining
    Sep  4 16:11:23.066: INFO: 70 pods has nil DeletionTimestamp
    Sep  4 16:11:23.066: INFO: 
    STEP: Gathering metrics 09/04/23 16:11:28.065
    W0904 16:11:28.100471    7754 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep  4 16:11:28.100: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep  4 16:11:28.100: INFO: Deleting pod "simpletest-rc-to-be-deleted-22gkb" in namespace "gc-3754"
    Sep  4 16:11:28.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-25h6h" in namespace "gc-3754"
    Sep  4 16:11:28.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-2knt9" in namespace "gc-3754"
    Sep  4 16:11:28.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-2l79f" in namespace "gc-3754"
    Sep  4 16:11:28.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tp9b" in namespace "gc-3754"
    Sep  4 16:11:28.194: INFO: Deleting pod "simpletest-rc-to-be-deleted-44qmb" in namespace "gc-3754"
    Sep  4 16:11:28.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dw8r" in namespace "gc-3754"
    Sep  4 16:11:28.231: INFO: Deleting pod "simpletest-rc-to-be-deleted-4m4dm" in namespace "gc-3754"
    Sep  4 16:11:28.251: INFO: Deleting pod "simpletest-rc-to-be-deleted-558mb" in namespace "gc-3754"
    Sep  4 16:11:28.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lvsh" in namespace "gc-3754"
    Sep  4 16:11:28.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vrlb" in namespace "gc-3754"
    Sep  4 16:11:28.306: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lf95" in namespace "gc-3754"
    Sep  4 16:11:28.326: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nt6x" in namespace "gc-3754"
    Sep  4 16:11:28.345: INFO: Deleting pod "simpletest-rc-to-be-deleted-7z58c" in namespace "gc-3754"
    Sep  4 16:11:28.363: INFO: Deleting pod "simpletest-rc-to-be-deleted-82dzw" in namespace "gc-3754"
    Sep  4 16:11:28.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cq9s" in namespace "gc-3754"
    Sep  4 16:11:28.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-8sdnj" in namespace "gc-3754"
    Sep  4 16:11:28.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-9425n" in namespace "gc-3754"
    Sep  4 16:11:28.441: INFO: Deleting pod "simpletest-rc-to-be-deleted-98nsc" in namespace "gc-3754"
    Sep  4 16:11:28.458: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fn4g" in namespace "gc-3754"
    Sep  4 16:11:28.478: INFO: Deleting pod "simpletest-rc-to-be-deleted-9hdg9" in namespace "gc-3754"
    Sep  4 16:11:28.497: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lz76" in namespace "gc-3754"
    Sep  4 16:11:28.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-9q6b2" in namespace "gc-3754"
    Sep  4 16:11:28.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-b52pc" in namespace "gc-3754"
    Sep  4 16:11:28.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6b7p" in namespace "gc-3754"
    Sep  4 16:11:28.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7ksx" in namespace "gc-3754"
    Sep  4 16:11:28.589: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7qgn" in namespace "gc-3754"
    Sep  4 16:11:28.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-bc5fm" in namespace "gc-3754"
    Sep  4 16:11:28.625: INFO: Deleting pod "simpletest-rc-to-be-deleted-bf98s" in namespace "gc-3754"
    Sep  4 16:11:28.644: INFO: Deleting pod "simpletest-rc-to-be-deleted-cl87q" in namespace "gc-3754"
    Sep  4 16:11:28.662: INFO: Deleting pod "simpletest-rc-to-be-deleted-csbf2" in namespace "gc-3754"
    Sep  4 16:11:28.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctv2l" in namespace "gc-3754"
    Sep  4 16:11:28.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8b2n" in namespace "gc-3754"
    Sep  4 16:11:28.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl7s9" in namespace "gc-3754"
    Sep  4 16:11:28.735: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnzkl" in namespace "gc-3754"
    Sep  4 16:11:28.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4897" in namespace "gc-3754"
    Sep  4 16:11:28.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbf7h" in namespace "gc-3754"
    Sep  4 16:11:28.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdqjc" in namespace "gc-3754"
    Sep  4 16:11:28.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-fw8zv" in namespace "gc-3754"
    Sep  4 16:11:28.827: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghssr" in namespace "gc-3754"
    Sep  4 16:11:28.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-gk6vv" in namespace "gc-3754"
    Sep  4 16:11:28.866: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkhkw" in namespace "gc-3754"
    Sep  4 16:11:28.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-gx6zc" in namespace "gc-3754"
    Sep  4 16:11:28.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8bds" in namespace "gc-3754"
    Sep  4 16:11:28.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjsjt" in namespace "gc-3754"
    Sep  4 16:11:28.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-htgqp" in namespace "gc-3754"
    Sep  4 16:11:28.956: INFO: Deleting pod "simpletest-rc-to-be-deleted-jblrm" in namespace "gc-3754"
    Sep  4 16:11:28.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-jflnq" in namespace "gc-3754"
    Sep  4 16:11:28.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-jxv2c" in namespace "gc-3754"
    Sep  4 16:11:29.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-k7vdv" in namespace "gc-3754"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:11:29.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3754" for this suite. 09/04/23 16:11:29.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:11:29.08
Sep  4 16:11:29.080: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 16:11:29.081
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:29.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:29.156
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 09/04/23 16:11:29.184
Sep  4 16:11:29.208: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6" in namespace "projected-7679" to be "Succeeded or Failed"
Sep  4 16:11:29.223: INFO: Pod "downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.88032ms
Sep  4 16:11:31.240: INFO: Pod "downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031561101s
Sep  4 16:11:33.240: INFO: Pod "downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031592899s
STEP: Saw pod success 09/04/23 16:11:33.24
Sep  4 16:11:33.240: INFO: Pod "downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6" satisfied condition "Succeeded or Failed"
Sep  4 16:11:33.255: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6 container client-container: <nil>
STEP: delete the pod 09/04/23 16:11:33.303
Sep  4 16:11:33.321: INFO: Waiting for pod downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6 to disappear
Sep  4 16:11:33.336: INFO: Pod downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 16:11:33.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7679" for this suite. 09/04/23 16:11:33.364
------------------------------
• [4.302 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:11:29.08
    Sep  4 16:11:29.080: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 16:11:29.081
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:29.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:29.156
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 09/04/23 16:11:29.184
    Sep  4 16:11:29.208: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6" in namespace "projected-7679" to be "Succeeded or Failed"
    Sep  4 16:11:29.223: INFO: Pod "downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.88032ms
    Sep  4 16:11:31.240: INFO: Pod "downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031561101s
    Sep  4 16:11:33.240: INFO: Pod "downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031592899s
    STEP: Saw pod success 09/04/23 16:11:33.24
    Sep  4 16:11:33.240: INFO: Pod "downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6" satisfied condition "Succeeded or Failed"
    Sep  4 16:11:33.255: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6 container client-container: <nil>
    STEP: delete the pod 09/04/23 16:11:33.303
    Sep  4 16:11:33.321: INFO: Waiting for pod downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6 to disappear
    Sep  4 16:11:33.336: INFO: Pod downwardapi-volume-3d5dfabc-f6fc-45a7-aa63-5fbb38fa8cd6 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:11:33.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7679" for this suite. 09/04/23 16:11:33.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:11:33.382
Sep  4 16:11:33.382: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers 09/04/23 16:11:33.383
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:33.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:33.458
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Sep  4 16:11:33.507: INFO: Waiting up to 5m0s for pod "client-containers-f9a13794-a505-4883-a37a-fdee06a381b6" in namespace "containers-6595" to be "running"
Sep  4 16:11:33.522: INFO: Pod "client-containers-f9a13794-a505-4883-a37a-fdee06a381b6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.215667ms
Sep  4 16:11:35.538: INFO: Pod "client-containers-f9a13794-a505-4883-a37a-fdee06a381b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.031326168s
Sep  4 16:11:35.538: INFO: Pod "client-containers-f9a13794-a505-4883-a37a-fdee06a381b6" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  4 16:11:35.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6595" for this suite. 09/04/23 16:11:35.603
------------------------------
• [2.237 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:11:33.382
    Sep  4 16:11:33.382: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename containers 09/04/23 16:11:33.383
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:33.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:33.458
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Sep  4 16:11:33.507: INFO: Waiting up to 5m0s for pod "client-containers-f9a13794-a505-4883-a37a-fdee06a381b6" in namespace "containers-6595" to be "running"
    Sep  4 16:11:33.522: INFO: Pod "client-containers-f9a13794-a505-4883-a37a-fdee06a381b6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.215667ms
    Sep  4 16:11:35.538: INFO: Pod "client-containers-f9a13794-a505-4883-a37a-fdee06a381b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.031326168s
    Sep  4 16:11:35.538: INFO: Pod "client-containers-f9a13794-a505-4883-a37a-fdee06a381b6" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:11:35.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6595" for this suite. 09/04/23 16:11:35.603
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:11:35.62
Sep  4 16:11:35.620: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 09/04/23 16:11:35.621
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:35.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:35.691
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 09/04/23 16:11:35.718
Sep  4 16:11:35.719: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7800 create -f -'
Sep  4 16:11:36.391: INFO: stderr: ""
Sep  4 16:11:36.391: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 09/04/23 16:11:36.391
Sep  4 16:11:36.391: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7800 diff -f -'
Sep  4 16:11:36.623: INFO: rc: 1
Sep  4 16:11:36.623: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7800 delete -f -'
Sep  4 16:11:36.726: INFO: stderr: ""
Sep  4 16:11:36.726: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  4 16:11:36.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7800" for this suite. 09/04/23 16:11:36.753
------------------------------
• [1.149 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:11:35.62
    Sep  4 16:11:35.620: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 09/04/23 16:11:35.621
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:35.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:35.691
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 09/04/23 16:11:35.718
    Sep  4 16:11:35.719: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7800 create -f -'
    Sep  4 16:11:36.391: INFO: stderr: ""
    Sep  4 16:11:36.391: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 09/04/23 16:11:36.391
    Sep  4 16:11:36.391: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7800 diff -f -'
    Sep  4 16:11:36.623: INFO: rc: 1
    Sep  4 16:11:36.623: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7800 delete -f -'
    Sep  4 16:11:36.726: INFO: stderr: ""
    Sep  4 16:11:36.726: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:11:36.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7800" for this suite. 09/04/23 16:11:36.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:11:36.77
Sep  4 16:11:36.771: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 09/04/23 16:11:36.771
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:36.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:36.843
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Sep  4 16:11:36.892: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs0a2112e9-c94f-47eb-9937-d1898bcb1782" in namespace "kubelet-test-5537" to be "running and ready"
Sep  4 16:11:36.908: INFO: Pod "busybox-readonly-fs0a2112e9-c94f-47eb-9937-d1898bcb1782": Phase="Pending", Reason="", readiness=false. Elapsed: 15.615483ms
Sep  4 16:11:36.908: INFO: The phase of Pod busybox-readonly-fs0a2112e9-c94f-47eb-9937-d1898bcb1782 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 16:11:38.924: INFO: Pod "busybox-readonly-fs0a2112e9-c94f-47eb-9937-d1898bcb1782": Phase="Running", Reason="", readiness=true. Elapsed: 2.032140242s
Sep  4 16:11:38.924: INFO: The phase of Pod busybox-readonly-fs0a2112e9-c94f-47eb-9937-d1898bcb1782 is Running (Ready = true)
Sep  4 16:11:38.924: INFO: Pod "busybox-readonly-fs0a2112e9-c94f-47eb-9937-d1898bcb1782" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  4 16:11:39.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5537" for this suite. 09/04/23 16:11:39.045
------------------------------
• [2.290 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:11:36.77
    Sep  4 16:11:36.771: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 09/04/23 16:11:36.771
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:36.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:36.843
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Sep  4 16:11:36.892: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs0a2112e9-c94f-47eb-9937-d1898bcb1782" in namespace "kubelet-test-5537" to be "running and ready"
    Sep  4 16:11:36.908: INFO: Pod "busybox-readonly-fs0a2112e9-c94f-47eb-9937-d1898bcb1782": Phase="Pending", Reason="", readiness=false. Elapsed: 15.615483ms
    Sep  4 16:11:36.908: INFO: The phase of Pod busybox-readonly-fs0a2112e9-c94f-47eb-9937-d1898bcb1782 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 16:11:38.924: INFO: Pod "busybox-readonly-fs0a2112e9-c94f-47eb-9937-d1898bcb1782": Phase="Running", Reason="", readiness=true. Elapsed: 2.032140242s
    Sep  4 16:11:38.924: INFO: The phase of Pod busybox-readonly-fs0a2112e9-c94f-47eb-9937-d1898bcb1782 is Running (Ready = true)
    Sep  4 16:11:38.924: INFO: Pod "busybox-readonly-fs0a2112e9-c94f-47eb-9937-d1898bcb1782" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:11:39.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5537" for this suite. 09/04/23 16:11:39.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:11:39.062
Sep  4 16:11:39.062: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 09/04/23 16:11:39.062
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:39.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:39.135
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  4 16:11:39.210: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  4 16:12:39.353: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:12:39.367
Sep  4 16:12:39.367: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path 09/04/23 16:12:39.368
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:12:39.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:12:39.437
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 09/04/23 16:12:39.463
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/04/23 16:12:39.463
Sep  4 16:12:39.483: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3929" to be "running"
Sep  4 16:12:39.498: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 14.319972ms
Sep  4 16:12:41.513: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.029898263s
Sep  4 16:12:41.513: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/04/23 16:12:41.528
Sep  4 16:12:41.546: INFO: found a healthy node: shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Sep  4 16:12:47.787: INFO: pods created so far: [1 1 1]
Sep  4 16:12:47.787: INFO: length of pods created so far: 3
Sep  4 16:12:49.822: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Sep  4 16:12:56.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:12:56.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-3929" for this suite. 09/04/23 16:12:57.043
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-903" for this suite. 09/04/23 16:12:57.059
------------------------------
• [78.012 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:11:39.062
    Sep  4 16:11:39.062: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 09/04/23 16:11:39.062
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:11:39.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:11:39.135
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  4 16:11:39.210: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  4 16:12:39.353: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:12:39.367
    Sep  4 16:12:39.367: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption-path 09/04/23 16:12:39.368
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:12:39.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:12:39.437
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 09/04/23 16:12:39.463
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/04/23 16:12:39.463
    Sep  4 16:12:39.483: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3929" to be "running"
    Sep  4 16:12:39.498: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 14.319972ms
    Sep  4 16:12:41.513: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.029898263s
    Sep  4 16:12:41.513: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/04/23 16:12:41.528
    Sep  4 16:12:41.546: INFO: found a healthy node: shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Sep  4 16:12:47.787: INFO: pods created so far: [1 1 1]
    Sep  4 16:12:47.787: INFO: length of pods created so far: 3
    Sep  4 16:12:49.822: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:12:56.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:12:56.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-3929" for this suite. 09/04/23 16:12:57.043
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-903" for this suite. 09/04/23 16:12:57.059
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:12:57.074
Sep  4 16:12:57.074: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 09/04/23 16:12:57.075
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:12:57.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:12:57.145
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Sep  4 16:12:57.172: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod 09/04/23 16:12:57.173
STEP: submitting the pod to kubernetes 09/04/23 16:12:57.173
Sep  4 16:12:57.193: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-94962e03-c07a-4453-810e-4c140d326683" in namespace "pods-3572" to be "running and ready"
Sep  4 16:12:57.207: INFO: Pod "pod-exec-websocket-94962e03-c07a-4453-810e-4c140d326683": Phase="Pending", Reason="", readiness=false. Elapsed: 13.575032ms
Sep  4 16:12:57.207: INFO: The phase of Pod pod-exec-websocket-94962e03-c07a-4453-810e-4c140d326683 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 16:12:59.222: INFO: Pod "pod-exec-websocket-94962e03-c07a-4453-810e-4c140d326683": Phase="Running", Reason="", readiness=true. Elapsed: 2.028731767s
Sep  4 16:12:59.222: INFO: The phase of Pod pod-exec-websocket-94962e03-c07a-4453-810e-4c140d326683 is Running (Ready = true)
Sep  4 16:12:59.222: INFO: Pod "pod-exec-websocket-94962e03-c07a-4453-810e-4c140d326683" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 16:12:59.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3572" for this suite. 09/04/23 16:12:59.558
------------------------------
• [2.500 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:12:57.074
    Sep  4 16:12:57.074: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 09/04/23 16:12:57.075
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:12:57.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:12:57.145
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Sep  4 16:12:57.172: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: creating the pod 09/04/23 16:12:57.173
    STEP: submitting the pod to kubernetes 09/04/23 16:12:57.173
    Sep  4 16:12:57.193: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-94962e03-c07a-4453-810e-4c140d326683" in namespace "pods-3572" to be "running and ready"
    Sep  4 16:12:57.207: INFO: Pod "pod-exec-websocket-94962e03-c07a-4453-810e-4c140d326683": Phase="Pending", Reason="", readiness=false. Elapsed: 13.575032ms
    Sep  4 16:12:57.207: INFO: The phase of Pod pod-exec-websocket-94962e03-c07a-4453-810e-4c140d326683 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 16:12:59.222: INFO: Pod "pod-exec-websocket-94962e03-c07a-4453-810e-4c140d326683": Phase="Running", Reason="", readiness=true. Elapsed: 2.028731767s
    Sep  4 16:12:59.222: INFO: The phase of Pod pod-exec-websocket-94962e03-c07a-4453-810e-4c140d326683 is Running (Ready = true)
    Sep  4 16:12:59.222: INFO: Pod "pod-exec-websocket-94962e03-c07a-4453-810e-4c140d326683" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:12:59.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3572" for this suite. 09/04/23 16:12:59.558
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:12:59.574
Sep  4 16:12:59.574: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 09/04/23 16:12:59.575
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:12:59.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:12:59.644
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 09/04/23 16:12:59.671
Sep  4 16:12:59.690: INFO: Waiting up to 5m0s for pod "pod-jf5h9" in namespace "pods-3396" to be "running"
Sep  4 16:12:59.704: INFO: Pod "pod-jf5h9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.858232ms
Sep  4 16:13:01.718: INFO: Pod "pod-jf5h9": Phase="Running", Reason="", readiness=true. Elapsed: 2.028371387s
Sep  4 16:13:01.718: INFO: Pod "pod-jf5h9" satisfied condition "running"
STEP: patching /status 09/04/23 16:13:01.719
Sep  4 16:13:01.736: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  4 16:13:01.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3396" for this suite. 09/04/23 16:13:01.763
------------------------------
• [2.204 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:12:59.574
    Sep  4 16:12:59.574: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 09/04/23 16:12:59.575
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:12:59.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:12:59.644
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 09/04/23 16:12:59.671
    Sep  4 16:12:59.690: INFO: Waiting up to 5m0s for pod "pod-jf5h9" in namespace "pods-3396" to be "running"
    Sep  4 16:12:59.704: INFO: Pod "pod-jf5h9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.858232ms
    Sep  4 16:13:01.718: INFO: Pod "pod-jf5h9": Phase="Running", Reason="", readiness=true. Elapsed: 2.028371387s
    Sep  4 16:13:01.718: INFO: Pod "pod-jf5h9" satisfied condition "running"
    STEP: patching /status 09/04/23 16:13:01.719
    Sep  4 16:13:01.736: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:13:01.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3396" for this suite. 09/04/23 16:13:01.763
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:13:01.779
Sep  4 16:13:01.779: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 16:13:01.78
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:01.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:01.85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Sep  4 16:13:01.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:13:01.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6625" for this suite. 09/04/23 16:13:02.002
------------------------------
• [0.243 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:13:01.779
    Sep  4 16:13:01.779: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 16:13:01.78
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:01.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:01.85
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Sep  4 16:13:01.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:13:01.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6625" for this suite. 09/04/23 16:13:02.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:13:02.022
Sep  4 16:13:02.022: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context 09/04/23 16:13:02.023
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:02.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:02.094
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/04/23 16:13:02.12
Sep  4 16:13:02.139: INFO: Waiting up to 5m0s for pod "security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e" in namespace "security-context-2613" to be "Succeeded or Failed"
Sep  4 16:13:02.153: INFO: Pod "security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.832296ms
Sep  4 16:13:04.169: INFO: Pod "security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02990919s
Sep  4 16:13:06.168: INFO: Pod "security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029204665s
STEP: Saw pod success 09/04/23 16:13:06.169
Sep  4 16:13:06.169: INFO: Pod "security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e" satisfied condition "Succeeded or Failed"
Sep  4 16:13:06.184: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e container test-container: <nil>
STEP: delete the pod 09/04/23 16:13:06.22
Sep  4 16:13:06.238: INFO: Waiting for pod security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e to disappear
Sep  4 16:13:06.252: INFO: Pod security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  4 16:13:06.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-2613" for this suite. 09/04/23 16:13:06.279
------------------------------
• [4.273 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:13:02.022
    Sep  4 16:13:02.022: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context 09/04/23 16:13:02.023
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:02.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:02.094
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/04/23 16:13:02.12
    Sep  4 16:13:02.139: INFO: Waiting up to 5m0s for pod "security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e" in namespace "security-context-2613" to be "Succeeded or Failed"
    Sep  4 16:13:02.153: INFO: Pod "security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.832296ms
    Sep  4 16:13:04.169: INFO: Pod "security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02990919s
    Sep  4 16:13:06.168: INFO: Pod "security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029204665s
    STEP: Saw pod success 09/04/23 16:13:06.169
    Sep  4 16:13:06.169: INFO: Pod "security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e" satisfied condition "Succeeded or Failed"
    Sep  4 16:13:06.184: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e container test-container: <nil>
    STEP: delete the pod 09/04/23 16:13:06.22
    Sep  4 16:13:06.238: INFO: Waiting for pod security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e to disappear
    Sep  4 16:13:06.252: INFO: Pod security-context-4d4f8550-9b3e-4a89-8d5b-886f4331f15e no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:13:06.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-2613" for this suite. 09/04/23 16:13:06.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:13:06.296
Sep  4 16:13:06.296: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 09/04/23 16:13:06.297
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:06.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:06.369
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/04/23 16:13:06.397
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-f7p8 09/04/23 16:13:06.427
STEP: Creating a pod to test atomic-volume-subpath 09/04/23 16:13:06.427
Sep  4 16:13:06.449: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-f7p8" in namespace "subpath-5908" to be "Succeeded or Failed"
Sep  4 16:13:06.463: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.528063ms
Sep  4 16:13:08.479: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.02991995s
Sep  4 16:13:10.480: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 4.031029998s
Sep  4 16:13:12.478: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 6.029641414s
Sep  4 16:13:14.481: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 8.032143008s
Sep  4 16:13:16.479: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 10.030585193s
Sep  4 16:13:18.481: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 12.031801309s
Sep  4 16:13:20.479: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 14.030207542s
Sep  4 16:13:22.480: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 16.031126243s
Sep  4 16:13:24.479: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 18.029925184s
Sep  4 16:13:26.480: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 20.030760614s
Sep  4 16:13:28.479: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=false. Elapsed: 22.030251339s
Sep  4 16:13:30.483: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.033933349s
STEP: Saw pod success 09/04/23 16:13:30.483
Sep  4 16:13:30.483: INFO: Pod "pod-subpath-test-projected-f7p8" satisfied condition "Succeeded or Failed"
Sep  4 16:13:30.498: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-subpath-test-projected-f7p8 container test-container-subpath-projected-f7p8: <nil>
STEP: delete the pod 09/04/23 16:13:30.535
Sep  4 16:13:30.553: INFO: Waiting for pod pod-subpath-test-projected-f7p8 to disappear
Sep  4 16:13:30.566: INFO: Pod pod-subpath-test-projected-f7p8 no longer exists
STEP: Deleting pod pod-subpath-test-projected-f7p8 09/04/23 16:13:30.566
Sep  4 16:13:30.567: INFO: Deleting pod "pod-subpath-test-projected-f7p8" in namespace "subpath-5908"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  4 16:13:30.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5908" for this suite. 09/04/23 16:13:30.608
------------------------------
• [24.335 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:13:06.296
    Sep  4 16:13:06.296: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 09/04/23 16:13:06.297
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:06.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:06.369
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/04/23 16:13:06.397
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-f7p8 09/04/23 16:13:06.427
    STEP: Creating a pod to test atomic-volume-subpath 09/04/23 16:13:06.427
    Sep  4 16:13:06.449: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-f7p8" in namespace "subpath-5908" to be "Succeeded or Failed"
    Sep  4 16:13:06.463: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.528063ms
    Sep  4 16:13:08.479: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.02991995s
    Sep  4 16:13:10.480: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 4.031029998s
    Sep  4 16:13:12.478: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 6.029641414s
    Sep  4 16:13:14.481: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 8.032143008s
    Sep  4 16:13:16.479: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 10.030585193s
    Sep  4 16:13:18.481: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 12.031801309s
    Sep  4 16:13:20.479: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 14.030207542s
    Sep  4 16:13:22.480: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 16.031126243s
    Sep  4 16:13:24.479: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 18.029925184s
    Sep  4 16:13:26.480: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=true. Elapsed: 20.030760614s
    Sep  4 16:13:28.479: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Running", Reason="", readiness=false. Elapsed: 22.030251339s
    Sep  4 16:13:30.483: INFO: Pod "pod-subpath-test-projected-f7p8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.033933349s
    STEP: Saw pod success 09/04/23 16:13:30.483
    Sep  4 16:13:30.483: INFO: Pod "pod-subpath-test-projected-f7p8" satisfied condition "Succeeded or Failed"
    Sep  4 16:13:30.498: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-subpath-test-projected-f7p8 container test-container-subpath-projected-f7p8: <nil>
    STEP: delete the pod 09/04/23 16:13:30.535
    Sep  4 16:13:30.553: INFO: Waiting for pod pod-subpath-test-projected-f7p8 to disappear
    Sep  4 16:13:30.566: INFO: Pod pod-subpath-test-projected-f7p8 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-f7p8 09/04/23 16:13:30.566
    Sep  4 16:13:30.567: INFO: Deleting pod "pod-subpath-test-projected-f7p8" in namespace "subpath-5908"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:13:30.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5908" for this suite. 09/04/23 16:13:30.608
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:13:30.631
Sep  4 16:13:30.631: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency 09/04/23 16:13:30.632
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:30.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:30.705
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Sep  4 16:13:30.731: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating replication controller svc-latency-rc in namespace svc-latency-294 09/04/23 16:13:30.732
I0904 16:13:30.748971    7754 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-294, replica count: 1
I0904 16:13:31.800725    7754 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0904 16:13:32.800914    7754 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  4 16:13:32.920: INFO: Created: latency-svc-zq4sx
Sep  4 16:13:32.924: INFO: Got endpoints: latency-svc-zq4sx [23.602606ms]
Sep  4 16:13:32.944: INFO: Created: latency-svc-g75js
Sep  4 16:13:32.947: INFO: Created: latency-svc-gvqcj
Sep  4 16:13:32.947: INFO: Got endpoints: latency-svc-g75js [22.772052ms]
Sep  4 16:13:32.950: INFO: Created: latency-svc-6rlcb
Sep  4 16:13:32.951: INFO: Got endpoints: latency-svc-gvqcj [26.471499ms]
Sep  4 16:13:32.955: INFO: Created: latency-svc-q8tz7
Sep  4 16:13:32.955: INFO: Got endpoints: latency-svc-6rlcb [30.902992ms]
Sep  4 16:13:32.958: INFO: Created: latency-svc-hs9t6
Sep  4 16:13:32.958: INFO: Got endpoints: latency-svc-q8tz7 [33.76342ms]
Sep  4 16:13:32.961: INFO: Got endpoints: latency-svc-hs9t6 [36.386969ms]
Sep  4 16:13:32.962: INFO: Created: latency-svc-m7wm2
Sep  4 16:13:32.966: INFO: Got endpoints: latency-svc-m7wm2 [41.217121ms]
Sep  4 16:13:32.966: INFO: Created: latency-svc-mnl8k
Sep  4 16:13:32.969: INFO: Got endpoints: latency-svc-mnl8k [44.198442ms]
Sep  4 16:13:32.972: INFO: Created: latency-svc-7gpfm
Sep  4 16:13:32.975: INFO: Created: latency-svc-pm5bl
Sep  4 16:13:32.976: INFO: Got endpoints: latency-svc-7gpfm [51.660243ms]
Sep  4 16:13:32.978: INFO: Got endpoints: latency-svc-pm5bl [53.569486ms]
Sep  4 16:13:32.978: INFO: Created: latency-svc-826bg
Sep  4 16:13:32.981: INFO: Got endpoints: latency-svc-826bg [55.745682ms]
Sep  4 16:13:32.983: INFO: Created: latency-svc-8gr5k
Sep  4 16:13:32.987: INFO: Created: latency-svc-8mlzz
Sep  4 16:13:32.987: INFO: Got endpoints: latency-svc-8gr5k [62.558026ms]
Sep  4 16:13:32.990: INFO: Got endpoints: latency-svc-8mlzz [65.549093ms]
Sep  4 16:13:32.992: INFO: Created: latency-svc-5khzl
Sep  4 16:13:32.995: INFO: Created: latency-svc-h5bkz
Sep  4 16:13:32.996: INFO: Got endpoints: latency-svc-5khzl [70.90387ms]
Sep  4 16:13:32.999: INFO: Created: latency-svc-qqzzj
Sep  4 16:13:33.001: INFO: Got endpoints: latency-svc-h5bkz [76.217319ms]
Sep  4 16:13:33.004: INFO: Got endpoints: latency-svc-qqzzj [79.298341ms]
Sep  4 16:13:33.006: INFO: Created: latency-svc-mgw4d
Sep  4 16:13:33.016: INFO: Got endpoints: latency-svc-mgw4d [68.846921ms]
Sep  4 16:13:33.019: INFO: Created: latency-svc-zwg97
Sep  4 16:13:33.022: INFO: Got endpoints: latency-svc-zwg97 [71.1847ms]
Sep  4 16:13:33.022: INFO: Created: latency-svc-nksww
Sep  4 16:13:33.025: INFO: Got endpoints: latency-svc-nksww [69.651913ms]
Sep  4 16:13:33.026: INFO: Created: latency-svc-g7slq
Sep  4 16:13:33.029: INFO: Got endpoints: latency-svc-g7slq [70.563308ms]
Sep  4 16:13:33.029: INFO: Created: latency-svc-n7g9z
Sep  4 16:13:33.032: INFO: Got endpoints: latency-svc-n7g9z [71.199655ms]
Sep  4 16:13:33.033: INFO: Created: latency-svc-wb6m2
Sep  4 16:13:33.041: INFO: Got endpoints: latency-svc-wb6m2 [74.990053ms]
Sep  4 16:13:33.041: INFO: Created: latency-svc-9xx7q
Sep  4 16:13:33.044: INFO: Got endpoints: latency-svc-9xx7q [74.841992ms]
Sep  4 16:13:33.045: INFO: Created: latency-svc-5t9kl
Sep  4 16:13:33.046: INFO: Got endpoints: latency-svc-5t9kl [69.959996ms]
Sep  4 16:13:33.049: INFO: Created: latency-svc-x8ldn
Sep  4 16:13:33.053: INFO: Got endpoints: latency-svc-x8ldn [74.457752ms]
Sep  4 16:13:33.053: INFO: Created: latency-svc-gbpjn
Sep  4 16:13:33.056: INFO: Got endpoints: latency-svc-gbpjn [75.777774ms]
Sep  4 16:13:33.057: INFO: Created: latency-svc-7jk97
Sep  4 16:13:33.062: INFO: Got endpoints: latency-svc-7jk97 [74.910569ms]
Sep  4 16:13:33.114: INFO: Created: latency-svc-kq74s
Sep  4 16:13:33.114: INFO: Created: latency-svc-pdcrq
Sep  4 16:13:33.114: INFO: Created: latency-svc-hr6v7
Sep  4 16:13:33.114: INFO: Created: latency-svc-86vpf
Sep  4 16:13:33.115: INFO: Created: latency-svc-8n9wc
Sep  4 16:13:33.115: INFO: Created: latency-svc-g9h76
Sep  4 16:13:33.114: INFO: Created: latency-svc-rtmg9
Sep  4 16:13:33.114: INFO: Created: latency-svc-jhtlv
Sep  4 16:13:33.115: INFO: Created: latency-svc-k77kp
Sep  4 16:13:33.115: INFO: Created: latency-svc-jwkmk
Sep  4 16:13:33.115: INFO: Created: latency-svc-j757d
Sep  4 16:13:33.114: INFO: Created: latency-svc-jb6lv
Sep  4 16:13:33.115: INFO: Created: latency-svc-5mvqk
Sep  4 16:13:33.115: INFO: Created: latency-svc-jddxw
Sep  4 16:13:33.115: INFO: Created: latency-svc-t9fsv
Sep  4 16:13:33.116: INFO: Got endpoints: latency-svc-jwkmk [75.35164ms]
Sep  4 16:13:33.117: INFO: Got endpoints: latency-svc-jddxw [88.232781ms]
Sep  4 16:13:33.117: INFO: Got endpoints: latency-svc-jhtlv [70.951166ms]
Sep  4 16:13:33.117: INFO: Got endpoints: latency-svc-5mvqk [73.735434ms]
Sep  4 16:13:33.118: INFO: Got endpoints: latency-svc-86vpf [64.928884ms]
Sep  4 16:13:33.118: INFO: Got endpoints: latency-svc-j757d [92.580166ms]
Sep  4 16:13:33.125: INFO: Got endpoints: latency-svc-hr6v7 [124.286761ms]
Sep  4 16:13:33.137: INFO: Created: latency-svc-bj8zf
Sep  4 16:13:33.147: INFO: Created: latency-svc-wnhj7
Sep  4 16:13:33.152: INFO: Created: latency-svc-w2kk8
Sep  4 16:13:33.155: INFO: Created: latency-svc-xktns
Sep  4 16:13:33.158: INFO: Created: latency-svc-ndp9r
Sep  4 16:13:33.164: INFO: Created: latency-svc-xmqzw
Sep  4 16:13:33.169: INFO: Created: latency-svc-8pn2c
Sep  4 16:13:33.174: INFO: Got endpoints: latency-svc-kq74s [183.5361ms]
Sep  4 16:13:33.193: INFO: Created: latency-svc-czb4w
Sep  4 16:13:33.224: INFO: Got endpoints: latency-svc-jb6lv [201.639474ms]
Sep  4 16:13:33.242: INFO: Created: latency-svc-9mzjh
Sep  4 16:13:33.277: INFO: Got endpoints: latency-svc-8n9wc [215.043887ms]
Sep  4 16:13:33.297: INFO: Created: latency-svc-r97hz
Sep  4 16:13:33.323: INFO: Got endpoints: latency-svc-rtmg9 [319.413531ms]
Sep  4 16:13:33.343: INFO: Created: latency-svc-nn7bc
Sep  4 16:13:33.376: INFO: Got endpoints: latency-svc-g9h76 [379.90102ms]
Sep  4 16:13:33.393: INFO: Created: latency-svc-2db59
Sep  4 16:13:33.426: INFO: Got endpoints: latency-svc-t9fsv [369.297852ms]
Sep  4 16:13:33.445: INFO: Created: latency-svc-hzzdd
Sep  4 16:13:33.475: INFO: Got endpoints: latency-svc-pdcrq [459.135764ms]
Sep  4 16:13:33.495: INFO: Created: latency-svc-p9r6p
Sep  4 16:13:33.524: INFO: Got endpoints: latency-svc-k77kp [491.621666ms]
Sep  4 16:13:33.543: INFO: Created: latency-svc-8946c
Sep  4 16:13:33.575: INFO: Got endpoints: latency-svc-bj8zf [458.38755ms]
Sep  4 16:13:33.593: INFO: Created: latency-svc-2z2kx
Sep  4 16:13:33.624: INFO: Got endpoints: latency-svc-wnhj7 [506.894651ms]
Sep  4 16:13:33.643: INFO: Created: latency-svc-8vdt6
Sep  4 16:13:33.675: INFO: Got endpoints: latency-svc-w2kk8 [557.353046ms]
Sep  4 16:13:33.693: INFO: Created: latency-svc-57pjx
Sep  4 16:13:33.724: INFO: Got endpoints: latency-svc-xktns [606.746685ms]
Sep  4 16:13:33.746: INFO: Created: latency-svc-456vx
Sep  4 16:13:33.774: INFO: Got endpoints: latency-svc-ndp9r [656.60244ms]
Sep  4 16:13:33.794: INFO: Created: latency-svc-wqsdg
Sep  4 16:13:33.824: INFO: Got endpoints: latency-svc-xmqzw [706.686483ms]
Sep  4 16:13:33.844: INFO: Created: latency-svc-jkmr6
Sep  4 16:13:33.880: INFO: Got endpoints: latency-svc-8pn2c [755.077352ms]
Sep  4 16:13:33.899: INFO: Created: latency-svc-mhf9b
Sep  4 16:13:33.923: INFO: Got endpoints: latency-svc-czb4w [749.585453ms]
Sep  4 16:13:33.942: INFO: Created: latency-svc-4s6xz
Sep  4 16:13:33.974: INFO: Got endpoints: latency-svc-9mzjh [750.007698ms]
Sep  4 16:13:33.992: INFO: Created: latency-svc-zjdh7
Sep  4 16:13:34.025: INFO: Got endpoints: latency-svc-r97hz [748.098309ms]
Sep  4 16:13:34.046: INFO: Created: latency-svc-rt8xf
Sep  4 16:13:34.075: INFO: Got endpoints: latency-svc-nn7bc [751.589686ms]
Sep  4 16:13:34.094: INFO: Created: latency-svc-jslhf
Sep  4 16:13:34.124: INFO: Got endpoints: latency-svc-2db59 [748.634109ms]
Sep  4 16:13:34.145: INFO: Created: latency-svc-n5hxp
Sep  4 16:13:34.174: INFO: Got endpoints: latency-svc-hzzdd [748.586325ms]
Sep  4 16:13:34.193: INFO: Created: latency-svc-phjqp
Sep  4 16:13:34.224: INFO: Got endpoints: latency-svc-p9r6p [748.843846ms]
Sep  4 16:13:34.244: INFO: Created: latency-svc-9kj6n
Sep  4 16:13:34.275: INFO: Got endpoints: latency-svc-8946c [751.264516ms]
Sep  4 16:13:34.293: INFO: Created: latency-svc-lh942
Sep  4 16:13:34.325: INFO: Got endpoints: latency-svc-2z2kx [750.055739ms]
Sep  4 16:13:34.344: INFO: Created: latency-svc-wtv9l
Sep  4 16:13:34.376: INFO: Got endpoints: latency-svc-8vdt6 [751.411245ms]
Sep  4 16:13:34.395: INFO: Created: latency-svc-p6dm2
Sep  4 16:13:34.425: INFO: Got endpoints: latency-svc-57pjx [750.791223ms]
Sep  4 16:13:34.445: INFO: Created: latency-svc-zc65p
Sep  4 16:13:34.474: INFO: Got endpoints: latency-svc-456vx [749.947009ms]
Sep  4 16:13:34.495: INFO: Created: latency-svc-gtlwc
Sep  4 16:13:34.524: INFO: Got endpoints: latency-svc-wqsdg [749.753367ms]
Sep  4 16:13:34.544: INFO: Created: latency-svc-zll4c
Sep  4 16:13:34.575: INFO: Got endpoints: latency-svc-jkmr6 [750.820753ms]
Sep  4 16:13:34.603: INFO: Created: latency-svc-p845z
Sep  4 16:13:34.624: INFO: Got endpoints: latency-svc-mhf9b [743.599988ms]
Sep  4 16:13:34.642: INFO: Created: latency-svc-ckmj5
Sep  4 16:13:34.673: INFO: Got endpoints: latency-svc-4s6xz [749.81809ms]
Sep  4 16:13:34.692: INFO: Created: latency-svc-m8k7n
Sep  4 16:13:34.723: INFO: Got endpoints: latency-svc-zjdh7 [749.40109ms]
Sep  4 16:13:34.742: INFO: Created: latency-svc-n7bdm
Sep  4 16:13:34.774: INFO: Got endpoints: latency-svc-rt8xf [748.041264ms]
Sep  4 16:13:34.797: INFO: Created: latency-svc-5vp6r
Sep  4 16:13:34.825: INFO: Got endpoints: latency-svc-jslhf [749.505688ms]
Sep  4 16:13:34.844: INFO: Created: latency-svc-r6gdc
Sep  4 16:13:34.873: INFO: Got endpoints: latency-svc-n5hxp [748.984467ms]
Sep  4 16:13:34.894: INFO: Created: latency-svc-cjbwr
Sep  4 16:13:34.924: INFO: Got endpoints: latency-svc-phjqp [749.928579ms]
Sep  4 16:13:34.943: INFO: Created: latency-svc-48p6m
Sep  4 16:13:34.976: INFO: Got endpoints: latency-svc-9kj6n [752.05559ms]
Sep  4 16:13:34.996: INFO: Created: latency-svc-v24ms
Sep  4 16:13:35.024: INFO: Got endpoints: latency-svc-lh942 [749.238218ms]
Sep  4 16:13:35.044: INFO: Created: latency-svc-76r7b
Sep  4 16:13:35.074: INFO: Got endpoints: latency-svc-wtv9l [749.285242ms]
Sep  4 16:13:35.096: INFO: Created: latency-svc-nl7hf
Sep  4 16:13:35.124: INFO: Got endpoints: latency-svc-p6dm2 [748.066812ms]
Sep  4 16:13:35.143: INFO: Created: latency-svc-4d5qk
Sep  4 16:13:35.175: INFO: Got endpoints: latency-svc-zc65p [749.21089ms]
Sep  4 16:13:35.193: INFO: Created: latency-svc-mvkbv
Sep  4 16:13:35.224: INFO: Got endpoints: latency-svc-gtlwc [749.816181ms]
Sep  4 16:13:35.243: INFO: Created: latency-svc-rpf6h
Sep  4 16:13:35.274: INFO: Got endpoints: latency-svc-zll4c [749.965599ms]
Sep  4 16:13:35.294: INFO: Created: latency-svc-6fjv9
Sep  4 16:13:35.325: INFO: Got endpoints: latency-svc-p845z [749.651116ms]
Sep  4 16:13:35.345: INFO: Created: latency-svc-w5c6s
Sep  4 16:13:35.378: INFO: Got endpoints: latency-svc-ckmj5 [753.650918ms]
Sep  4 16:13:35.396: INFO: Created: latency-svc-2zj45
Sep  4 16:13:35.423: INFO: Got endpoints: latency-svc-m8k7n [749.743761ms]
Sep  4 16:13:35.442: INFO: Created: latency-svc-ztv6l
Sep  4 16:13:35.474: INFO: Got endpoints: latency-svc-n7bdm [751.181918ms]
Sep  4 16:13:35.493: INFO: Created: latency-svc-pzvjf
Sep  4 16:13:35.523: INFO: Got endpoints: latency-svc-5vp6r [749.658586ms]
Sep  4 16:13:35.544: INFO: Created: latency-svc-sjjb6
Sep  4 16:13:35.573: INFO: Got endpoints: latency-svc-r6gdc [748.539599ms]
Sep  4 16:13:35.592: INFO: Created: latency-svc-j7cgl
Sep  4 16:13:35.625: INFO: Got endpoints: latency-svc-cjbwr [752.224514ms]
Sep  4 16:13:35.644: INFO: Created: latency-svc-7sdvl
Sep  4 16:13:35.674: INFO: Got endpoints: latency-svc-48p6m [749.993937ms]
Sep  4 16:13:35.694: INFO: Created: latency-svc-s82v2
Sep  4 16:13:35.723: INFO: Got endpoints: latency-svc-v24ms [747.062611ms]
Sep  4 16:13:35.744: INFO: Created: latency-svc-htvvr
Sep  4 16:13:35.774: INFO: Got endpoints: latency-svc-76r7b [749.398029ms]
Sep  4 16:13:35.793: INFO: Created: latency-svc-zfxd7
Sep  4 16:13:35.824: INFO: Got endpoints: latency-svc-nl7hf [750.008237ms]
Sep  4 16:13:35.844: INFO: Created: latency-svc-fdr7w
Sep  4 16:13:35.875: INFO: Got endpoints: latency-svc-4d5qk [751.103826ms]
Sep  4 16:13:35.895: INFO: Created: latency-svc-44tfw
Sep  4 16:13:36.507: INFO: Got endpoints: latency-svc-mvkbv [1.332683491s]
Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-6fjv9 [1.235581827s]
Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-rpf6h [1.285642512s]
Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-2zj45 [1.132158742s]
Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-w5c6s [1.185197497s]
Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-pzvjf [1.035605309s]
Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-ztv6l [1.086968919s]
Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-j7cgl [936.983068ms]
Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-sjjb6 [986.928653ms]
Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-s82v2 [835.949956ms]
Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-htvvr [786.907217ms]
Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-7sdvl [884.804569ms]
Sep  4 16:13:36.524: INFO: Got endpoints: latency-svc-zfxd7 [749.912152ms]
Sep  4 16:13:36.527: INFO: Created: latency-svc-8xrrs
Sep  4 16:13:36.532: INFO: Created: latency-svc-htsjf
Sep  4 16:13:36.536: INFO: Created: latency-svc-g7dcl
Sep  4 16:13:36.540: INFO: Created: latency-svc-zcd4v
Sep  4 16:13:36.544: INFO: Created: latency-svc-rxpb5
Sep  4 16:13:36.547: INFO: Created: latency-svc-twr2z
Sep  4 16:13:36.550: INFO: Created: latency-svc-mpd6j
Sep  4 16:13:36.554: INFO: Created: latency-svc-khd5z
Sep  4 16:13:36.558: INFO: Created: latency-svc-r8bng
Sep  4 16:13:36.563: INFO: Created: latency-svc-kzg8j
Sep  4 16:13:36.567: INFO: Created: latency-svc-wx8zl
Sep  4 16:13:36.570: INFO: Created: latency-svc-4s7b8
Sep  4 16:13:36.575: INFO: Got endpoints: latency-svc-fdr7w [750.564776ms]
Sep  4 16:13:36.575: INFO: Created: latency-svc-dn8s2
Sep  4 16:13:36.594: INFO: Created: latency-svc-bm6tr
Sep  4 16:13:36.624: INFO: Got endpoints: latency-svc-44tfw [749.539431ms]
Sep  4 16:13:36.643: INFO: Created: latency-svc-x2p54
Sep  4 16:13:36.674: INFO: Got endpoints: latency-svc-8xrrs [166.99087ms]
Sep  4 16:13:36.694: INFO: Created: latency-svc-6pgbr
Sep  4 16:13:36.724: INFO: Got endpoints: latency-svc-htsjf [214.294163ms]
Sep  4 16:13:36.744: INFO: Created: latency-svc-b989s
Sep  4 16:13:36.774: INFO: Got endpoints: latency-svc-g7dcl [264.580265ms]
Sep  4 16:13:36.794: INFO: Created: latency-svc-n77hr
Sep  4 16:13:36.824: INFO: Got endpoints: latency-svc-zcd4v [314.41572ms]
Sep  4 16:13:36.846: INFO: Created: latency-svc-g6g6c
Sep  4 16:13:36.876: INFO: Got endpoints: latency-svc-rxpb5 [366.045655ms]
Sep  4 16:13:36.896: INFO: Created: latency-svc-cwlkl
Sep  4 16:13:36.923: INFO: Got endpoints: latency-svc-twr2z [413.511298ms]
Sep  4 16:13:36.945: INFO: Created: latency-svc-wf2bx
Sep  4 16:13:36.974: INFO: Got endpoints: latency-svc-mpd6j [463.874001ms]
Sep  4 16:13:36.993: INFO: Created: latency-svc-kkb5c
Sep  4 16:13:37.027: INFO: Got endpoints: latency-svc-khd5z [516.94178ms]
Sep  4 16:13:37.048: INFO: Created: latency-svc-6vlrj
Sep  4 16:13:37.074: INFO: Got endpoints: latency-svc-r8bng [564.207465ms]
Sep  4 16:13:37.094: INFO: Created: latency-svc-25n5b
Sep  4 16:13:37.124: INFO: Got endpoints: latency-svc-kzg8j [613.996427ms]
Sep  4 16:13:37.144: INFO: Created: latency-svc-g5s7l
Sep  4 16:13:37.174: INFO: Got endpoints: latency-svc-wx8zl [664.136932ms]
Sep  4 16:13:37.195: INFO: Created: latency-svc-ncq4x
Sep  4 16:13:37.224: INFO: Got endpoints: latency-svc-4s7b8 [713.394552ms]
Sep  4 16:13:37.244: INFO: Created: latency-svc-678gt
Sep  4 16:13:37.274: INFO: Got endpoints: latency-svc-dn8s2 [750.620163ms]
Sep  4 16:13:37.294: INFO: Created: latency-svc-tx7lj
Sep  4 16:13:37.324: INFO: Got endpoints: latency-svc-bm6tr [749.432527ms]
Sep  4 16:13:37.344: INFO: Created: latency-svc-6r4j7
Sep  4 16:13:37.376: INFO: Got endpoints: latency-svc-x2p54 [751.01817ms]
Sep  4 16:13:37.396: INFO: Created: latency-svc-pdm4v
Sep  4 16:13:37.425: INFO: Got endpoints: latency-svc-6pgbr [750.366273ms]
Sep  4 16:13:37.445: INFO: Created: latency-svc-kfbrv
Sep  4 16:13:37.475: INFO: Got endpoints: latency-svc-b989s [750.503805ms]
Sep  4 16:13:37.494: INFO: Created: latency-svc-s2m4w
Sep  4 16:13:37.524: INFO: Got endpoints: latency-svc-n77hr [749.216584ms]
Sep  4 16:13:37.544: INFO: Created: latency-svc-4f6qd
Sep  4 16:13:37.574: INFO: Got endpoints: latency-svc-g6g6c [749.410091ms]
Sep  4 16:13:37.593: INFO: Created: latency-svc-zm2p8
Sep  4 16:13:37.624: INFO: Got endpoints: latency-svc-cwlkl [747.668293ms]
Sep  4 16:13:37.643: INFO: Created: latency-svc-6fhf8
Sep  4 16:13:37.674: INFO: Got endpoints: latency-svc-wf2bx [750.935619ms]
Sep  4 16:13:37.694: INFO: Created: latency-svc-89gnj
Sep  4 16:13:37.725: INFO: Got endpoints: latency-svc-kkb5c [751.214859ms]
Sep  4 16:13:37.747: INFO: Created: latency-svc-rjf6c
Sep  4 16:13:37.774: INFO: Got endpoints: latency-svc-6vlrj [746.853469ms]
Sep  4 16:13:37.793: INFO: Created: latency-svc-pj7j9
Sep  4 16:13:37.824: INFO: Got endpoints: latency-svc-25n5b [749.978557ms]
Sep  4 16:13:37.843: INFO: Created: latency-svc-pb5jl
Sep  4 16:13:37.874: INFO: Got endpoints: latency-svc-g5s7l [749.944021ms]
Sep  4 16:13:37.895: INFO: Created: latency-svc-t4lbj
Sep  4 16:13:37.924: INFO: Got endpoints: latency-svc-ncq4x [749.603338ms]
Sep  4 16:13:37.945: INFO: Created: latency-svc-xcvgf
Sep  4 16:13:37.974: INFO: Got endpoints: latency-svc-678gt [750.68356ms]
Sep  4 16:13:37.994: INFO: Created: latency-svc-kv6vq
Sep  4 16:13:38.029: INFO: Got endpoints: latency-svc-tx7lj [754.734331ms]
Sep  4 16:13:38.049: INFO: Created: latency-svc-6gr7x
Sep  4 16:13:38.075: INFO: Got endpoints: latency-svc-6r4j7 [750.574269ms]
Sep  4 16:13:38.094: INFO: Created: latency-svc-jvzh7
Sep  4 16:13:38.124: INFO: Got endpoints: latency-svc-pdm4v [748.739426ms]
Sep  4 16:13:38.143: INFO: Created: latency-svc-pwp5m
Sep  4 16:13:38.174: INFO: Got endpoints: latency-svc-kfbrv [749.540611ms]
Sep  4 16:13:38.194: INFO: Created: latency-svc-6kjh5
Sep  4 16:13:38.224: INFO: Got endpoints: latency-svc-s2m4w [749.413576ms]
Sep  4 16:13:38.244: INFO: Created: latency-svc-7ts8n
Sep  4 16:13:38.273: INFO: Got endpoints: latency-svc-4f6qd [749.543084ms]
Sep  4 16:13:38.294: INFO: Created: latency-svc-fv9bz
Sep  4 16:13:38.324: INFO: Got endpoints: latency-svc-zm2p8 [750.215989ms]
Sep  4 16:13:38.345: INFO: Created: latency-svc-d4b92
Sep  4 16:13:38.374: INFO: Got endpoints: latency-svc-6fhf8 [750.360397ms]
Sep  4 16:13:38.394: INFO: Created: latency-svc-vbnsg
Sep  4 16:13:38.425: INFO: Got endpoints: latency-svc-89gnj [750.156909ms]
Sep  4 16:13:38.444: INFO: Created: latency-svc-2slwm
Sep  4 16:13:38.473: INFO: Got endpoints: latency-svc-rjf6c [748.290505ms]
Sep  4 16:13:38.493: INFO: Created: latency-svc-mc65s
Sep  4 16:13:38.523: INFO: Got endpoints: latency-svc-pj7j9 [748.732938ms]
Sep  4 16:13:38.543: INFO: Created: latency-svc-65gvh
Sep  4 16:13:38.575: INFO: Got endpoints: latency-svc-pb5jl [751.054298ms]
Sep  4 16:13:38.595: INFO: Created: latency-svc-htzz4
Sep  4 16:13:38.624: INFO: Got endpoints: latency-svc-t4lbj [749.714023ms]
Sep  4 16:13:38.644: INFO: Created: latency-svc-kt76v
Sep  4 16:13:38.674: INFO: Got endpoints: latency-svc-xcvgf [750.132616ms]
Sep  4 16:13:38.694: INFO: Created: latency-svc-jsdsb
Sep  4 16:13:38.724: INFO: Got endpoints: latency-svc-kv6vq [749.689523ms]
Sep  4 16:13:38.744: INFO: Created: latency-svc-zsnzp
Sep  4 16:13:38.774: INFO: Got endpoints: latency-svc-6gr7x [744.675762ms]
Sep  4 16:13:38.795: INFO: Created: latency-svc-j5snz
Sep  4 16:13:38.824: INFO: Got endpoints: latency-svc-jvzh7 [749.121439ms]
Sep  4 16:13:38.843: INFO: Created: latency-svc-cj84k
Sep  4 16:13:38.874: INFO: Got endpoints: latency-svc-pwp5m [749.575436ms]
Sep  4 16:13:38.893: INFO: Created: latency-svc-gmz94
Sep  4 16:13:38.925: INFO: Got endpoints: latency-svc-6kjh5 [750.788121ms]
Sep  4 16:13:38.944: INFO: Created: latency-svc-pkmdf
Sep  4 16:13:38.974: INFO: Got endpoints: latency-svc-7ts8n [750.04643ms]
Sep  4 16:13:38.994: INFO: Created: latency-svc-9ngqm
Sep  4 16:13:39.029: INFO: Got endpoints: latency-svc-fv9bz [755.900811ms]
Sep  4 16:13:39.049: INFO: Created: latency-svc-h8prs
Sep  4 16:13:39.077: INFO: Got endpoints: latency-svc-d4b92 [752.947188ms]
Sep  4 16:13:39.096: INFO: Created: latency-svc-4th7t
Sep  4 16:13:39.124: INFO: Got endpoints: latency-svc-vbnsg [750.304093ms]
Sep  4 16:13:39.147: INFO: Created: latency-svc-mkvhh
Sep  4 16:13:39.180: INFO: Got endpoints: latency-svc-2slwm [755.807567ms]
Sep  4 16:13:39.201: INFO: Created: latency-svc-jkwkd
Sep  4 16:13:39.224: INFO: Got endpoints: latency-svc-mc65s [750.257819ms]
Sep  4 16:13:39.243: INFO: Created: latency-svc-8j2jp
Sep  4 16:13:39.275: INFO: Got endpoints: latency-svc-65gvh [752.465901ms]
Sep  4 16:13:39.296: INFO: Created: latency-svc-cbl8l
Sep  4 16:13:39.326: INFO: Got endpoints: latency-svc-htzz4 [750.441218ms]
Sep  4 16:13:39.345: INFO: Created: latency-svc-s9rwd
Sep  4 16:13:39.377: INFO: Got endpoints: latency-svc-kt76v [753.289435ms]
Sep  4 16:13:39.396: INFO: Created: latency-svc-lrbzs
Sep  4 16:13:39.424: INFO: Got endpoints: latency-svc-jsdsb [749.941952ms]
Sep  4 16:13:39.443: INFO: Created: latency-svc-lbwmp
Sep  4 16:13:39.474: INFO: Got endpoints: latency-svc-zsnzp [749.48498ms]
Sep  4 16:13:39.493: INFO: Created: latency-svc-44trf
Sep  4 16:13:39.524: INFO: Got endpoints: latency-svc-j5snz [750.147692ms]
Sep  4 16:13:39.544: INFO: Created: latency-svc-qnzk2
Sep  4 16:13:39.575: INFO: Got endpoints: latency-svc-cj84k [750.952573ms]
Sep  4 16:13:39.595: INFO: Created: latency-svc-bq6qr
Sep  4 16:13:39.625: INFO: Got endpoints: latency-svc-gmz94 [751.379491ms]
Sep  4 16:13:39.646: INFO: Created: latency-svc-wqg77
Sep  4 16:13:39.675: INFO: Got endpoints: latency-svc-pkmdf [750.017419ms]
Sep  4 16:13:39.697: INFO: Created: latency-svc-gc8pq
Sep  4 16:13:39.724: INFO: Got endpoints: latency-svc-9ngqm [749.731708ms]
Sep  4 16:13:39.743: INFO: Created: latency-svc-67kgd
Sep  4 16:13:39.774: INFO: Got endpoints: latency-svc-h8prs [745.012518ms]
Sep  4 16:13:39.794: INFO: Created: latency-svc-wlwnj
Sep  4 16:13:39.824: INFO: Got endpoints: latency-svc-4th7t [747.123817ms]
Sep  4 16:13:39.844: INFO: Created: latency-svc-694p7
Sep  4 16:13:39.874: INFO: Got endpoints: latency-svc-mkvhh [749.911391ms]
Sep  4 16:13:39.893: INFO: Created: latency-svc-7j6jw
Sep  4 16:13:39.926: INFO: Got endpoints: latency-svc-jkwkd [745.940254ms]
Sep  4 16:13:39.946: INFO: Created: latency-svc-c9czp
Sep  4 16:13:39.975: INFO: Got endpoints: latency-svc-8j2jp [750.785353ms]
Sep  4 16:13:39.993: INFO: Created: latency-svc-7cbbk
Sep  4 16:13:40.024: INFO: Got endpoints: latency-svc-cbl8l [748.933864ms]
Sep  4 16:13:40.043: INFO: Created: latency-svc-cmw7n
Sep  4 16:13:40.075: INFO: Got endpoints: latency-svc-s9rwd [749.015734ms]
Sep  4 16:13:40.101: INFO: Created: latency-svc-h868z
Sep  4 16:13:40.126: INFO: Got endpoints: latency-svc-lrbzs [748.259056ms]
Sep  4 16:13:40.144: INFO: Created: latency-svc-5d978
Sep  4 16:13:40.174: INFO: Got endpoints: latency-svc-lbwmp [749.713234ms]
Sep  4 16:13:40.193: INFO: Created: latency-svc-g4jlx
Sep  4 16:13:40.224: INFO: Got endpoints: latency-svc-44trf [750.702158ms]
Sep  4 16:13:40.243: INFO: Created: latency-svc-8llwl
Sep  4 16:13:40.274: INFO: Got endpoints: latency-svc-qnzk2 [750.408967ms]
Sep  4 16:13:40.297: INFO: Created: latency-svc-vm2mj
Sep  4 16:13:40.324: INFO: Got endpoints: latency-svc-bq6qr [749.398778ms]
Sep  4 16:13:40.344: INFO: Created: latency-svc-m4mgs
Sep  4 16:13:40.379: INFO: Got endpoints: latency-svc-wqg77 [753.224371ms]
Sep  4 16:13:40.398: INFO: Created: latency-svc-df8rn
Sep  4 16:13:40.424: INFO: Got endpoints: latency-svc-gc8pq [749.151957ms]
Sep  4 16:13:40.444: INFO: Created: latency-svc-m2nww
Sep  4 16:13:40.475: INFO: Got endpoints: latency-svc-67kgd [751.186414ms]
Sep  4 16:13:40.494: INFO: Created: latency-svc-rh7f7
Sep  4 16:13:40.526: INFO: Got endpoints: latency-svc-wlwnj [752.14285ms]
Sep  4 16:13:40.545: INFO: Created: latency-svc-sw584
Sep  4 16:13:40.574: INFO: Got endpoints: latency-svc-694p7 [750.107518ms]
Sep  4 16:13:40.593: INFO: Created: latency-svc-tmg45
Sep  4 16:13:40.624: INFO: Got endpoints: latency-svc-7j6jw [749.950152ms]
Sep  4 16:13:40.644: INFO: Created: latency-svc-5cp2h
Sep  4 16:13:40.674: INFO: Got endpoints: latency-svc-c9czp [747.967011ms]
Sep  4 16:13:40.693: INFO: Created: latency-svc-nbz2p
Sep  4 16:13:40.725: INFO: Got endpoints: latency-svc-7cbbk [750.011477ms]
Sep  4 16:13:40.744: INFO: Created: latency-svc-dx4hm
Sep  4 16:13:40.774: INFO: Got endpoints: latency-svc-cmw7n [750.068396ms]
Sep  4 16:13:40.825: INFO: Got endpoints: latency-svc-h868z [749.97027ms]
Sep  4 16:13:40.874: INFO: Got endpoints: latency-svc-5d978 [748.82426ms]
Sep  4 16:13:40.925: INFO: Got endpoints: latency-svc-g4jlx [750.779092ms]
Sep  4 16:13:40.975: INFO: Got endpoints: latency-svc-8llwl [750.823893ms]
Sep  4 16:13:41.024: INFO: Got endpoints: latency-svc-vm2mj [749.681734ms]
Sep  4 16:13:41.074: INFO: Got endpoints: latency-svc-m4mgs [750.113621ms]
Sep  4 16:13:41.125: INFO: Got endpoints: latency-svc-df8rn [746.097888ms]
Sep  4 16:13:41.175: INFO: Got endpoints: latency-svc-m2nww [750.632105ms]
Sep  4 16:13:41.224: INFO: Got endpoints: latency-svc-rh7f7 [748.328407ms]
Sep  4 16:13:41.275: INFO: Got endpoints: latency-svc-sw584 [748.32211ms]
Sep  4 16:13:41.324: INFO: Got endpoints: latency-svc-tmg45 [749.471813ms]
Sep  4 16:13:41.375: INFO: Got endpoints: latency-svc-5cp2h [751.056484ms]
Sep  4 16:13:41.426: INFO: Got endpoints: latency-svc-nbz2p [751.132816ms]
Sep  4 16:13:41.475: INFO: Got endpoints: latency-svc-dx4hm [750.231345ms]
Sep  4 16:13:41.475: INFO: Latencies: [22.772052ms 26.471499ms 30.902992ms 33.76342ms 36.386969ms 41.217121ms 44.198442ms 51.660243ms 53.569486ms 55.745682ms 62.558026ms 64.928884ms 65.549093ms 68.846921ms 69.651913ms 69.959996ms 70.563308ms 70.90387ms 70.951166ms 71.1847ms 71.199655ms 73.735434ms 74.457752ms 74.841992ms 74.910569ms 74.990053ms 75.35164ms 75.777774ms 76.217319ms 79.298341ms 88.232781ms 92.580166ms 124.286761ms 166.99087ms 183.5361ms 201.639474ms 214.294163ms 215.043887ms 264.580265ms 314.41572ms 319.413531ms 366.045655ms 369.297852ms 379.90102ms 413.511298ms 458.38755ms 459.135764ms 463.874001ms 491.621666ms 506.894651ms 516.94178ms 557.353046ms 564.207465ms 606.746685ms 613.996427ms 656.60244ms 664.136932ms 706.686483ms 713.394552ms 743.599988ms 744.675762ms 745.012518ms 745.940254ms 746.097888ms 746.853469ms 747.062611ms 747.123817ms 747.668293ms 747.967011ms 748.041264ms 748.066812ms 748.098309ms 748.259056ms 748.290505ms 748.32211ms 748.328407ms 748.539599ms 748.586325ms 748.634109ms 748.732938ms 748.739426ms 748.82426ms 748.843846ms 748.933864ms 748.984467ms 749.015734ms 749.121439ms 749.151957ms 749.21089ms 749.216584ms 749.238218ms 749.285242ms 749.398029ms 749.398778ms 749.40109ms 749.410091ms 749.413576ms 749.432527ms 749.471813ms 749.48498ms 749.505688ms 749.539431ms 749.540611ms 749.543084ms 749.575436ms 749.585453ms 749.603338ms 749.651116ms 749.658586ms 749.681734ms 749.689523ms 749.713234ms 749.714023ms 749.731708ms 749.743761ms 749.753367ms 749.816181ms 749.81809ms 749.911391ms 749.912152ms 749.928579ms 749.941952ms 749.944021ms 749.947009ms 749.950152ms 749.965599ms 749.97027ms 749.978557ms 749.993937ms 750.007698ms 750.008237ms 750.011477ms 750.017419ms 750.04643ms 750.055739ms 750.068396ms 750.107518ms 750.113621ms 750.132616ms 750.147692ms 750.156909ms 750.215989ms 750.231345ms 750.257819ms 750.304093ms 750.360397ms 750.366273ms 750.408967ms 750.441218ms 750.503805ms 750.564776ms 750.574269ms 750.620163ms 750.632105ms 750.68356ms 750.702158ms 750.779092ms 750.785353ms 750.788121ms 750.791223ms 750.820753ms 750.823893ms 750.935619ms 750.952573ms 751.01817ms 751.054298ms 751.056484ms 751.103826ms 751.132816ms 751.181918ms 751.186414ms 751.214859ms 751.264516ms 751.379491ms 751.411245ms 751.589686ms 752.05559ms 752.14285ms 752.224514ms 752.465901ms 752.947188ms 753.224371ms 753.289435ms 753.650918ms 754.734331ms 755.077352ms 755.807567ms 755.900811ms 786.907217ms 835.949956ms 884.804569ms 936.983068ms 986.928653ms 1.035605309s 1.086968919s 1.132158742s 1.185197497s 1.235581827s 1.285642512s 1.332683491s]
Sep  4 16:13:41.475: INFO: 50 %ile: 749.505688ms
Sep  4 16:13:41.475: INFO: 90 %ile: 752.947188ms
Sep  4 16:13:41.475: INFO: 99 %ile: 1.285642512s
Sep  4 16:13:41.475: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Sep  4 16:13:41.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-294" for this suite. 09/04/23 16:13:41.503
------------------------------
• [10.888 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:13:30.631
    Sep  4 16:13:30.631: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svc-latency 09/04/23 16:13:30.632
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:30.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:30.705
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Sep  4 16:13:30.731: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-294 09/04/23 16:13:30.732
    I0904 16:13:30.748971    7754 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-294, replica count: 1
    I0904 16:13:31.800725    7754 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0904 16:13:32.800914    7754 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  4 16:13:32.920: INFO: Created: latency-svc-zq4sx
    Sep  4 16:13:32.924: INFO: Got endpoints: latency-svc-zq4sx [23.602606ms]
    Sep  4 16:13:32.944: INFO: Created: latency-svc-g75js
    Sep  4 16:13:32.947: INFO: Created: latency-svc-gvqcj
    Sep  4 16:13:32.947: INFO: Got endpoints: latency-svc-g75js [22.772052ms]
    Sep  4 16:13:32.950: INFO: Created: latency-svc-6rlcb
    Sep  4 16:13:32.951: INFO: Got endpoints: latency-svc-gvqcj [26.471499ms]
    Sep  4 16:13:32.955: INFO: Created: latency-svc-q8tz7
    Sep  4 16:13:32.955: INFO: Got endpoints: latency-svc-6rlcb [30.902992ms]
    Sep  4 16:13:32.958: INFO: Created: latency-svc-hs9t6
    Sep  4 16:13:32.958: INFO: Got endpoints: latency-svc-q8tz7 [33.76342ms]
    Sep  4 16:13:32.961: INFO: Got endpoints: latency-svc-hs9t6 [36.386969ms]
    Sep  4 16:13:32.962: INFO: Created: latency-svc-m7wm2
    Sep  4 16:13:32.966: INFO: Got endpoints: latency-svc-m7wm2 [41.217121ms]
    Sep  4 16:13:32.966: INFO: Created: latency-svc-mnl8k
    Sep  4 16:13:32.969: INFO: Got endpoints: latency-svc-mnl8k [44.198442ms]
    Sep  4 16:13:32.972: INFO: Created: latency-svc-7gpfm
    Sep  4 16:13:32.975: INFO: Created: latency-svc-pm5bl
    Sep  4 16:13:32.976: INFO: Got endpoints: latency-svc-7gpfm [51.660243ms]
    Sep  4 16:13:32.978: INFO: Got endpoints: latency-svc-pm5bl [53.569486ms]
    Sep  4 16:13:32.978: INFO: Created: latency-svc-826bg
    Sep  4 16:13:32.981: INFO: Got endpoints: latency-svc-826bg [55.745682ms]
    Sep  4 16:13:32.983: INFO: Created: latency-svc-8gr5k
    Sep  4 16:13:32.987: INFO: Created: latency-svc-8mlzz
    Sep  4 16:13:32.987: INFO: Got endpoints: latency-svc-8gr5k [62.558026ms]
    Sep  4 16:13:32.990: INFO: Got endpoints: latency-svc-8mlzz [65.549093ms]
    Sep  4 16:13:32.992: INFO: Created: latency-svc-5khzl
    Sep  4 16:13:32.995: INFO: Created: latency-svc-h5bkz
    Sep  4 16:13:32.996: INFO: Got endpoints: latency-svc-5khzl [70.90387ms]
    Sep  4 16:13:32.999: INFO: Created: latency-svc-qqzzj
    Sep  4 16:13:33.001: INFO: Got endpoints: latency-svc-h5bkz [76.217319ms]
    Sep  4 16:13:33.004: INFO: Got endpoints: latency-svc-qqzzj [79.298341ms]
    Sep  4 16:13:33.006: INFO: Created: latency-svc-mgw4d
    Sep  4 16:13:33.016: INFO: Got endpoints: latency-svc-mgw4d [68.846921ms]
    Sep  4 16:13:33.019: INFO: Created: latency-svc-zwg97
    Sep  4 16:13:33.022: INFO: Got endpoints: latency-svc-zwg97 [71.1847ms]
    Sep  4 16:13:33.022: INFO: Created: latency-svc-nksww
    Sep  4 16:13:33.025: INFO: Got endpoints: latency-svc-nksww [69.651913ms]
    Sep  4 16:13:33.026: INFO: Created: latency-svc-g7slq
    Sep  4 16:13:33.029: INFO: Got endpoints: latency-svc-g7slq [70.563308ms]
    Sep  4 16:13:33.029: INFO: Created: latency-svc-n7g9z
    Sep  4 16:13:33.032: INFO: Got endpoints: latency-svc-n7g9z [71.199655ms]
    Sep  4 16:13:33.033: INFO: Created: latency-svc-wb6m2
    Sep  4 16:13:33.041: INFO: Got endpoints: latency-svc-wb6m2 [74.990053ms]
    Sep  4 16:13:33.041: INFO: Created: latency-svc-9xx7q
    Sep  4 16:13:33.044: INFO: Got endpoints: latency-svc-9xx7q [74.841992ms]
    Sep  4 16:13:33.045: INFO: Created: latency-svc-5t9kl
    Sep  4 16:13:33.046: INFO: Got endpoints: latency-svc-5t9kl [69.959996ms]
    Sep  4 16:13:33.049: INFO: Created: latency-svc-x8ldn
    Sep  4 16:13:33.053: INFO: Got endpoints: latency-svc-x8ldn [74.457752ms]
    Sep  4 16:13:33.053: INFO: Created: latency-svc-gbpjn
    Sep  4 16:13:33.056: INFO: Got endpoints: latency-svc-gbpjn [75.777774ms]
    Sep  4 16:13:33.057: INFO: Created: latency-svc-7jk97
    Sep  4 16:13:33.062: INFO: Got endpoints: latency-svc-7jk97 [74.910569ms]
    Sep  4 16:13:33.114: INFO: Created: latency-svc-kq74s
    Sep  4 16:13:33.114: INFO: Created: latency-svc-pdcrq
    Sep  4 16:13:33.114: INFO: Created: latency-svc-hr6v7
    Sep  4 16:13:33.114: INFO: Created: latency-svc-86vpf
    Sep  4 16:13:33.115: INFO: Created: latency-svc-8n9wc
    Sep  4 16:13:33.115: INFO: Created: latency-svc-g9h76
    Sep  4 16:13:33.114: INFO: Created: latency-svc-rtmg9
    Sep  4 16:13:33.114: INFO: Created: latency-svc-jhtlv
    Sep  4 16:13:33.115: INFO: Created: latency-svc-k77kp
    Sep  4 16:13:33.115: INFO: Created: latency-svc-jwkmk
    Sep  4 16:13:33.115: INFO: Created: latency-svc-j757d
    Sep  4 16:13:33.114: INFO: Created: latency-svc-jb6lv
    Sep  4 16:13:33.115: INFO: Created: latency-svc-5mvqk
    Sep  4 16:13:33.115: INFO: Created: latency-svc-jddxw
    Sep  4 16:13:33.115: INFO: Created: latency-svc-t9fsv
    Sep  4 16:13:33.116: INFO: Got endpoints: latency-svc-jwkmk [75.35164ms]
    Sep  4 16:13:33.117: INFO: Got endpoints: latency-svc-jddxw [88.232781ms]
    Sep  4 16:13:33.117: INFO: Got endpoints: latency-svc-jhtlv [70.951166ms]
    Sep  4 16:13:33.117: INFO: Got endpoints: latency-svc-5mvqk [73.735434ms]
    Sep  4 16:13:33.118: INFO: Got endpoints: latency-svc-86vpf [64.928884ms]
    Sep  4 16:13:33.118: INFO: Got endpoints: latency-svc-j757d [92.580166ms]
    Sep  4 16:13:33.125: INFO: Got endpoints: latency-svc-hr6v7 [124.286761ms]
    Sep  4 16:13:33.137: INFO: Created: latency-svc-bj8zf
    Sep  4 16:13:33.147: INFO: Created: latency-svc-wnhj7
    Sep  4 16:13:33.152: INFO: Created: latency-svc-w2kk8
    Sep  4 16:13:33.155: INFO: Created: latency-svc-xktns
    Sep  4 16:13:33.158: INFO: Created: latency-svc-ndp9r
    Sep  4 16:13:33.164: INFO: Created: latency-svc-xmqzw
    Sep  4 16:13:33.169: INFO: Created: latency-svc-8pn2c
    Sep  4 16:13:33.174: INFO: Got endpoints: latency-svc-kq74s [183.5361ms]
    Sep  4 16:13:33.193: INFO: Created: latency-svc-czb4w
    Sep  4 16:13:33.224: INFO: Got endpoints: latency-svc-jb6lv [201.639474ms]
    Sep  4 16:13:33.242: INFO: Created: latency-svc-9mzjh
    Sep  4 16:13:33.277: INFO: Got endpoints: latency-svc-8n9wc [215.043887ms]
    Sep  4 16:13:33.297: INFO: Created: latency-svc-r97hz
    Sep  4 16:13:33.323: INFO: Got endpoints: latency-svc-rtmg9 [319.413531ms]
    Sep  4 16:13:33.343: INFO: Created: latency-svc-nn7bc
    Sep  4 16:13:33.376: INFO: Got endpoints: latency-svc-g9h76 [379.90102ms]
    Sep  4 16:13:33.393: INFO: Created: latency-svc-2db59
    Sep  4 16:13:33.426: INFO: Got endpoints: latency-svc-t9fsv [369.297852ms]
    Sep  4 16:13:33.445: INFO: Created: latency-svc-hzzdd
    Sep  4 16:13:33.475: INFO: Got endpoints: latency-svc-pdcrq [459.135764ms]
    Sep  4 16:13:33.495: INFO: Created: latency-svc-p9r6p
    Sep  4 16:13:33.524: INFO: Got endpoints: latency-svc-k77kp [491.621666ms]
    Sep  4 16:13:33.543: INFO: Created: latency-svc-8946c
    Sep  4 16:13:33.575: INFO: Got endpoints: latency-svc-bj8zf [458.38755ms]
    Sep  4 16:13:33.593: INFO: Created: latency-svc-2z2kx
    Sep  4 16:13:33.624: INFO: Got endpoints: latency-svc-wnhj7 [506.894651ms]
    Sep  4 16:13:33.643: INFO: Created: latency-svc-8vdt6
    Sep  4 16:13:33.675: INFO: Got endpoints: latency-svc-w2kk8 [557.353046ms]
    Sep  4 16:13:33.693: INFO: Created: latency-svc-57pjx
    Sep  4 16:13:33.724: INFO: Got endpoints: latency-svc-xktns [606.746685ms]
    Sep  4 16:13:33.746: INFO: Created: latency-svc-456vx
    Sep  4 16:13:33.774: INFO: Got endpoints: latency-svc-ndp9r [656.60244ms]
    Sep  4 16:13:33.794: INFO: Created: latency-svc-wqsdg
    Sep  4 16:13:33.824: INFO: Got endpoints: latency-svc-xmqzw [706.686483ms]
    Sep  4 16:13:33.844: INFO: Created: latency-svc-jkmr6
    Sep  4 16:13:33.880: INFO: Got endpoints: latency-svc-8pn2c [755.077352ms]
    Sep  4 16:13:33.899: INFO: Created: latency-svc-mhf9b
    Sep  4 16:13:33.923: INFO: Got endpoints: latency-svc-czb4w [749.585453ms]
    Sep  4 16:13:33.942: INFO: Created: latency-svc-4s6xz
    Sep  4 16:13:33.974: INFO: Got endpoints: latency-svc-9mzjh [750.007698ms]
    Sep  4 16:13:33.992: INFO: Created: latency-svc-zjdh7
    Sep  4 16:13:34.025: INFO: Got endpoints: latency-svc-r97hz [748.098309ms]
    Sep  4 16:13:34.046: INFO: Created: latency-svc-rt8xf
    Sep  4 16:13:34.075: INFO: Got endpoints: latency-svc-nn7bc [751.589686ms]
    Sep  4 16:13:34.094: INFO: Created: latency-svc-jslhf
    Sep  4 16:13:34.124: INFO: Got endpoints: latency-svc-2db59 [748.634109ms]
    Sep  4 16:13:34.145: INFO: Created: latency-svc-n5hxp
    Sep  4 16:13:34.174: INFO: Got endpoints: latency-svc-hzzdd [748.586325ms]
    Sep  4 16:13:34.193: INFO: Created: latency-svc-phjqp
    Sep  4 16:13:34.224: INFO: Got endpoints: latency-svc-p9r6p [748.843846ms]
    Sep  4 16:13:34.244: INFO: Created: latency-svc-9kj6n
    Sep  4 16:13:34.275: INFO: Got endpoints: latency-svc-8946c [751.264516ms]
    Sep  4 16:13:34.293: INFO: Created: latency-svc-lh942
    Sep  4 16:13:34.325: INFO: Got endpoints: latency-svc-2z2kx [750.055739ms]
    Sep  4 16:13:34.344: INFO: Created: latency-svc-wtv9l
    Sep  4 16:13:34.376: INFO: Got endpoints: latency-svc-8vdt6 [751.411245ms]
    Sep  4 16:13:34.395: INFO: Created: latency-svc-p6dm2
    Sep  4 16:13:34.425: INFO: Got endpoints: latency-svc-57pjx [750.791223ms]
    Sep  4 16:13:34.445: INFO: Created: latency-svc-zc65p
    Sep  4 16:13:34.474: INFO: Got endpoints: latency-svc-456vx [749.947009ms]
    Sep  4 16:13:34.495: INFO: Created: latency-svc-gtlwc
    Sep  4 16:13:34.524: INFO: Got endpoints: latency-svc-wqsdg [749.753367ms]
    Sep  4 16:13:34.544: INFO: Created: latency-svc-zll4c
    Sep  4 16:13:34.575: INFO: Got endpoints: latency-svc-jkmr6 [750.820753ms]
    Sep  4 16:13:34.603: INFO: Created: latency-svc-p845z
    Sep  4 16:13:34.624: INFO: Got endpoints: latency-svc-mhf9b [743.599988ms]
    Sep  4 16:13:34.642: INFO: Created: latency-svc-ckmj5
    Sep  4 16:13:34.673: INFO: Got endpoints: latency-svc-4s6xz [749.81809ms]
    Sep  4 16:13:34.692: INFO: Created: latency-svc-m8k7n
    Sep  4 16:13:34.723: INFO: Got endpoints: latency-svc-zjdh7 [749.40109ms]
    Sep  4 16:13:34.742: INFO: Created: latency-svc-n7bdm
    Sep  4 16:13:34.774: INFO: Got endpoints: latency-svc-rt8xf [748.041264ms]
    Sep  4 16:13:34.797: INFO: Created: latency-svc-5vp6r
    Sep  4 16:13:34.825: INFO: Got endpoints: latency-svc-jslhf [749.505688ms]
    Sep  4 16:13:34.844: INFO: Created: latency-svc-r6gdc
    Sep  4 16:13:34.873: INFO: Got endpoints: latency-svc-n5hxp [748.984467ms]
    Sep  4 16:13:34.894: INFO: Created: latency-svc-cjbwr
    Sep  4 16:13:34.924: INFO: Got endpoints: latency-svc-phjqp [749.928579ms]
    Sep  4 16:13:34.943: INFO: Created: latency-svc-48p6m
    Sep  4 16:13:34.976: INFO: Got endpoints: latency-svc-9kj6n [752.05559ms]
    Sep  4 16:13:34.996: INFO: Created: latency-svc-v24ms
    Sep  4 16:13:35.024: INFO: Got endpoints: latency-svc-lh942 [749.238218ms]
    Sep  4 16:13:35.044: INFO: Created: latency-svc-76r7b
    Sep  4 16:13:35.074: INFO: Got endpoints: latency-svc-wtv9l [749.285242ms]
    Sep  4 16:13:35.096: INFO: Created: latency-svc-nl7hf
    Sep  4 16:13:35.124: INFO: Got endpoints: latency-svc-p6dm2 [748.066812ms]
    Sep  4 16:13:35.143: INFO: Created: latency-svc-4d5qk
    Sep  4 16:13:35.175: INFO: Got endpoints: latency-svc-zc65p [749.21089ms]
    Sep  4 16:13:35.193: INFO: Created: latency-svc-mvkbv
    Sep  4 16:13:35.224: INFO: Got endpoints: latency-svc-gtlwc [749.816181ms]
    Sep  4 16:13:35.243: INFO: Created: latency-svc-rpf6h
    Sep  4 16:13:35.274: INFO: Got endpoints: latency-svc-zll4c [749.965599ms]
    Sep  4 16:13:35.294: INFO: Created: latency-svc-6fjv9
    Sep  4 16:13:35.325: INFO: Got endpoints: latency-svc-p845z [749.651116ms]
    Sep  4 16:13:35.345: INFO: Created: latency-svc-w5c6s
    Sep  4 16:13:35.378: INFO: Got endpoints: latency-svc-ckmj5 [753.650918ms]
    Sep  4 16:13:35.396: INFO: Created: latency-svc-2zj45
    Sep  4 16:13:35.423: INFO: Got endpoints: latency-svc-m8k7n [749.743761ms]
    Sep  4 16:13:35.442: INFO: Created: latency-svc-ztv6l
    Sep  4 16:13:35.474: INFO: Got endpoints: latency-svc-n7bdm [751.181918ms]
    Sep  4 16:13:35.493: INFO: Created: latency-svc-pzvjf
    Sep  4 16:13:35.523: INFO: Got endpoints: latency-svc-5vp6r [749.658586ms]
    Sep  4 16:13:35.544: INFO: Created: latency-svc-sjjb6
    Sep  4 16:13:35.573: INFO: Got endpoints: latency-svc-r6gdc [748.539599ms]
    Sep  4 16:13:35.592: INFO: Created: latency-svc-j7cgl
    Sep  4 16:13:35.625: INFO: Got endpoints: latency-svc-cjbwr [752.224514ms]
    Sep  4 16:13:35.644: INFO: Created: latency-svc-7sdvl
    Sep  4 16:13:35.674: INFO: Got endpoints: latency-svc-48p6m [749.993937ms]
    Sep  4 16:13:35.694: INFO: Created: latency-svc-s82v2
    Sep  4 16:13:35.723: INFO: Got endpoints: latency-svc-v24ms [747.062611ms]
    Sep  4 16:13:35.744: INFO: Created: latency-svc-htvvr
    Sep  4 16:13:35.774: INFO: Got endpoints: latency-svc-76r7b [749.398029ms]
    Sep  4 16:13:35.793: INFO: Created: latency-svc-zfxd7
    Sep  4 16:13:35.824: INFO: Got endpoints: latency-svc-nl7hf [750.008237ms]
    Sep  4 16:13:35.844: INFO: Created: latency-svc-fdr7w
    Sep  4 16:13:35.875: INFO: Got endpoints: latency-svc-4d5qk [751.103826ms]
    Sep  4 16:13:35.895: INFO: Created: latency-svc-44tfw
    Sep  4 16:13:36.507: INFO: Got endpoints: latency-svc-mvkbv [1.332683491s]
    Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-6fjv9 [1.235581827s]
    Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-rpf6h [1.285642512s]
    Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-2zj45 [1.132158742s]
    Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-w5c6s [1.185197497s]
    Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-pzvjf [1.035605309s]
    Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-ztv6l [1.086968919s]
    Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-j7cgl [936.983068ms]
    Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-sjjb6 [986.928653ms]
    Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-s82v2 [835.949956ms]
    Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-htvvr [786.907217ms]
    Sep  4 16:13:36.510: INFO: Got endpoints: latency-svc-7sdvl [884.804569ms]
    Sep  4 16:13:36.524: INFO: Got endpoints: latency-svc-zfxd7 [749.912152ms]
    Sep  4 16:13:36.527: INFO: Created: latency-svc-8xrrs
    Sep  4 16:13:36.532: INFO: Created: latency-svc-htsjf
    Sep  4 16:13:36.536: INFO: Created: latency-svc-g7dcl
    Sep  4 16:13:36.540: INFO: Created: latency-svc-zcd4v
    Sep  4 16:13:36.544: INFO: Created: latency-svc-rxpb5
    Sep  4 16:13:36.547: INFO: Created: latency-svc-twr2z
    Sep  4 16:13:36.550: INFO: Created: latency-svc-mpd6j
    Sep  4 16:13:36.554: INFO: Created: latency-svc-khd5z
    Sep  4 16:13:36.558: INFO: Created: latency-svc-r8bng
    Sep  4 16:13:36.563: INFO: Created: latency-svc-kzg8j
    Sep  4 16:13:36.567: INFO: Created: latency-svc-wx8zl
    Sep  4 16:13:36.570: INFO: Created: latency-svc-4s7b8
    Sep  4 16:13:36.575: INFO: Got endpoints: latency-svc-fdr7w [750.564776ms]
    Sep  4 16:13:36.575: INFO: Created: latency-svc-dn8s2
    Sep  4 16:13:36.594: INFO: Created: latency-svc-bm6tr
    Sep  4 16:13:36.624: INFO: Got endpoints: latency-svc-44tfw [749.539431ms]
    Sep  4 16:13:36.643: INFO: Created: latency-svc-x2p54
    Sep  4 16:13:36.674: INFO: Got endpoints: latency-svc-8xrrs [166.99087ms]
    Sep  4 16:13:36.694: INFO: Created: latency-svc-6pgbr
    Sep  4 16:13:36.724: INFO: Got endpoints: latency-svc-htsjf [214.294163ms]
    Sep  4 16:13:36.744: INFO: Created: latency-svc-b989s
    Sep  4 16:13:36.774: INFO: Got endpoints: latency-svc-g7dcl [264.580265ms]
    Sep  4 16:13:36.794: INFO: Created: latency-svc-n77hr
    Sep  4 16:13:36.824: INFO: Got endpoints: latency-svc-zcd4v [314.41572ms]
    Sep  4 16:13:36.846: INFO: Created: latency-svc-g6g6c
    Sep  4 16:13:36.876: INFO: Got endpoints: latency-svc-rxpb5 [366.045655ms]
    Sep  4 16:13:36.896: INFO: Created: latency-svc-cwlkl
    Sep  4 16:13:36.923: INFO: Got endpoints: latency-svc-twr2z [413.511298ms]
    Sep  4 16:13:36.945: INFO: Created: latency-svc-wf2bx
    Sep  4 16:13:36.974: INFO: Got endpoints: latency-svc-mpd6j [463.874001ms]
    Sep  4 16:13:36.993: INFO: Created: latency-svc-kkb5c
    Sep  4 16:13:37.027: INFO: Got endpoints: latency-svc-khd5z [516.94178ms]
    Sep  4 16:13:37.048: INFO: Created: latency-svc-6vlrj
    Sep  4 16:13:37.074: INFO: Got endpoints: latency-svc-r8bng [564.207465ms]
    Sep  4 16:13:37.094: INFO: Created: latency-svc-25n5b
    Sep  4 16:13:37.124: INFO: Got endpoints: latency-svc-kzg8j [613.996427ms]
    Sep  4 16:13:37.144: INFO: Created: latency-svc-g5s7l
    Sep  4 16:13:37.174: INFO: Got endpoints: latency-svc-wx8zl [664.136932ms]
    Sep  4 16:13:37.195: INFO: Created: latency-svc-ncq4x
    Sep  4 16:13:37.224: INFO: Got endpoints: latency-svc-4s7b8 [713.394552ms]
    Sep  4 16:13:37.244: INFO: Created: latency-svc-678gt
    Sep  4 16:13:37.274: INFO: Got endpoints: latency-svc-dn8s2 [750.620163ms]
    Sep  4 16:13:37.294: INFO: Created: latency-svc-tx7lj
    Sep  4 16:13:37.324: INFO: Got endpoints: latency-svc-bm6tr [749.432527ms]
    Sep  4 16:13:37.344: INFO: Created: latency-svc-6r4j7
    Sep  4 16:13:37.376: INFO: Got endpoints: latency-svc-x2p54 [751.01817ms]
    Sep  4 16:13:37.396: INFO: Created: latency-svc-pdm4v
    Sep  4 16:13:37.425: INFO: Got endpoints: latency-svc-6pgbr [750.366273ms]
    Sep  4 16:13:37.445: INFO: Created: latency-svc-kfbrv
    Sep  4 16:13:37.475: INFO: Got endpoints: latency-svc-b989s [750.503805ms]
    Sep  4 16:13:37.494: INFO: Created: latency-svc-s2m4w
    Sep  4 16:13:37.524: INFO: Got endpoints: latency-svc-n77hr [749.216584ms]
    Sep  4 16:13:37.544: INFO: Created: latency-svc-4f6qd
    Sep  4 16:13:37.574: INFO: Got endpoints: latency-svc-g6g6c [749.410091ms]
    Sep  4 16:13:37.593: INFO: Created: latency-svc-zm2p8
    Sep  4 16:13:37.624: INFO: Got endpoints: latency-svc-cwlkl [747.668293ms]
    Sep  4 16:13:37.643: INFO: Created: latency-svc-6fhf8
    Sep  4 16:13:37.674: INFO: Got endpoints: latency-svc-wf2bx [750.935619ms]
    Sep  4 16:13:37.694: INFO: Created: latency-svc-89gnj
    Sep  4 16:13:37.725: INFO: Got endpoints: latency-svc-kkb5c [751.214859ms]
    Sep  4 16:13:37.747: INFO: Created: latency-svc-rjf6c
    Sep  4 16:13:37.774: INFO: Got endpoints: latency-svc-6vlrj [746.853469ms]
    Sep  4 16:13:37.793: INFO: Created: latency-svc-pj7j9
    Sep  4 16:13:37.824: INFO: Got endpoints: latency-svc-25n5b [749.978557ms]
    Sep  4 16:13:37.843: INFO: Created: latency-svc-pb5jl
    Sep  4 16:13:37.874: INFO: Got endpoints: latency-svc-g5s7l [749.944021ms]
    Sep  4 16:13:37.895: INFO: Created: latency-svc-t4lbj
    Sep  4 16:13:37.924: INFO: Got endpoints: latency-svc-ncq4x [749.603338ms]
    Sep  4 16:13:37.945: INFO: Created: latency-svc-xcvgf
    Sep  4 16:13:37.974: INFO: Got endpoints: latency-svc-678gt [750.68356ms]
    Sep  4 16:13:37.994: INFO: Created: latency-svc-kv6vq
    Sep  4 16:13:38.029: INFO: Got endpoints: latency-svc-tx7lj [754.734331ms]
    Sep  4 16:13:38.049: INFO: Created: latency-svc-6gr7x
    Sep  4 16:13:38.075: INFO: Got endpoints: latency-svc-6r4j7 [750.574269ms]
    Sep  4 16:13:38.094: INFO: Created: latency-svc-jvzh7
    Sep  4 16:13:38.124: INFO: Got endpoints: latency-svc-pdm4v [748.739426ms]
    Sep  4 16:13:38.143: INFO: Created: latency-svc-pwp5m
    Sep  4 16:13:38.174: INFO: Got endpoints: latency-svc-kfbrv [749.540611ms]
    Sep  4 16:13:38.194: INFO: Created: latency-svc-6kjh5
    Sep  4 16:13:38.224: INFO: Got endpoints: latency-svc-s2m4w [749.413576ms]
    Sep  4 16:13:38.244: INFO: Created: latency-svc-7ts8n
    Sep  4 16:13:38.273: INFO: Got endpoints: latency-svc-4f6qd [749.543084ms]
    Sep  4 16:13:38.294: INFO: Created: latency-svc-fv9bz
    Sep  4 16:13:38.324: INFO: Got endpoints: latency-svc-zm2p8 [750.215989ms]
    Sep  4 16:13:38.345: INFO: Created: latency-svc-d4b92
    Sep  4 16:13:38.374: INFO: Got endpoints: latency-svc-6fhf8 [750.360397ms]
    Sep  4 16:13:38.394: INFO: Created: latency-svc-vbnsg
    Sep  4 16:13:38.425: INFO: Got endpoints: latency-svc-89gnj [750.156909ms]
    Sep  4 16:13:38.444: INFO: Created: latency-svc-2slwm
    Sep  4 16:13:38.473: INFO: Got endpoints: latency-svc-rjf6c [748.290505ms]
    Sep  4 16:13:38.493: INFO: Created: latency-svc-mc65s
    Sep  4 16:13:38.523: INFO: Got endpoints: latency-svc-pj7j9 [748.732938ms]
    Sep  4 16:13:38.543: INFO: Created: latency-svc-65gvh
    Sep  4 16:13:38.575: INFO: Got endpoints: latency-svc-pb5jl [751.054298ms]
    Sep  4 16:13:38.595: INFO: Created: latency-svc-htzz4
    Sep  4 16:13:38.624: INFO: Got endpoints: latency-svc-t4lbj [749.714023ms]
    Sep  4 16:13:38.644: INFO: Created: latency-svc-kt76v
    Sep  4 16:13:38.674: INFO: Got endpoints: latency-svc-xcvgf [750.132616ms]
    Sep  4 16:13:38.694: INFO: Created: latency-svc-jsdsb
    Sep  4 16:13:38.724: INFO: Got endpoints: latency-svc-kv6vq [749.689523ms]
    Sep  4 16:13:38.744: INFO: Created: latency-svc-zsnzp
    Sep  4 16:13:38.774: INFO: Got endpoints: latency-svc-6gr7x [744.675762ms]
    Sep  4 16:13:38.795: INFO: Created: latency-svc-j5snz
    Sep  4 16:13:38.824: INFO: Got endpoints: latency-svc-jvzh7 [749.121439ms]
    Sep  4 16:13:38.843: INFO: Created: latency-svc-cj84k
    Sep  4 16:13:38.874: INFO: Got endpoints: latency-svc-pwp5m [749.575436ms]
    Sep  4 16:13:38.893: INFO: Created: latency-svc-gmz94
    Sep  4 16:13:38.925: INFO: Got endpoints: latency-svc-6kjh5 [750.788121ms]
    Sep  4 16:13:38.944: INFO: Created: latency-svc-pkmdf
    Sep  4 16:13:38.974: INFO: Got endpoints: latency-svc-7ts8n [750.04643ms]
    Sep  4 16:13:38.994: INFO: Created: latency-svc-9ngqm
    Sep  4 16:13:39.029: INFO: Got endpoints: latency-svc-fv9bz [755.900811ms]
    Sep  4 16:13:39.049: INFO: Created: latency-svc-h8prs
    Sep  4 16:13:39.077: INFO: Got endpoints: latency-svc-d4b92 [752.947188ms]
    Sep  4 16:13:39.096: INFO: Created: latency-svc-4th7t
    Sep  4 16:13:39.124: INFO: Got endpoints: latency-svc-vbnsg [750.304093ms]
    Sep  4 16:13:39.147: INFO: Created: latency-svc-mkvhh
    Sep  4 16:13:39.180: INFO: Got endpoints: latency-svc-2slwm [755.807567ms]
    Sep  4 16:13:39.201: INFO: Created: latency-svc-jkwkd
    Sep  4 16:13:39.224: INFO: Got endpoints: latency-svc-mc65s [750.257819ms]
    Sep  4 16:13:39.243: INFO: Created: latency-svc-8j2jp
    Sep  4 16:13:39.275: INFO: Got endpoints: latency-svc-65gvh [752.465901ms]
    Sep  4 16:13:39.296: INFO: Created: latency-svc-cbl8l
    Sep  4 16:13:39.326: INFO: Got endpoints: latency-svc-htzz4 [750.441218ms]
    Sep  4 16:13:39.345: INFO: Created: latency-svc-s9rwd
    Sep  4 16:13:39.377: INFO: Got endpoints: latency-svc-kt76v [753.289435ms]
    Sep  4 16:13:39.396: INFO: Created: latency-svc-lrbzs
    Sep  4 16:13:39.424: INFO: Got endpoints: latency-svc-jsdsb [749.941952ms]
    Sep  4 16:13:39.443: INFO: Created: latency-svc-lbwmp
    Sep  4 16:13:39.474: INFO: Got endpoints: latency-svc-zsnzp [749.48498ms]
    Sep  4 16:13:39.493: INFO: Created: latency-svc-44trf
    Sep  4 16:13:39.524: INFO: Got endpoints: latency-svc-j5snz [750.147692ms]
    Sep  4 16:13:39.544: INFO: Created: latency-svc-qnzk2
    Sep  4 16:13:39.575: INFO: Got endpoints: latency-svc-cj84k [750.952573ms]
    Sep  4 16:13:39.595: INFO: Created: latency-svc-bq6qr
    Sep  4 16:13:39.625: INFO: Got endpoints: latency-svc-gmz94 [751.379491ms]
    Sep  4 16:13:39.646: INFO: Created: latency-svc-wqg77
    Sep  4 16:13:39.675: INFO: Got endpoints: latency-svc-pkmdf [750.017419ms]
    Sep  4 16:13:39.697: INFO: Created: latency-svc-gc8pq
    Sep  4 16:13:39.724: INFO: Got endpoints: latency-svc-9ngqm [749.731708ms]
    Sep  4 16:13:39.743: INFO: Created: latency-svc-67kgd
    Sep  4 16:13:39.774: INFO: Got endpoints: latency-svc-h8prs [745.012518ms]
    Sep  4 16:13:39.794: INFO: Created: latency-svc-wlwnj
    Sep  4 16:13:39.824: INFO: Got endpoints: latency-svc-4th7t [747.123817ms]
    Sep  4 16:13:39.844: INFO: Created: latency-svc-694p7
    Sep  4 16:13:39.874: INFO: Got endpoints: latency-svc-mkvhh [749.911391ms]
    Sep  4 16:13:39.893: INFO: Created: latency-svc-7j6jw
    Sep  4 16:13:39.926: INFO: Got endpoints: latency-svc-jkwkd [745.940254ms]
    Sep  4 16:13:39.946: INFO: Created: latency-svc-c9czp
    Sep  4 16:13:39.975: INFO: Got endpoints: latency-svc-8j2jp [750.785353ms]
    Sep  4 16:13:39.993: INFO: Created: latency-svc-7cbbk
    Sep  4 16:13:40.024: INFO: Got endpoints: latency-svc-cbl8l [748.933864ms]
    Sep  4 16:13:40.043: INFO: Created: latency-svc-cmw7n
    Sep  4 16:13:40.075: INFO: Got endpoints: latency-svc-s9rwd [749.015734ms]
    Sep  4 16:13:40.101: INFO: Created: latency-svc-h868z
    Sep  4 16:13:40.126: INFO: Got endpoints: latency-svc-lrbzs [748.259056ms]
    Sep  4 16:13:40.144: INFO: Created: latency-svc-5d978
    Sep  4 16:13:40.174: INFO: Got endpoints: latency-svc-lbwmp [749.713234ms]
    Sep  4 16:13:40.193: INFO: Created: latency-svc-g4jlx
    Sep  4 16:13:40.224: INFO: Got endpoints: latency-svc-44trf [750.702158ms]
    Sep  4 16:13:40.243: INFO: Created: latency-svc-8llwl
    Sep  4 16:13:40.274: INFO: Got endpoints: latency-svc-qnzk2 [750.408967ms]
    Sep  4 16:13:40.297: INFO: Created: latency-svc-vm2mj
    Sep  4 16:13:40.324: INFO: Got endpoints: latency-svc-bq6qr [749.398778ms]
    Sep  4 16:13:40.344: INFO: Created: latency-svc-m4mgs
    Sep  4 16:13:40.379: INFO: Got endpoints: latency-svc-wqg77 [753.224371ms]
    Sep  4 16:13:40.398: INFO: Created: latency-svc-df8rn
    Sep  4 16:13:40.424: INFO: Got endpoints: latency-svc-gc8pq [749.151957ms]
    Sep  4 16:13:40.444: INFO: Created: latency-svc-m2nww
    Sep  4 16:13:40.475: INFO: Got endpoints: latency-svc-67kgd [751.186414ms]
    Sep  4 16:13:40.494: INFO: Created: latency-svc-rh7f7
    Sep  4 16:13:40.526: INFO: Got endpoints: latency-svc-wlwnj [752.14285ms]
    Sep  4 16:13:40.545: INFO: Created: latency-svc-sw584
    Sep  4 16:13:40.574: INFO: Got endpoints: latency-svc-694p7 [750.107518ms]
    Sep  4 16:13:40.593: INFO: Created: latency-svc-tmg45
    Sep  4 16:13:40.624: INFO: Got endpoints: latency-svc-7j6jw [749.950152ms]
    Sep  4 16:13:40.644: INFO: Created: latency-svc-5cp2h
    Sep  4 16:13:40.674: INFO: Got endpoints: latency-svc-c9czp [747.967011ms]
    Sep  4 16:13:40.693: INFO: Created: latency-svc-nbz2p
    Sep  4 16:13:40.725: INFO: Got endpoints: latency-svc-7cbbk [750.011477ms]
    Sep  4 16:13:40.744: INFO: Created: latency-svc-dx4hm
    Sep  4 16:13:40.774: INFO: Got endpoints: latency-svc-cmw7n [750.068396ms]
    Sep  4 16:13:40.825: INFO: Got endpoints: latency-svc-h868z [749.97027ms]
    Sep  4 16:13:40.874: INFO: Got endpoints: latency-svc-5d978 [748.82426ms]
    Sep  4 16:13:40.925: INFO: Got endpoints: latency-svc-g4jlx [750.779092ms]
    Sep  4 16:13:40.975: INFO: Got endpoints: latency-svc-8llwl [750.823893ms]
    Sep  4 16:13:41.024: INFO: Got endpoints: latency-svc-vm2mj [749.681734ms]
    Sep  4 16:13:41.074: INFO: Got endpoints: latency-svc-m4mgs [750.113621ms]
    Sep  4 16:13:41.125: INFO: Got endpoints: latency-svc-df8rn [746.097888ms]
    Sep  4 16:13:41.175: INFO: Got endpoints: latency-svc-m2nww [750.632105ms]
    Sep  4 16:13:41.224: INFO: Got endpoints: latency-svc-rh7f7 [748.328407ms]
    Sep  4 16:13:41.275: INFO: Got endpoints: latency-svc-sw584 [748.32211ms]
    Sep  4 16:13:41.324: INFO: Got endpoints: latency-svc-tmg45 [749.471813ms]
    Sep  4 16:13:41.375: INFO: Got endpoints: latency-svc-5cp2h [751.056484ms]
    Sep  4 16:13:41.426: INFO: Got endpoints: latency-svc-nbz2p [751.132816ms]
    Sep  4 16:13:41.475: INFO: Got endpoints: latency-svc-dx4hm [750.231345ms]
    Sep  4 16:13:41.475: INFO: Latencies: [22.772052ms 26.471499ms 30.902992ms 33.76342ms 36.386969ms 41.217121ms 44.198442ms 51.660243ms 53.569486ms 55.745682ms 62.558026ms 64.928884ms 65.549093ms 68.846921ms 69.651913ms 69.959996ms 70.563308ms 70.90387ms 70.951166ms 71.1847ms 71.199655ms 73.735434ms 74.457752ms 74.841992ms 74.910569ms 74.990053ms 75.35164ms 75.777774ms 76.217319ms 79.298341ms 88.232781ms 92.580166ms 124.286761ms 166.99087ms 183.5361ms 201.639474ms 214.294163ms 215.043887ms 264.580265ms 314.41572ms 319.413531ms 366.045655ms 369.297852ms 379.90102ms 413.511298ms 458.38755ms 459.135764ms 463.874001ms 491.621666ms 506.894651ms 516.94178ms 557.353046ms 564.207465ms 606.746685ms 613.996427ms 656.60244ms 664.136932ms 706.686483ms 713.394552ms 743.599988ms 744.675762ms 745.012518ms 745.940254ms 746.097888ms 746.853469ms 747.062611ms 747.123817ms 747.668293ms 747.967011ms 748.041264ms 748.066812ms 748.098309ms 748.259056ms 748.290505ms 748.32211ms 748.328407ms 748.539599ms 748.586325ms 748.634109ms 748.732938ms 748.739426ms 748.82426ms 748.843846ms 748.933864ms 748.984467ms 749.015734ms 749.121439ms 749.151957ms 749.21089ms 749.216584ms 749.238218ms 749.285242ms 749.398029ms 749.398778ms 749.40109ms 749.410091ms 749.413576ms 749.432527ms 749.471813ms 749.48498ms 749.505688ms 749.539431ms 749.540611ms 749.543084ms 749.575436ms 749.585453ms 749.603338ms 749.651116ms 749.658586ms 749.681734ms 749.689523ms 749.713234ms 749.714023ms 749.731708ms 749.743761ms 749.753367ms 749.816181ms 749.81809ms 749.911391ms 749.912152ms 749.928579ms 749.941952ms 749.944021ms 749.947009ms 749.950152ms 749.965599ms 749.97027ms 749.978557ms 749.993937ms 750.007698ms 750.008237ms 750.011477ms 750.017419ms 750.04643ms 750.055739ms 750.068396ms 750.107518ms 750.113621ms 750.132616ms 750.147692ms 750.156909ms 750.215989ms 750.231345ms 750.257819ms 750.304093ms 750.360397ms 750.366273ms 750.408967ms 750.441218ms 750.503805ms 750.564776ms 750.574269ms 750.620163ms 750.632105ms 750.68356ms 750.702158ms 750.779092ms 750.785353ms 750.788121ms 750.791223ms 750.820753ms 750.823893ms 750.935619ms 750.952573ms 751.01817ms 751.054298ms 751.056484ms 751.103826ms 751.132816ms 751.181918ms 751.186414ms 751.214859ms 751.264516ms 751.379491ms 751.411245ms 751.589686ms 752.05559ms 752.14285ms 752.224514ms 752.465901ms 752.947188ms 753.224371ms 753.289435ms 753.650918ms 754.734331ms 755.077352ms 755.807567ms 755.900811ms 786.907217ms 835.949956ms 884.804569ms 936.983068ms 986.928653ms 1.035605309s 1.086968919s 1.132158742s 1.185197497s 1.235581827s 1.285642512s 1.332683491s]
    Sep  4 16:13:41.475: INFO: 50 %ile: 749.505688ms
    Sep  4 16:13:41.475: INFO: 90 %ile: 752.947188ms
    Sep  4 16:13:41.475: INFO: 99 %ile: 1.285642512s
    Sep  4 16:13:41.475: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:13:41.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-294" for this suite. 09/04/23 16:13:41.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:13:41.519
Sep  4 16:13:41.520: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 09/04/23 16:13:41.52
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:41.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:41.592
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-3a87a448-59d4-4be4-861d-62467363415f 09/04/23 16:13:41.62
STEP: Creating a pod to test consume secrets 09/04/23 16:13:41.636
Sep  4 16:13:41.658: INFO: Waiting up to 5m0s for pod "pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe" in namespace "secrets-2963" to be "Succeeded or Failed"
Sep  4 16:13:41.672: INFO: Pod "pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe": Phase="Pending", Reason="", readiness=false. Elapsed: 14.305443ms
Sep  4 16:13:43.688: INFO: Pod "pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe": Phase="Running", Reason="", readiness=false. Elapsed: 2.030062643s
Sep  4 16:13:45.688: INFO: Pod "pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030672141s
STEP: Saw pod success 09/04/23 16:13:45.688
Sep  4 16:13:45.688: INFO: Pod "pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe" satisfied condition "Succeeded or Failed"
Sep  4 16:13:45.703: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe container secret-volume-test: <nil>
STEP: delete the pod 09/04/23 16:13:45.787
Sep  4 16:13:45.807: INFO: Waiting for pod pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe to disappear
Sep  4 16:13:45.821: INFO: Pod pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  4 16:13:45.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2963" for this suite. 09/04/23 16:13:45.849
------------------------------
• [4.345 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:13:41.519
    Sep  4 16:13:41.520: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 09/04/23 16:13:41.52
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:41.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:41.592
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-3a87a448-59d4-4be4-861d-62467363415f 09/04/23 16:13:41.62
    STEP: Creating a pod to test consume secrets 09/04/23 16:13:41.636
    Sep  4 16:13:41.658: INFO: Waiting up to 5m0s for pod "pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe" in namespace "secrets-2963" to be "Succeeded or Failed"
    Sep  4 16:13:41.672: INFO: Pod "pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe": Phase="Pending", Reason="", readiness=false. Elapsed: 14.305443ms
    Sep  4 16:13:43.688: INFO: Pod "pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe": Phase="Running", Reason="", readiness=false. Elapsed: 2.030062643s
    Sep  4 16:13:45.688: INFO: Pod "pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030672141s
    STEP: Saw pod success 09/04/23 16:13:45.688
    Sep  4 16:13:45.688: INFO: Pod "pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe" satisfied condition "Succeeded or Failed"
    Sep  4 16:13:45.703: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe container secret-volume-test: <nil>
    STEP: delete the pod 09/04/23 16:13:45.787
    Sep  4 16:13:45.807: INFO: Waiting for pod pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe to disappear
    Sep  4 16:13:45.821: INFO: Pod pod-secrets-af83a2e1-bddf-4a93-bdf1-95c9833bcfbe no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:13:45.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2963" for this suite. 09/04/23 16:13:45.849
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:13:45.865
Sep  4 16:13:45.865: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 09/04/23 16:13:45.865
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:45.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:45.94
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/04/23 16:13:45.968
Sep  4 16:13:45.990: INFO: Waiting up to 5m0s for pod "pod-64e21202-02d4-477d-ac6a-0bbb9200db58" in namespace "emptydir-6990" to be "Succeeded or Failed"
Sep  4 16:13:46.005: INFO: Pod "pod-64e21202-02d4-477d-ac6a-0bbb9200db58": Phase="Pending", Reason="", readiness=false. Elapsed: 14.697715ms
Sep  4 16:13:48.022: INFO: Pod "pod-64e21202-02d4-477d-ac6a-0bbb9200db58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032470506s
Sep  4 16:13:50.021: INFO: Pod "pod-64e21202-02d4-477d-ac6a-0bbb9200db58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031030982s
STEP: Saw pod success 09/04/23 16:13:50.021
Sep  4 16:13:50.021: INFO: Pod "pod-64e21202-02d4-477d-ac6a-0bbb9200db58" satisfied condition "Succeeded or Failed"
Sep  4 16:13:50.036: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-64e21202-02d4-477d-ac6a-0bbb9200db58 container test-container: <nil>
STEP: delete the pod 09/04/23 16:13:50.073
Sep  4 16:13:50.092: INFO: Waiting for pod pod-64e21202-02d4-477d-ac6a-0bbb9200db58 to disappear
Sep  4 16:13:50.106: INFO: Pod pod-64e21202-02d4-477d-ac6a-0bbb9200db58 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  4 16:13:50.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6990" for this suite. 09/04/23 16:13:50.134
------------------------------
• [4.285 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:13:45.865
    Sep  4 16:13:45.865: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 09/04/23 16:13:45.865
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:45.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:45.94
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/04/23 16:13:45.968
    Sep  4 16:13:45.990: INFO: Waiting up to 5m0s for pod "pod-64e21202-02d4-477d-ac6a-0bbb9200db58" in namespace "emptydir-6990" to be "Succeeded or Failed"
    Sep  4 16:13:46.005: INFO: Pod "pod-64e21202-02d4-477d-ac6a-0bbb9200db58": Phase="Pending", Reason="", readiness=false. Elapsed: 14.697715ms
    Sep  4 16:13:48.022: INFO: Pod "pod-64e21202-02d4-477d-ac6a-0bbb9200db58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032470506s
    Sep  4 16:13:50.021: INFO: Pod "pod-64e21202-02d4-477d-ac6a-0bbb9200db58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031030982s
    STEP: Saw pod success 09/04/23 16:13:50.021
    Sep  4 16:13:50.021: INFO: Pod "pod-64e21202-02d4-477d-ac6a-0bbb9200db58" satisfied condition "Succeeded or Failed"
    Sep  4 16:13:50.036: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod pod-64e21202-02d4-477d-ac6a-0bbb9200db58 container test-container: <nil>
    STEP: delete the pod 09/04/23 16:13:50.073
    Sep  4 16:13:50.092: INFO: Waiting for pod pod-64e21202-02d4-477d-ac6a-0bbb9200db58 to disappear
    Sep  4 16:13:50.106: INFO: Pod pod-64e21202-02d4-477d-ac6a-0bbb9200db58 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:13:50.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6990" for this suite. 09/04/23 16:13:50.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:13:50.15
Sep  4 16:13:50.150: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 09/04/23 16:13:50.151
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:50.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:50.221
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 09/04/23 16:13:50.249
Sep  4 16:13:50.271: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6" in namespace "downward-api-2313" to be "Succeeded or Failed"
Sep  4 16:13:50.286: INFO: Pod "downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.655701ms
Sep  4 16:13:52.301: INFO: Pod "downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029544086s
Sep  4 16:13:54.302: INFO: Pod "downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030782638s
STEP: Saw pod success 09/04/23 16:13:54.302
Sep  4 16:13:54.302: INFO: Pod "downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6" satisfied condition "Succeeded or Failed"
Sep  4 16:13:54.317: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6 container client-container: <nil>
STEP: delete the pod 09/04/23 16:13:54.354
Sep  4 16:13:54.373: INFO: Waiting for pod downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6 to disappear
Sep  4 16:13:54.387: INFO: Pod downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  4 16:13:54.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2313" for this suite. 09/04/23 16:13:54.414
------------------------------
• [4.280 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:13:50.15
    Sep  4 16:13:50.150: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 09/04/23 16:13:50.151
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:50.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:50.221
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 09/04/23 16:13:50.249
    Sep  4 16:13:50.271: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6" in namespace "downward-api-2313" to be "Succeeded or Failed"
    Sep  4 16:13:50.286: INFO: Pod "downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.655701ms
    Sep  4 16:13:52.301: INFO: Pod "downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029544086s
    Sep  4 16:13:54.302: INFO: Pod "downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030782638s
    STEP: Saw pod success 09/04/23 16:13:54.302
    Sep  4 16:13:54.302: INFO: Pod "downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6" satisfied condition "Succeeded or Failed"
    Sep  4 16:13:54.317: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6 container client-container: <nil>
    STEP: delete the pod 09/04/23 16:13:54.354
    Sep  4 16:13:54.373: INFO: Waiting for pod downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6 to disappear
    Sep  4 16:13:54.387: INFO: Pod downwardapi-volume-b378970f-24c7-4710-b39f-ce40256ad5a6 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:13:54.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2313" for this suite. 09/04/23 16:13:54.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:13:54.43
Sep  4 16:13:54.430: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 09/04/23 16:13:54.431
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:54.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:54.501
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8815 09/04/23 16:13:54.529
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/04/23 16:13:54.549
STEP: creating service externalsvc in namespace services-8815 09/04/23 16:13:54.549
STEP: creating replication controller externalsvc in namespace services-8815 09/04/23 16:13:54.568
I0904 16:13:54.584940    7754 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8815, replica count: 2
I0904 16:13:57.635583    7754 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 09/04/23 16:13:57.649
Sep  4 16:13:57.685: INFO: Creating new exec pod
Sep  4 16:13:57.704: INFO: Waiting up to 5m0s for pod "execpodm2p4l" in namespace "services-8815" to be "running"
Sep  4 16:13:57.718: INFO: Pod "execpodm2p4l": Phase="Pending", Reason="", readiness=false. Elapsed: 14.609716ms
Sep  4 16:13:59.734: INFO: Pod "execpodm2p4l": Phase="Running", Reason="", readiness=true. Elapsed: 2.030229794s
Sep  4 16:13:59.734: INFO: Pod "execpodm2p4l" satisfied condition "running"
Sep  4 16:13:59.734: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8815 exec execpodm2p4l -- /bin/sh -x -c nslookup nodeport-service.services-8815.svc.cluster.local'
Sep  4 16:14:00.214: INFO: stderr: "+ nslookup nodeport-service.services-8815.svc.cluster.local\n"
Sep  4 16:14:00.214: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nnodeport-service.services-8815.svc.cluster.local.svc.cluster.local\tcanonical name = externalsvc.services-8815.svc.cluster.local.svc.cluster.local.\nName:\texternalsvc.services-8815.svc.cluster.local.svc.cluster.local\nAddress: 100.111.16.208\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8815, will wait for the garbage collector to delete the pods 09/04/23 16:14:00.214
Sep  4 16:14:00.296: INFO: Deleting ReplicationController externalsvc took: 16.585692ms
Sep  4 16:14:00.397: INFO: Terminating ReplicationController externalsvc pods took: 100.612334ms
Sep  4 16:14:02.519: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  4 16:14:02.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8815" for this suite. 09/04/23 16:14:02.564
------------------------------
• [8.150 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:13:54.43
    Sep  4 16:13:54.430: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 09/04/23 16:13:54.431
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:13:54.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:13:54.501
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-8815 09/04/23 16:13:54.529
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/04/23 16:13:54.549
    STEP: creating service externalsvc in namespace services-8815 09/04/23 16:13:54.549
    STEP: creating replication controller externalsvc in namespace services-8815 09/04/23 16:13:54.568
    I0904 16:13:54.584940    7754 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8815, replica count: 2
    I0904 16:13:57.635583    7754 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 09/04/23 16:13:57.649
    Sep  4 16:13:57.685: INFO: Creating new exec pod
    Sep  4 16:13:57.704: INFO: Waiting up to 5m0s for pod "execpodm2p4l" in namespace "services-8815" to be "running"
    Sep  4 16:13:57.718: INFO: Pod "execpodm2p4l": Phase="Pending", Reason="", readiness=false. Elapsed: 14.609716ms
    Sep  4 16:13:59.734: INFO: Pod "execpodm2p4l": Phase="Running", Reason="", readiness=true. Elapsed: 2.030229794s
    Sep  4 16:13:59.734: INFO: Pod "execpodm2p4l" satisfied condition "running"
    Sep  4 16:13:59.734: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmud5-dd2.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8815 exec execpodm2p4l -- /bin/sh -x -c nslookup nodeport-service.services-8815.svc.cluster.local'
    Sep  4 16:14:00.214: INFO: stderr: "+ nslookup nodeport-service.services-8815.svc.cluster.local\n"
    Sep  4 16:14:00.214: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nnodeport-service.services-8815.svc.cluster.local.svc.cluster.local\tcanonical name = externalsvc.services-8815.svc.cluster.local.svc.cluster.local.\nName:\texternalsvc.services-8815.svc.cluster.local.svc.cluster.local\nAddress: 100.111.16.208\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8815, will wait for the garbage collector to delete the pods 09/04/23 16:14:00.214
    Sep  4 16:14:00.296: INFO: Deleting ReplicationController externalsvc took: 16.585692ms
    Sep  4 16:14:00.397: INFO: Terminating ReplicationController externalsvc pods took: 100.612334ms
    Sep  4 16:14:02.519: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:14:02.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8815" for this suite. 09/04/23 16:14:02.564
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:14:02.58
Sep  4 16:14:02.580: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook 09/04/23 16:14:02.581
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:14:02.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:14:02.652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/04/23 16:14:02.68
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/04/23 16:14:03.351
STEP: Deploying the custom resource conversion webhook pod 09/04/23 16:14:03.367
STEP: Wait for the deployment to be ready 09/04/23 16:14:03.398
Sep  4 16:14:03.431: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/04/23 16:14:05.476
STEP: Verifying the service has paired with the endpoint 09/04/23 16:14:05.497
Sep  4 16:14:06.497: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Sep  4 16:14:06.513: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource 09/04/23 16:14:08.965
STEP: Create a v2 custom resource 09/04/23 16:14:09.023
STEP: List CRs in v1 09/04/23 16:14:09.135
STEP: List CRs in v2 09/04/23 16:14:09.169
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:14:09.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-3507" for this suite. 09/04/23 16:14:09.87
------------------------------
• [7.310 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:14:02.58
    Sep  4 16:14:02.580: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-webhook 09/04/23 16:14:02.581
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:14:02.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:14:02.652
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/04/23 16:14:02.68
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/04/23 16:14:03.351
    STEP: Deploying the custom resource conversion webhook pod 09/04/23 16:14:03.367
    STEP: Wait for the deployment to be ready 09/04/23 16:14:03.398
    Sep  4 16:14:03.431: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/04/23 16:14:05.476
    STEP: Verifying the service has paired with the endpoint 09/04/23 16:14:05.497
    Sep  4 16:14:06.497: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Sep  4 16:14:06.513: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Creating a v1 custom resource 09/04/23 16:14:08.965
    STEP: Create a v2 custom resource 09/04/23 16:14:09.023
    STEP: List CRs in v1 09/04/23 16:14:09.135
    STEP: List CRs in v2 09/04/23 16:14:09.169
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:14:09.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-3507" for this suite. 09/04/23 16:14:09.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:14:09.89
Sep  4 16:14:09.890: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-single-pod 09/04/23 16:14:09.891
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:14:09.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:14:09.963
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Sep  4 16:14:09.991: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  4 16:15:10.116: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Sep  4 16:15:10.130: INFO: Starting informer...
STEP: Starting pod... 09/04/23 16:15:10.13
Sep  4 16:15:10.165: INFO: Pod is running on shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh. Tainting Node
STEP: Trying to apply a taint on the Node 09/04/23 16:15:10.165
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 16:15:10.2
STEP: Waiting short time to make sure Pod is queued for deletion 09/04/23 16:15:10.215
Sep  4 16:15:10.215: INFO: Pod wasn't evicted. Proceeding
Sep  4 16:15:10.215: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 16:15:10.248
STEP: Waiting some time to make sure that toleration time passed. 09/04/23 16:15:10.263
Sep  4 16:16:25.267: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:16:25.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-3886" for this suite. 09/04/23 16:16:25.295
------------------------------
• [135.420 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:14:09.89
    Sep  4 16:14:09.890: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename taint-single-pod 09/04/23 16:14:09.891
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:14:09.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:14:09.963
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Sep  4 16:14:09.991: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  4 16:15:10.116: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Sep  4 16:15:10.130: INFO: Starting informer...
    STEP: Starting pod... 09/04/23 16:15:10.13
    Sep  4 16:15:10.165: INFO: Pod is running on shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh. Tainting Node
    STEP: Trying to apply a taint on the Node 09/04/23 16:15:10.165
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 16:15:10.2
    STEP: Waiting short time to make sure Pod is queued for deletion 09/04/23 16:15:10.215
    Sep  4 16:15:10.215: INFO: Pod wasn't evicted. Proceeding
    Sep  4 16:15:10.215: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/04/23 16:15:10.248
    STEP: Waiting some time to make sure that toleration time passed. 09/04/23 16:15:10.263
    Sep  4 16:16:25.267: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:16:25.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-3886" for this suite. 09/04/23 16:16:25.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:16:25.311
Sep  4 16:16:25.311: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 16:16:25.312
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:16:25.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:16:25.382
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-123ac782-9871-448a-8635-8e90b655bdac 09/04/23 16:16:25.425
STEP: Creating secret with name s-test-opt-upd-09c830b4-c643-4195-b469-bd507d9e06ca 09/04/23 16:16:25.439
STEP: Creating the pod 09/04/23 16:16:25.454
Sep  4 16:16:25.476: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-60d8d204-3b19-45a8-af60-840146ed8621" in namespace "projected-9198" to be "running and ready"
Sep  4 16:16:25.490: INFO: Pod "pod-projected-secrets-60d8d204-3b19-45a8-af60-840146ed8621": Phase="Pending", Reason="", readiness=false. Elapsed: 14.07172ms
Sep  4 16:16:25.490: INFO: The phase of Pod pod-projected-secrets-60d8d204-3b19-45a8-af60-840146ed8621 is Pending, waiting for it to be Running (with Ready = true)
Sep  4 16:16:27.507: INFO: Pod "pod-projected-secrets-60d8d204-3b19-45a8-af60-840146ed8621": Phase="Running", Reason="", readiness=true. Elapsed: 2.030340399s
Sep  4 16:16:27.507: INFO: The phase of Pod pod-projected-secrets-60d8d204-3b19-45a8-af60-840146ed8621 is Running (Ready = true)
Sep  4 16:16:27.507: INFO: Pod "pod-projected-secrets-60d8d204-3b19-45a8-af60-840146ed8621" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-123ac782-9871-448a-8635-8e90b655bdac 09/04/23 16:16:27.766
STEP: Updating secret s-test-opt-upd-09c830b4-c643-4195-b469-bd507d9e06ca 09/04/23 16:16:27.782
STEP: Creating secret with name s-test-opt-create-6f272af0-eda1-4f5d-a35a-650fd38c76ca 09/04/23 16:16:27.797
STEP: waiting to observe update in volume 09/04/23 16:16:27.812
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  4 16:17:41.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9198" for this suite. 09/04/23 16:17:41.389
------------------------------
• [76.093 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:16:25.311
    Sep  4 16:16:25.311: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 16:16:25.312
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:16:25.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:16:25.382
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-123ac782-9871-448a-8635-8e90b655bdac 09/04/23 16:16:25.425
    STEP: Creating secret with name s-test-opt-upd-09c830b4-c643-4195-b469-bd507d9e06ca 09/04/23 16:16:25.439
    STEP: Creating the pod 09/04/23 16:16:25.454
    Sep  4 16:16:25.476: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-60d8d204-3b19-45a8-af60-840146ed8621" in namespace "projected-9198" to be "running and ready"
    Sep  4 16:16:25.490: INFO: Pod "pod-projected-secrets-60d8d204-3b19-45a8-af60-840146ed8621": Phase="Pending", Reason="", readiness=false. Elapsed: 14.07172ms
    Sep  4 16:16:25.490: INFO: The phase of Pod pod-projected-secrets-60d8d204-3b19-45a8-af60-840146ed8621 is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 16:16:27.507: INFO: Pod "pod-projected-secrets-60d8d204-3b19-45a8-af60-840146ed8621": Phase="Running", Reason="", readiness=true. Elapsed: 2.030340399s
    Sep  4 16:16:27.507: INFO: The phase of Pod pod-projected-secrets-60d8d204-3b19-45a8-af60-840146ed8621 is Running (Ready = true)
    Sep  4 16:16:27.507: INFO: Pod "pod-projected-secrets-60d8d204-3b19-45a8-af60-840146ed8621" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-123ac782-9871-448a-8635-8e90b655bdac 09/04/23 16:16:27.766
    STEP: Updating secret s-test-opt-upd-09c830b4-c643-4195-b469-bd507d9e06ca 09/04/23 16:16:27.782
    STEP: Creating secret with name s-test-opt-create-6f272af0-eda1-4f5d-a35a-650fd38c76ca 09/04/23 16:16:27.797
    STEP: waiting to observe update in volume 09/04/23 16:16:27.812
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:17:41.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9198" for this suite. 09/04/23 16:17:41.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:17:41.405
Sep  4 16:17:41.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ingress 09/04/23 16:17:41.406
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:17:41.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:17:41.476
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 09/04/23 16:17:41.503
STEP: getting /apis/networking.k8s.io 09/04/23 16:17:41.53
STEP: getting /apis/networking.k8s.iov1 09/04/23 16:17:41.542
STEP: creating 09/04/23 16:17:41.556
STEP: getting 09/04/23 16:17:41.602
STEP: listing 09/04/23 16:17:41.616
STEP: watching 09/04/23 16:17:41.631
Sep  4 16:17:41.631: INFO: starting watch
STEP: cluster-wide listing 09/04/23 16:17:41.644
STEP: cluster-wide watching 09/04/23 16:17:41.659
Sep  4 16:17:41.659: INFO: starting watch
STEP: patching 09/04/23 16:17:41.673
STEP: updating 09/04/23 16:17:41.688
Sep  4 16:17:41.717: INFO: waiting for watch events with expected annotations
Sep  4 16:17:41.717: INFO: saw patched and updated annotations
STEP: patching /status 09/04/23 16:17:41.718
STEP: updating /status 09/04/23 16:17:41.733
STEP: get /status 09/04/23 16:17:41.763
STEP: deleting 09/04/23 16:17:41.777
STEP: deleting a collection 09/04/23 16:17:41.82
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Sep  4 16:17:41.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-8613" for this suite. 09/04/23 16:17:41.868
------------------------------
• [0.478 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:17:41.405
    Sep  4 16:17:41.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename ingress 09/04/23 16:17:41.406
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:17:41.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:17:41.476
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 09/04/23 16:17:41.503
    STEP: getting /apis/networking.k8s.io 09/04/23 16:17:41.53
    STEP: getting /apis/networking.k8s.iov1 09/04/23 16:17:41.542
    STEP: creating 09/04/23 16:17:41.556
    STEP: getting 09/04/23 16:17:41.602
    STEP: listing 09/04/23 16:17:41.616
    STEP: watching 09/04/23 16:17:41.631
    Sep  4 16:17:41.631: INFO: starting watch
    STEP: cluster-wide listing 09/04/23 16:17:41.644
    STEP: cluster-wide watching 09/04/23 16:17:41.659
    Sep  4 16:17:41.659: INFO: starting watch
    STEP: patching 09/04/23 16:17:41.673
    STEP: updating 09/04/23 16:17:41.688
    Sep  4 16:17:41.717: INFO: waiting for watch events with expected annotations
    Sep  4 16:17:41.717: INFO: saw patched and updated annotations
    STEP: patching /status 09/04/23 16:17:41.718
    STEP: updating /status 09/04/23 16:17:41.733
    STEP: get /status 09/04/23 16:17:41.763
    STEP: deleting 09/04/23 16:17:41.777
    STEP: deleting a collection 09/04/23 16:17:41.82
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:17:41.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-8613" for this suite. 09/04/23 16:17:41.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:17:41.884
Sep  4 16:17:41.884: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 09/04/23 16:17:41.884
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:17:41.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:17:41.954
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  4 16:17:42.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3707" for this suite. 09/04/23 16:17:42.036
------------------------------
• [0.167 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:17:41.884
    Sep  4 16:17:41.884: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 09/04/23 16:17:41.884
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:17:41.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:17:41.954
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:17:42.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3707" for this suite. 09/04/23 16:17:42.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:17:42.052
Sep  4 16:17:42.052: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 09/04/23 16:17:42.053
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:17:42.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:17:42.123
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  4 16:17:42.150: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  4 16:17:42.192: INFO: Waiting for terminating namespaces to be deleted...
Sep  4 16:17:42.206: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx before test
Sep  4 16:17:42.228: INFO: addons-nginx-ingress-controller-56b5dc8f6c-tr9n6 from kube-system started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep  4 16:17:42.228: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-58685978db-gpxdq from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Sep  4 16:17:42.228: INFO: apiserver-proxy-bc5jh from kube-system started at 2023-09-04 14:34:44 +0000 UTC (2 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container proxy ready: true, restart count 0
Sep  4 16:17:42.228: INFO: 	Container sidecar ready: true, restart count 0
Sep  4 16:17:42.228: INFO: blackbox-exporter-585854d657-fvtbg from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container blackbox-exporter ready: true, restart count 0
Sep  4 16:17:42.228: INFO: blackbox-exporter-585854d657-vz8rw from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container blackbox-exporter ready: true, restart count 0
Sep  4 16:17:42.228: INFO: calico-kube-controllers-684b9f4889-f24wb from kube-system started at 2023-09-04 14:34:42 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  4 16:17:42.228: INFO: calico-node-959qs from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 16:17:42.228: INFO: calico-node-vertical-autoscaler-7bbd54698f-zb6st from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container autoscaler ready: true, restart count 0
Sep  4 16:17:42.228: INFO: calico-typha-deploy-6f4475c6d5-z7t2m from kube-system started at 2023-09-04 14:35:50 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container calico-typha ready: true, restart count 0
Sep  4 16:17:42.228: INFO: calico-typha-horizontal-autoscaler-7b89d5ff97-gdvkd from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container autoscaler ready: true, restart count 0
Sep  4 16:17:42.228: INFO: calico-typha-vertical-autoscaler-656479b7b5-khd8g from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container autoscaler ready: true, restart count 0
Sep  4 16:17:42.228: INFO: coredns-89679867b-2fr9s from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container coredns ready: true, restart count 0
Sep  4 16:17:42.228: INFO: coredns-89679867b-5qtl7 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container coredns ready: true, restart count 0
Sep  4 16:17:42.228: INFO: csi-driver-node-5kfkx from kube-system started at 2023-09-04 14:34:46 +0000 UTC (3 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container csi-driver ready: true, restart count 0
Sep  4 16:17:42.228: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Sep  4 16:17:42.228: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Sep  4 16:17:42.228: INFO: egress-filter-applier-n4kfc from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container egress-filter-applier ready: true, restart count 0
Sep  4 16:17:42.228: INFO: kube-proxy-worker-1-v1.26.8-ch6px from kube-system started at 2023-09-04 15:01:11 +0000 UTC (2 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container conntrack-fix ready: true, restart count 0
Sep  4 16:17:42.228: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 16:17:42.228: INFO: metrics-server-78947f8d7c-9cdcs from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container metrics-server ready: true, restart count 0
Sep  4 16:17:42.228: INFO: metrics-server-78947f8d7c-w8gr6 from kube-system started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container metrics-server ready: true, restart count 0
Sep  4 16:17:42.228: INFO: network-problem-detector-host-xf9l5 from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Sep  4 16:17:42.228: INFO: network-problem-detector-pod-lbp4l from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Sep  4 16:17:42.228: INFO: node-exporter-ktjnf from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container node-exporter ready: true, restart count 0
Sep  4 16:17:42.228: INFO: node-local-dns-zhl7r from kube-system started at 2023-09-04 15:35:11 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container node-cache ready: true, restart count 0
Sep  4 16:17:42.228: INFO: node-problem-detector-rqmbk from kube-system started at 2023-09-04 14:41:10 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep  4 16:17:42.228: INFO: vpn-shoot-5d596dbb88-5vx52 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container vpn-shoot ready: true, restart count 0
Sep  4 16:17:42.228: INFO: dashboard-metrics-scraper-6c889fdd54-555v8 from kubernetes-dashboard started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Sep  4 16:17:42.228: INFO: kubernetes-dashboard-b9859c4d7-bq9mj from kubernetes-dashboard started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.228: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep  4 16:17:42.228: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh before test
Sep  4 16:17:42.259: INFO: apiserver-proxy-4xjvl from kube-system started at 2023-09-04 14:35:03 +0000 UTC (2 container statuses recorded)
Sep  4 16:17:42.259: INFO: 	Container proxy ready: true, restart count 0
Sep  4 16:17:42.259: INFO: 	Container sidecar ready: true, restart count 0
Sep  4 16:17:42.259: INFO: calico-node-dqrx8 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.259: INFO: 	Container calico-node ready: true, restart count 0
Sep  4 16:17:42.259: INFO: calico-typha-deploy-6f4475c6d5-sj648 from kube-system started at 2023-09-04 16:15:11 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.260: INFO: 	Container calico-typha ready: true, restart count 0
Sep  4 16:17:42.260: INFO: csi-driver-node-fcbsh from kube-system started at 2023-09-04 14:35:03 +0000 UTC (3 container statuses recorded)
Sep  4 16:17:42.260: INFO: 	Container csi-driver ready: true, restart count 0
Sep  4 16:17:42.260: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Sep  4 16:17:42.260: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Sep  4 16:17:42.260: INFO: egress-filter-applier-42dhz from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.260: INFO: 	Container egress-filter-applier ready: true, restart count 0
Sep  4 16:17:42.260: INFO: kube-proxy-worker-1-v1.26.8-fsg4b from kube-system started at 2023-09-04 15:45:12 +0000 UTC (2 container statuses recorded)
Sep  4 16:17:42.260: INFO: 	Container conntrack-fix ready: true, restart count 0
Sep  4 16:17:42.260: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  4 16:17:42.260: INFO: network-problem-detector-host-4k5k5 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.260: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Sep  4 16:17:42.260: INFO: network-problem-detector-pod-sv5cs from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.260: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Sep  4 16:17:42.260: INFO: node-exporter-mp64n from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.260: INFO: 	Container node-exporter ready: true, restart count 0
Sep  4 16:17:42.260: INFO: node-local-dns-xnc4s from kube-system started at 2023-09-04 15:36:10 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.260: INFO: 	Container node-cache ready: true, restart count 0
Sep  4 16:17:42.260: INFO: node-problem-detector-gqcc9 from kube-system started at 2023-09-04 14:42:11 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.260: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep  4 16:17:42.260: INFO: bin-false237a310b-4563-4f42-8770-91403cb3318c from kubelet-test-3707 started at 2023-09-04 16:17:42 +0000 UTC (1 container statuses recorded)
Sep  4 16:17:42.260: INFO: 	Container bin-false237a310b-4563-4f42-8770-91403cb3318c ready: false, restart count 0
Sep  4 16:17:42.260: INFO: pod-projected-secrets-60d8d204-3b19-45a8-af60-840146ed8621 from projected-9198 started at 2023-09-04 16:16:25 +0000 UTC (3 container statuses recorded)
Sep  4 16:17:42.260: INFO: 	Container creates-volume-test ready: true, restart count 0
Sep  4 16:17:42.260: INFO: 	Container dels-volume-test ready: true, restart count 0
Sep  4 16:17:42.260: INFO: 	Container upds-volume-test ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/04/23 16:17:42.26
Sep  4 16:17:42.281: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6735" to be "running"
Sep  4 16:17:42.296: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 14.454491ms
Sep  4 16:17:44.311: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.029684499s
Sep  4 16:17:44.311: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/04/23 16:17:44.325
STEP: Trying to apply a random label on the found node. 09/04/23 16:17:44.371
STEP: verifying the node has the label kubernetes.io/e2e-e3735a74-d507-44d7-a588-53f958ff0512 95 09/04/23 16:17:44.394
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/04/23 16:17:44.409
Sep  4 16:17:44.429: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6735" to be "not pending"
Sep  4 16:17:44.443: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.125172ms
Sep  4 16:17:46.459: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.029584119s
Sep  4 16:17:46.459: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.1.231 on the node which pod4 resides and expect not scheduled 09/04/23 16:17:46.459
Sep  4 16:17:46.478: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6735" to be "not pending"
Sep  4 16:17:46.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.514817ms
Sep  4 16:17:48.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030257256s
Sep  4 16:17:50.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031425664s
Sep  4 16:17:52.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030685446s
Sep  4 16:17:54.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030627147s
Sep  4 16:17:56.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029696662s
Sep  4 16:17:58.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.03096354s
Sep  4 16:18:00.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.029560703s
Sep  4 16:18:02.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.029486288s
Sep  4 16:18:04.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.029486709s
Sep  4 16:18:06.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.030579936s
Sep  4 16:18:08.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.031961947s
Sep  4 16:18:10.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.030721389s
Sep  4 16:18:12.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.030251433s
Sep  4 16:18:14.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.029519428s
Sep  4 16:18:16.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.030323965s
Sep  4 16:18:18.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.029868921s
Sep  4 16:18:20.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.029640366s
Sep  4 16:18:22.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.02954078s
Sep  4 16:18:24.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.030031665s
Sep  4 16:18:26.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.030418789s
Sep  4 16:18:28.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.030345477s
Sep  4 16:18:30.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.03001613s
Sep  4 16:18:32.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.030185302s
Sep  4 16:18:34.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.0297689s
Sep  4 16:18:36.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.031077974s
Sep  4 16:18:38.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.029981893s
Sep  4 16:18:40.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.029633672s
Sep  4 16:18:42.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.029569076s
Sep  4 16:18:44.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.031110387s
Sep  4 16:18:46.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.031162887s
Sep  4 16:18:48.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.031060897s
Sep  4 16:18:50.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.030118807s
Sep  4 16:18:52.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.030333387s
Sep  4 16:18:54.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.030210711s
Sep  4 16:18:56.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.030112801s
Sep  4 16:18:58.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.02960305s
Sep  4 16:19:00.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.030580026s
Sep  4 16:19:02.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.029386249s
Sep  4 16:19:04.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.030937707s
Sep  4 16:19:06.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.02994744s
Sep  4 16:19:08.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.031528819s
Sep  4 16:19:10.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.031347579s
Sep  4 16:19:12.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.029924372s
Sep  4 16:19:14.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.029972669s
Sep  4 16:19:16.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.030287355s
Sep  4 16:19:18.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.031613497s
Sep  4 16:19:20.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.031871257s
Sep  4 16:19:22.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.030252776s
Sep  4 16:19:24.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.030457543s
Sep  4 16:19:26.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.030255962s
Sep  4 16:19:28.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.032102126s
Sep  4 16:19:30.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.031086781s
Sep  4 16:19:32.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.030131879s
Sep  4 16:19:34.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.030802297s
Sep  4 16:19:36.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.030054713s
Sep  4 16:19:38.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.030825841s
Sep  4 16:19:40.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.030818337s
Sep  4 16:19:42.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.0305055s
Sep  4 16:19:44.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.029787117s
Sep  4 16:19:46.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.030268297s
Sep  4 16:19:48.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.029951363s
Sep  4 16:19:50.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.030822357s
Sep  4 16:19:52.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.029129783s
Sep  4 16:19:54.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.029865959s
Sep  4 16:19:56.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.030204849s
Sep  4 16:19:58.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.029899816s
Sep  4 16:20:00.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.029482554s
Sep  4 16:20:02.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.028600207s
Sep  4 16:20:04.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.03124219s
Sep  4 16:20:06.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.030847288s
Sep  4 16:20:08.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.044097184s
Sep  4 16:20:10.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.029569629s
Sep  4 16:20:12.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.029298963s
Sep  4 16:20:14.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.02952727s
Sep  4 16:20:16.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.030266364s
Sep  4 16:20:18.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.03045906s
Sep  4 16:20:20.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.031387133s
Sep  4 16:20:22.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.030414021s
Sep  4 16:20:24.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.030787982s
Sep  4 16:20:26.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.030668392s
Sep  4 16:20:28.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.030226656s
Sep  4 16:20:30.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.031779704s
Sep  4 16:20:32.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.030880402s
Sep  4 16:20:34.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.029311992s
Sep  4 16:20:36.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.030603063s
Sep  4 16:20:38.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.029736226s
Sep  4 16:20:40.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.030066533s
Sep  4 16:20:42.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.029567122s
Sep  4 16:20:44.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.030342155s
Sep  4 16:20:46.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.030824616s
Sep  4 16:20:48.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.029395641s
Sep  4 16:20:50.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.030130124s
Sep  4 16:20:52.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.029979131s
Sep  4 16:20:54.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.030643098s
Sep  4 16:20:56.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.031162876s
Sep  4 16:20:58.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.029990132s
Sep  4 16:21:00.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.030139632s
Sep  4 16:21:02.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.030297007s
Sep  4 16:21:04.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.028602837s
Sep  4 16:21:06.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.030444408s
Sep  4 16:21:08.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.030673501s
Sep  4 16:21:10.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.030381319s
Sep  4 16:21:12.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.030093921s
Sep  4 16:21:14.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.031736758s
Sep  4 16:21:16.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.030377271s
Sep  4 16:21:18.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.030469593s
Sep  4 16:21:20.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.030442564s
Sep  4 16:21:22.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.029947048s
Sep  4 16:21:24.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.031079752s
Sep  4 16:21:26.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.030754928s
Sep  4 16:21:28.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.029503483s
Sep  4 16:21:30.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.030091609s
Sep  4 16:21:32.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.030231404s
Sep  4 16:21:34.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.028998203s
Sep  4 16:21:36.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.030719931s
Sep  4 16:21:38.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.029904541s
Sep  4 16:21:40.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.0299752s
Sep  4 16:21:42.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.03047201s
Sep  4 16:21:44.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.031763617s
Sep  4 16:21:46.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.03033736s
Sep  4 16:21:48.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.030015083s
Sep  4 16:21:50.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.030069875s
Sep  4 16:21:52.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.030885126s
Sep  4 16:21:54.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.030824302s
Sep  4 16:21:56.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.029798271s
Sep  4 16:21:58.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.031775875s
Sep  4 16:22:00.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.030381545s
Sep  4 16:22:02.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.029945744s
Sep  4 16:22:04.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.030897055s
Sep  4 16:22:06.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.029610811s
Sep  4 16:22:08.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.031828952s
Sep  4 16:22:10.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.030287093s
Sep  4 16:22:12.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.031044989s
Sep  4 16:22:14.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.03039309s
Sep  4 16:22:16.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.030149161s
Sep  4 16:22:18.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.029842735s
Sep  4 16:22:20.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.031921895s
Sep  4 16:22:22.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.031349631s
Sep  4 16:22:24.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.031735326s
Sep  4 16:22:26.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.030331843s
Sep  4 16:22:28.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.031863953s
Sep  4 16:22:30.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.03046386s
Sep  4 16:22:32.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.031046483s
Sep  4 16:22:34.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.030759568s
Sep  4 16:22:36.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.030131038s
Sep  4 16:22:38.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.031169287s
Sep  4 16:22:40.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.033094735s
------------------------------
Automatically polling progress:
  [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance] (Spec Runtime: 5m0.209s)
    test/e2e/scheduling/predicates.go:704
    In [It] (Node Runtime: 5m0.001s)
      test/e2e/scheduling/predicates.go:704
      At [By Step] Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.1.231 on the node which pod4 resides and expect not scheduled (Step Runtime: 4m55.802s)
        test/e2e/scheduling/predicates.go:723

      Spec Goroutine
      goroutine 36242 [select]
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.WaitForWithContext({0x817c768, 0xc0001a6000}, 0xc002805da0, 0x306620a?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:660
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.poll({0x817c768, 0xc0001a6000}, 0x98?, 0x3064da5?, 0xc002805d88?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:596
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.PollImmediateWithContext({0x817c768, 0xc0001a6000}, 0x0?, 0xc0041dfbe8?, 0x26724e7?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:528
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.PollImmediate(0x77b6667?, 0x4?, 0x78f7a1c?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:514
        k8s.io/kubernetes/test/e2e/framework/pod.WaitForPodCondition({0x81ba5a8?, 0xc00353a4e0}, {0xc006650590, 0xf}, {0x77b6fef, 0x4}, {0x77ce4aa, 0xb}, 0xc00177d0f0?, 0x7a9bc70)
          test/e2e/framework/pod/wait.go:290
        k8s.io/kubernetes/test/e2e/framework/pod.WaitForPodNotPending({0x81ba5a8?, 0xc00353a4e0?}, {0xc006650590?, 0x0?}, {0x77b6fef?, 0x0?})
          test/e2e/framework/pod/wait.go:585
      > k8s.io/kubernetes/test/e2e/scheduling.createHostPortPodOnNode(0xc00092f3b0, {0x77b6fef, 0x4}, {0xc006650590, 0xf}, {0xc00234a350, 0xc}, 0xd432, {0x77b5a29, 0x3}, ...)
          test/e2e/scheduling/predicates.go:1153
      > k8s.io/kubernetes/test/e2e/scheduling.glob..func4.13()
          test/e2e/scheduling/predicates.go:724
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0x2dc5bee, 0xc005946d80})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
Sep  4 16:22:42.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.03014691s
Sep  4 16:22:44.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.033509983s
Sep  4 16:22:46.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.030148783s
Sep  4 16:22:46.524: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.045977167s
STEP: removing the label kubernetes.io/e2e-e3735a74-d507-44d7-a588-53f958ff0512 off the node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh 09/04/23 16:22:46.524
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e3735a74-d507-44d7-a588-53f958ff0512 09/04/23 16:22:46.582
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:22:46.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6735" for this suite. 09/04/23 16:22:46.612
• [SLOW TEST] [304.576 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:17:42.052
    Sep  4 16:17:42.052: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 09/04/23 16:17:42.053
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:17:42.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:17:42.123
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  4 16:17:42.150: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  4 16:17:42.192: INFO: Waiting for terminating namespaces to be deleted...
    Sep  4 16:17:42.206: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx before test
    Sep  4 16:17:42.228: INFO: addons-nginx-ingress-controller-56b5dc8f6c-tr9n6 from kube-system started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-58685978db-gpxdq from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: apiserver-proxy-bc5jh from kube-system started at 2023-09-04 14:34:44 +0000 UTC (2 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container proxy ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: 	Container sidecar ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: blackbox-exporter-585854d657-fvtbg from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: blackbox-exporter-585854d657-vz8rw from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: calico-kube-controllers-684b9f4889-f24wb from kube-system started at 2023-09-04 14:34:42 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: calico-node-959qs from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: calico-node-vertical-autoscaler-7bbd54698f-zb6st from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: calico-typha-deploy-6f4475c6d5-z7t2m from kube-system started at 2023-09-04 14:35:50 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container calico-typha ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: calico-typha-horizontal-autoscaler-7b89d5ff97-gdvkd from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: calico-typha-vertical-autoscaler-656479b7b5-khd8g from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container autoscaler ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: coredns-89679867b-2fr9s from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: coredns-89679867b-5qtl7 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container coredns ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: csi-driver-node-5kfkx from kube-system started at 2023-09-04 14:34:46 +0000 UTC (3 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container csi-driver ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: egress-filter-applier-n4kfc from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: kube-proxy-worker-1-v1.26.8-ch6px from kube-system started at 2023-09-04 15:01:11 +0000 UTC (2 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container conntrack-fix ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: metrics-server-78947f8d7c-9cdcs from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container metrics-server ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: metrics-server-78947f8d7c-w8gr6 from kube-system started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container metrics-server ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: network-problem-detector-host-xf9l5 from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: network-problem-detector-pod-lbp4l from kube-system started at 2023-09-04 14:34:43 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: node-exporter-ktjnf from kube-system started at 2023-09-04 14:34:45 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: node-local-dns-zhl7r from kube-system started at 2023-09-04 15:35:11 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container node-cache ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: node-problem-detector-rqmbk from kube-system started at 2023-09-04 14:41:10 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: vpn-shoot-5d596dbb88-5vx52 from kube-system started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container vpn-shoot ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: dashboard-metrics-scraper-6c889fdd54-555v8 from kubernetes-dashboard started at 2023-09-04 14:35:22 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: kubernetes-dashboard-b9859c4d7-bq9mj from kubernetes-dashboard started at 2023-09-04 14:51:12 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.228: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Sep  4 16:17:42.228: INFO: 
    Logging pods the apiserver thinks is on node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh before test
    Sep  4 16:17:42.259: INFO: apiserver-proxy-4xjvl from kube-system started at 2023-09-04 14:35:03 +0000 UTC (2 container statuses recorded)
    Sep  4 16:17:42.259: INFO: 	Container proxy ready: true, restart count 0
    Sep  4 16:17:42.259: INFO: 	Container sidecar ready: true, restart count 0
    Sep  4 16:17:42.259: INFO: calico-node-dqrx8 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.259: INFO: 	Container calico-node ready: true, restart count 0
    Sep  4 16:17:42.259: INFO: calico-typha-deploy-6f4475c6d5-sj648 from kube-system started at 2023-09-04 16:15:11 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.260: INFO: 	Container calico-typha ready: true, restart count 0
    Sep  4 16:17:42.260: INFO: csi-driver-node-fcbsh from kube-system started at 2023-09-04 14:35:03 +0000 UTC (3 container statuses recorded)
    Sep  4 16:17:42.260: INFO: 	Container csi-driver ready: true, restart count 0
    Sep  4 16:17:42.260: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Sep  4 16:17:42.260: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Sep  4 16:17:42.260: INFO: egress-filter-applier-42dhz from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.260: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Sep  4 16:17:42.260: INFO: kube-proxy-worker-1-v1.26.8-fsg4b from kube-system started at 2023-09-04 15:45:12 +0000 UTC (2 container statuses recorded)
    Sep  4 16:17:42.260: INFO: 	Container conntrack-fix ready: true, restart count 0
    Sep  4 16:17:42.260: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  4 16:17:42.260: INFO: network-problem-detector-host-4k5k5 from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.260: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Sep  4 16:17:42.260: INFO: network-problem-detector-pod-sv5cs from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.260: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Sep  4 16:17:42.260: INFO: node-exporter-mp64n from kube-system started at 2023-09-04 14:35:03 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.260: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  4 16:17:42.260: INFO: node-local-dns-xnc4s from kube-system started at 2023-09-04 15:36:10 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.260: INFO: 	Container node-cache ready: true, restart count 0
    Sep  4 16:17:42.260: INFO: node-problem-detector-gqcc9 from kube-system started at 2023-09-04 14:42:11 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.260: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep  4 16:17:42.260: INFO: bin-false237a310b-4563-4f42-8770-91403cb3318c from kubelet-test-3707 started at 2023-09-04 16:17:42 +0000 UTC (1 container statuses recorded)
    Sep  4 16:17:42.260: INFO: 	Container bin-false237a310b-4563-4f42-8770-91403cb3318c ready: false, restart count 0
    Sep  4 16:17:42.260: INFO: pod-projected-secrets-60d8d204-3b19-45a8-af60-840146ed8621 from projected-9198 started at 2023-09-04 16:16:25 +0000 UTC (3 container statuses recorded)
    Sep  4 16:17:42.260: INFO: 	Container creates-volume-test ready: true, restart count 0
    Sep  4 16:17:42.260: INFO: 	Container dels-volume-test ready: true, restart count 0
    Sep  4 16:17:42.260: INFO: 	Container upds-volume-test ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/04/23 16:17:42.26
    Sep  4 16:17:42.281: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6735" to be "running"
    Sep  4 16:17:42.296: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 14.454491ms
    Sep  4 16:17:44.311: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.029684499s
    Sep  4 16:17:44.311: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/04/23 16:17:44.325
    STEP: Trying to apply a random label on the found node. 09/04/23 16:17:44.371
    STEP: verifying the node has the label kubernetes.io/e2e-e3735a74-d507-44d7-a588-53f958ff0512 95 09/04/23 16:17:44.394
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/04/23 16:17:44.409
    Sep  4 16:17:44.429: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6735" to be "not pending"
    Sep  4 16:17:44.443: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.125172ms
    Sep  4 16:17:46.459: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.029584119s
    Sep  4 16:17:46.459: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.1.231 on the node which pod4 resides and expect not scheduled 09/04/23 16:17:46.459
    Sep  4 16:17:46.478: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6735" to be "not pending"
    Sep  4 16:17:46.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.514817ms
    Sep  4 16:17:48.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030257256s
    Sep  4 16:17:50.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031425664s
    Sep  4 16:17:52.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030685446s
    Sep  4 16:17:54.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030627147s
    Sep  4 16:17:56.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029696662s
    Sep  4 16:17:58.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.03096354s
    Sep  4 16:18:00.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.029560703s
    Sep  4 16:18:02.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.029486288s
    Sep  4 16:18:04.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.029486709s
    Sep  4 16:18:06.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.030579936s
    Sep  4 16:18:08.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.031961947s
    Sep  4 16:18:10.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.030721389s
    Sep  4 16:18:12.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.030251433s
    Sep  4 16:18:14.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.029519428s
    Sep  4 16:18:16.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.030323965s
    Sep  4 16:18:18.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.029868921s
    Sep  4 16:18:20.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.029640366s
    Sep  4 16:18:22.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.02954078s
    Sep  4 16:18:24.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.030031665s
    Sep  4 16:18:26.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.030418789s
    Sep  4 16:18:28.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.030345477s
    Sep  4 16:18:30.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.03001613s
    Sep  4 16:18:32.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.030185302s
    Sep  4 16:18:34.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.0297689s
    Sep  4 16:18:36.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.031077974s
    Sep  4 16:18:38.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.029981893s
    Sep  4 16:18:40.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.029633672s
    Sep  4 16:18:42.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.029569076s
    Sep  4 16:18:44.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.031110387s
    Sep  4 16:18:46.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.031162887s
    Sep  4 16:18:48.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.031060897s
    Sep  4 16:18:50.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.030118807s
    Sep  4 16:18:52.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.030333387s
    Sep  4 16:18:54.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.030210711s
    Sep  4 16:18:56.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.030112801s
    Sep  4 16:18:58.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.02960305s
    Sep  4 16:19:00.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.030580026s
    Sep  4 16:19:02.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.029386249s
    Sep  4 16:19:04.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.030937707s
    Sep  4 16:19:06.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.02994744s
    Sep  4 16:19:08.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.031528819s
    Sep  4 16:19:10.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.031347579s
    Sep  4 16:19:12.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.029924372s
    Sep  4 16:19:14.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.029972669s
    Sep  4 16:19:16.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.030287355s
    Sep  4 16:19:18.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.031613497s
    Sep  4 16:19:20.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.031871257s
    Sep  4 16:19:22.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.030252776s
    Sep  4 16:19:24.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.030457543s
    Sep  4 16:19:26.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.030255962s
    Sep  4 16:19:28.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.032102126s
    Sep  4 16:19:30.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.031086781s
    Sep  4 16:19:32.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.030131879s
    Sep  4 16:19:34.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.030802297s
    Sep  4 16:19:36.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.030054713s
    Sep  4 16:19:38.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.030825841s
    Sep  4 16:19:40.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.030818337s
    Sep  4 16:19:42.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.0305055s
    Sep  4 16:19:44.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.029787117s
    Sep  4 16:19:46.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.030268297s
    Sep  4 16:19:48.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.029951363s
    Sep  4 16:19:50.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.030822357s
    Sep  4 16:19:52.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.029129783s
    Sep  4 16:19:54.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.029865959s
    Sep  4 16:19:56.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.030204849s
    Sep  4 16:19:58.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.029899816s
    Sep  4 16:20:00.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.029482554s
    Sep  4 16:20:02.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.028600207s
    Sep  4 16:20:04.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.03124219s
    Sep  4 16:20:06.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.030847288s
    Sep  4 16:20:08.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.044097184s
    Sep  4 16:20:10.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.029569629s
    Sep  4 16:20:12.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.029298963s
    Sep  4 16:20:14.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.02952727s
    Sep  4 16:20:16.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.030266364s
    Sep  4 16:20:18.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.03045906s
    Sep  4 16:20:20.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.031387133s
    Sep  4 16:20:22.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.030414021s
    Sep  4 16:20:24.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.030787982s
    Sep  4 16:20:26.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.030668392s
    Sep  4 16:20:28.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.030226656s
    Sep  4 16:20:30.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.031779704s
    Sep  4 16:20:32.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.030880402s
    Sep  4 16:20:34.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.029311992s
    Sep  4 16:20:36.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.030603063s
    Sep  4 16:20:38.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.029736226s
    Sep  4 16:20:40.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.030066533s
    Sep  4 16:20:42.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.029567122s
    Sep  4 16:20:44.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.030342155s
    Sep  4 16:20:46.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.030824616s
    Sep  4 16:20:48.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.029395641s
    Sep  4 16:20:50.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.030130124s
    Sep  4 16:20:52.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.029979131s
    Sep  4 16:20:54.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.030643098s
    Sep  4 16:20:56.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.031162876s
    Sep  4 16:20:58.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.029990132s
    Sep  4 16:21:00.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.030139632s
    Sep  4 16:21:02.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.030297007s
    Sep  4 16:21:04.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.028602837s
    Sep  4 16:21:06.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.030444408s
    Sep  4 16:21:08.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.030673501s
    Sep  4 16:21:10.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.030381319s
    Sep  4 16:21:12.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.030093921s
    Sep  4 16:21:14.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.031736758s
    Sep  4 16:21:16.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.030377271s
    Sep  4 16:21:18.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.030469593s
    Sep  4 16:21:20.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.030442564s
    Sep  4 16:21:22.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.029947048s
    Sep  4 16:21:24.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.031079752s
    Sep  4 16:21:26.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.030754928s
    Sep  4 16:21:28.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.029503483s
    Sep  4 16:21:30.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.030091609s
    Sep  4 16:21:32.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.030231404s
    Sep  4 16:21:34.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.028998203s
    Sep  4 16:21:36.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.030719931s
    Sep  4 16:21:38.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.029904541s
    Sep  4 16:21:40.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.0299752s
    Sep  4 16:21:42.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.03047201s
    Sep  4 16:21:44.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.031763617s
    Sep  4 16:21:46.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.03033736s
    Sep  4 16:21:48.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.030015083s
    Sep  4 16:21:50.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.030069875s
    Sep  4 16:21:52.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.030885126s
    Sep  4 16:21:54.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.030824302s
    Sep  4 16:21:56.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.029798271s
    Sep  4 16:21:58.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.031775875s
    Sep  4 16:22:00.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.030381545s
    Sep  4 16:22:02.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.029945744s
    Sep  4 16:22:04.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.030897055s
    Sep  4 16:22:06.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.029610811s
    Sep  4 16:22:08.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.031828952s
    Sep  4 16:22:10.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.030287093s
    Sep  4 16:22:12.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.031044989s
    Sep  4 16:22:14.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.03039309s
    Sep  4 16:22:16.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.030149161s
    Sep  4 16:22:18.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.029842735s
    Sep  4 16:22:20.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.031921895s
    Sep  4 16:22:22.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.031349631s
    Sep  4 16:22:24.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.031735326s
    Sep  4 16:22:26.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.030331843s
    Sep  4 16:22:28.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.031863953s
    Sep  4 16:22:30.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.03046386s
    Sep  4 16:22:32.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.031046483s
    Sep  4 16:22:34.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.030759568s
    Sep  4 16:22:36.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.030131038s
    Sep  4 16:22:38.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.031169287s
    Sep  4 16:22:40.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.033094735s
    Sep  4 16:22:42.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.03014691s
    Sep  4 16:22:44.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.033509983s
    Sep  4 16:22:46.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.030148783s
    Sep  4 16:22:46.524: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.045977167s
    STEP: removing the label kubernetes.io/e2e-e3735a74-d507-44d7-a588-53f958ff0512 off the node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh 09/04/23 16:22:46.524
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-e3735a74-d507-44d7-a588-53f958ff0512 09/04/23 16:22:46.582
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:22:46.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6735" for this suite. 09/04/23 16:22:46.612
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:22:46.628
Sep  4 16:22:46.628: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 09/04/23 16:22:46.629
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:22:46.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:22:46.699
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/04/23 16:22:46.759
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 16:22:46.928
STEP: Deploying the webhook pod 09/04/23 16:22:46.945
STEP: Wait for the deployment to be ready 09/04/23 16:22:46.976
Sep  4 16:22:47.019: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 16, 22, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 22, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 16, 22, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 22, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/04/23 16:22:49.034
STEP: Verifying the service has paired with the endpoint 09/04/23 16:22:49.052
Sep  4 16:22:50.053: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Sep  4 16:22:50.068: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1276-crds.webhook.example.com via the AdmissionRegistration API 09/04/23 16:22:50.098
STEP: Creating a custom resource that should be mutated by the webhook 09/04/23 16:22:50.228
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:22:52.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3612" for this suite. 09/04/23 16:22:53.087
STEP: Destroying namespace "webhook-3612-markers" for this suite. 09/04/23 16:22:53.103
------------------------------
• [6.490 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:22:46.628
    Sep  4 16:22:46.628: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 09/04/23 16:22:46.629
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:22:46.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:22:46.699
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/04/23 16:22:46.759
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/04/23 16:22:46.928
    STEP: Deploying the webhook pod 09/04/23 16:22:46.945
    STEP: Wait for the deployment to be ready 09/04/23 16:22:46.976
    Sep  4 16:22:47.019: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 4, 16, 22, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 22, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 4, 16, 22, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 4, 16, 22, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/04/23 16:22:49.034
    STEP: Verifying the service has paired with the endpoint 09/04/23 16:22:49.052
    Sep  4 16:22:50.053: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Sep  4 16:22:50.068: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1276-crds.webhook.example.com via the AdmissionRegistration API 09/04/23 16:22:50.098
    STEP: Creating a custom resource that should be mutated by the webhook 09/04/23 16:22:50.228
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:22:52.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3612" for this suite. 09/04/23 16:22:53.087
    STEP: Destroying namespace "webhook-3612-markers" for this suite. 09/04/23 16:22:53.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:22:53.118
Sep  4 16:22:53.118: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ingressclass 09/04/23 16:22:53.119
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:22:53.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:22:53.189
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 09/04/23 16:22:53.216
STEP: getting /apis/networking.k8s.io 09/04/23 16:22:53.243
STEP: getting /apis/networking.k8s.iov1 09/04/23 16:22:53.256
STEP: creating 09/04/23 16:22:53.269
STEP: getting 09/04/23 16:22:53.313
STEP: listing 09/04/23 16:22:53.328
STEP: watching 09/04/23 16:22:53.343
Sep  4 16:22:53.343: INFO: starting watch
STEP: patching 09/04/23 16:22:53.356
STEP: updating 09/04/23 16:22:53.372
Sep  4 16:22:53.387: INFO: waiting for watch events with expected annotations
Sep  4 16:22:53.387: INFO: saw patched and updated annotations
STEP: deleting 09/04/23 16:22:53.387
STEP: deleting a collection 09/04/23 16:22:53.432
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Sep  4 16:22:53.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-6008" for this suite. 09/04/23 16:22:53.479
------------------------------
• [0.377 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:22:53.118
    Sep  4 16:22:53.118: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename ingressclass 09/04/23 16:22:53.119
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:22:53.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:22:53.189
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 09/04/23 16:22:53.216
    STEP: getting /apis/networking.k8s.io 09/04/23 16:22:53.243
    STEP: getting /apis/networking.k8s.iov1 09/04/23 16:22:53.256
    STEP: creating 09/04/23 16:22:53.269
    STEP: getting 09/04/23 16:22:53.313
    STEP: listing 09/04/23 16:22:53.328
    STEP: watching 09/04/23 16:22:53.343
    Sep  4 16:22:53.343: INFO: starting watch
    STEP: patching 09/04/23 16:22:53.356
    STEP: updating 09/04/23 16:22:53.372
    Sep  4 16:22:53.387: INFO: waiting for watch events with expected annotations
    Sep  4 16:22:53.387: INFO: saw patched and updated annotations
    STEP: deleting 09/04/23 16:22:53.387
    STEP: deleting a collection 09/04/23 16:22:53.432
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:22:53.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-6008" for this suite. 09/04/23 16:22:53.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:22:53.495
Sep  4 16:22:53.495: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename csiinlinevolumes 09/04/23 16:22:53.496
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:22:53.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:22:53.566
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 09/04/23 16:22:53.592
STEP: getting 09/04/23 16:22:53.633
STEP: listing in namespace 09/04/23 16:22:53.647
STEP: patching 09/04/23 16:22:53.662
STEP: deleting 09/04/23 16:22:53.678
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Sep  4 16:22:53.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2285" for this suite. 09/04/23 16:22:53.726
------------------------------
• [0.247 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:22:53.495
    Sep  4 16:22:53.495: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename csiinlinevolumes 09/04/23 16:22:53.496
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:22:53.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:22:53.566
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 09/04/23 16:22:53.592
    STEP: getting 09/04/23 16:22:53.633
    STEP: listing in namespace 09/04/23 16:22:53.647
    STEP: patching 09/04/23 16:22:53.662
    STEP: deleting 09/04/23 16:22:53.678
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:22:53.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2285" for this suite. 09/04/23 16:22:53.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:22:53.75
Sep  4 16:22:53.750: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers 09/04/23 16:22:53.751
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:22:53.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:22:53.822
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 09/04/23 16:22:53.849
Sep  4 16:22:53.870: INFO: Waiting up to 5m0s for pod "client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8" in namespace "containers-2111" to be "Succeeded or Failed"
Sep  4 16:22:53.884: INFO: Pod "client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.680274ms
Sep  4 16:22:55.900: INFO: Pod "client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029759572s
Sep  4 16:22:57.900: INFO: Pod "client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0294259s
STEP: Saw pod success 09/04/23 16:22:57.9
Sep  4 16:22:57.900: INFO: Pod "client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8" satisfied condition "Succeeded or Failed"
Sep  4 16:22:57.914: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8 container agnhost-container: <nil>
STEP: delete the pod 09/04/23 16:22:57.988
Sep  4 16:22:58.006: INFO: Waiting for pod client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8 to disappear
Sep  4 16:22:58.020: INFO: Pod client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  4 16:22:58.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2111" for this suite. 09/04/23 16:22:58.047
------------------------------
• [4.313 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:22:53.75
    Sep  4 16:22:53.750: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename containers 09/04/23 16:22:53.751
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:22:53.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:22:53.822
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 09/04/23 16:22:53.849
    Sep  4 16:22:53.870: INFO: Waiting up to 5m0s for pod "client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8" in namespace "containers-2111" to be "Succeeded or Failed"
    Sep  4 16:22:53.884: INFO: Pod "client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.680274ms
    Sep  4 16:22:55.900: INFO: Pod "client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029759572s
    Sep  4 16:22:57.900: INFO: Pod "client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0294259s
    STEP: Saw pod success 09/04/23 16:22:57.9
    Sep  4 16:22:57.900: INFO: Pod "client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8" satisfied condition "Succeeded or Failed"
    Sep  4 16:22:57.914: INFO: Trying to get logs from node shoot--it--tmud5-dd2-worker-1-z1-79cbc-pp5wh pod client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8 container agnhost-container: <nil>
    STEP: delete the pod 09/04/23 16:22:57.988
    Sep  4 16:22:58.006: INFO: Waiting for pod client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8 to disappear
    Sep  4 16:22:58.020: INFO: Pod client-containers-46a1282d-4714-400b-bccc-1ce2e13eb4e8 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:22:58.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2111" for this suite. 09/04/23 16:22:58.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:22:58.064
Sep  4 16:22:58.064: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 16:22:58.064
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:22:58.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:22:58.133
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Sep  4 16:22:58.160: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:23:00.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-607" for this suite. 09/04/23 16:23:00.93
------------------------------
• [2.883 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:22:58.064
    Sep  4 16:22:58.064: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 09/04/23 16:22:58.064
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:22:58.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:22:58.133
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Sep  4 16:22:58.160: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:23:00.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-607" for this suite. 09/04/23 16:23:00.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:23:00.947
Sep  4 16:23:00.947: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 09/04/23 16:23:00.948
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:23:00.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:23:01.019
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
Sep  4 16:23:01.105: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 16:23:01.121
Sep  4 16:23:01.150: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 16:23:01.150: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 16:23:02.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 16:23:02.193: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 16:23:03.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 16:23:03.191: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 09/04/23 16:23:03.248
STEP: Check that daemon pods images are updated. 09/04/23 16:23:03.279
Sep  4 16:23:03.293: INFO: Wrong image for pod: daemon-set-w2pv4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  4 16:23:04.323: INFO: Wrong image for pod: daemon-set-w2pv4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  4 16:23:05.323: INFO: Wrong image for pod: daemon-set-w2pv4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  4 16:23:06.323: INFO: Pod daemon-set-26ksc is not available
Sep  4 16:23:06.323: INFO: Wrong image for pod: daemon-set-w2pv4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  4 16:23:08.324: INFO: Pod daemon-set-h7kjx is not available
STEP: Check that daemon pods are still running on every node of the cluster. 09/04/23 16:23:08.35
Sep  4 16:23:08.381: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  4 16:23:08.381: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
Sep  4 16:23:09.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  4 16:23:09.423: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/04/23 16:23:09.495
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4967, will wait for the garbage collector to delete the pods 09/04/23 16:23:09.496
Sep  4 16:23:09.576: INFO: Deleting DaemonSet.extensions daemon-set took: 16.429008ms
Sep  4 16:23:09.677: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.927741ms
Sep  4 16:23:12.693: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  4 16:23:12.693: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  4 16:23:12.708: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"53415"},"items":null}

Sep  4 16:23:12.722: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"53416"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  4 16:23:12.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4967" for this suite. 09/04/23 16:23:12.794
------------------------------
• [11.862 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:23:00.947
    Sep  4 16:23:00.947: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 09/04/23 16:23:00.948
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:23:00.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:23:01.019
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:385
    Sep  4 16:23:01.105: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 09/04/23 16:23:01.121
    Sep  4 16:23:01.150: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 16:23:01.150: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 16:23:02.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 16:23:02.193: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 16:23:03.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 16:23:03.191: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 09/04/23 16:23:03.248
    STEP: Check that daemon pods images are updated. 09/04/23 16:23:03.279
    Sep  4 16:23:03.293: INFO: Wrong image for pod: daemon-set-w2pv4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  4 16:23:04.323: INFO: Wrong image for pod: daemon-set-w2pv4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  4 16:23:05.323: INFO: Wrong image for pod: daemon-set-w2pv4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  4 16:23:06.323: INFO: Pod daemon-set-26ksc is not available
    Sep  4 16:23:06.323: INFO: Wrong image for pod: daemon-set-w2pv4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  4 16:23:08.324: INFO: Pod daemon-set-h7kjx is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 09/04/23 16:23:08.35
    Sep  4 16:23:08.381: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  4 16:23:08.381: INFO: Node shoot--it--tmud5-dd2-worker-1-z1-79cbc-lnprx is running 0 daemon pod, expected 1
    Sep  4 16:23:09.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  4 16:23:09.423: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/04/23 16:23:09.495
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4967, will wait for the garbage collector to delete the pods 09/04/23 16:23:09.496
    Sep  4 16:23:09.576: INFO: Deleting DaemonSet.extensions daemon-set took: 16.429008ms
    Sep  4 16:23:09.677: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.927741ms
    Sep  4 16:23:12.693: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  4 16:23:12.693: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  4 16:23:12.708: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"53415"},"items":null}

    Sep  4 16:23:12.722: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"53416"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:23:12.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4967" for this suite. 09/04/23 16:23:12.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:23:12.809
Sep  4 16:23:12.809: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 09/04/23 16:23:12.81
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:23:12.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:23:12.88
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 09/04/23 16:23:12.907
STEP: When the matched label of one of its pods change 09/04/23 16:23:12.923
Sep  4 16:23:12.939: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 09/04/23 16:23:14.002
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  4 16:23:14.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4572" for this suite. 09/04/23 16:23:14.044
------------------------------
• [1.250 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:23:12.809
    Sep  4 16:23:12.809: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 09/04/23 16:23:12.81
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:23:12.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:23:12.88
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 09/04/23 16:23:12.907
    STEP: When the matched label of one of its pods change 09/04/23 16:23:12.923
    Sep  4 16:23:12.939: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/04/23 16:23:14.002
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:23:14.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4572" for this suite. 09/04/23 16:23:14.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/04/23 16:23:14.059
Sep  4 16:23:14.060: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 09/04/23 16:23:14.06
STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:23:14.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:23:14.13
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 09/04/23 16:23:14.156
Sep  4 16:23:14.178: INFO: Waiting up to 5m0s for pod "labelsupdated166ac9c-a4f5-44b9-9097-ff9fe5524d5c" in namespace "projected-4748" to be "running and ready"
Sep  4 16:23:14.192: INFO: Pod "labelsupdated166ac9c-a4f5-44b9-9097-ff9fe5524d5c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.458977ms
Sep  4 16:23:14.192: INFO: The phase of Pod labelsupdated166ac9c-a4f5-44b9-9097-ff9fe5524d5c is Pending, waiting for it to be Running (with Ready = true)
Sep  4 16:23:16.207: INFO: Pod "labelsupdated166ac9c-a4f5-44b9-9097-ff9fe5524d5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.02965641s
Sep  4 16:23:16.207: INFO: The phase of Pod labelsupdated166ac9c-a4f5-44b9-9097-ff9fe5524d5c is Running (Ready = true)
Sep  4 16:23:16.207: INFO: Pod "labelsupdated166ac9c-a4f5-44b9-9097-ff9fe5524d5c" satisfied condition "running and ready"
Sep  4 16:23:16.791: INFO: Successfully updated pod "labelsupdated166ac9c-a4f5-44b9-9097-ff9fe5524d5c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  4 16:23:18.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4748" for this suite. 09/04/23 16:23:18.893
------------------------------
• [4.853 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/04/23 16:23:14.059
    Sep  4 16:23:14.060: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 09/04/23 16:23:14.06
    STEP: Waiting for a default service account to be provisioned in namespace 09/04/23 16:23:14.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/04/23 16:23:14.13
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 09/04/23 16:23:14.156
    Sep  4 16:23:14.178: INFO: Waiting up to 5m0s for pod "labelsupdated166ac9c-a4f5-44b9-9097-ff9fe5524d5c" in namespace "projected-4748" to be "running and ready"
    Sep  4 16:23:14.192: INFO: Pod "labelsupdated166ac9c-a4f5-44b9-9097-ff9fe5524d5c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.458977ms
    Sep  4 16:23:14.192: INFO: The phase of Pod labelsupdated166ac9c-a4f5-44b9-9097-ff9fe5524d5c is Pending, waiting for it to be Running (with Ready = true)
    Sep  4 16:23:16.207: INFO: Pod "labelsupdated166ac9c-a4f5-44b9-9097-ff9fe5524d5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.02965641s
    Sep  4 16:23:16.207: INFO: The phase of Pod labelsupdated166ac9c-a4f5-44b9-9097-ff9fe5524d5c is Running (Ready = true)
    Sep  4 16:23:16.207: INFO: Pod "labelsupdated166ac9c-a4f5-44b9-9097-ff9fe5524d5c" satisfied condition "running and ready"
    Sep  4 16:23:16.791: INFO: Successfully updated pod "labelsupdated166ac9c-a4f5-44b9-9097-ff9fe5524d5c"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  4 16:23:18.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4748" for this suite. 09/04/23 16:23:18.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Sep  4 16:23:18.914: INFO: Running AfterSuite actions on node 1
Sep  4 16:23:18.914: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Sep  4 16:23:18.914: INFO: Running AfterSuite actions on node 1
    Sep  4 16:23:18.914: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.081 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6002.598 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--ginkgo.dryRun is deprecated, use --ginkgo.dry-run instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m
  [38;5;11m--ginkgo.flakeAttempts is deprecated, use --ginkgo.flake-attempts instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m


Ginkgo ran 1 suite in 1h40m2.970072201s
Test Suite Passed
